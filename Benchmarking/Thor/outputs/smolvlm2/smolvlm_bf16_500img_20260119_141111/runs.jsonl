{"image_index": 0, "image_name": "000000000139.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000139.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2100.706, "latencies_ms": [2100.706], "images_per_second": 0.476, "prompt_tokens": 1099, "response_tokens_est": 31, "n_tiles": 1, "output_text": " In the image, there is a woman standing in a kitchen, with a dining table and chairs in front of her, and a television on the wall.", "error": null, "sys_before": {"cpu_percent": 5.5, "ram_used_mb": 26723.7, "ram_available_mb": 99048.5, "ram_percent": 21.2}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26723.1, "ram_available_mb": 99049.1, "ram_percent": 21.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6054.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 6054.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.45, "peak": 37.82, "min": 16.16}, "VIN": {"avg": 68.4, "peak": 99.98, "min": 27.95}, "VIN_SYS_5V0": {"avg": 14.35, "peak": 15.34, "min": 12.36}, "VDD_CPU_SOC_MSS": {"avg": 15.04, "peak": 16.14, "min": 13.39}}, "power_watts_avg": 27.45, "energy_joules_est": 57.68, "sample_count": 21, "duration_seconds": 2.101}, "timestamp": "2026-01-19T14:11:46.820472"}
{"image_index": 0, "image_name": "000000000139.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000139.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 5008.689, "latencies_ms": [5008.689], "images_per_second": 0.2, "prompt_tokens": 1113, "response_tokens_est": 128, "n_tiles": 1, "output_text": " television: 1, dining table: 1, chairs: 4, vase: 1, flowers: 1, television stand: 1, television screen: 1, window: 1, wall: 1, floor: 1, rug: 1, fireplace: 1, ceiling: 1, light fixture: 1, wall clock: 1, wall art: 1, wall: 1, wall socket: 1, wall socket cover: 1, wall socket cover: 1, wall socket cover: 1, wall socket cover: 1, wall socket cover", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26723.1, "ram_available_mb": 99049.1, "ram_percent": 21.2}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 26725.3, "ram_available_mb": 99046.9, "ram_percent": 21.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6054.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 6054.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 21.85, "peak": 37.42, "min": 14.99}, "VIN": {"avg": 60.57, "peak": 102.67, "min": 28.01}, "VIN_SYS_5V0": {"avg": 14.09, "peak": 15.24, "min": 11.24}, "VDD_CPU_SOC_MSS": {"avg": 14.64, "peak": 16.14, "min": 12.22}}, "power_watts_avg": 21.85, "energy_joules_est": 109.45, "sample_count": 49, "duration_seconds": 5.009}, "timestamp": "2026-01-19T14:11:51.924262"}
{"image_index": 0, "image_name": "000000000139.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000139.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2904.107, "latencies_ms": [2904.107], "images_per_second": 0.344, "prompt_tokens": 1117, "response_tokens_est": 57, "n_tiles": 1, "output_text": " The dining table is located in the center of the room, with the woman standing near it. The television is on the left side of the room, while the fireplace is on the right side. The woman is standing near the dining table, which is in the center of the room.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26725.3, "ram_available_mb": 99046.9, "ram_percent": 21.2}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 26725.3, "ram_available_mb": 99046.9, "ram_percent": 21.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6054.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 6054.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 24.83, "peak": 37.42, "min": 13.8}, "VIN": {"avg": 67.29, "peak": 102.05, "min": 29.38}, "VIN_SYS_5V0": {"avg": 14.12, "peak": 15.24, "min": 11.14}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 16.14, "min": 12.22}}, "power_watts_avg": 24.83, "energy_joules_est": 72.12, "sample_count": 28, "duration_seconds": 2.905}, "timestamp": "2026-01-19T14:11:54.844616"}
{"image_index": 0, "image_name": "000000000139.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000139.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1876.032, "latencies_ms": [1876.032], "images_per_second": 0.533, "prompt_tokens": 1111, "response_tokens_est": 23, "n_tiles": 1, "output_text": " A woman is standing in a living room with a dining table and chairs. There is a fireplace in the room.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26725.3, "ram_available_mb": 99046.9, "ram_percent": 21.2}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26728.7, "ram_available_mb": 99043.5, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6054.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 6054.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.98, "peak": 37.42, "min": 15.38}, "VIN": {"avg": 67.11, "peak": 96.34, "min": 27.18}, "VIN_SYS_5V0": {"avg": 14.33, "peak": 15.34, "min": 12.15}, "VDD_CPU_SOC_MSS": {"avg": 15.13, "peak": 16.14, "min": 13.39}}, "power_watts_avg": 27.98, "energy_joules_est": 52.51, "sample_count": 19, "duration_seconds": 1.877}, "timestamp": "2026-01-19T14:11:56.824127"}
{"image_index": 0, "image_name": "000000000139.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000139.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2353.192, "latencies_ms": [2353.192], "images_per_second": 0.425, "prompt_tokens": 1109, "response_tokens_est": 38, "n_tiles": 1, "output_text": " The room is bathed in warm yellow light, with hardwood floors and a large window letting in natural light. The walls are painted a bright yellow, and the furniture is made of wood.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26728.7, "ram_available_mb": 99043.5, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26724.0, "ram_available_mb": 99048.2, "ram_percent": 21.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6054.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 6054.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.42, "peak": 37.42, "min": 15.77}, "VIN": {"avg": 67.48, "peak": 116.05, "min": 28.09}, "VIN_SYS_5V0": {"avg": 14.01, "peak": 15.14, "min": 11.04}, "VDD_CPU_SOC_MSS": {"avg": 14.66, "peak": 16.14, "min": 11.82}}, "power_watts_avg": 26.42, "energy_joules_est": 62.18, "sample_count": 23, "duration_seconds": 2.354}, "timestamp": "2026-01-19T14:11:59.233560"}
{"image_index": 1, "image_name": "000000000285.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000285.jpg", "image_width": 586, "image_height": 640, "image_resolution": "586x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2367.098, "latencies_ms": [2367.098], "images_per_second": 0.422, "prompt_tokens": 1432, "response_tokens_est": 26, "n_tiles": 1, "output_text": " The image features a large brown bear with a thick coat of fur, standing on a grassy field and looking directly at the camera.", "error": null, "sys_before": {"cpu_percent": 6.3, "ram_used_mb": 26724.0, "ram_available_mb": 99048.2, "ram_percent": 21.2}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 26697.7, "ram_available_mb": 99074.5, "ram_percent": 21.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.2, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.83, "peak": 36.63, "min": 14.98}, "VIN": {"avg": 71.72, "peak": 110.36, "min": 31.58}, "VIN_SYS_5V0": {"avg": 14.43, "peak": 15.75, "min": 11.34}, "VDD_CPU_SOC_MSS": {"avg": 15.38, "peak": 16.93, "min": 12.22}}, "power_watts_avg": 28.83, "energy_joules_est": 68.27, "sample_count": 23, "duration_seconds": 2.368}, "timestamp": "2026-01-19T14:12:01.636160"}
{"image_index": 1, "image_name": "000000000285.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000285.jpg", "image_width": 586, "image_height": 640, "image_resolution": "586x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3258.736, "latencies_ms": [3258.736], "images_per_second": 0.307, "prompt_tokens": 1446, "response_tokens_est": 57, "n_tiles": 1, "output_text": " 1. bear: 1\n2. grass: 1\n3. fur: 1\n4. nose: 1\n5. eyes: 1\n6. mouth: 1\n7. ears: 1\n8. fur texture: 1", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26697.7, "ram_available_mb": 99074.5, "ram_percent": 21.2}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26715.3, "ram_available_mb": 99056.9, "ram_percent": 21.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.05, "peak": 38.6, "min": 17.35}, "VIN": {"avg": 66.78, "peak": 124.6, "min": 27.99}, "VIN_SYS_5V0": {"avg": 14.51, "peak": 15.95, "min": 12.25}, "VDD_CPU_SOC_MSS": {"avg": 15.38, "peak": 16.93, "min": 13.4}}, "power_watts_avg": 27.05, "energy_joules_est": 88.16, "sample_count": 32, "duration_seconds": 3.259}, "timestamp": "2026-01-19T14:12:04.951333"}
{"image_index": 1, "image_name": "000000000285.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000285.jpg", "image_width": 586, "image_height": 640, "image_resolution": "586x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2448.547, "latencies_ms": [2448.547], "images_per_second": 0.408, "prompt_tokens": 1450, "response_tokens_est": 30, "n_tiles": 1, "output_text": " The bear is in the foreground, with the grass in the background. The bear is facing the camera, with its head turned slightly to the right.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26715.3, "ram_available_mb": 99056.9, "ram_percent": 21.2}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26744.3, "ram_available_mb": 99027.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.88, "peak": 39.39, "min": 14.98}, "VIN": {"avg": 70.17, "peak": 110.54, "min": 28.29}, "VIN_SYS_5V0": {"avg": 14.45, "peak": 16.05, "min": 11.34}, "VDD_CPU_SOC_MSS": {"avg": 15.42, "peak": 17.32, "min": 12.22}}, "power_watts_avg": 28.88, "energy_joules_est": 70.73, "sample_count": 24, "duration_seconds": 2.449}, "timestamp": "2026-01-19T14:12:07.449163"}
{"image_index": 1, "image_name": "000000000285.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000285.jpg", "image_width": 586, "image_height": 640, "image_resolution": "586x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2798.232, "latencies_ms": [2798.232], "images_per_second": 0.357, "prompt_tokens": 1444, "response_tokens_est": 43, "n_tiles": 1, "output_text": " In the image, a large brown bear is sitting on a grassy field, looking directly at the camera. The bear's fur is a mix of brown and beige colors, and it has a large black nose.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26744.3, "ram_available_mb": 99027.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26743.2, "ram_available_mb": 99029.0, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.22, "peak": 38.6, "min": 16.56}, "VIN": {"avg": 71.19, "peak": 112.0, "min": 28.62}, "VIN_SYS_5V0": {"avg": 14.49, "peak": 15.85, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 15.4, "peak": 16.93, "min": 12.61}}, "power_watts_avg": 28.22, "energy_joules_est": 78.98, "sample_count": 27, "duration_seconds": 2.799}, "timestamp": "2026-01-19T14:12:10.266245"}
{"image_index": 1, "image_name": "000000000285.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000285.jpg", "image_width": 586, "image_height": 640, "image_resolution": "586x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1790.246, "latencies_ms": [1790.246], "images_per_second": 0.559, "prompt_tokens": 1442, "response_tokens_est": 13, "n_tiles": 1, "output_text": " The bear has a brown fur, and the grass is green.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26743.2, "ram_available_mb": 99029.0, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 26743.7, "ram_available_mb": 99028.5, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.24, "peak": 39.78, "min": 16.95}, "VIN": {"avg": 76.91, "peak": 123.57, "min": 27.06}, "VIN_SYS_5V0": {"avg": 14.86, "peak": 16.15, "min": 12.36}, "VDD_CPU_SOC_MSS": {"avg": 16.21, "peak": 17.72, "min": 13.39}}, "power_watts_avg": 32.24, "energy_joules_est": 57.74, "sample_count": 18, "duration_seconds": 1.791}, "timestamp": "2026-01-19T14:12:12.154457"}
{"image_index": 2, "image_name": "000000000632.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000632.jpg", "image_width": 640, "image_height": 483, "image_resolution": "640x483", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2685.896, "latencies_ms": [2685.896], "images_per_second": 0.372, "prompt_tokens": 1432, "response_tokens_est": 45, "n_tiles": 1, "output_text": " The image depicts a cozy bedroom with a blue comforter on the bed, a wooden dresser with a mirror, a bookshelf filled with books, and a window that offers a view of a lush green tree outside.", "error": null, "sys_before": {"cpu_percent": 15.4, "ram_used_mb": 26743.7, "ram_available_mb": 99028.5, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26743.7, "ram_available_mb": 99028.5, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.2, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.38, "peak": 39.78, "min": 19.71}, "VIN": {"avg": 73.53, "peak": 127.62, "min": 29.2}, "VIN_SYS_5V0": {"avg": 14.72, "peak": 16.26, "min": 11.44}, "VDD_CPU_SOC_MSS": {"avg": 16.03, "peak": 17.72, "min": 12.61}}, "power_watts_avg": 29.38, "energy_joules_est": 78.94, "sample_count": 26, "duration_seconds": 2.687}, "timestamp": "2026-01-19T14:12:14.873693"}
{"image_index": 2, "image_name": "000000000632.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000632.jpg", "image_width": 640, "image_height": 483, "image_resolution": "640x483", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3120.651, "latencies_ms": [3120.651], "images_per_second": 0.32, "prompt_tokens": 1446, "response_tokens_est": 62, "n_tiles": 1, "output_text": " 1. bed: 1\n2. dresser: 1\n3. mirror: 1\n4. bookshelf: 1\n5. books: 100\n6. potted plant: 4\n7. window: 1\n8. wall: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26743.7, "ram_available_mb": 99028.5, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26743.7, "ram_available_mb": 99028.5, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.71, "peak": 39.78, "min": 16.95}, "VIN": {"avg": 68.74, "peak": 113.69, "min": 29.66}, "VIN_SYS_5V0": {"avg": 14.71, "peak": 16.15, "min": 12.15}, "VDD_CPU_SOC_MSS": {"avg": 15.99, "peak": 17.72, "min": 13.79}}, "power_watts_avg": 27.71, "energy_joules_est": 86.49, "sample_count": 31, "duration_seconds": 3.121}, "timestamp": "2026-01-19T14:12:18.100894"}
{"image_index": 2, "image_name": "000000000632.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000632.jpg", "image_width": 640, "image_height": 483, "image_resolution": "640x483", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3017.464, "latencies_ms": [3017.464], "images_per_second": 0.331, "prompt_tokens": 1450, "response_tokens_est": 58, "n_tiles": 1, "output_text": " The bed is located on the left side of the room, with the window on the right side. The bookshelf is positioned in the background, while the dresser is situated near the bed. The plants are placed in the foreground, with the window being the closest object to the viewer.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26743.7, "ram_available_mb": 99028.5, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26744.2, "ram_available_mb": 99028.0, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.57, "peak": 39.39, "min": 14.99}, "VIN": {"avg": 69.76, "peak": 118.9, "min": 27.61}, "VIN_SYS_5V0": {"avg": 14.56, "peak": 16.05, "min": 11.14}, "VDD_CPU_SOC_MSS": {"avg": 15.78, "peak": 17.32, "min": 12.61}}, "power_watts_avg": 27.57, "energy_joules_est": 83.21, "sample_count": 30, "duration_seconds": 3.018}, "timestamp": "2026-01-19T14:12:21.220699"}
{"image_index": 2, "image_name": "000000000632.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000632.jpg", "image_width": 640, "image_height": 483, "image_resolution": "640x483", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2539.49, "latencies_ms": [2539.49], "images_per_second": 0.394, "prompt_tokens": 1444, "response_tokens_est": 40, "n_tiles": 1, "output_text": " The bedroom is furnished with a bed, dresser, mirror, and bookshelf. The room is decorated with a blue comforter, floral wallpaper, and a window that lets in natural light.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26744.2, "ram_available_mb": 99028.0, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26745.1, "ram_available_mb": 99027.1, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.06, "peak": 39.39, "min": 14.99}, "VIN": {"avg": 72.34, "peak": 129.08, "min": 28.62}, "VIN_SYS_5V0": {"avg": 14.58, "peak": 16.05, "min": 11.04}, "VDD_CPU_SOC_MSS": {"avg": 15.85, "peak": 17.32, "min": 12.22}}, "power_watts_avg": 29.06, "energy_joules_est": 73.81, "sample_count": 25, "duration_seconds": 2.54}, "timestamp": "2026-01-19T14:12:23.831911"}
{"image_index": 2, "image_name": "000000000632.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000632.jpg", "image_width": 640, "image_height": 483, "image_resolution": "640x483", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2178.638, "latencies_ms": [2178.638], "images_per_second": 0.459, "prompt_tokens": 1442, "response_tokens_est": 27, "n_tiles": 1, "output_text": " The room is bathed in natural light from a window, the walls are adorned with floral wallpaper, and the floor is carpeted.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26745.1, "ram_available_mb": 99027.1, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26747.8, "ram_available_mb": 99024.4, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.01, "peak": 39.78, "min": 16.56}, "VIN": {"avg": 75.43, "peak": 125.89, "min": 29.19}, "VIN_SYS_5V0": {"avg": 14.78, "peak": 16.15, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 16.09, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 31.01, "energy_joules_est": 67.57, "sample_count": 21, "duration_seconds": 2.179}, "timestamp": "2026-01-19T14:12:26.023992"}
{"image_index": 3, "image_name": "000000000724.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000724.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1406.553, "latencies_ms": [1406.553], "images_per_second": 0.711, "prompt_tokens": 1100, "response_tokens_est": 14, "n_tiles": 1, "output_text": " A stop sign is on a pole in front of a parking lot.", "error": null, "sys_before": {"cpu_percent": 14.3, "ram_used_mb": 26747.8, "ram_available_mb": 99024.4, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 26747.8, "ram_available_mb": 99024.4, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.31, "peak": 39.39, "min": 19.32}, "VIN": {"avg": 76.46, "peak": 120.72, "min": 28.41}, "VIN_SYS_5V0": {"avg": 14.88, "peak": 15.85, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 16.23, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 32.31, "energy_joules_est": 45.46, "sample_count": 14, "duration_seconds": 1.407}, "timestamp": "2026-01-19T14:12:27.489601"}
{"image_index": 3, "image_name": "000000000724.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000724.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1313.52, "latencies_ms": [1313.52], "images_per_second": 0.761, "prompt_tokens": 1114, "response_tokens_est": 5, "n_tiles": 1, "output_text": " stop sign: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26747.8, "ram_available_mb": 99024.4, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.1, "ram_used_mb": 26747.8, "ram_available_mb": 99024.4, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.06, "peak": 38.21, "min": 20.89}, "VIN": {"avg": 71.84, "peak": 108.02, "min": 27.73}, "VIN_SYS_5V0": {"avg": 14.44, "peak": 15.54, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.36, "peak": 16.54, "min": 13.4}}, "power_watts_avg": 32.06, "energy_joules_est": 42.13, "sample_count": 13, "duration_seconds": 1.314}, "timestamp": "2026-01-19T14:12:28.848330"}
{"image_index": 3, "image_name": "000000000724.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000724.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2283.706, "latencies_ms": [2283.706], "images_per_second": 0.438, "prompt_tokens": 1118, "response_tokens_est": 47, "n_tiles": 1, "output_text": " The stop sign is in the foreground, to the left of the street. The street is in the middle of the image, with the stop sign on the right side. The background includes trees and buildings, with the sky visible above.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26747.8, "ram_available_mb": 99024.4, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 26747.8, "ram_available_mb": 99024.4, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.59, "peak": 40.57, "min": 20.1}, "VIN": {"avg": 71.26, "peak": 113.32, "min": 31.98}, "VIN_SYS_5V0": {"avg": 14.75, "peak": 15.85, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.91, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 29.59, "energy_joules_est": 67.59, "sample_count": 22, "duration_seconds": 2.284}, "timestamp": "2026-01-19T14:12:31.137520"}
{"image_index": 3, "image_name": "000000000724.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000724.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1408.451, "latencies_ms": [1408.451], "images_per_second": 0.71, "prompt_tokens": 1112, "response_tokens_est": 14, "n_tiles": 1, "output_text": " A stop sign is on a pole in front of a parking lot.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26747.8, "ram_available_mb": 99024.4, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 26747.5, "ram_available_mb": 99024.7, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.57, "peak": 38.6, "min": 18.13}, "VIN": {"avg": 72.17, "peak": 95.57, "min": 28.13}, "VIN_SYS_5V0": {"avg": 14.98, "peak": 15.85, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 16.37, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 31.57, "energy_joules_est": 44.47, "sample_count": 14, "duration_seconds": 1.409}, "timestamp": "2026-01-19T14:12:32.599220"}
{"image_index": 3, "image_name": "000000000724.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000724.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1597.444, "latencies_ms": [1597.444], "images_per_second": 0.626, "prompt_tokens": 1110, "response_tokens_est": 16, "n_tiles": 1, "output_text": " The stop sign is red and white, and it is in a sunny environment.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26747.5, "ram_available_mb": 99024.7, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 26747.5, "ram_available_mb": 99024.7, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.86, "peak": 38.6, "min": 20.5}, "VIN": {"avg": 74.63, "peak": 106.86, "min": 29.32}, "VIN_SYS_5V0": {"avg": 14.43, "peak": 15.54, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.43, "peak": 16.54, "min": 13.39}}, "power_watts_avg": 30.86, "energy_joules_est": 49.31, "sample_count": 16, "duration_seconds": 1.598}, "timestamp": "2026-01-19T14:12:34.269185"}
{"image_index": 4, "image_name": "000000000776.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000776.jpg", "image_width": 428, "image_height": 640, "image_resolution": "428x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1513.214, "latencies_ms": [1513.214], "images_per_second": 0.661, "prompt_tokens": 1100, "response_tokens_est": 17, "n_tiles": 1, "output_text": " Three teddy bears of different sizes and colors are huddled together on a blue blanket.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26747.5, "ram_available_mb": 99024.7, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26748.2, "ram_available_mb": 99023.9, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.81, "peak": 40.57, "min": 19.32}, "VIN": {"avg": 74.29, "peak": 120.27, "min": 28.1}, "VIN_SYS_5V0": {"avg": 14.46, "peak": 15.65, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.83, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 31.81, "energy_joules_est": 48.16, "sample_count": 15, "duration_seconds": 1.514}, "timestamp": "2026-01-19T14:12:35.839473"}
{"image_index": 4, "image_name": "000000000776.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000776.jpg", "image_width": 428, "image_height": 640, "image_resolution": "428x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1209.533, "latencies_ms": [1209.533], "images_per_second": 0.827, "prompt_tokens": 1114, "response_tokens_est": 5, "n_tiles": 1, "output_text": " teddy bear: 4", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26748.2, "ram_available_mb": 99023.9, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.3, "ram_used_mb": 26747.4, "ram_available_mb": 99024.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 33.23, "peak": 40.18, "min": 20.11}, "VIN": {"avg": 81.09, "peak": 129.45, "min": 28.17}, "VIN_SYS_5V0": {"avg": 14.59, "peak": 15.85, "min": 12.15}, "VDD_CPU_SOC_MSS": {"avg": 15.95, "peak": 17.32, "min": 13.4}}, "power_watts_avg": 33.23, "energy_joules_est": 40.21, "sample_count": 12, "duration_seconds": 1.21}, "timestamp": "2026-01-19T14:12:37.096732"}
{"image_index": 4, "image_name": "000000000776.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000776.jpg", "image_width": 428, "image_height": 640, "image_resolution": "428x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2734.873, "latencies_ms": [2734.873], "images_per_second": 0.366, "prompt_tokens": 1118, "response_tokens_est": 64, "n_tiles": 1, "output_text": " The teddy bears are positioned in a close-knit arrangement, with the largest bear in the center and the smallest bear on the left. The largest bear is in the foreground, while the smallest bear is in the background. The teddy bears are arranged in a way that suggests they are huddled together for comfort or companionship.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26747.4, "ram_available_mb": 99024.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 26747.4, "ram_available_mb": 99024.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.8, "peak": 40.97, "min": 17.35}, "VIN": {"avg": 69.23, "peak": 126.64, "min": 28.65}, "VIN_SYS_5V0": {"avg": 14.66, "peak": 15.85, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.84, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 27.8, "energy_joules_est": 76.04, "sample_count": 27, "duration_seconds": 2.735}, "timestamp": "2026-01-19T14:12:39.911578"}
{"image_index": 4, "image_name": "000000000776.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000776.jpg", "image_width": 428, "image_height": 640, "image_resolution": "428x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1401.887, "latencies_ms": [1401.887], "images_per_second": 0.713, "prompt_tokens": 1112, "response_tokens_est": 12, "n_tiles": 1, "output_text": " Three teddy bears are huddled together on a blue blanket.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26747.4, "ram_available_mb": 99024.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 26748.1, "ram_available_mb": 99024.0, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.45, "peak": 39.39, "min": 14.98}, "VIN": {"avg": 73.7, "peak": 119.12, "min": 28.06}, "VIN_SYS_5V0": {"avg": 14.39, "peak": 15.75, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 15.64, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 30.45, "energy_joules_est": 42.7, "sample_count": 14, "duration_seconds": 1.402}, "timestamp": "2026-01-19T14:12:41.380218"}
{"image_index": 4, "image_name": "000000000776.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000776.jpg", "image_width": 428, "image_height": 640, "image_resolution": "428x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1516.181, "latencies_ms": [1516.181], "images_per_second": 0.66, "prompt_tokens": 1110, "response_tokens_est": 17, "n_tiles": 1, "output_text": " The teddy bears are brown and beige, and they are on a blue blanket.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26748.1, "ram_available_mb": 99024.0, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 26748.2, "ram_available_mb": 99024.0, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.2, "peak": 40.18, "min": 20.89}, "VIN": {"avg": 73.77, "peak": 125.28, "min": 29.09}, "VIN_SYS_5V0": {"avg": 14.6, "peak": 15.75, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.88, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 32.2, "energy_joules_est": 48.83, "sample_count": 15, "duration_seconds": 1.517}, "timestamp": "2026-01-19T14:12:42.943060"}
{"image_index": 5, "image_name": "000000000785.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000785.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1646.296, "latencies_ms": [1646.296], "images_per_second": 0.607, "prompt_tokens": 1099, "response_tokens_est": 22, "n_tiles": 1, "output_text": " A woman wearing a red jacket and black pants is skiing down a snowy hill with ski poles in her hands.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26748.2, "ram_available_mb": 99024.0, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26748.2, "ram_available_mb": 99024.0, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.62, "peak": 40.57, "min": 20.11}, "VIN": {"avg": 72.37, "peak": 124.92, "min": 29.89}, "VIN_SYS_5V0": {"avg": 14.66, "peak": 15.75, "min": 12.25}, "VDD_CPU_SOC_MSS": {"avg": 15.87, "peak": 16.93, "min": 13.79}}, "power_watts_avg": 31.62, "energy_joules_est": 52.07, "sample_count": 16, "duration_seconds": 1.647}, "timestamp": "2026-01-19T14:12:44.611057"}
{"image_index": 5, "image_name": "000000000785.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000785.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2889.081, "latencies_ms": [2889.081], "images_per_second": 0.346, "prompt_tokens": 1113, "response_tokens_est": 71, "n_tiles": 1, "output_text": " 1. Ski pole: 2\n2. Ski: 2\n3. Ski pole: 2\n4. Ski pole: 2\n5. Ski pole: 2\n6. Ski pole: 2\n7. Ski pole: 2\n8. Ski pole: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26748.2, "ram_available_mb": 99024.0, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 26748.0, "ram_available_mb": 99024.1, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.78, "peak": 40.57, "min": 18.53}, "VIN": {"avg": 66.76, "peak": 114.05, "min": 30.06}, "VIN_SYS_5V0": {"avg": 14.82, "peak": 15.85, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 15.98, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 26.78, "energy_joules_est": 77.38, "sample_count": 28, "duration_seconds": 2.889}, "timestamp": "2026-01-19T14:12:47.528234"}
{"image_index": 5, "image_name": "000000000785.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000785.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2273.663, "latencies_ms": [2273.663], "images_per_second": 0.44, "prompt_tokens": 1117, "response_tokens_est": 42, "n_tiles": 1, "output_text": " The skier is positioned in the foreground of the image, with the ski slope stretching out into the background. The skier is to the left of the image, with the ski poles extending towards the right.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26748.0, "ram_available_mb": 99024.1, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26748.0, "ram_available_mb": 99024.1, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.56, "peak": 37.82, "min": 16.16}, "VIN": {"avg": 72.11, "peak": 106.44, "min": 30.18}, "VIN_SYS_5V0": {"avg": 14.63, "peak": 15.44, "min": 12.56}, "VDD_CPU_SOC_MSS": {"avg": 15.47, "peak": 16.14, "min": 13.79}}, "power_watts_avg": 27.56, "energy_joules_est": 62.67, "sample_count": 22, "duration_seconds": 2.274}, "timestamp": "2026-01-19T14:12:49.823893"}
{"image_index": 5, "image_name": "000000000785.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000785.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1490.62, "latencies_ms": [1490.62], "images_per_second": 0.671, "prompt_tokens": 1111, "response_tokens_est": 16, "n_tiles": 1, "output_text": " A woman wearing a red jacket and black pants is skiing down a snowy hill.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26748.0, "ram_available_mb": 99024.1, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 26748.0, "ram_available_mb": 99024.1, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.12, "peak": 40.18, "min": 16.95}, "VIN": {"avg": 73.35, "peak": 118.14, "min": 28.75}, "VIN_SYS_5V0": {"avg": 14.85, "peak": 15.85, "min": 12.76}, "VDD_CPU_SOC_MSS": {"avg": 15.99, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 31.12, "energy_joules_est": 46.4, "sample_count": 15, "duration_seconds": 1.491}, "timestamp": "2026-01-19T14:12:51.387553"}
{"image_index": 5, "image_name": "000000000785.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000785.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1623.072, "latencies_ms": [1623.072], "images_per_second": 0.616, "prompt_tokens": 1109, "response_tokens_est": 20, "n_tiles": 1, "output_text": " The skier is wearing a red jacket and black pants, and the snow is white and fluffy.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 26748.0, "ram_available_mb": 99024.1, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26747.8, "ram_available_mb": 99024.4, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.42, "peak": 40.18, "min": 19.71}, "VIN": {"avg": 73.16, "peak": 116.79, "min": 29.15}, "VIN_SYS_5V0": {"avg": 14.59, "peak": 15.65, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.73, "peak": 16.93, "min": 13.39}}, "power_watts_avg": 31.42, "energy_joules_est": 51.0, "sample_count": 16, "duration_seconds": 1.623}, "timestamp": "2026-01-19T14:12:53.052456"}
{"image_index": 6, "image_name": "000000000802.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000802.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1973.45, "latencies_ms": [1973.45], "images_per_second": 0.507, "prompt_tokens": 1100, "response_tokens_est": 35, "n_tiles": 1, "output_text": " The image depicts a kitchen with a white refrigerator, a white stove, and a white oven, all set against a backdrop of wooden cabinets and a beige tiled floor.", "error": null, "sys_before": {"cpu_percent": 14.3, "ram_used_mb": 26747.8, "ram_available_mb": 99024.4, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26747.8, "ram_available_mb": 99024.4, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.55, "peak": 39.78, "min": 18.92}, "VIN": {"avg": 61.34, "peak": 98.41, "min": 29.19}, "VIN_SYS_5V0": {"avg": 14.73, "peak": 15.85, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.88, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 29.55, "energy_joules_est": 58.33, "sample_count": 19, "duration_seconds": 1.974}, "timestamp": "2026-01-19T14:12:55.048064"}
{"image_index": 6, "image_name": "000000000802.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000802.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2776.216, "latencies_ms": [2776.216], "images_per_second": 0.36, "prompt_tokens": 1114, "response_tokens_est": 66, "n_tiles": 1, "output_text": " 1. white refrigerator: 1\n2. white stove: 1\n3. white oven: 1\n4. white cabinet: 2\n5. white dishwasher: 1\n6. white sink: 1\n7. white countertop: 1\n8. white tile floor: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26747.8, "ram_available_mb": 99024.4, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 26748.5, "ram_available_mb": 99023.6, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.44, "peak": 39.78, "min": 17.74}, "VIN": {"avg": 71.29, "peak": 117.68, "min": 29.74}, "VIN_SYS_5V0": {"avg": 14.81, "peak": 15.85, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 16.01, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 26.44, "energy_joules_est": 73.41, "sample_count": 27, "duration_seconds": 2.777}, "timestamp": "2026-01-19T14:12:57.875851"}
{"image_index": 6, "image_name": "000000000802.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000802.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2280.777, "latencies_ms": [2280.777], "images_per_second": 0.438, "prompt_tokens": 1118, "response_tokens_est": 46, "n_tiles": 1, "output_text": " The stove is located to the left of the refrigerator, and the sink is situated in the middle of the kitchen. The refrigerator is positioned to the right of the stove, and the oven is located to the left of the sink.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26748.5, "ram_available_mb": 99023.6, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26748.5, "ram_available_mb": 99023.7, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.69, "peak": 39.78, "min": 15.38}, "VIN": {"avg": 71.68, "peak": 115.44, "min": 32.05}, "VIN_SYS_5V0": {"avg": 14.65, "peak": 15.85, "min": 12.15}, "VDD_CPU_SOC_MSS": {"avg": 15.88, "peak": 17.32, "min": 13.4}}, "power_watts_avg": 27.69, "energy_joules_est": 63.17, "sample_count": 22, "duration_seconds": 2.281}, "timestamp": "2026-01-19T14:13:00.163070"}
{"image_index": 6, "image_name": "000000000802.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000802.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1444.224, "latencies_ms": [1444.224], "images_per_second": 0.692, "prompt_tokens": 1112, "response_tokens_est": 15, "n_tiles": 1, "output_text": " A small kitchen with white appliances and wood cabinets is shown in the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26748.5, "ram_available_mb": 99023.7, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 26748.5, "ram_available_mb": 99023.7, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.46, "peak": 39.39, "min": 16.95}, "VIN": {"avg": 71.23, "peak": 116.71, "min": 31.45}, "VIN_SYS_5V0": {"avg": 15.02, "peak": 15.95, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 16.34, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 31.46, "energy_joules_est": 45.45, "sample_count": 14, "duration_seconds": 1.445}, "timestamp": "2026-01-19T14:13:01.614199"}
{"image_index": 6, "image_name": "000000000802.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000802.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1521.298, "latencies_ms": [1521.298], "images_per_second": 0.657, "prompt_tokens": 1110, "response_tokens_est": 18, "n_tiles": 1, "output_text": " The kitchen is well-lit with natural light, and the cabinets are made of wood.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26748.5, "ram_available_mb": 99023.7, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 26748.5, "ram_available_mb": 99023.7, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.62, "peak": 40.18, "min": 22.08}, "VIN": {"avg": 77.9, "peak": 129.0, "min": 28.64}, "VIN_SYS_5V0": {"avg": 15.06, "peak": 15.95, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 16.38, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 32.62, "energy_joules_est": 49.63, "sample_count": 15, "duration_seconds": 1.522}, "timestamp": "2026-01-19T14:13:03.176822"}
{"image_index": 7, "image_name": "000000000872.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000872.jpg", "image_width": 621, "image_height": 640, "image_resolution": "621x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2166.909, "latencies_ms": [2166.909], "images_per_second": 0.461, "prompt_tokens": 1432, "response_tokens_est": 26, "n_tiles": 1, "output_text": " Two baseball players are running on the field, one of them is wearing a green shirt and the other is wearing a white shirt.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 26748.5, "ram_available_mb": 99023.7, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 26748.5, "ram_available_mb": 99023.7, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.2, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.59, "peak": 40.18, "min": 19.71}, "VIN": {"avg": 74.68, "peak": 123.25, "min": 28.57}, "VIN_SYS_5V0": {"avg": 15.16, "peak": 16.46, "min": 12.46}, "VDD_CPU_SOC_MSS": {"avg": 16.37, "peak": 17.72, "min": 13.79}}, "power_watts_avg": 31.59, "energy_joules_est": 68.46, "sample_count": 21, "duration_seconds": 2.167}, "timestamp": "2026-01-19T14:13:05.369470"}
{"image_index": 7, "image_name": "000000000872.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000872.jpg", "image_width": 621, "image_height": 640, "image_resolution": "621x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3269.522, "latencies_ms": [3269.522], "images_per_second": 0.306, "prompt_tokens": 1446, "response_tokens_est": 68, "n_tiles": 1, "output_text": " 1. baseball player: 2\n2. baseball glove: 1\n3. baseball bat: 1\n4. baseball cap: 1\n5. baseball helmet: 1\n6. baseball field: 1\n7. baseball player's pants: 1\n8. baseball player's shirt: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26748.5, "ram_available_mb": 99023.7, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26749.3, "ram_available_mb": 99022.9, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.85, "peak": 40.18, "min": 18.53}, "VIN": {"avg": 68.09, "peak": 131.37, "min": 27.78}, "VIN_SYS_5V0": {"avg": 14.98, "peak": 16.46, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 16.17, "peak": 17.72, "min": 14.18}}, "power_watts_avg": 27.85, "energy_joules_est": 91.07, "sample_count": 32, "duration_seconds": 3.27}, "timestamp": "2026-01-19T14:13:08.698728"}
{"image_index": 7, "image_name": "000000000872.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000872.jpg", "image_width": 621, "image_height": 640, "image_resolution": "621x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2773.111, "latencies_ms": [2773.111], "images_per_second": 0.361, "prompt_tokens": 1450, "response_tokens_est": 43, "n_tiles": 1, "output_text": " The baseball player in the foreground is running towards the right side of the image, while the other player is running towards the left side. The baseball player in the foreground is closer to the camera than the other player.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26749.3, "ram_available_mb": 99022.9, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26749.5, "ram_available_mb": 99022.7, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.21, "peak": 39.78, "min": 15.77}, "VIN": {"avg": 70.12, "peak": 127.72, "min": 27.13}, "VIN_SYS_5V0": {"avg": 14.62, "peak": 16.26, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.81, "peak": 17.72, "min": 13.0}}, "power_watts_avg": 28.21, "energy_joules_est": 78.24, "sample_count": 27, "duration_seconds": 2.773}, "timestamp": "2026-01-19T14:13:11.511724"}
{"image_index": 7, "image_name": "000000000872.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000872.jpg", "image_width": 621, "image_height": 640, "image_resolution": "621x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2148.146, "latencies_ms": [2148.146], "images_per_second": 0.466, "prompt_tokens": 1444, "response_tokens_est": 25, "n_tiles": 1, "output_text": " A baseball game is taking place on a field with a boy wearing a green shirt and a baseball glove running towards the camera.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26749.5, "ram_available_mb": 99022.7, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 26749.5, "ram_available_mb": 99022.7, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.94, "peak": 39.78, "min": 15.77}, "VIN": {"avg": 71.4, "peak": 117.18, "min": 28.05}, "VIN_SYS_5V0": {"avg": 14.92, "peak": 16.26, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 16.09, "peak": 17.72, "min": 13.39}}, "power_watts_avg": 30.94, "energy_joules_est": 66.48, "sample_count": 21, "duration_seconds": 2.149}, "timestamp": "2026-01-19T14:13:13.703619"}
{"image_index": 7, "image_name": "000000000872.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000872.jpg", "image_width": 621, "image_height": 640, "image_resolution": "621x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2553.831, "latencies_ms": [2553.831], "images_per_second": 0.392, "prompt_tokens": 1442, "response_tokens_est": 41, "n_tiles": 1, "output_text": " The image features a baseball game with two players running on the field, one wearing a green shirt and the other wearing a white shirt. The field is covered in green grass, and the sky is clear.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26749.5, "ram_available_mb": 99022.7, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26749.5, "ram_available_mb": 99022.7, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.77, "peak": 40.18, "min": 18.14}, "VIN": {"avg": 68.82, "peak": 107.02, "min": 28.58}, "VIN_SYS_5V0": {"avg": 14.95, "peak": 16.36, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 16.14, "peak": 17.71, "min": 13.39}}, "power_watts_avg": 29.77, "energy_joules_est": 76.04, "sample_count": 25, "duration_seconds": 2.554}, "timestamp": "2026-01-19T14:13:16.307590"}
{"image_index": 8, "image_name": "000000000885.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000885.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1697.739, "latencies_ms": [1697.739], "images_per_second": 0.589, "prompt_tokens": 1099, "response_tokens_est": 24, "n_tiles": 1, "output_text": " A tennis player is preparing to hit a ball on a court with a J.P. Morgan advertisement in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26749.7, "ram_available_mb": 99022.4, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26749.7, "ram_available_mb": 99022.4, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.1, "peak": 40.18, "min": 16.56}, "VIN": {"avg": 71.21, "peak": 106.78, "min": 29.05}, "VIN_SYS_5V0": {"avg": 14.7, "peak": 15.85, "min": 12.25}, "VDD_CPU_SOC_MSS": {"avg": 15.82, "peak": 16.93, "min": 13.39}}, "power_watts_avg": 30.1, "energy_joules_est": 51.12, "sample_count": 17, "duration_seconds": 1.698}, "timestamp": "2026-01-19T14:13:18.082387"}
{"image_index": 8, "image_name": "000000000885.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000885.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2150.558, "latencies_ms": [2150.558], "images_per_second": 0.465, "prompt_tokens": 1113, "response_tokens_est": 41, "n_tiles": 1, "output_text": " 1. tennis player\n2. tennis racket\n3. ball\n4. tennis court\n5. J.P. Morgan\n6. referee\n7. spectator\n8. advertisement", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26749.7, "ram_available_mb": 99022.4, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26749.7, "ram_available_mb": 99022.4, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.65, "peak": 40.18, "min": 17.74}, "VIN": {"avg": 69.36, "peak": 120.74, "min": 28.33}, "VIN_SYS_5V0": {"avg": 14.66, "peak": 15.85, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.83, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 28.65, "energy_joules_est": 61.62, "sample_count": 21, "duration_seconds": 2.151}, "timestamp": "2026-01-19T14:13:20.265107"}
{"image_index": 8, "image_name": "000000000885.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000885.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2062.01, "latencies_ms": [2062.01], "images_per_second": 0.485, "prompt_tokens": 1117, "response_tokens_est": 38, "n_tiles": 1, "output_text": " The tennis player is positioned in the foreground, with the ball and the umpire in the background. The player is near the baseline, while the umpire is near the net.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26749.7, "ram_available_mb": 99022.4, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26750.0, "ram_available_mb": 99022.2, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.96, "peak": 40.18, "min": 16.95}, "VIN": {"avg": 70.67, "peak": 108.73, "min": 28.43}, "VIN_SYS_5V0": {"avg": 14.83, "peak": 15.85, "min": 12.56}, "VDD_CPU_SOC_MSS": {"avg": 16.05, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 28.96, "energy_joules_est": 59.73, "sample_count": 20, "duration_seconds": 2.063}, "timestamp": "2026-01-19T14:13:22.356510"}
{"image_index": 8, "image_name": "000000000885.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000885.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1593.161, "latencies_ms": [1593.161], "images_per_second": 0.628, "prompt_tokens": 1111, "response_tokens_est": 20, "n_tiles": 1, "output_text": " A tennis player is playing on a court with a J.P. Morgan advertisement in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26750.0, "ram_available_mb": 99022.2, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 26750.0, "ram_available_mb": 99022.2, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.71, "peak": 40.18, "min": 17.35}, "VIN": {"avg": 66.71, "peak": 115.11, "min": 27.01}, "VIN_SYS_5V0": {"avg": 14.82, "peak": 15.85, "min": 12.66}, "VDD_CPU_SOC_MSS": {"avg": 16.04, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 30.71, "energy_joules_est": 48.93, "sample_count": 16, "duration_seconds": 1.593}, "timestamp": "2026-01-19T14:13:24.035570"}
{"image_index": 8, "image_name": "000000000885.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000885.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3443.519, "latencies_ms": [3443.519], "images_per_second": 0.29, "prompt_tokens": 1109, "response_tokens_est": 90, "n_tiles": 1, "output_text": " The image captures a dynamic moment on a vibrant blue tennis court, where a player in a crisp white outfit is in the midst of a powerful swing, his body leaning forward in anticipation. The court is bathed in bright sunlight, casting sharp shadows and highlighting the texture of the green surface. In the background, a large blue wall stands out, adorned with the logo of J.P. Morgan, adding a touch of corporate elegance to the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26750.0, "ram_available_mb": 99022.2, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 26749.3, "ram_available_mb": 99022.9, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 25.04, "peak": 40.18, "min": 16.95}, "VIN": {"avg": 67.09, "peak": 97.37, "min": 28.11}, "VIN_SYS_5V0": {"avg": 14.55, "peak": 15.75, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.67, "peak": 16.93, "min": 13.0}}, "power_watts_avg": 25.04, "energy_joules_est": 86.24, "sample_count": 34, "duration_seconds": 3.444}, "timestamp": "2026-01-19T14:13:27.575004"}
{"image_index": 9, "image_name": "000000001000.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001000.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1719.738, "latencies_ms": [1719.738], "images_per_second": 0.581, "prompt_tokens": 1099, "response_tokens_est": 25, "n_tiles": 1, "output_text": " A group of children and adults are posing for a picture on a tennis court, with one of the adults holding a trophy.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 26749.3, "ram_available_mb": 99022.9, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26749.1, "ram_available_mb": 99023.1, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.62, "peak": 39.78, "min": 14.59}, "VIN": {"avg": 70.5, "peak": 113.26, "min": 28.44}, "VIN_SYS_5V0": {"avg": 14.48, "peak": 15.65, "min": 11.54}, "VDD_CPU_SOC_MSS": {"avg": 15.63, "peak": 16.93, "min": 12.61}}, "power_watts_avg": 29.62, "energy_joules_est": 50.96, "sample_count": 17, "duration_seconds": 1.72}, "timestamp": "2026-01-19T14:13:29.356648"}
{"image_index": 9, "image_name": "000000001000.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001000.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2505.758, "latencies_ms": [2505.758], "images_per_second": 0.399, "prompt_tokens": 1113, "response_tokens_est": 56, "n_tiles": 1, "output_text": " 1. boy: 3\n2. girl: 2\n3. boy: 2\n4. boy: 1\n5. boy: 1\n6. boy: 1\n7. boy: 1\n8. boy: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26749.1, "ram_available_mb": 99023.1, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26749.1, "ram_available_mb": 99023.1, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.28, "peak": 40.18, "min": 16.95}, "VIN": {"avg": 72.59, "peak": 115.75, "min": 26.87}, "VIN_SYS_5V0": {"avg": 14.72, "peak": 15.85, "min": 12.36}, "VDD_CPU_SOC_MSS": {"avg": 15.86, "peak": 16.93, "min": 13.39}}, "power_watts_avg": 27.28, "energy_joules_est": 68.37, "sample_count": 25, "duration_seconds": 2.506}, "timestamp": "2026-01-19T14:13:31.967417"}
{"image_index": 9, "image_name": "000000001000.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001000.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2792.475, "latencies_ms": [2792.475], "images_per_second": 0.358, "prompt_tokens": 1117, "response_tokens_est": 66, "n_tiles": 1, "output_text": " The group of children and adults are standing on a tennis court, with the tennis rackets held by the children and adults. The tennis rackets are positioned in the foreground, with the group of children and adults standing behind them. The tennis court is located in the background, with the tennis net and trees visible in the distance.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26749.1, "ram_available_mb": 99023.1, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 26748.8, "ram_available_mb": 99023.4, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.21, "peak": 39.78, "min": 14.99}, "VIN": {"avg": 70.99, "peak": 103.79, "min": 29.55}, "VIN_SYS_5V0": {"avg": 14.55, "peak": 15.75, "min": 11.44}, "VDD_CPU_SOC_MSS": {"avg": 15.67, "peak": 16.93, "min": 12.61}}, "power_watts_avg": 26.21, "energy_joules_est": 73.2, "sample_count": 27, "duration_seconds": 2.793}, "timestamp": "2026-01-19T14:13:34.788013"}
{"image_index": 9, "image_name": "000000001000.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001000.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1484.12, "latencies_ms": [1484.12], "images_per_second": 0.674, "prompt_tokens": 1111, "response_tokens_est": 16, "n_tiles": 1, "output_text": " A group of children and adults are posing for a picture on a tennis court.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26748.8, "ram_available_mb": 99023.4, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 26749.3, "ram_available_mb": 99022.9, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.1, "peak": 40.18, "min": 16.56}, "VIN": {"avg": 82.14, "peak": 128.35, "min": 29.0}, "VIN_SYS_5V0": {"avg": 14.83, "peak": 15.85, "min": 12.56}, "VDD_CPU_SOC_MSS": {"avg": 16.07, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 31.1, "energy_joules_est": 46.17, "sample_count": 15, "duration_seconds": 1.484}, "timestamp": "2026-01-19T14:13:36.347390"}
{"image_index": 9, "image_name": "000000001000.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001000.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2140.896, "latencies_ms": [2140.896], "images_per_second": 0.467, "prompt_tokens": 1109, "response_tokens_est": 41, "n_tiles": 1, "output_text": " The image features a group of children and adults standing on a blue tennis court, with the children holding tennis rackets and wearing tennis attire. The lighting is natural, and the weather appears to be sunny.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26749.3, "ram_available_mb": 99022.9, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26749.7, "ram_available_mb": 99022.5, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.1, "peak": 40.57, "min": 19.32}, "VIN": {"avg": 68.12, "peak": 118.88, "min": 28.03}, "VIN_SYS_5V0": {"avg": 14.63, "peak": 15.85, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.77, "peak": 16.93, "min": 13.4}}, "power_watts_avg": 29.1, "energy_joules_est": 62.31, "sample_count": 21, "duration_seconds": 2.141}, "timestamp": "2026-01-19T14:13:38.531847"}
{"image_index": 10, "image_name": "000000001268.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001268.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1929.054, "latencies_ms": [1929.054], "images_per_second": 0.518, "prompt_tokens": 1099, "response_tokens_est": 33, "n_tiles": 1, "output_text": " In the image, a group of people are gathered by a river, with a bridge arching above them, and a bird is seen walking on the riverbank.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 26749.7, "ram_available_mb": 99022.5, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26749.7, "ram_available_mb": 99022.5, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.24, "peak": 40.18, "min": 16.56}, "VIN": {"avg": 69.89, "peak": 115.67, "min": 28.11}, "VIN_SYS_5V0": {"avg": 14.72, "peak": 15.85, "min": 12.25}, "VDD_CPU_SOC_MSS": {"avg": 15.88, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 29.24, "energy_joules_est": 56.42, "sample_count": 19, "duration_seconds": 1.93}, "timestamp": "2026-01-19T14:13:40.516194"}
{"image_index": 10, "image_name": "000000001268.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001268.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2539.157, "latencies_ms": [2539.157], "images_per_second": 0.394, "prompt_tokens": 1113, "response_tokens_est": 56, "n_tiles": 1, "output_text": " 1. bridge: 1\n2. people: 4\n3. birds: 1\n4. rocks: 2\n5. water: 1\n6. trees: 1\n7. buildings: 1\n8. bridge: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26749.7, "ram_available_mb": 99022.5, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 26750.2, "ram_available_mb": 99022.0, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.01, "peak": 39.78, "min": 16.95}, "VIN": {"avg": 69.51, "peak": 122.76, "min": 28.47}, "VIN_SYS_5V0": {"avg": 14.74, "peak": 15.85, "min": 12.36}, "VDD_CPU_SOC_MSS": {"avg": 15.81, "peak": 16.93, "min": 13.39}}, "power_watts_avg": 27.01, "energy_joules_est": 68.6, "sample_count": 25, "duration_seconds": 2.54}, "timestamp": "2026-01-19T14:13:43.113468"}
{"image_index": 10, "image_name": "000000001268.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001268.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2456.038, "latencies_ms": [2456.038], "images_per_second": 0.407, "prompt_tokens": 1117, "response_tokens_est": 53, "n_tiles": 1, "output_text": " The main objects are positioned in a way that the bridge is in the foreground, with the people and the bird in the background. The bridge is located to the left of the people and the bird, and the people are sitting on the right side of the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26750.2, "ram_available_mb": 99022.0, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26750.2, "ram_available_mb": 99022.0, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.12, "peak": 39.78, "min": 15.77}, "VIN": {"avg": 67.16, "peak": 123.5, "min": 29.01}, "VIN_SYS_5V0": {"avg": 14.72, "peak": 15.85, "min": 12.15}, "VDD_CPU_SOC_MSS": {"avg": 15.78, "peak": 16.93, "min": 13.39}}, "power_watts_avg": 27.12, "energy_joules_est": 66.62, "sample_count": 24, "duration_seconds": 2.456}, "timestamp": "2026-01-19T14:13:45.610395"}
{"image_index": 10, "image_name": "000000001268.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001268.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1711.177, "latencies_ms": [1711.177], "images_per_second": 0.584, "prompt_tokens": 1111, "response_tokens_est": 24, "n_tiles": 1, "output_text": " A group of people are sitting on a stone ledge by a river, watching a white bird swim in the water.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26750.2, "ram_available_mb": 99022.0, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26749.6, "ram_available_mb": 99022.5, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.94, "peak": 39.78, "min": 16.16}, "VIN": {"avg": 67.29, "peak": 122.56, "min": 28.16}, "VIN_SYS_5V0": {"avg": 14.71, "peak": 15.85, "min": 12.36}, "VDD_CPU_SOC_MSS": {"avg": 15.94, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 29.94, "energy_joules_est": 51.24, "sample_count": 17, "duration_seconds": 1.712}, "timestamp": "2026-01-19T14:13:47.381633"}
{"image_index": 10, "image_name": "000000001268.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001268.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2116.011, "latencies_ms": [2116.011], "images_per_second": 0.473, "prompt_tokens": 1109, "response_tokens_est": 40, "n_tiles": 1, "output_text": " The image features a bridge with a metal structure, a river with a white bird, and a group of people sitting on the bank. The lighting is natural, and the weather appears to be clear.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26749.6, "ram_available_mb": 99022.5, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26750.9, "ram_available_mb": 99021.3, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.76, "peak": 40.18, "min": 18.14}, "VIN": {"avg": 68.71, "peak": 118.66, "min": 28.27}, "VIN_SYS_5V0": {"avg": 14.66, "peak": 15.85, "min": 12.15}, "VDD_CPU_SOC_MSS": {"avg": 15.82, "peak": 16.93, "min": 13.39}}, "power_watts_avg": 28.76, "energy_joules_est": 60.87, "sample_count": 21, "duration_seconds": 2.116}, "timestamp": "2026-01-19T14:13:49.567199"}
{"image_index": 11, "image_name": "000000001296.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001296.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1423.887, "latencies_ms": [1423.887], "images_per_second": 0.702, "prompt_tokens": 1100, "response_tokens_est": 14, "n_tiles": 1, "output_text": " A woman is looking at her phone with a Hello Kitty case.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26750.9, "ram_available_mb": 99021.3, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26750.9, "ram_available_mb": 99021.3, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.29, "peak": 39.78, "min": 16.16}, "VIN": {"avg": 73.71, "peak": 122.5, "min": 29.56}, "VIN_SYS_5V0": {"avg": 14.68, "peak": 15.95, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.92, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 31.29, "energy_joules_est": 44.56, "sample_count": 14, "duration_seconds": 1.424}, "timestamp": "2026-01-19T14:13:51.031621"}
{"image_index": 11, "image_name": "000000001296.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001296.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2728.68, "latencies_ms": [2728.68], "images_per_second": 0.366, "prompt_tokens": 1114, "response_tokens_est": 61, "n_tiles": 1, "output_text": " 1. woman: 1\n2. hair: 1\n3. earring: 1\n4. wristwatch: 1\n5. bracelet: 2\n6. phone: 1\n7. Hello Kitty: 1\n8. background: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26750.9, "ram_available_mb": 99021.3, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26750.6, "ram_available_mb": 99021.6, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.06, "peak": 40.57, "min": 16.95}, "VIN": {"avg": 67.77, "peak": 126.0, "min": 28.54}, "VIN_SYS_5V0": {"avg": 14.67, "peak": 15.95, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 15.74, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 27.06, "energy_joules_est": 73.85, "sample_count": 27, "duration_seconds": 2.729}, "timestamp": "2026-01-19T14:13:53.847250"}
{"image_index": 11, "image_name": "000000001296.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001296.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2307.732, "latencies_ms": [2307.732], "images_per_second": 0.433, "prompt_tokens": 1118, "response_tokens_est": 47, "n_tiles": 1, "output_text": " The woman is in the foreground of the image, holding a phone in her hand. The phone is in front of her, and she is looking at it. The background is blurred, but it appears to be a crowd of people.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26750.6, "ram_available_mb": 99021.6, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26751.1, "ram_available_mb": 99021.1, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.14, "peak": 39.78, "min": 14.59}, "VIN": {"avg": 64.59, "peak": 100.52, "min": 27.37}, "VIN_SYS_5V0": {"avg": 14.54, "peak": 15.85, "min": 11.54}, "VDD_CPU_SOC_MSS": {"avg": 15.62, "peak": 16.93, "min": 12.61}}, "power_watts_avg": 27.14, "energy_joules_est": 62.65, "sample_count": 23, "duration_seconds": 2.308}, "timestamp": "2026-01-19T14:13:56.239495"}
{"image_index": 11, "image_name": "000000001296.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001296.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1502.073, "latencies_ms": [1502.073], "images_per_second": 0.666, "prompt_tokens": 1112, "response_tokens_est": 16, "n_tiles": 1, "output_text": " A woman is taking a picture of herself with a Hello Kitty phone case.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26751.1, "ram_available_mb": 99021.1, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 26750.3, "ram_available_mb": 99021.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.21, "peak": 39.39, "min": 15.38}, "VIN": {"avg": 65.43, "peak": 103.27, "min": 28.07}, "VIN_SYS_5V0": {"avg": 14.48, "peak": 15.95, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.75, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 30.21, "energy_joules_est": 45.39, "sample_count": 15, "duration_seconds": 1.502}, "timestamp": "2026-01-19T14:13:57.804842"}
{"image_index": 11, "image_name": "000000001296.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001296.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2064.849, "latencies_ms": [2064.849], "images_per_second": 0.484, "prompt_tokens": 1110, "response_tokens_est": 38, "n_tiles": 1, "output_text": " The woman is wearing a white shirt with a black and white print, and she is holding a Hello Kitty phone case. The lighting is natural, and the weather appears to be sunny.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26750.3, "ram_available_mb": 99021.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26749.2, "ram_available_mb": 99022.9, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.39, "peak": 40.18, "min": 19.71}, "VIN": {"avg": 70.8, "peak": 119.7, "min": 29.52}, "VIN_SYS_5V0": {"avg": 14.73, "peak": 15.85, "min": 12.15}, "VDD_CPU_SOC_MSS": {"avg": 15.85, "peak": 17.32, "min": 13.4}}, "power_watts_avg": 29.39, "energy_joules_est": 60.7, "sample_count": 20, "duration_seconds": 2.065}, "timestamp": "2026-01-19T14:13:59.890026"}
{"image_index": 12, "image_name": "000000001353.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001353.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1692.366, "latencies_ms": [1692.366], "images_per_second": 0.591, "prompt_tokens": 1100, "response_tokens_est": 24, "n_tiles": 1, "output_text": " A group of children are riding in a red and yellow train car on a track in a room with a brown wall.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26749.2, "ram_available_mb": 99022.9, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26749.7, "ram_available_mb": 99022.5, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.31, "peak": 39.39, "min": 17.74}, "VIN": {"avg": 77.54, "peak": 125.84, "min": 29.46}, "VIN_SYS_5V0": {"avg": 14.8, "peak": 15.85, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 16.01, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 30.31, "energy_joules_est": 51.32, "sample_count": 17, "duration_seconds": 1.693}, "timestamp": "2026-01-19T14:14:01.663680"}
{"image_index": 12, "image_name": "000000001353.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001353.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2434.17, "latencies_ms": [2434.17], "images_per_second": 0.411, "prompt_tokens": 1114, "response_tokens_est": 53, "n_tiles": 1, "output_text": " 1. children: 5\n2. train: 1\n3. track: 1\n4. wall: 1\n5. floor: 1\n6. children's clothing: 5\n7. children's hair: 5", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26749.7, "ram_available_mb": 99022.5, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26749.7, "ram_available_mb": 99022.5, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.5, "peak": 40.18, "min": 17.35}, "VIN": {"avg": 66.89, "peak": 105.67, "min": 30.75}, "VIN_SYS_5V0": {"avg": 14.68, "peak": 15.85, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.79, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 27.5, "energy_joules_est": 66.95, "sample_count": 24, "duration_seconds": 2.435}, "timestamp": "2026-01-19T14:14:04.158463"}
{"image_index": 12, "image_name": "000000001353.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001353.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2156.628, "latencies_ms": [2156.628], "images_per_second": 0.464, "prompt_tokens": 1118, "response_tokens_est": 42, "n_tiles": 1, "output_text": " The children are in the foreground, riding on a train that is in the middle of the image. The train is moving towards the right side of the image, and the children are looking towards the left side.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26749.7, "ram_available_mb": 99022.5, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26750.0, "ram_available_mb": 99022.2, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.14, "peak": 39.78, "min": 15.77}, "VIN": {"avg": 64.89, "peak": 126.21, "min": 29.55}, "VIN_SYS_5V0": {"avg": 14.75, "peak": 15.95, "min": 12.15}, "VDD_CPU_SOC_MSS": {"avg": 15.88, "peak": 17.32, "min": 13.4}}, "power_watts_avg": 28.14, "energy_joules_est": 60.71, "sample_count": 21, "duration_seconds": 2.157}, "timestamp": "2026-01-19T14:14:06.352571"}
{"image_index": 12, "image_name": "000000001353.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001353.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1674.749, "latencies_ms": [1674.749], "images_per_second": 0.597, "prompt_tokens": 1112, "response_tokens_est": 24, "n_tiles": 1, "output_text": " A group of children are riding on a train car in a room with a wooden floor and a wall in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26750.0, "ram_available_mb": 99022.2, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26750.2, "ram_available_mb": 99022.0, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.39, "peak": 39.0, "min": 16.56}, "VIN": {"avg": 74.64, "peak": 107.27, "min": 36.48}, "VIN_SYS_5V0": {"avg": 14.96, "peak": 15.95, "min": 12.56}, "VDD_CPU_SOC_MSS": {"avg": 16.19, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 30.39, "energy_joules_est": 50.91, "sample_count": 16, "duration_seconds": 1.675}, "timestamp": "2026-01-19T14:14:08.033289"}
{"image_index": 12, "image_name": "000000001353.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001353.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2381.015, "latencies_ms": [2381.015], "images_per_second": 0.42, "prompt_tokens": 1110, "response_tokens_est": 52, "n_tiles": 1, "output_text": " The image features a group of children riding on a red and yellow train car, with the train car being the main focus of the image. The lighting in the image is dim, and the children are wearing jackets, indicating that it might be a cold day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26750.2, "ram_available_mb": 99022.0, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26752.7, "ram_available_mb": 99019.5, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.39, "peak": 40.18, "min": 19.71}, "VIN": {"avg": 65.87, "peak": 97.25, "min": 32.58}, "VIN_SYS_5V0": {"avg": 15.0, "peak": 16.05, "min": 13.29}, "VDD_CPU_SOC_MSS": {"avg": 16.23, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 28.39, "energy_joules_est": 67.6, "sample_count": 23, "duration_seconds": 2.381}, "timestamp": "2026-01-19T14:14:10.428505"}
{"image_index": 13, "image_name": "000000001425.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001425.jpg", "image_width": 640, "image_height": 512, "image_resolution": "640x512", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1962.935, "latencies_ms": [1962.935], "images_per_second": 0.509, "prompt_tokens": 1432, "response_tokens_est": 19, "n_tiles": 1, "output_text": " A black and white photo of a plate with a slice of cake and a side of salad.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26752.7, "ram_available_mb": 99019.5, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 26752.9, "ram_available_mb": 99019.2, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.2, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.96, "peak": 39.78, "min": 16.56}, "VIN": {"avg": 78.35, "peak": 130.4, "min": 29.28}, "VIN_SYS_5V0": {"avg": 15.37, "peak": 16.56, "min": 12.76}, "VDD_CPU_SOC_MSS": {"avg": 16.64, "peak": 17.71, "min": 14.18}}, "power_watts_avg": 31.96, "energy_joules_est": 62.76, "sample_count": 19, "duration_seconds": 1.964}, "timestamp": "2026-01-19T14:14:12.409575"}
{"image_index": 13, "image_name": "000000001425.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001425.jpg", "image_width": 640, "image_height": 512, "image_resolution": "640x512", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2932.445, "latencies_ms": [2932.445], "images_per_second": 0.341, "prompt_tokens": 1446, "response_tokens_est": 56, "n_tiles": 1, "output_text": " 1. plate: 1\n2. food: 1\n3. cup: 1\n4. fork: 1\n5. knife: 1\n6. table: 1\n7. background: 1\n8. glass: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26752.9, "ram_available_mb": 99019.2, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26752.9, "ram_available_mb": 99019.2, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.95, "peak": 40.18, "min": 18.13}, "VIN": {"avg": 69.62, "peak": 115.72, "min": 28.37}, "VIN_SYS_5V0": {"avg": 15.07, "peak": 16.46, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 16.25, "peak": 17.72, "min": 14.18}}, "power_watts_avg": 28.95, "energy_joules_est": 84.91, "sample_count": 29, "duration_seconds": 2.933}, "timestamp": "2026-01-19T14:14:15.430226"}
{"image_index": 13, "image_name": "000000001425.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001425.jpg", "image_width": 640, "image_height": 512, "image_resolution": "640x512", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2729.35, "latencies_ms": [2729.35], "images_per_second": 0.366, "prompt_tokens": 1450, "response_tokens_est": 45, "n_tiles": 1, "output_text": " The main object, a slice of cake, is in the foreground, with a small dish of food in the background. The cake is to the left of the dish, and the dish is to the right of the cake.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26752.9, "ram_available_mb": 99019.2, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26754.7, "ram_available_mb": 99017.5, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.46, "peak": 39.39, "min": 15.77}, "VIN": {"avg": 69.44, "peak": 125.41, "min": 28.31}, "VIN_SYS_5V0": {"avg": 14.73, "peak": 16.26, "min": 11.44}, "VDD_CPU_SOC_MSS": {"avg": 15.81, "peak": 17.71, "min": 12.61}}, "power_watts_avg": 28.46, "energy_joules_est": 77.7, "sample_count": 27, "duration_seconds": 2.73}, "timestamp": "2026-01-19T14:14:18.247285"}
{"image_index": 13, "image_name": "000000001425.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001425.jpg", "image_width": 640, "image_height": 512, "image_resolution": "640x512", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1891.549, "latencies_ms": [1891.549], "images_per_second": 0.529, "prompt_tokens": 1444, "response_tokens_est": 16, "n_tiles": 1, "output_text": " A black and white photo of a plate of food with a side of salad.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26754.7, "ram_available_mb": 99017.5, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.5, "ram_used_mb": 26752.7, "ram_available_mb": 99019.4, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.52, "peak": 39.78, "min": 15.77}, "VIN": {"avg": 75.85, "peak": 130.63, "min": 27.49}, "VIN_SYS_5V0": {"avg": 14.8, "peak": 16.26, "min": 11.44}, "VDD_CPU_SOC_MSS": {"avg": 16.0, "peak": 17.71, "min": 12.61}}, "power_watts_avg": 31.52, "energy_joules_est": 59.64, "sample_count": 19, "duration_seconds": 1.892}, "timestamp": "2026-01-19T14:14:20.229535"}
{"image_index": 13, "image_name": "000000001425.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001425.jpg", "image_width": 640, "image_height": 512, "image_resolution": "640x512", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2478.589, "latencies_ms": [2478.589], "images_per_second": 0.403, "prompt_tokens": 1442, "response_tokens_est": 38, "n_tiles": 1, "output_text": " The image is in black and white, with a focus on a slice of cake on a white plate. The lighting is soft and natural, and the cake appears to be moist and delicious.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26752.7, "ram_available_mb": 99019.4, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26753.0, "ram_available_mb": 99019.2, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.45, "peak": 40.18, "min": 19.32}, "VIN": {"avg": 72.96, "peak": 119.41, "min": 29.91}, "VIN_SYS_5V0": {"avg": 14.9, "peak": 16.26, "min": 11.77}, "VDD_CPU_SOC_MSS": {"avg": 16.13, "peak": 17.71, "min": 13.0}}, "power_watts_avg": 30.45, "energy_joules_est": 75.48, "sample_count": 24, "duration_seconds": 2.479}, "timestamp": "2026-01-19T14:14:22.728925"}
{"image_index": 14, "image_name": "000000001490.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001490.jpg", "image_width": 640, "image_height": 315, "image_resolution": "640x315", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1285.919, "latencies_ms": [1285.919], "images_per_second": 0.778, "prompt_tokens": 766, "response_tokens_est": 23, "n_tiles": 1, "output_text": " A person is standing on a paddleboard in the water, holding a paddle, and wearing a wetsuit.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26753.0, "ram_available_mb": 99019.2, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26753.0, "ram_available_mb": 99019.2, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5179.1, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.19, "peak": 36.63, "min": 18.92}, "VIN": {"avg": 74.64, "peak": 126.97, "min": 28.7}, "VIN_SYS_5V0": {"avg": 14.47, "peak": 15.24, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 15.66, "peak": 16.54, "min": 14.18}}, "power_watts_avg": 28.19, "energy_joules_est": 36.27, "sample_count": 13, "duration_seconds": 1.286}, "timestamp": "2026-01-19T14:14:24.095402"}
{"image_index": 14, "image_name": "000000001490.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001490.jpg", "image_width": 640, "image_height": 315, "image_resolution": "640x315", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2145.951, "latencies_ms": [2145.951], "images_per_second": 0.466, "prompt_tokens": 780, "response_tokens_est": 57, "n_tiles": 1, "output_text": " 1. person: 1\n2. paddle: 1\n3. surfboard: 1\n4. water: 1\n5. land: 1\n6. sky: 1\n7. city: 1\n8. buildings: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26753.0, "ram_available_mb": 99019.2, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 26753.5, "ram_available_mb": 99018.7, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5180.1, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 24.97, "peak": 36.24, "min": 17.74}, "VIN": {"avg": 68.72, "peak": 127.18, "min": 28.36}, "VIN_SYS_5V0": {"avg": 14.4, "peak": 14.94, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.64, "peak": 16.16, "min": 13.0}}, "power_watts_avg": 24.97, "energy_joules_est": 53.6, "sample_count": 21, "duration_seconds": 2.146}, "timestamp": "2026-01-19T14:14:26.281198"}
{"image_index": 14, "image_name": "000000001490.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001490.jpg", "image_width": 640, "image_height": 315, "image_resolution": "640x315", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1372.728, "latencies_ms": [1372.728], "images_per_second": 0.728, "prompt_tokens": 784, "response_tokens_est": 26, "n_tiles": 1, "output_text": " The person is in the foreground, paddling a paddleboard on the water, while the shoreline and buildings are in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26753.5, "ram_available_mb": 99018.7, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26753.2, "ram_available_mb": 99019.0, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5180.1, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.02, "peak": 35.45, "min": 16.56}, "VIN": {"avg": 66.37, "peak": 93.08, "min": 26.87}, "VIN_SYS_5V0": {"avg": 14.35, "peak": 15.04, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 15.61, "peak": 16.54, "min": 13.79}}, "power_watts_avg": 27.02, "energy_joules_est": 37.1, "sample_count": 14, "duration_seconds": 1.373}, "timestamp": "2026-01-19T14:14:27.743887"}
{"image_index": 14, "image_name": "000000001490.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001490.jpg", "image_width": 640, "image_height": 315, "image_resolution": "640x315", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1142.758, "latencies_ms": [1142.758], "images_per_second": 0.875, "prompt_tokens": 778, "response_tokens_est": 16, "n_tiles": 1, "output_text": " A man is standing on a paddleboard in the water, holding a paddle.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26753.2, "ram_available_mb": 99019.0, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26755.0, "ram_available_mb": 99017.1, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5180.1, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.81, "peak": 36.63, "min": 16.56}, "VIN": {"avg": 66.92, "peak": 102.69, "min": 29.29}, "VIN_SYS_5V0": {"avg": 14.12, "peak": 14.94, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.21, "peak": 16.14, "min": 13.0}}, "power_watts_avg": 28.81, "energy_joules_est": 32.94, "sample_count": 11, "duration_seconds": 1.143}, "timestamp": "2026-01-19T14:14:28.897040"}
{"image_index": 14, "image_name": "000000001490.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001490.jpg", "image_width": 640, "image_height": 315, "image_resolution": "640x315", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1599.512, "latencies_ms": [1599.512], "images_per_second": 0.625, "prompt_tokens": 776, "response_tokens_est": 36, "n_tiles": 1, "output_text": " The image is in black and white, with the water being a dark shade and the sky being a lighter shade. The person is wearing a wetsuit, which is black.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26755.0, "ram_available_mb": 99017.1, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26755.0, "ram_available_mb": 99017.1, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5180.1, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.76, "peak": 37.42, "min": 18.53}, "VIN": {"avg": 67.4, "peak": 98.82, "min": 27.12}, "VIN_SYS_5V0": {"avg": 14.71, "peak": 15.34, "min": 13.49}, "VDD_CPU_SOC_MSS": {"avg": 15.87, "peak": 16.93, "min": 14.57}}, "power_watts_avg": 27.76, "energy_joules_est": 44.41, "sample_count": 16, "duration_seconds": 1.6}, "timestamp": "2026-01-19T14:14:30.564377"}
{"image_index": 15, "image_name": "000000001503.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001503.jpg", "image_width": 320, "image_height": 240, "image_resolution": "320x240", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1694.402, "latencies_ms": [1694.402], "images_per_second": 0.59, "prompt_tokens": 1099, "response_tokens_est": 24, "n_tiles": 1, "output_text": " A white computer desk with a laptop, keyboard, mouse, and computer monitor, and a lamp on the right side.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26755.0, "ram_available_mb": 99017.1, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26755.5, "ram_available_mb": 99016.6, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.94, "peak": 40.18, "min": 15.77}, "VIN": {"avg": 72.18, "peak": 105.92, "min": 27.93}, "VIN_SYS_5V0": {"avg": 14.66, "peak": 15.85, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 15.98, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 29.94, "energy_joules_est": 50.74, "sample_count": 17, "duration_seconds": 1.695}, "timestamp": "2026-01-19T14:14:32.339974"}
{"image_index": 15, "image_name": "000000001503.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001503.jpg", "image_width": 320, "image_height": 240, "image_resolution": "320x240", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2004.536, "latencies_ms": [2004.536], "images_per_second": 0.499, "prompt_tokens": 1113, "response_tokens_est": 34, "n_tiles": 1, "output_text": " laptop: 1, monitor: 1, keyboard: 1, mouse: 1, speakers: 2, printer: 1, lamp: 1", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26755.5, "ram_available_mb": 99016.6, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26755.5, "ram_available_mb": 99016.6, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.08, "peak": 40.18, "min": 17.74}, "VIN": {"avg": 70.28, "peak": 125.0, "min": 28.13}, "VIN_SYS_5V0": {"avg": 14.61, "peak": 15.75, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.65, "peak": 16.93, "min": 13.0}}, "power_watts_avg": 29.08, "energy_joules_est": 58.31, "sample_count": 20, "duration_seconds": 2.005}, "timestamp": "2026-01-19T14:14:34.420008"}
{"image_index": 15, "image_name": "000000001503.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001503.jpg", "image_width": 320, "image_height": 240, "image_resolution": "320x240", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1883.113, "latencies_ms": [1883.113], "images_per_second": 0.531, "prompt_tokens": 1117, "response_tokens_est": 31, "n_tiles": 1, "output_text": " The laptop is to the left of the computer monitor, the keyboard is in front of the monitor, and the speakers are to the right of the monitor.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26755.5, "ram_available_mb": 99016.6, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26755.5, "ram_available_mb": 99016.6, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.18, "peak": 40.18, "min": 16.16}, "VIN": {"avg": 71.32, "peak": 114.5, "min": 26.93}, "VIN_SYS_5V0": {"avg": 14.62, "peak": 15.75, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.77, "peak": 16.93, "min": 13.0}}, "power_watts_avg": 29.18, "energy_joules_est": 54.96, "sample_count": 19, "duration_seconds": 1.883}, "timestamp": "2026-01-19T14:14:36.402997"}
{"image_index": 15, "image_name": "000000001503.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001503.jpg", "image_width": 320, "image_height": 240, "image_resolution": "320x240", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1455.435, "latencies_ms": [1455.435], "images_per_second": 0.687, "prompt_tokens": 1111, "response_tokens_est": 14, "n_tiles": 1, "output_text": " A white desk with a computer, laptop, and speakers on it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26755.5, "ram_available_mb": 99016.6, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26755.5, "ram_available_mb": 99016.7, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.52, "peak": 40.18, "min": 16.16}, "VIN": {"avg": 76.6, "peak": 120.11, "min": 31.84}, "VIN_SYS_5V0": {"avg": 14.57, "peak": 15.75, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 15.78, "peak": 16.93, "min": 12.61}}, "power_watts_avg": 31.52, "energy_joules_est": 45.89, "sample_count": 14, "duration_seconds": 1.456}, "timestamp": "2026-01-19T14:14:37.863127"}
{"image_index": 15, "image_name": "000000001503.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001503.jpg", "image_width": 320, "image_height": 240, "image_resolution": "320x240", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1640.359, "latencies_ms": [1640.359], "images_per_second": 0.61, "prompt_tokens": 1109, "response_tokens_est": 23, "n_tiles": 1, "output_text": " The room is well-lit with natural light coming through the window, and the desk is made of white wood.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26755.5, "ram_available_mb": 99016.7, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 26754.6, "ram_available_mb": 99017.6, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.62, "peak": 40.57, "min": 23.25}, "VIN": {"avg": 76.66, "peak": 120.52, "min": 28.58}, "VIN_SYS_5V0": {"avg": 15.18, "peak": 16.05, "min": 13.39}, "VDD_CPU_SOC_MSS": {"avg": 16.41, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 32.62, "energy_joules_est": 53.53, "sample_count": 16, "duration_seconds": 1.641}, "timestamp": "2026-01-19T14:14:39.532584"}
{"image_index": 16, "image_name": "000000001532.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001532.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1674.53, "latencies_ms": [1674.53], "images_per_second": 0.597, "prompt_tokens": 1099, "response_tokens_est": 24, "n_tiles": 1, "output_text": " A highway with multiple lanes, a bridge with green signs pointing to different directions, and a few cars on the road.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 26754.6, "ram_available_mb": 99017.6, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26754.8, "ram_available_mb": 99017.3, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.01, "peak": 40.18, "min": 19.71}, "VIN": {"avg": 77.27, "peak": 128.26, "min": 29.45}, "VIN_SYS_5V0": {"avg": 14.88, "peak": 15.85, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 16.21, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 31.01, "energy_joules_est": 51.95, "sample_count": 17, "duration_seconds": 1.675}, "timestamp": "2026-01-19T14:14:41.311629"}
{"image_index": 16, "image_name": "000000001532.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001532.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2256.021, "latencies_ms": [2256.021], "images_per_second": 0.443, "prompt_tokens": 1113, "response_tokens_est": 45, "n_tiles": 1, "output_text": " TAXI: 1\nSUV: 1\nCAR: 2\nVEHICLE: 1\nBUS: 1\nTRAFFIC: 1\nEXIT: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26754.8, "ram_available_mb": 99017.3, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26754.8, "ram_available_mb": 99017.3, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.26, "peak": 40.18, "min": 17.35}, "VIN": {"avg": 73.44, "peak": 125.07, "min": 28.74}, "VIN_SYS_5V0": {"avg": 14.63, "peak": 15.75, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.8, "peak": 16.93, "min": 13.0}}, "power_watts_avg": 28.26, "energy_joules_est": 63.77, "sample_count": 22, "duration_seconds": 2.256}, "timestamp": "2026-01-19T14:14:43.596342"}
{"image_index": 16, "image_name": "000000001532.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001532.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1968.539, "latencies_ms": [1968.539], "images_per_second": 0.508, "prompt_tokens": 1117, "response_tokens_est": 35, "n_tiles": 1, "output_text": " The sign is located in the foreground of the image, while the vehicles are in the background. The sign is also above the vehicles, indicating that it is a traffic sign.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26754.8, "ram_available_mb": 99017.3, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26754.8, "ram_available_mb": 99017.3, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.59, "peak": 40.18, "min": 16.95}, "VIN": {"avg": 78.88, "peak": 126.39, "min": 37.89}, "VIN_SYS_5V0": {"avg": 14.97, "peak": 15.95, "min": 12.66}, "VDD_CPU_SOC_MSS": {"avg": 16.14, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 29.59, "energy_joules_est": 58.26, "sample_count": 19, "duration_seconds": 1.969}, "timestamp": "2026-01-19T14:14:45.574483"}
{"image_index": 16, "image_name": "000000001532.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001532.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1520.288, "latencies_ms": [1520.288], "images_per_second": 0.658, "prompt_tokens": 1111, "response_tokens_est": 18, "n_tiles": 1, "output_text": " A busy highway with cars and trucks driving under a bridge with signs pointing to different locations.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26754.8, "ram_available_mb": 99017.3, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 26754.6, "ram_available_mb": 99017.6, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.86, "peak": 40.18, "min": 18.52}, "VIN": {"avg": 73.03, "peak": 121.03, "min": 29.47}, "VIN_SYS_5V0": {"avg": 15.09, "peak": 15.95, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 16.43, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 31.86, "energy_joules_est": 48.45, "sample_count": 15, "duration_seconds": 1.521}, "timestamp": "2026-01-19T14:14:47.143935"}
{"image_index": 16, "image_name": "000000001532.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001532.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2261.53, "latencies_ms": [2261.53], "images_per_second": 0.442, "prompt_tokens": 1109, "response_tokens_est": 46, "n_tiles": 1, "output_text": " The image depicts a busy highway with several vehicles, including a black SUV, under a green highway sign that indicates the direction to North Ventura and Hollywood Blvd. The sky is clear and blue, suggesting a sunny day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26754.6, "ram_available_mb": 99017.6, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26754.5, "ram_available_mb": 99017.7, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.87, "peak": 40.57, "min": 19.71}, "VIN": {"avg": 70.44, "peak": 105.4, "min": 28.83}, "VIN_SYS_5V0": {"avg": 14.88, "peak": 15.95, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 16.0, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 28.87, "energy_joules_est": 65.3, "sample_count": 22, "duration_seconds": 2.262}, "timestamp": "2026-01-19T14:14:49.429250"}
{"image_index": 17, "image_name": "000000001584.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001584.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2163.195, "latencies_ms": [2163.195], "images_per_second": 0.462, "prompt_tokens": 1432, "response_tokens_est": 27, "n_tiles": 1, "output_text": " A red double-decker bus is driving down a street with a sign on the front that says \"15 Aldwych\".", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26754.5, "ram_available_mb": 99017.7, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 26754.2, "ram_available_mb": 99017.9, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.2, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.14, "peak": 40.18, "min": 16.56}, "VIN": {"avg": 72.7, "peak": 117.06, "min": 29.33}, "VIN_SYS_5V0": {"avg": 15.37, "peak": 16.56, "min": 12.66}, "VDD_CPU_SOC_MSS": {"avg": 16.67, "peak": 18.11, "min": 14.18}}, "power_watts_avg": 31.14, "energy_joules_est": 67.38, "sample_count": 21, "duration_seconds": 2.164}, "timestamp": "2026-01-19T14:14:51.616368"}
{"image_index": 17, "image_name": "000000001584.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001584.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2467.754, "latencies_ms": [2467.754], "images_per_second": 0.405, "prompt_tokens": 1446, "response_tokens_est": 39, "n_tiles": 1, "output_text": " 1. red double decker bus\n2. people\n3. trees\n4. buildings\n5. street\n6. license plate\n7. bus number\n8. bus route", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26754.2, "ram_available_mb": 99017.9, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26754.2, "ram_available_mb": 99017.9, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.58, "peak": 40.57, "min": 18.92}, "VIN": {"avg": 73.62, "peak": 127.86, "min": 28.35}, "VIN_SYS_5V0": {"avg": 15.26, "peak": 16.56, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 16.42, "peak": 17.71, "min": 14.18}}, "power_watts_avg": 30.58, "energy_joules_est": 75.47, "sample_count": 24, "duration_seconds": 2.468}, "timestamp": "2026-01-19T14:14:54.119078"}
{"image_index": 17, "image_name": "000000001584.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001584.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2408.538, "latencies_ms": [2408.538], "images_per_second": 0.415, "prompt_tokens": 1450, "response_tokens_est": 36, "n_tiles": 1, "output_text": " The red double-decker bus is in the foreground, driving on the road. The building is in the background, and the trees are on the left side of the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26754.2, "ram_available_mb": 99017.9, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26754.7, "ram_available_mb": 99017.5, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.19, "peak": 39.78, "min": 17.35}, "VIN": {"avg": 70.96, "peak": 122.65, "min": 28.13}, "VIN_SYS_5V0": {"avg": 15.11, "peak": 16.56, "min": 12.56}, "VDD_CPU_SOC_MSS": {"avg": 16.27, "peak": 17.71, "min": 13.79}}, "power_watts_avg": 30.19, "energy_joules_est": 72.73, "sample_count": 24, "duration_seconds": 2.409}, "timestamp": "2026-01-19T14:14:56.620867"}
{"image_index": 17, "image_name": "000000001584.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001584.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2229.688, "latencies_ms": [2229.688], "images_per_second": 0.448, "prompt_tokens": 1444, "response_tokens_est": 29, "n_tiles": 1, "output_text": " A red double-decker bus is driving down a street in a city, passing by a park with trees and a building in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26754.7, "ram_available_mb": 99017.5, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26754.5, "ram_available_mb": 99017.7, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.64, "peak": 39.78, "min": 16.16}, "VIN": {"avg": 73.12, "peak": 105.7, "min": 29.35}, "VIN_SYS_5V0": {"avg": 14.94, "peak": 16.36, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 16.14, "peak": 17.72, "min": 13.0}}, "power_watts_avg": 30.64, "energy_joules_est": 68.33, "sample_count": 22, "duration_seconds": 2.23}, "timestamp": "2026-01-19T14:14:58.907096"}
{"image_index": 17, "image_name": "000000001584.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001584.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1864.659, "latencies_ms": [1864.659], "images_per_second": 0.536, "prompt_tokens": 1442, "response_tokens_est": 15, "n_tiles": 1, "output_text": " The bus is red with a yellow stripe, and the sky is cloudy.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26754.5, "ram_available_mb": 99017.7, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 26757.9, "ram_available_mb": 99014.3, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.68, "peak": 40.18, "min": 17.74}, "VIN": {"avg": 76.35, "peak": 126.48, "min": 29.05}, "VIN_SYS_5V0": {"avg": 15.17, "peak": 16.46, "min": 12.25}, "VDD_CPU_SOC_MSS": {"avg": 16.32, "peak": 17.71, "min": 13.39}}, "power_watts_avg": 32.68, "energy_joules_est": 60.96, "sample_count": 18, "duration_seconds": 1.865}, "timestamp": "2026-01-19T14:15:00.784275"}
{"image_index": 18, "image_name": "000000001675.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001675.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1422.462, "latencies_ms": [1422.462], "images_per_second": 0.703, "prompt_tokens": 1099, "response_tokens_est": 14, "n_tiles": 1, "output_text": " A black and white cat is laying on top of a laptop computer.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 26757.9, "ram_available_mb": 99014.3, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 26757.6, "ram_available_mb": 99014.6, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 33.41, "peak": 40.18, "min": 22.08}, "VIN": {"avg": 78.42, "peak": 125.04, "min": 29.23}, "VIN_SYS_5V0": {"avg": 15.14, "peak": 16.05, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 16.31, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 33.41, "energy_joules_est": 47.55, "sample_count": 14, "duration_seconds": 1.423}, "timestamp": "2026-01-19T14:15:02.251361"}
{"image_index": 18, "image_name": "000000001675.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001675.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2881.83, "latencies_ms": [2881.83], "images_per_second": 0.347, "prompt_tokens": 1113, "response_tokens_est": 67, "n_tiles": 1, "output_text": " 1. black and white cat: 1\n2. laptop: 1\n3. keyboard: 1\n4. white wall: 1\n5. white baseboard: 1\n6. white door frame: 1\n7. white door: 1\n8. white wall panel: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26757.6, "ram_available_mb": 99014.6, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 26757.1, "ram_available_mb": 99015.1, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.95, "peak": 40.57, "min": 18.52}, "VIN": {"avg": 64.84, "peak": 96.01, "min": 28.27}, "VIN_SYS_5V0": {"avg": 14.82, "peak": 15.95, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.79, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 26.95, "energy_joules_est": 77.68, "sample_count": 28, "duration_seconds": 2.882}, "timestamp": "2026-01-19T14:15:05.165136"}
{"image_index": 18, "image_name": "000000001675.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001675.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2177.778, "latencies_ms": [2177.778], "images_per_second": 0.459, "prompt_tokens": 1117, "response_tokens_est": 43, "n_tiles": 1, "output_text": " The black and white cat is positioned to the left of the laptop, which is situated in the middle of the image. The laptop is located in the foreground of the image, while the cat is in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26757.1, "ram_available_mb": 99015.1, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26757.4, "ram_available_mb": 99014.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.46, "peak": 40.18, "min": 15.77}, "VIN": {"avg": 69.54, "peak": 128.27, "min": 29.19}, "VIN_SYS_5V0": {"avg": 14.96, "peak": 16.05, "min": 12.56}, "VDD_CPU_SOC_MSS": {"avg": 16.07, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 28.46, "energy_joules_est": 61.99, "sample_count": 21, "duration_seconds": 2.178}, "timestamp": "2026-01-19T14:15:07.353441"}
{"image_index": 18, "image_name": "000000001675.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001675.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1417.872, "latencies_ms": [1417.872], "images_per_second": 0.705, "prompt_tokens": 1111, "response_tokens_est": 14, "n_tiles": 1, "output_text": " A black and white cat is laying on top of a laptop computer.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26757.4, "ram_available_mb": 99014.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 26757.4, "ram_available_mb": 99014.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.08, "peak": 39.78, "min": 18.13}, "VIN": {"avg": 71.31, "peak": 104.41, "min": 28.62}, "VIN_SYS_5V0": {"avg": 15.17, "peak": 16.15, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 16.31, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 32.08, "energy_joules_est": 45.5, "sample_count": 14, "duration_seconds": 1.418}, "timestamp": "2026-01-19T14:15:08.823453"}
{"image_index": 18, "image_name": "000000001675.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001675.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1421.715, "latencies_ms": [1421.715], "images_per_second": 0.703, "prompt_tokens": 1109, "response_tokens_est": 13, "n_tiles": 1, "output_text": " The cat is black and white, and the laptop is silver.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26757.4, "ram_available_mb": 99014.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 26757.1, "ram_available_mb": 99015.1, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.9, "peak": 40.57, "min": 20.89}, "VIN": {"avg": 71.45, "peak": 97.89, "min": 28.28}, "VIN_SYS_5V0": {"avg": 14.9, "peak": 15.95, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.95, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 32.9, "energy_joules_est": 46.78, "sample_count": 14, "duration_seconds": 1.422}, "timestamp": "2026-01-19T14:15:10.287566"}
{"image_index": 19, "image_name": "000000001761.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001761.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1593.5, "latencies_ms": [1593.5], "images_per_second": 0.628, "prompt_tokens": 1100, "response_tokens_est": 20, "n_tiles": 1, "output_text": " Two airplanes fly over a bridge and the Sydney Opera House, with a cloudy sky in the background.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 26757.1, "ram_available_mb": 99015.1, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26758.1, "ram_available_mb": 99014.1, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.86, "peak": 40.56, "min": 21.68}, "VIN": {"avg": 76.17, "peak": 120.06, "min": 28.7}, "VIN_SYS_5V0": {"avg": 14.91, "peak": 16.15, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 16.02, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 31.86, "energy_joules_est": 50.78, "sample_count": 16, "duration_seconds": 1.594}, "timestamp": "2026-01-19T14:15:11.959986"}
{"image_index": 19, "image_name": "000000001761.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001761.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2589.922, "latencies_ms": [2589.922], "images_per_second": 0.386, "prompt_tokens": 1114, "response_tokens_est": 58, "n_tiles": 1, "output_text": " 1. airplane: 2\n2. bridge: 1\n3. building: 1\n4. city skyline: 1\n5. water: 1\n6. clouds: 1\n7. sky: 1\n8. flags: 1", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26758.1, "ram_available_mb": 99014.1, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 26758.3, "ram_available_mb": 99013.9, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.35, "peak": 40.18, "min": 18.92}, "VIN": {"avg": 66.18, "peak": 117.48, "min": 33.76}, "VIN_SYS_5V0": {"avg": 14.78, "peak": 15.95, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 15.88, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 27.35, "energy_joules_est": 70.85, "sample_count": 25, "duration_seconds": 2.59}, "timestamp": "2026-01-19T14:15:14.556715"}
{"image_index": 19, "image_name": "000000001761.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001761.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2000.646, "latencies_ms": [2000.646], "images_per_second": 0.5, "prompt_tokens": 1118, "response_tokens_est": 37, "n_tiles": 1, "output_text": " The two airplanes are flying above the Sydney Harbour Bridge, which is positioned in the foreground of the image. The Sydney Opera House is situated in the background, far away from the camera.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26758.3, "ram_available_mb": 99013.9, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26759.5, "ram_available_mb": 99012.6, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.0, "peak": 39.39, "min": 17.35}, "VIN": {"avg": 75.6, "peak": 120.15, "min": 28.45}, "VIN_SYS_5V0": {"avg": 15.06, "peak": 16.05, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 16.34, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 29.0, "energy_joules_est": 58.03, "sample_count": 20, "duration_seconds": 2.001}, "timestamp": "2026-01-19T14:15:16.634093"}
{"image_index": 19, "image_name": "000000001761.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001761.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1626.965, "latencies_ms": [1626.965], "images_per_second": 0.615, "prompt_tokens": 1112, "response_tokens_est": 21, "n_tiles": 1, "output_text": " Two airplanes fly over a large bridge and the Sydney Opera House, with a cloudy sky in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26759.5, "ram_available_mb": 99012.6, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26759.3, "ram_available_mb": 99012.9, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.19, "peak": 39.39, "min": 16.56}, "VIN": {"avg": 73.71, "peak": 128.7, "min": 29.22}, "VIN_SYS_5V0": {"avg": 14.71, "peak": 15.95, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.9, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 30.19, "energy_joules_est": 49.13, "sample_count": 16, "duration_seconds": 1.627}, "timestamp": "2026-01-19T14:15:18.308174"}
{"image_index": 19, "image_name": "000000001761.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001761.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1598.987, "latencies_ms": [1598.987], "images_per_second": 0.625, "prompt_tokens": 1110, "response_tokens_est": 20, "n_tiles": 1, "output_text": " The sky is overcast with a grayish hue, and the bridge is a dark brown color.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26759.3, "ram_available_mb": 99012.9, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 26759.0, "ram_available_mb": 99013.1, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.08, "peak": 40.18, "min": 18.92}, "VIN": {"avg": 78.4, "peak": 131.37, "min": 30.46}, "VIN_SYS_5V0": {"avg": 14.83, "peak": 16.05, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.97, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 31.08, "energy_joules_est": 49.71, "sample_count": 16, "duration_seconds": 1.599}, "timestamp": "2026-01-19T14:15:19.988257"}
{"image_index": 20, "image_name": "000000001818.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001818.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1508.638, "latencies_ms": [1508.638], "images_per_second": 0.663, "prompt_tokens": 1099, "response_tokens_est": 16, "n_tiles": 1, "output_text": " A black and white photo of a zebra nursing its young in a grassy field.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 26759.0, "ram_available_mb": 99013.1, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 26759.9, "ram_available_mb": 99012.2, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.57, "peak": 40.57, "min": 18.14}, "VIN": {"avg": 76.66, "peak": 121.53, "min": 30.17}, "VIN_SYS_5V0": {"avg": 14.63, "peak": 15.95, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.88, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 31.57, "energy_joules_est": 47.65, "sample_count": 15, "duration_seconds": 1.509}, "timestamp": "2026-01-19T14:15:21.559795"}
{"image_index": 20, "image_name": "000000001818.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001818.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1469.06, "latencies_ms": [1469.06], "images_per_second": 0.681, "prompt_tokens": 1113, "response_tokens_est": 15, "n_tiles": 1, "output_text": " zebra: 1\nbaby zebra: 1\ngrass: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26759.9, "ram_available_mb": 99012.2, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 26759.7, "ram_available_mb": 99012.5, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.17, "peak": 40.57, "min": 20.1}, "VIN": {"avg": 69.87, "peak": 88.35, "min": 28.54}, "VIN_SYS_5V0": {"avg": 14.76, "peak": 15.95, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 16.04, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 32.17, "energy_joules_est": 47.27, "sample_count": 15, "duration_seconds": 1.469}, "timestamp": "2026-01-19T14:15:23.130766"}
{"image_index": 20, "image_name": "000000001818.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001818.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2811.736, "latencies_ms": [2811.736], "images_per_second": 0.356, "prompt_tokens": 1117, "response_tokens_est": 66, "n_tiles": 1, "output_text": " The zebra is positioned on the left side of the image, with its body facing towards the right side. The baby zebra is positioned on the right side of the image, with its body facing towards the left side. The baby zebra is positioned very close to the mother zebra, with its head near the mother's udder.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26759.7, "ram_available_mb": 99012.5, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 26759.7, "ram_available_mb": 99012.5, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.67, "peak": 40.57, "min": 16.95}, "VIN": {"avg": 70.06, "peak": 126.01, "min": 28.08}, "VIN_SYS_5V0": {"avg": 14.72, "peak": 15.95, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.78, "peak": 16.93, "min": 13.0}}, "power_watts_avg": 26.67, "energy_joules_est": 75.0, "sample_count": 28, "duration_seconds": 2.812}, "timestamp": "2026-01-19T14:15:26.042687"}
{"image_index": 20, "image_name": "000000001818.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001818.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1389.023, "latencies_ms": [1389.023], "images_per_second": 0.72, "prompt_tokens": 1111, "response_tokens_est": 11, "n_tiles": 1, "output_text": " A zebra is nursing its young in a grassy field.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26759.7, "ram_available_mb": 99012.5, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 26760.2, "ram_available_mb": 99012.0, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.73, "peak": 39.78, "min": 14.59}, "VIN": {"avg": 76.41, "peak": 118.82, "min": 28.57}, "VIN_SYS_5V0": {"avg": 14.49, "peak": 15.85, "min": 11.54}, "VDD_CPU_SOC_MSS": {"avg": 15.61, "peak": 16.93, "min": 12.61}}, "power_watts_avg": 30.73, "energy_joules_est": 42.69, "sample_count": 14, "duration_seconds": 1.389}, "timestamp": "2026-01-19T14:15:27.505665"}
{"image_index": 20, "image_name": "000000001818.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001818.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2217.366, "latencies_ms": [2217.366], "images_per_second": 0.451, "prompt_tokens": 1109, "response_tokens_est": 42, "n_tiles": 1, "output_text": " The image is in black and white, with the zebra's stripes standing out against the white background. The lighting is natural, coming from the side, casting shadows and highlighting the texture of the zebra's fur.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26760.2, "ram_available_mb": 99012.0, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26760.2, "ram_available_mb": 99012.0, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.96, "peak": 40.97, "min": 18.14}, "VIN": {"avg": 70.7, "peak": 121.96, "min": 28.84}, "VIN_SYS_5V0": {"avg": 14.69, "peak": 15.85, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.77, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 28.96, "energy_joules_est": 64.22, "sample_count": 22, "duration_seconds": 2.218}, "timestamp": "2026-01-19T14:15:29.796127"}
{"image_index": 21, "image_name": "000000001993.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001993.jpg", "image_width": 640, "image_height": 419, "image_resolution": "640x419", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1942.35, "latencies_ms": [1942.35], "images_per_second": 0.515, "prompt_tokens": 1099, "response_tokens_est": 33, "n_tiles": 1, "output_text": " The image shows a room with a bed, a small round table, and a chair, all placed in front of a window with a view of a building outside.", "error": null, "sys_before": {"cpu_percent": 14.3, "ram_used_mb": 26760.2, "ram_available_mb": 99012.0, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26760.2, "ram_available_mb": 99011.9, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.03, "peak": 40.18, "min": 15.77}, "VIN": {"avg": 72.88, "peak": 115.21, "min": 29.93}, "VIN_SYS_5V0": {"avg": 14.71, "peak": 15.95, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.79, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 29.03, "energy_joules_est": 56.41, "sample_count": 19, "duration_seconds": 1.943}, "timestamp": "2026-01-19T14:15:31.776112"}
{"image_index": 21, "image_name": "000000001993.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001993.jpg", "image_width": 640, "image_height": 419, "image_resolution": "640x419", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2524.623, "latencies_ms": [2524.623], "images_per_second": 0.396, "prompt_tokens": 1113, "response_tokens_est": 56, "n_tiles": 1, "output_text": " 1. bed: 1\n2. chair: 2\n3. table: 1\n4. floor: 1\n5. wall: 1\n6. window: 1\n7. door: 1\n8. lamp: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26760.2, "ram_available_mb": 99011.9, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26760.0, "ram_available_mb": 99012.2, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.31, "peak": 40.18, "min": 17.74}, "VIN": {"avg": 68.3, "peak": 105.8, "min": 28.83}, "VIN_SYS_5V0": {"avg": 14.9, "peak": 16.05, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 16.02, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 27.31, "energy_joules_est": 68.95, "sample_count": 25, "duration_seconds": 2.525}, "timestamp": "2026-01-19T14:15:34.372135"}
{"image_index": 21, "image_name": "000000001993.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001993.jpg", "image_width": 640, "image_height": 419, "image_resolution": "640x419", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2246.818, "latencies_ms": [2246.818], "images_per_second": 0.445, "prompt_tokens": 1117, "response_tokens_est": 45, "n_tiles": 1, "output_text": " The bed is positioned in the center of the room, with the window and door to the left and right, respectively. The table and chairs are located in the foreground, while the purple wall and window are in the background.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26760.0, "ram_available_mb": 99012.2, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26761.7, "ram_available_mb": 99010.5, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.94, "peak": 40.18, "min": 15.77}, "VIN": {"avg": 67.05, "peak": 121.75, "min": 29.68}, "VIN_SYS_5V0": {"avg": 14.75, "peak": 15.95, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 15.82, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 27.94, "energy_joules_est": 62.78, "sample_count": 22, "duration_seconds": 2.247}, "timestamp": "2026-01-19T14:15:36.660723"}
{"image_index": 21, "image_name": "000000001993.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001993.jpg", "image_width": 640, "image_height": 419, "image_resolution": "640x419", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1446.872, "latencies_ms": [1446.872], "images_per_second": 0.691, "prompt_tokens": 1111, "response_tokens_est": 14, "n_tiles": 1, "output_text": " A small room with a bed, a table, and a chair.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26761.7, "ram_available_mb": 99010.5, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 26761.7, "ram_available_mb": 99010.5, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.66, "peak": 40.18, "min": 16.56}, "VIN": {"avg": 70.09, "peak": 94.73, "min": 29.19}, "VIN_SYS_5V0": {"avg": 14.92, "peak": 15.95, "min": 12.46}, "VDD_CPU_SOC_MSS": {"avg": 16.14, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 31.66, "energy_joules_est": 45.82, "sample_count": 14, "duration_seconds": 1.447}, "timestamp": "2026-01-19T14:15:38.119834"}
{"image_index": 21, "image_name": "000000001993.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001993.jpg", "image_width": 640, "image_height": 419, "image_resolution": "640x419", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1444.886, "latencies_ms": [1444.886], "images_per_second": 0.692, "prompt_tokens": 1109, "response_tokens_est": 14, "n_tiles": 1, "output_text": " The room is painted purple, and the bed has a colorful quilt.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26761.7, "ram_available_mb": 99010.5, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 26761.7, "ram_available_mb": 99010.5, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 33.57, "peak": 40.97, "min": 22.07}, "VIN": {"avg": 78.37, "peak": 122.02, "min": 30.26}, "VIN_SYS_5V0": {"avg": 15.16, "peak": 16.05, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 16.39, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 33.57, "energy_joules_est": 48.52, "sample_count": 14, "duration_seconds": 1.445}, "timestamp": "2026-01-19T14:15:39.586794"}
{"image_index": 22, "image_name": "000000002006.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002006.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1493.228, "latencies_ms": [1493.228], "images_per_second": 0.67, "prompt_tokens": 1099, "response_tokens_est": 17, "n_tiles": 1, "output_text": " A purple bus with the number 96 on it is driving down the street.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 26761.7, "ram_available_mb": 99010.5, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 26763.6, "ram_available_mb": 99008.6, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.94, "peak": 40.57, "min": 22.07}, "VIN": {"avg": 76.26, "peak": 118.95, "min": 29.19}, "VIN_SYS_5V0": {"avg": 15.09, "peak": 16.05, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 16.27, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 32.94, "energy_joules_est": 49.2, "sample_count": 15, "duration_seconds": 1.494}, "timestamp": "2026-01-19T14:15:41.155668"}
{"image_index": 22, "image_name": "000000002006.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002006.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2524.557, "latencies_ms": [2524.557], "images_per_second": 0.396, "prompt_tokens": 1113, "response_tokens_est": 56, "n_tiles": 1, "output_text": " 1. bus: 1\n2. person: 1\n3. pole: 1\n4. sign: 1\n5. tree: 1\n6. building: 1\n7. street: 1\n8. sidewalk: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26763.6, "ram_available_mb": 99008.6, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26763.3, "ram_available_mb": 99008.9, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.75, "peak": 40.57, "min": 18.14}, "VIN": {"avg": 68.37, "peak": 91.13, "min": 28.58}, "VIN_SYS_5V0": {"avg": 14.82, "peak": 16.05, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.99, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 27.75, "energy_joules_est": 70.07, "sample_count": 25, "duration_seconds": 2.525}, "timestamp": "2026-01-19T14:15:43.751782"}
{"image_index": 22, "image_name": "000000002006.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002006.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2120.57, "latencies_ms": [2120.57], "images_per_second": 0.472, "prompt_tokens": 1117, "response_tokens_est": 40, "n_tiles": 1, "output_text": " The bus is positioned on the left side of the image, with the driver's side facing the camera. The sidewalk is located in the foreground, while the background features a street with buildings and trees.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26763.3, "ram_available_mb": 99008.9, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26763.3, "ram_available_mb": 99008.9, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.24, "peak": 40.18, "min": 15.38}, "VIN": {"avg": 76.19, "peak": 125.71, "min": 28.53}, "VIN_SYS_5V0": {"avg": 14.79, "peak": 15.95, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.77, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 28.24, "energy_joules_est": 59.89, "sample_count": 21, "duration_seconds": 2.121}, "timestamp": "2026-01-19T14:15:45.943508"}
{"image_index": 22, "image_name": "000000002006.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002006.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1528.155, "latencies_ms": [1528.155], "images_per_second": 0.654, "prompt_tokens": 1111, "response_tokens_est": 17, "n_tiles": 1, "output_text": " A purple bus with the number 96 on it is driving down a street.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26763.3, "ram_available_mb": 99008.9, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26764.0, "ram_available_mb": 99008.2, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.05, "peak": 40.18, "min": 16.17}, "VIN": {"avg": 71.55, "peak": 105.33, "min": 28.96}, "VIN_SYS_5V0": {"avg": 14.78, "peak": 15.95, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 15.96, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 31.05, "energy_joules_est": 47.46, "sample_count": 15, "duration_seconds": 1.529}, "timestamp": "2026-01-19T14:15:47.512606"}
{"image_index": 22, "image_name": "000000002006.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002006.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1819.338, "latencies_ms": [1819.338], "images_per_second": 0.55, "prompt_tokens": 1109, "response_tokens_est": 29, "n_tiles": 1, "output_text": " The bus is purple with a white roof and has a white license plate. The sky is clear and blue, and the sun is shining brightly.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26764.0, "ram_available_mb": 99008.2, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26764.0, "ram_available_mb": 99008.2, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.77, "peak": 40.97, "min": 20.11}, "VIN": {"avg": 72.54, "peak": 118.94, "min": 27.25}, "VIN_SYS_5V0": {"avg": 14.94, "peak": 15.95, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 16.14, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 30.77, "energy_joules_est": 55.99, "sample_count": 18, "duration_seconds": 1.82}, "timestamp": "2026-01-19T14:15:49.389414"}
{"image_index": 23, "image_name": "000000002149.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002149.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1665.461, "latencies_ms": [1665.461], "images_per_second": 0.6, "prompt_tokens": 1099, "response_tokens_est": 22, "n_tiles": 1, "output_text": " The image shows a white bowl filled with green apples, with a few apples placed on top of the bowl.", "error": null, "sys_before": {"cpu_percent": 16.7, "ram_used_mb": 26764.0, "ram_available_mb": 99008.2, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26770.8, "ram_available_mb": 99001.3, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.78, "peak": 40.18, "min": 17.35}, "VIN": {"avg": 74.22, "peak": 119.78, "min": 33.34}, "VIN_SYS_5V0": {"avg": 14.81, "peak": 15.95, "min": 12.25}, "VDD_CPU_SOC_MSS": {"avg": 15.97, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 30.78, "energy_joules_est": 51.29, "sample_count": 16, "duration_seconds": 1.666}, "timestamp": "2026-01-19T14:15:51.062785"}
{"image_index": 23, "image_name": "000000002149.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002149.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1188.772, "latencies_ms": [1188.772], "images_per_second": 0.841, "prompt_tokens": 1113, "response_tokens_est": 5, "n_tiles": 1, "output_text": " apple: 10", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26770.8, "ram_available_mb": 99001.3, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.2, "ram_used_mb": 26771.0, "ram_available_mb": 99001.1, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 33.78, "peak": 40.18, "min": 20.5}, "VIN": {"avg": 78.79, "peak": 127.54, "min": 28.47}, "VIN_SYS_5V0": {"avg": 15.18, "peak": 16.15, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 16.47, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 33.78, "energy_joules_est": 40.18, "sample_count": 12, "duration_seconds": 1.189}, "timestamp": "2026-01-19T14:15:52.324939"}
{"image_index": 23, "image_name": "000000002149.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002149.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2122.124, "latencies_ms": [2122.124], "images_per_second": 0.471, "prompt_tokens": 1117, "response_tokens_est": 40, "n_tiles": 1, "output_text": " The apples are in the foreground, with the bowl placed in the middle of the image. The apples are positioned in a way that they are close to each other, with some apples overlapping each other.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.0, "ram_available_mb": 99001.1, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26771.0, "ram_available_mb": 99001.1, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.24, "peak": 40.97, "min": 19.32}, "VIN": {"avg": 72.89, "peak": 124.7, "min": 28.22}, "VIN_SYS_5V0": {"avg": 14.85, "peak": 15.95, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.95, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 30.24, "energy_joules_est": 64.19, "sample_count": 21, "duration_seconds": 2.123}, "timestamp": "2026-01-19T14:15:54.515289"}
{"image_index": 23, "image_name": "000000002149.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002149.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2856.148, "latencies_ms": [2856.148], "images_per_second": 0.35, "prompt_tokens": 1111, "response_tokens_est": 65, "n_tiles": 1, "output_text": " The image captures a close-up view of a white bowl filled with green apples, which are arranged in a way that showcases their glossy skin and the intricate details of their texture. The background is blurred, drawing focus to the apples and the bowl, creating a sense of depth and emphasizing the freshness and quality of the fruit.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26771.0, "ram_available_mb": 99001.1, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 26771.8, "ram_available_mb": 99000.4, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.05, "peak": 40.18, "min": 16.56}, "VIN": {"avg": 68.36, "peak": 126.0, "min": 29.51}, "VIN_SYS_5V0": {"avg": 14.69, "peak": 15.85, "min": 12.15}, "VDD_CPU_SOC_MSS": {"avg": 15.74, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 26.05, "energy_joules_est": 74.41, "sample_count": 28, "duration_seconds": 2.857}, "timestamp": "2026-01-19T14:15:57.436669"}
{"image_index": 23, "image_name": "000000002149.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002149.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1422.172, "latencies_ms": [1422.172], "images_per_second": 0.703, "prompt_tokens": 1109, "response_tokens_est": 13, "n_tiles": 1, "output_text": " The apples are green and shiny, and the bowl is white.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.8, "ram_available_mb": 99000.4, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26771.8, "ram_available_mb": 99000.4, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.16, "peak": 40.18, "min": 15.38}, "VIN": {"avg": 73.92, "peak": 121.32, "min": 28.45}, "VIN_SYS_5V0": {"avg": 14.69, "peak": 15.85, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 15.83, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 31.16, "energy_joules_est": 44.33, "sample_count": 14, "duration_seconds": 1.423}, "timestamp": "2026-01-19T14:15:58.904233"}
{"image_index": 24, "image_name": "000000002153.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002153.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1954.188, "latencies_ms": [1954.188], "images_per_second": 0.512, "prompt_tokens": 1099, "response_tokens_est": 33, "n_tiles": 1, "output_text": " The image captures a dynamic moment in a baseball game, with the batter, catcher, and umpire all in position, ready to react to the pitch.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 26771.8, "ram_available_mb": 99000.4, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 26775.0, "ram_available_mb": 98997.2, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.36, "peak": 40.18, "min": 20.5}, "VIN": {"avg": 67.64, "peak": 117.04, "min": 31.9}, "VIN_SYS_5V0": {"avg": 14.96, "peak": 16.05, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 16.0, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 30.36, "energy_joules_est": 59.35, "sample_count": 19, "duration_seconds": 1.955}, "timestamp": "2026-01-19T14:16:00.898223"}
{"image_index": 24, "image_name": "000000002153.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002153.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2750.385, "latencies_ms": [2750.385], "images_per_second": 0.364, "prompt_tokens": 1113, "response_tokens_est": 60, "n_tiles": 1, "output_text": " 1. batter: 1\n2. catcher: 1\n3. umpire: 1\n4. pitcher: 1\n5. home plate: 1\n6. baseball: 1\n7. grass: 1\n8. dirt: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26775.0, "ram_available_mb": 98997.2, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 26774.7, "ram_available_mb": 98997.5, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.4, "peak": 40.57, "min": 16.95}, "VIN": {"avg": 69.56, "peak": 111.22, "min": 30.06}, "VIN_SYS_5V0": {"avg": 14.8, "peak": 16.05, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 15.97, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 26.4, "energy_joules_est": 72.62, "sample_count": 27, "duration_seconds": 2.751}, "timestamp": "2026-01-19T14:16:03.712795"}
{"image_index": 24, "image_name": "000000002153.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002153.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2101.999, "latencies_ms": [2101.999], "images_per_second": 0.476, "prompt_tokens": 1117, "response_tokens_est": 38, "n_tiles": 1, "output_text": " The batter is positioned in the foreground, with the catcher and umpire in the background. The pitcher is standing on the mound, which is located in the middle of the field.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26774.7, "ram_available_mb": 98997.5, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26775.7, "ram_available_mb": 98996.5, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.97, "peak": 40.18, "min": 14.59}, "VIN": {"avg": 72.52, "peak": 124.88, "min": 27.88}, "VIN_SYS_5V0": {"avg": 14.68, "peak": 15.95, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.73, "peak": 16.93, "min": 13.0}}, "power_watts_avg": 27.97, "energy_joules_est": 58.8, "sample_count": 21, "duration_seconds": 2.102}, "timestamp": "2026-01-19T14:16:05.912017"}
{"image_index": 24, "image_name": "000000002153.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002153.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3131.362, "latencies_ms": [3131.362], "images_per_second": 0.319, "prompt_tokens": 1111, "response_tokens_est": 79, "n_tiles": 1, "output_text": " The image captures a moment in a baseball game, with the batter, wearing a red helmet and white uniform, in the midst of swinging his bat at a pitched ball. The umpire, dressed in a blue shirt and black pants, stands behind the catcher, who is crouched in anticipation. The field is a lush green, contrasting with the brown dirt of the infield.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26775.7, "ram_available_mb": 98996.5, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 26775.3, "ram_available_mb": 98996.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 25.38, "peak": 40.18, "min": 15.38}, "VIN": {"avg": 70.36, "peak": 126.54, "min": 28.53}, "VIN_SYS_5V0": {"avg": 14.76, "peak": 15.95, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 15.73, "peak": 17.32, "min": 12.61}}, "power_watts_avg": 25.38, "energy_joules_est": 79.48, "sample_count": 31, "duration_seconds": 3.132}, "timestamp": "2026-01-19T14:16:09.147179"}
{"image_index": 24, "image_name": "000000002153.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002153.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2439.748, "latencies_ms": [2439.748], "images_per_second": 0.41, "prompt_tokens": 1109, "response_tokens_est": 53, "n_tiles": 1, "output_text": " The image captures a vibrant scene of a baseball game, with the lush green of the field contrasting against the brown dirt of the infield. The bright sunlight casts a warm glow on the scene, highlighting the players' uniforms and the crisp lines of the baseball diamond.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26775.3, "ram_available_mb": 98996.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 26776.0, "ram_available_mb": 98996.2, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.12, "peak": 40.18, "min": 14.59}, "VIN": {"avg": 68.32, "peak": 106.24, "min": 29.87}, "VIN_SYS_5V0": {"avg": 14.79, "peak": 15.95, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 15.88, "peak": 17.32, "min": 12.61}}, "power_watts_avg": 27.12, "energy_joules_est": 66.17, "sample_count": 24, "duration_seconds": 2.44}, "timestamp": "2026-01-19T14:16:11.648666"}
{"image_index": 25, "image_name": "000000002157.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002157.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1862.938, "latencies_ms": [1862.938], "images_per_second": 0.537, "prompt_tokens": 1099, "response_tokens_est": 30, "n_tiles": 1, "output_text": " A white cake with red and blue berries is on a table with a red tablecloth, surrounded by wine glasses, plates of cheese, and bread.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26776.0, "ram_available_mb": 98996.2, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26775.7, "ram_available_mb": 98996.4, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.62, "peak": 40.18, "min": 15.77}, "VIN": {"avg": 71.35, "peak": 130.26, "min": 33.72}, "VIN_SYS_5V0": {"avg": 14.84, "peak": 15.95, "min": 12.25}, "VDD_CPU_SOC_MSS": {"avg": 15.97, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 29.62, "energy_joules_est": 55.2, "sample_count": 18, "duration_seconds": 1.864}, "timestamp": "2026-01-19T14:16:13.525253"}
{"image_index": 25, "image_name": "000000002157.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002157.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2196.998, "latencies_ms": [2196.998], "images_per_second": 0.455, "prompt_tokens": 1113, "response_tokens_est": 44, "n_tiles": 1, "output_text": " cake: 1, glasses: 10, plates: 10, knives: 2, grapes: 10, cheese: 10, bread: 1, tablecloth: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26775.7, "ram_available_mb": 98996.4, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26777.5, "ram_available_mb": 98994.7, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.62, "peak": 40.18, "min": 18.14}, "VIN": {"avg": 70.75, "peak": 119.77, "min": 27.77}, "VIN_SYS_5V0": {"avg": 15.03, "peak": 16.05, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 16.16, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 28.62, "energy_joules_est": 62.89, "sample_count": 22, "duration_seconds": 2.197}, "timestamp": "2026-01-19T14:16:15.818276"}
{"image_index": 25, "image_name": "000000002157.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002157.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2965.778, "latencies_ms": [2965.778], "images_per_second": 0.337, "prompt_tokens": 1117, "response_tokens_est": 72, "n_tiles": 1, "output_text": " The cake is located in the foreground, to the left of the plates of cheese and grapes. The glasses are arranged in a line behind the cake, with some of them placed on the table and others on the plates. The plates of cheese and grapes are positioned to the right of the cake, with the cheese on the left and the grapes on the right.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26777.5, "ram_available_mb": 98994.7, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 26776.8, "ram_available_mb": 98995.4, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 25.83, "peak": 39.78, "min": 15.38}, "VIN": {"avg": 67.86, "peak": 107.88, "min": 28.58}, "VIN_SYS_5V0": {"avg": 14.79, "peak": 15.95, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.75, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 25.83, "energy_joules_est": 76.62, "sample_count": 29, "duration_seconds": 2.966}, "timestamp": "2026-01-19T14:16:18.836997"}
{"image_index": 25, "image_name": "000000002157.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002157.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1518.131, "latencies_ms": [1518.131], "images_per_second": 0.659, "prompt_tokens": 1111, "response_tokens_est": 17, "n_tiles": 1, "output_text": " A table is set with a cake, plates of food, and glasses of water.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26776.8, "ram_available_mb": 98995.4, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 26776.8, "ram_available_mb": 98995.4, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.99, "peak": 40.18, "min": 15.77}, "VIN": {"avg": 70.55, "peak": 88.57, "min": 28.58}, "VIN_SYS_5V0": {"avg": 14.92, "peak": 16.05, "min": 12.36}, "VDD_CPU_SOC_MSS": {"avg": 15.99, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 30.99, "energy_joules_est": 47.06, "sample_count": 15, "duration_seconds": 1.518}, "timestamp": "2026-01-19T14:16:20.400843"}
{"image_index": 25, "image_name": "000000002157.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002157.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1938.332, "latencies_ms": [1938.332], "images_per_second": 0.516, "prompt_tokens": 1109, "response_tokens_est": 33, "n_tiles": 1, "output_text": " The image features a red tablecloth, with a white cake and a plate of food on it. The lighting is natural, and the weather appears to be sunny.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26776.8, "ram_available_mb": 98995.4, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26777.2, "ram_available_mb": 98994.9, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.27, "peak": 40.57, "min": 20.49}, "VIN": {"avg": 75.05, "peak": 128.64, "min": 30.3}, "VIN_SYS_5V0": {"avg": 14.91, "peak": 15.95, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 16.04, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 30.27, "energy_joules_est": 58.69, "sample_count": 19, "duration_seconds": 1.939}, "timestamp": "2026-01-19T14:16:22.391238"}
{"image_index": 26, "image_name": "000000002261.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002261.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1468.246, "latencies_ms": [1468.246], "images_per_second": 0.681, "prompt_tokens": 1099, "response_tokens_est": 15, "n_tiles": 1, "output_text": " A person is riding a wave on a blue surfboard in the ocean.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 26777.2, "ram_available_mb": 98994.9, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 26777.2, "ram_available_mb": 98994.9, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.28, "peak": 40.57, "min": 17.35}, "VIN": {"avg": 80.16, "peak": 124.16, "min": 29.15}, "VIN_SYS_5V0": {"avg": 14.77, "peak": 15.95, "min": 12.36}, "VDD_CPU_SOC_MSS": {"avg": 15.91, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 31.28, "energy_joules_est": 45.95, "sample_count": 15, "duration_seconds": 1.469}, "timestamp": "2026-01-19T14:16:23.963268"}
{"image_index": 26, "image_name": "000000002261.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002261.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2564.94, "latencies_ms": [2564.94], "images_per_second": 0.39, "prompt_tokens": 1113, "response_tokens_est": 57, "n_tiles": 1, "output_text": " 1. person: 1\n2. surfboard: 1\n3. wave: 1\n4. water: 1\n5. sky: 0\n6. sand: 0\n7. rocks: 0\n8. sky: 0", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26777.2, "ram_available_mb": 98994.9, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 26777.5, "ram_available_mb": 98994.7, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.67, "peak": 40.57, "min": 18.91}, "VIN": {"avg": 66.38, "peak": 106.29, "min": 28.68}, "VIN_SYS_5V0": {"avg": 14.82, "peak": 15.95, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.8, "peak": 16.93, "min": 13.0}}, "power_watts_avg": 27.67, "energy_joules_est": 70.99, "sample_count": 25, "duration_seconds": 2.566}, "timestamp": "2026-01-19T14:16:26.563945"}
{"image_index": 26, "image_name": "000000002261.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002261.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2111.985, "latencies_ms": [2111.985], "images_per_second": 0.473, "prompt_tokens": 1117, "response_tokens_est": 40, "n_tiles": 1, "output_text": " The surfer is positioned in the foreground of the image, with the wave in the background. The surfer is on the left side of the wave, and the wave is on the right side.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26777.5, "ram_available_mb": 98994.7, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26777.5, "ram_available_mb": 98994.7, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.46, "peak": 40.18, "min": 16.16}, "VIN": {"avg": 71.93, "peak": 125.36, "min": 28.84}, "VIN_SYS_5V0": {"avg": 14.95, "peak": 15.95, "min": 12.66}, "VDD_CPU_SOC_MSS": {"avg": 15.99, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 28.46, "energy_joules_est": 60.12, "sample_count": 21, "duration_seconds": 2.112}, "timestamp": "2026-01-19T14:16:28.759096"}
{"image_index": 26, "image_name": "000000002261.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002261.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1462.97, "latencies_ms": [1462.97], "images_per_second": 0.684, "prompt_tokens": 1111, "response_tokens_est": 14, "n_tiles": 1, "output_text": " A man is riding a wave on a surfboard in the ocean.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26777.5, "ram_available_mb": 98994.7, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 26777.5, "ram_available_mb": 98994.7, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.68, "peak": 39.78, "min": 16.16}, "VIN": {"avg": 72.08, "peak": 118.38, "min": 27.32}, "VIN_SYS_5V0": {"avg": 14.62, "peak": 15.95, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.72, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 30.68, "energy_joules_est": 44.9, "sample_count": 15, "duration_seconds": 1.463}, "timestamp": "2026-01-19T14:16:30.325636"}
{"image_index": 26, "image_name": "000000002261.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002261.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2438.588, "latencies_ms": [2438.588], "images_per_second": 0.41, "prompt_tokens": 1109, "response_tokens_est": 52, "n_tiles": 1, "output_text": " The image captures a dynamic scene of a surfer riding a wave, with the surfer's black wetsuit contrasting against the vibrant green of the ocean. The lighting is natural and bright, suggesting a sunny day, and the water appears calm and inviting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26777.5, "ram_available_mb": 98994.7, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 26778.9, "ram_available_mb": 98993.3, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.86, "peak": 40.18, "min": 18.52}, "VIN": {"avg": 66.05, "peak": 124.77, "min": 29.42}, "VIN_SYS_5V0": {"avg": 14.69, "peak": 15.95, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.87, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 27.86, "energy_joules_est": 67.95, "sample_count": 24, "duration_seconds": 2.439}, "timestamp": "2026-01-19T14:16:32.825815"}
{"image_index": 27, "image_name": "000000002299.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002299.jpg", "image_width": 500, "image_height": 302, "image_resolution": "500x302", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1576.51, "latencies_ms": [1576.51], "images_per_second": 0.634, "prompt_tokens": 1099, "response_tokens_est": 19, "n_tiles": 1, "output_text": " A group of children are posing for a black and white photo in front of a brick building.", "error": null, "sys_before": {"cpu_percent": 14.3, "ram_used_mb": 26778.9, "ram_available_mb": 98993.3, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 26778.9, "ram_available_mb": 98993.3, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.26, "peak": 40.18, "min": 15.77}, "VIN": {"avg": 79.72, "peak": 126.96, "min": 29.24}, "VIN_SYS_5V0": {"avg": 14.79, "peak": 15.95, "min": 12.25}, "VDD_CPU_SOC_MSS": {"avg": 15.8, "peak": 17.32, "min": 13.4}}, "power_watts_avg": 30.26, "energy_joules_est": 47.73, "sample_count": 16, "duration_seconds": 1.577}, "timestamp": "2026-01-19T14:16:34.503702"}
{"image_index": 27, "image_name": "000000002299.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002299.jpg", "image_width": 500, "image_height": 302, "image_resolution": "500x302", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2361.121, "latencies_ms": [2361.121], "images_per_second": 0.424, "prompt_tokens": 1113, "response_tokens_est": 47, "n_tiles": 1, "output_text": " 1. group of children\n2. brick wall\n3. children's clothing\n4. children's shoes\n5. children's hair\n6. children's faces\n7. children's eyes\n8. children's noses", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26778.9, "ram_available_mb": 98993.3, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 26779.1, "ram_available_mb": 98993.0, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.01, "peak": 40.18, "min": 18.14}, "VIN": {"avg": 66.75, "peak": 109.62, "min": 29.8}, "VIN_SYS_5V0": {"avg": 14.67, "peak": 15.85, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.7, "peak": 16.92, "min": 13.0}}, "power_watts_avg": 28.01, "energy_joules_est": 66.15, "sample_count": 23, "duration_seconds": 2.362}, "timestamp": "2026-01-19T14:16:36.893799"}
{"image_index": 27, "image_name": "000000002299.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002299.jpg", "image_width": 500, "image_height": 302, "image_resolution": "500x302", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2244.872, "latencies_ms": [2244.872], "images_per_second": 0.445, "prompt_tokens": 1117, "response_tokens_est": 45, "n_tiles": 1, "output_text": " The group of children is positioned in front of a brick building, with the children arranged in a grid-like formation. The children are standing and sitting in various poses, with some children positioned closer to the camera than others.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26779.1, "ram_available_mb": 98993.0, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26778.9, "ram_available_mb": 98993.3, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.1, "peak": 40.18, "min": 16.16}, "VIN": {"avg": 69.48, "peak": 103.63, "min": 28.41}, "VIN_SYS_5V0": {"avg": 14.96, "peak": 16.05, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 15.89, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 28.1, "energy_joules_est": 63.09, "sample_count": 22, "duration_seconds": 2.245}, "timestamp": "2026-01-19T14:16:39.190674"}
{"image_index": 27, "image_name": "000000002299.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002299.jpg", "image_width": 500, "image_height": 302, "image_resolution": "500x302", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1571.311, "latencies_ms": [1571.311], "images_per_second": 0.636, "prompt_tokens": 1111, "response_tokens_est": 19, "n_tiles": 1, "output_text": " A group of children are posing for a black and white photo in front of a brick building.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26778.9, "ram_available_mb": 98993.3, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26778.9, "ram_available_mb": 98993.3, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.17, "peak": 40.18, "min": 16.56}, "VIN": {"avg": 73.56, "peak": 104.22, "min": 33.8}, "VIN_SYS_5V0": {"avg": 14.96, "peak": 16.05, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 16.01, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 31.17, "energy_joules_est": 49.0, "sample_count": 15, "duration_seconds": 1.572}, "timestamp": "2026-01-19T14:16:40.765856"}
{"image_index": 27, "image_name": "000000002299.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002299.jpg", "image_width": 500, "image_height": 302, "image_resolution": "500x302", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2782.636, "latencies_ms": [2782.636], "images_per_second": 0.359, "prompt_tokens": 1109, "response_tokens_est": 67, "n_tiles": 1, "output_text": " The image is a black and white photograph with a grainy texture, capturing a moment of unity and camaraderie among the children. The lighting is soft and diffused, creating a sense of nostalgia and timelessness. The children are dressed in their school uniforms, which are simple yet elegant, reflecting the era in which the photograph was taken.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26778.9, "ram_available_mb": 98993.3, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 26778.9, "ram_available_mb": 98993.3, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.52, "peak": 40.57, "min": 18.91}, "VIN": {"avg": 71.15, "peak": 120.95, "min": 29.19}, "VIN_SYS_5V0": {"avg": 15.1, "peak": 16.15, "min": 13.29}, "VDD_CPU_SOC_MSS": {"avg": 16.19, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 27.52, "energy_joules_est": 76.59, "sample_count": 27, "duration_seconds": 2.783}, "timestamp": "2026-01-19T14:16:43.576504"}
{"image_index": 28, "image_name": "000000002431.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002431.jpg", "image_width": 457, "image_height": 640, "image_resolution": "457x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1534.677, "latencies_ms": [1534.677], "images_per_second": 0.652, "prompt_tokens": 1100, "response_tokens_est": 18, "n_tiles": 1, "output_text": " A plate of bread and a knife on a table with a wine glass in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26778.9, "ram_available_mb": 98993.3, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26778.9, "ram_available_mb": 98993.3, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.81, "peak": 39.78, "min": 16.16}, "VIN": {"avg": 68.41, "peak": 104.71, "min": 29.38}, "VIN_SYS_5V0": {"avg": 14.99, "peak": 16.05, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 16.14, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 30.81, "energy_joules_est": 47.3, "sample_count": 15, "duration_seconds": 1.535}, "timestamp": "2026-01-19T14:16:45.151908"}
{"image_index": 28, "image_name": "000000002431.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002431.jpg", "image_width": 457, "image_height": 640, "image_resolution": "457x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2038.171, "latencies_ms": [2038.171], "images_per_second": 0.491, "prompt_tokens": 1114, "response_tokens_est": 36, "n_tiles": 1, "output_text": " knife: 1, plate: 1, bread: 1, cup: 1, wine glass: 1, napkin: 1, butter: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26778.9, "ram_available_mb": 98993.3, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26778.6, "ram_available_mb": 98993.5, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.49, "peak": 40.18, "min": 19.31}, "VIN": {"avg": 65.89, "peak": 96.95, "min": 28.46}, "VIN_SYS_5V0": {"avg": 14.97, "peak": 16.05, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 16.02, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 29.49, "energy_joules_est": 60.12, "sample_count": 20, "duration_seconds": 2.038}, "timestamp": "2026-01-19T14:16:47.248695"}
{"image_index": 28, "image_name": "000000002431.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002431.jpg", "image_width": 457, "image_height": 640, "image_resolution": "457x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2547.796, "latencies_ms": [2547.796], "images_per_second": 0.392, "prompt_tokens": 1118, "response_tokens_est": 57, "n_tiles": 1, "output_text": " The main objects are positioned in the foreground, with the bread, knife, and wine glass in the background. The bread is placed on the left side of the plate, while the knife is on the right side. The wine glass is located in the upper right corner of the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26778.6, "ram_available_mb": 98993.5, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26778.6, "ram_available_mb": 98993.5, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.08, "peak": 40.18, "min": 16.56}, "VIN": {"avg": 72.3, "peak": 121.49, "min": 29.23}, "VIN_SYS_5V0": {"avg": 14.86, "peak": 16.05, "min": 12.25}, "VDD_CPU_SOC_MSS": {"avg": 15.84, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 27.08, "energy_joules_est": 69.01, "sample_count": 25, "duration_seconds": 2.548}, "timestamp": "2026-01-19T14:16:49.850170"}
{"image_index": 28, "image_name": "000000002431.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002431.jpg", "image_width": 457, "image_height": 640, "image_resolution": "457x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1723.959, "latencies_ms": [1723.959], "images_per_second": 0.58, "prompt_tokens": 1112, "response_tokens_est": 25, "n_tiles": 1, "output_text": " In a cozy restaurant, a plate of bread and cheese is served on a table, accompanied by a glass of red wine.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26778.6, "ram_available_mb": 98993.5, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26779.4, "ram_available_mb": 98992.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.76, "peak": 39.39, "min": 15.77}, "VIN": {"avg": 74.76, "peak": 123.23, "min": 28.78}, "VIN_SYS_5V0": {"avg": 14.91, "peak": 16.15, "min": 12.36}, "VDD_CPU_SOC_MSS": {"avg": 16.0, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 29.76, "energy_joules_est": 51.31, "sample_count": 17, "duration_seconds": 1.724}, "timestamp": "2026-01-19T14:16:51.619738"}
{"image_index": 28, "image_name": "000000002431.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002431.jpg", "image_width": 457, "image_height": 640, "image_resolution": "457x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1900.462, "latencies_ms": [1900.462], "images_per_second": 0.526, "prompt_tokens": 1110, "response_tokens_est": 32, "n_tiles": 1, "output_text": " The image features a wooden table with a white plate of food, a knife, and a wine glass. The lighting is natural, and the colors are warm.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26779.4, "ram_available_mb": 98992.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26779.4, "ram_available_mb": 98992.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.61, "peak": 40.18, "min": 18.14}, "VIN": {"avg": 66.43, "peak": 109.78, "min": 28.03}, "VIN_SYS_5V0": {"avg": 14.9, "peak": 16.15, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.98, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 29.61, "energy_joules_est": 56.28, "sample_count": 19, "duration_seconds": 1.901}, "timestamp": "2026-01-19T14:16:53.606365"}
{"image_index": 29, "image_name": "000000002473.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002473.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1811.401, "latencies_ms": [1811.401], "images_per_second": 0.552, "prompt_tokens": 1099, "response_tokens_est": 28, "n_tiles": 1, "output_text": " A skier in a colorful outfit is jumping in the air with skis attached, while another skier in a white outfit stands nearby.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26779.4, "ram_available_mb": 98992.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26779.4, "ram_available_mb": 98992.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.57, "peak": 40.18, "min": 16.56}, "VIN": {"avg": 67.3, "peak": 119.02, "min": 30.64}, "VIN_SYS_5V0": {"avg": 14.67, "peak": 15.85, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.73, "peak": 16.93, "min": 13.0}}, "power_watts_avg": 29.57, "energy_joules_est": 53.58, "sample_count": 18, "duration_seconds": 1.812}, "timestamp": "2026-01-19T14:16:55.489063"}
{"image_index": 29, "image_name": "000000002473.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002473.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2610.524, "latencies_ms": [2610.524], "images_per_second": 0.383, "prompt_tokens": 1113, "response_tokens_est": 59, "n_tiles": 1, "output_text": " 1. skier: 1\n2. ski poles: 2\n3. skis: 2\n4. snow: 1\n5. trees: 4\n6. person: 1\n7. helmet: 1\n8. goggles: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26779.4, "ram_available_mb": 98992.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26780.6, "ram_available_mb": 98991.6, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.91, "peak": 40.18, "min": 17.35}, "VIN": {"avg": 67.11, "peak": 122.09, "min": 28.41}, "VIN_SYS_5V0": {"avg": 14.79, "peak": 15.95, "min": 12.25}, "VDD_CPU_SOC_MSS": {"avg": 15.78, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 26.91, "energy_joules_est": 70.26, "sample_count": 26, "duration_seconds": 2.611}, "timestamp": "2026-01-19T14:16:58.192216"}
{"image_index": 29, "image_name": "000000002473.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002473.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2366.218, "latencies_ms": [2366.218], "images_per_second": 0.423, "prompt_tokens": 1117, "response_tokens_est": 48, "n_tiles": 1, "output_text": " The skier is in the foreground, jumping over a snow ramp, while the other two skiers are in the background, standing on the snow. The skier in the foreground is closer to the camera than the other two skiers.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26780.6, "ram_available_mb": 98991.6, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26780.6, "ram_available_mb": 98991.6, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.39, "peak": 40.18, "min": 14.98}, "VIN": {"avg": 67.7, "peak": 84.17, "min": 29.81}, "VIN_SYS_5V0": {"avg": 14.77, "peak": 15.95, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.71, "peak": 17.32, "min": 12.61}}, "power_watts_avg": 27.39, "energy_joules_est": 64.82, "sample_count": 23, "duration_seconds": 2.366}, "timestamp": "2026-01-19T14:17:00.594063"}
{"image_index": 29, "image_name": "000000002473.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002473.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1760.559, "latencies_ms": [1760.559], "images_per_second": 0.568, "prompt_tokens": 1111, "response_tokens_est": 26, "n_tiles": 1, "output_text": " A skier is jumping in the air with skis attached to their feet, while another skier is standing on the snow.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26780.6, "ram_available_mb": 98991.6, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26780.3, "ram_available_mb": 98991.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.29, "peak": 40.18, "min": 16.56}, "VIN": {"avg": 74.26, "peak": 126.67, "min": 31.82}, "VIN_SYS_5V0": {"avg": 14.99, "peak": 16.05, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 16.1, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 30.29, "energy_joules_est": 53.34, "sample_count": 17, "duration_seconds": 1.761}, "timestamp": "2026-01-19T14:17:02.361945"}
{"image_index": 29, "image_name": "000000002473.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002473.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2143.722, "latencies_ms": [2143.722], "images_per_second": 0.466, "prompt_tokens": 1109, "response_tokens_est": 42, "n_tiles": 1, "output_text": " The image features a skier in mid-air against a clear blue sky, with snow-covered trees in the background. The skier is wearing a colorful jacket and pants, and is holding ski poles.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26780.3, "ram_available_mb": 98991.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26780.3, "ram_available_mb": 98991.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.14, "peak": 40.57, "min": 19.31}, "VIN": {"avg": 73.0, "peak": 122.9, "min": 28.72}, "VIN_SYS_5V0": {"avg": 15.1, "peak": 16.15, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 16.31, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 29.14, "energy_joules_est": 62.48, "sample_count": 21, "duration_seconds": 2.144}, "timestamp": "2026-01-19T14:17:04.555542"}
{"image_index": 30, "image_name": "000000002532.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002532.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1508.643, "latencies_ms": [1508.643], "images_per_second": 0.663, "prompt_tokens": 1100, "response_tokens_est": 17, "n_tiles": 1, "output_text": " A person is walking on a snowy mountain with ski poles and wearing a green jacket.", "error": null, "sys_before": {"cpu_percent": 6.7, "ram_used_mb": 26780.3, "ram_available_mb": 98991.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 26780.6, "ram_available_mb": 98991.6, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.96, "peak": 40.16, "min": 16.56}, "VIN": {"avg": 67.74, "peak": 99.87, "min": 30.15}, "VIN_SYS_5V0": {"avg": 14.81, "peak": 15.95, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.99, "peak": 17.32, "min": 13.4}}, "power_watts_avg": 30.96, "energy_joules_est": 46.72, "sample_count": 15, "duration_seconds": 1.509}, "timestamp": "2026-01-19T14:17:06.135076"}
{"image_index": 30, "image_name": "000000002532.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002532.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2537.993, "latencies_ms": [2537.993], "images_per_second": 0.394, "prompt_tokens": 1114, "response_tokens_est": 57, "n_tiles": 1, "output_text": " 1. person: 1\n2. ski poles: 2\n3. backpack: 1\n4. snow: 1\n5. rocks: 2\n6. trees: 2\n7. clouds: 2\n8. sky: 1", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26780.6, "ram_available_mb": 98991.6, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 26780.6, "ram_available_mb": 98991.6, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.67, "peak": 40.57, "min": 18.14}, "VIN": {"avg": 67.81, "peak": 124.59, "min": 28.61}, "VIN_SYS_5V0": {"avg": 14.94, "peak": 16.15, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.89, "peak": 17.32, "min": 13.4}}, "power_watts_avg": 27.67, "energy_joules_est": 70.24, "sample_count": 25, "duration_seconds": 2.538}, "timestamp": "2026-01-19T14:17:08.741797"}
{"image_index": 30, "image_name": "000000002532.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002532.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2647.561, "latencies_ms": [2647.561], "images_per_second": 0.378, "prompt_tokens": 1118, "response_tokens_est": 60, "n_tiles": 1, "output_text": " The person is in the foreground of the image, with the snow-covered mountains and trees in the background. The person is facing away from the camera, with the mountains and trees in the distance. The person is standing on a snow-covered trail, with the mountains and trees in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26780.6, "ram_available_mb": 98991.6, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 26776.3, "ram_available_mb": 98995.9, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.41, "peak": 39.78, "min": 15.77}, "VIN": {"avg": 73.07, "peak": 126.77, "min": 28.97}, "VIN_SYS_5V0": {"avg": 14.86, "peak": 16.05, "min": 12.15}, "VDD_CPU_SOC_MSS": {"avg": 16.07, "peak": 17.32, "min": 13.4}}, "power_watts_avg": 26.41, "energy_joules_est": 69.93, "sample_count": 26, "duration_seconds": 2.648}, "timestamp": "2026-01-19T14:17:11.459951"}
{"image_index": 30, "image_name": "000000002532.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002532.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1621.681, "latencies_ms": [1621.681], "images_per_second": 0.617, "prompt_tokens": 1112, "response_tokens_est": 21, "n_tiles": 1, "output_text": " A person is walking on a snowy mountain trail with ski poles, wearing a green jacket and black pants.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26776.3, "ram_available_mb": 98995.9, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26776.3, "ram_available_mb": 98995.9, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.04, "peak": 39.39, "min": 15.38}, "VIN": {"avg": 75.6, "peak": 124.78, "min": 28.23}, "VIN_SYS_5V0": {"avg": 14.76, "peak": 16.05, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 15.97, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 30.04, "energy_joules_est": 48.73, "sample_count": 16, "duration_seconds": 1.622}, "timestamp": "2026-01-19T14:17:13.129435"}
{"image_index": 30, "image_name": "000000002532.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002532.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2313.483, "latencies_ms": [2313.483], "images_per_second": 0.432, "prompt_tokens": 1110, "response_tokens_est": 48, "n_tiles": 1, "output_text": " The image features a person wearing a green jacket and black pants, standing on a snowy mountain with a clear blue sky and white clouds in the background. The snow is pristine white, and the person is using ski poles to navigate the terrain.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26776.3, "ram_available_mb": 98995.9, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26776.3, "ram_available_mb": 98995.9, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.13, "peak": 40.57, "min": 18.13}, "VIN": {"avg": 72.22, "peak": 116.52, "min": 30.0}, "VIN_SYS_5V0": {"avg": 14.94, "peak": 16.15, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.94, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 28.13, "energy_joules_est": 65.09, "sample_count": 23, "duration_seconds": 2.314}, "timestamp": "2026-01-19T14:17:15.521486"}
{"image_index": 31, "image_name": "000000002587.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002587.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1439.816, "latencies_ms": [1439.816], "images_per_second": 0.695, "prompt_tokens": 1099, "response_tokens_est": 14, "n_tiles": 1, "output_text": " A banana and a chocolate donut are placed in a plastic bag.", "error": null, "sys_before": {"cpu_percent": 14.3, "ram_used_mb": 26776.3, "ram_available_mb": 98995.9, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26777.5, "ram_available_mb": 98994.7, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.32, "peak": 40.18, "min": 15.38}, "VIN": {"avg": 72.65, "peak": 115.02, "min": 27.92}, "VIN_SYS_5V0": {"avg": 14.74, "peak": 15.85, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.92, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 31.32, "energy_joules_est": 45.12, "sample_count": 14, "duration_seconds": 1.441}, "timestamp": "2026-01-19T14:17:16.985286"}
{"image_index": 31, "image_name": "000000002587.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002587.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1424.88, "latencies_ms": [1424.88], "images_per_second": 0.702, "prompt_tokens": 1113, "response_tokens_est": 14, "n_tiles": 1, "output_text": " banana: 1, apple: 1, bag: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26777.5, "ram_available_mb": 98994.7, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 26777.5, "ram_available_mb": 98994.7, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 33.63, "peak": 40.57, "min": 22.46}, "VIN": {"avg": 73.49, "peak": 127.01, "min": 27.13}, "VIN_SYS_5V0": {"avg": 15.21, "peak": 16.15, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 16.37, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 33.63, "energy_joules_est": 47.93, "sample_count": 14, "duration_seconds": 1.425}, "timestamp": "2026-01-19T14:17:18.446824"}
{"image_index": 31, "image_name": "000000002587.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002587.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2067.968, "latencies_ms": [2067.968], "images_per_second": 0.484, "prompt_tokens": 1117, "response_tokens_est": 37, "n_tiles": 1, "output_text": " The banana is positioned to the left of the donut, which is in the center of the image. The banana is in the foreground, while the donut is in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26777.5, "ram_available_mb": 98994.7, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26777.5, "ram_available_mb": 98994.7, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.04, "peak": 40.97, "min": 20.1}, "VIN": {"avg": 73.7, "peak": 123.89, "min": 30.02}, "VIN_SYS_5V0": {"avg": 15.02, "peak": 16.05, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 16.1, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 30.04, "energy_joules_est": 62.13, "sample_count": 20, "duration_seconds": 2.068}, "timestamp": "2026-01-19T14:17:20.540335"}
{"image_index": 31, "image_name": "000000002587.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002587.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1399.073, "latencies_ms": [1399.073], "images_per_second": 0.715, "prompt_tokens": 1111, "response_tokens_est": 13, "n_tiles": 1, "output_text": " A banana and a donut are placed in a plastic bag.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26777.5, "ram_available_mb": 98994.7, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 26778.0, "ram_available_mb": 98994.2, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.97, "peak": 40.18, "min": 17.35}, "VIN": {"avg": 75.82, "peak": 124.79, "min": 29.77}, "VIN_SYS_5V0": {"avg": 15.03, "peak": 16.05, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 16.23, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 31.97, "energy_joules_est": 44.74, "sample_count": 14, "duration_seconds": 1.4}, "timestamp": "2026-01-19T14:17:22.003656"}
{"image_index": 31, "image_name": "000000002587.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002587.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2196.858, "latencies_ms": [2196.858], "images_per_second": 0.455, "prompt_tokens": 1109, "response_tokens_est": 43, "n_tiles": 1, "output_text": " The image features a banana and a donut placed in a plastic bag, with the banana being yellow and the donut being brown. The lighting is dim, and the bag appears to be made of clear plastic.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26778.0, "ram_available_mb": 98994.2, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26777.7, "ram_available_mb": 98994.4, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.07, "peak": 40.97, "min": 18.14}, "VIN": {"avg": 72.78, "peak": 121.33, "min": 28.58}, "VIN_SYS_5V0": {"avg": 14.86, "peak": 16.05, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.86, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 29.07, "energy_joules_est": 63.87, "sample_count": 22, "duration_seconds": 2.197}, "timestamp": "2026-01-19T14:17:24.298305"}
{"image_index": 32, "image_name": "000000002592.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002592.jpg", "image_width": 640, "image_height": 366, "image_resolution": "640x366", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1982.962, "latencies_ms": [1982.962], "images_per_second": 0.504, "prompt_tokens": 1099, "response_tokens_est": 35, "n_tiles": 1, "output_text": " A white mug with a pirate skull and crossbones design and the words \"PIRATE PICKLES\" is placed on a glass surface next to a large knife.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 26777.7, "ram_available_mb": 98994.4, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26778.4, "ram_available_mb": 98993.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.58, "peak": 40.18, "min": 15.38}, "VIN": {"avg": 71.49, "peak": 119.26, "min": 30.29}, "VIN_SYS_5V0": {"avg": 14.72, "peak": 15.95, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.85, "peak": 16.93, "min": 13.0}}, "power_watts_avg": 28.58, "energy_joules_est": 56.68, "sample_count": 20, "duration_seconds": 1.983}, "timestamp": "2026-01-19T14:17:26.392718"}
{"image_index": 32, "image_name": "000000002592.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002592.jpg", "image_width": 640, "image_height": 366, "image_resolution": "640x366", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1608.174, "latencies_ms": [1608.174], "images_per_second": 0.622, "prompt_tokens": 1113, "response_tokens_est": 20, "n_tiles": 1, "output_text": " mug: 1, knife: 1, glass: 1, table: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26778.4, "ram_available_mb": 98993.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26778.2, "ram_available_mb": 98994.0, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.48, "peak": 40.18, "min": 15.77}, "VIN": {"avg": 70.44, "peak": 126.47, "min": 30.59}, "VIN_SYS_5V0": {"avg": 14.68, "peak": 15.95, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.8, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 30.48, "energy_joules_est": 49.03, "sample_count": 16, "duration_seconds": 1.609}, "timestamp": "2026-01-19T14:17:28.061661"}
{"image_index": 32, "image_name": "000000002592.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002592.jpg", "image_width": 640, "image_height": 366, "image_resolution": "640x366", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1993.5, "latencies_ms": [1993.5], "images_per_second": 0.502, "prompt_tokens": 1117, "response_tokens_est": 35, "n_tiles": 1, "output_text": " The mug is located to the left of the knife, which is placed in the foreground of the image. The mug is positioned closer to the camera than the knife.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26778.2, "ram_available_mb": 98994.0, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26778.2, "ram_available_mb": 98994.0, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.51, "peak": 40.56, "min": 18.91}, "VIN": {"avg": 71.37, "peak": 124.4, "min": 29.06}, "VIN_SYS_5V0": {"avg": 14.84, "peak": 16.05, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.94, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 29.51, "energy_joules_est": 58.84, "sample_count": 20, "duration_seconds": 1.994}, "timestamp": "2026-01-19T14:17:30.147601"}
{"image_index": 32, "image_name": "000000002592.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002592.jpg", "image_width": 640, "image_height": 366, "image_resolution": "640x366", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1932.371, "latencies_ms": [1932.371], "images_per_second": 0.517, "prompt_tokens": 1111, "response_tokens_est": 33, "n_tiles": 1, "output_text": " A white mug with a pirate skull and crossbones design is sitting on a glass table. Next to the mug is a large knife with a black handle.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26778.2, "ram_available_mb": 98994.0, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26778.4, "ram_available_mb": 98993.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.3, "peak": 40.18, "min": 16.16}, "VIN": {"avg": 75.54, "peak": 121.96, "min": 28.95}, "VIN_SYS_5V0": {"avg": 14.8, "peak": 15.95, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.9, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 29.3, "energy_joules_est": 56.63, "sample_count": 19, "duration_seconds": 1.933}, "timestamp": "2026-01-19T14:17:32.128167"}
{"image_index": 32, "image_name": "000000002592.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002592.jpg", "image_width": 640, "image_height": 366, "image_resolution": "640x366", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1596.46, "latencies_ms": [1596.46], "images_per_second": 0.626, "prompt_tokens": 1109, "response_tokens_est": 20, "n_tiles": 1, "output_text": " The mug is white and the knife is black. The mug is on a glass table.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26778.4, "ram_available_mb": 98993.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 26778.4, "ram_available_mb": 98993.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.98, "peak": 40.16, "min": 17.35}, "VIN": {"avg": 76.23, "peak": 119.93, "min": 29.34}, "VIN_SYS_5V0": {"avg": 14.95, "peak": 16.05, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.92, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 30.98, "energy_joules_est": 49.47, "sample_count": 16, "duration_seconds": 1.597}, "timestamp": "2026-01-19T14:17:33.795588"}
{"image_index": 33, "image_name": "000000002685.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002685.jpg", "image_width": 640, "image_height": 555, "image_resolution": "640x555", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2341.981, "latencies_ms": [2341.981], "images_per_second": 0.427, "prompt_tokens": 1432, "response_tokens_est": 33, "n_tiles": 1, "output_text": " A group of people are gathered around a wooden counter in a wine tasting room, with a man in a white shirt and a woman in a black shirt standing out.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 26778.4, "ram_available_mb": 98993.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26779.1, "ram_available_mb": 98993.0, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.2, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.63, "peak": 40.57, "min": 18.92}, "VIN": {"avg": 71.56, "peak": 118.02, "min": 29.19}, "VIN_SYS_5V0": {"avg": 15.25, "peak": 16.66, "min": 12.15}, "VDD_CPU_SOC_MSS": {"avg": 16.35, "peak": 17.71, "min": 13.39}}, "power_watts_avg": 30.63, "energy_joules_est": 71.75, "sample_count": 23, "duration_seconds": 2.343}, "timestamp": "2026-01-19T14:17:36.203084"}
{"image_index": 33, "image_name": "000000002685.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002685.jpg", "image_width": 640, "image_height": 555, "image_resolution": "640x555", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2937.76, "latencies_ms": [2937.76], "images_per_second": 0.34, "prompt_tokens": 1446, "response_tokens_est": 56, "n_tiles": 1, "output_text": " 1. man: 1\n2. woman: 1\n3. man: 1\n4. man: 1\n5. man: 1\n6. man: 1\n7. man: 1\n8. man: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26779.1, "ram_available_mb": 98993.0, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26779.1, "ram_available_mb": 98993.0, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.61, "peak": 40.18, "min": 17.35}, "VIN": {"avg": 71.68, "peak": 122.11, "min": 28.44}, "VIN_SYS_5V0": {"avg": 15.11, "peak": 16.56, "min": 12.36}, "VDD_CPU_SOC_MSS": {"avg": 16.2, "peak": 17.71, "min": 13.39}}, "power_watts_avg": 28.61, "energy_joules_est": 84.07, "sample_count": 29, "duration_seconds": 2.938}, "timestamp": "2026-01-19T14:17:39.222870"}
{"image_index": 33, "image_name": "000000002685.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002685.jpg", "image_width": 640, "image_height": 555, "image_resolution": "640x555", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3512.096, "latencies_ms": [3512.096], "images_per_second": 0.285, "prompt_tokens": 1450, "response_tokens_est": 77, "n_tiles": 1, "output_text": " The man in the white cap is standing to the left of the man in the blue shirt, who is standing in front of the counter. The woman in the black shirt is standing to the right of the man in the blue shirt, and the man in the white shirt is standing behind the counter. The wooden barrel is located in the background, behind the man in the white shirt.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26779.1, "ram_available_mb": 98993.0, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26778.9, "ram_available_mb": 98993.3, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.14, "peak": 39.78, "min": 15.77}, "VIN": {"avg": 71.6, "peak": 121.35, "min": 28.53}, "VIN_SYS_5V0": {"avg": 15.03, "peak": 16.56, "min": 11.87}, "VDD_CPU_SOC_MSS": {"avg": 16.08, "peak": 17.71, "min": 13.0}}, "power_watts_avg": 27.14, "energy_joules_est": 95.34, "sample_count": 34, "duration_seconds": 3.513}, "timestamp": "2026-01-19T14:17:42.760044"}
{"image_index": 33, "image_name": "000000002685.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002685.jpg", "image_width": 640, "image_height": 555, "image_resolution": "640x555", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2050.592, "latencies_ms": [2050.592], "images_per_second": 0.488, "prompt_tokens": 1444, "response_tokens_est": 23, "n_tiles": 1, "output_text": " A group of people are gathered in a wine tasting room, standing around a wooden counter with wine bottles on it.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26778.9, "ram_available_mb": 98993.3, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 26778.9, "ram_available_mb": 98993.3, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.85, "peak": 40.18, "min": 16.56}, "VIN": {"avg": 74.91, "peak": 119.89, "min": 28.71}, "VIN_SYS_5V0": {"avg": 15.36, "peak": 16.56, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 16.52, "peak": 17.71, "min": 13.79}}, "power_watts_avg": 31.85, "energy_joules_est": 65.33, "sample_count": 20, "duration_seconds": 2.051}, "timestamp": "2026-01-19T14:17:44.842841"}
{"image_index": 33, "image_name": "000000002685.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002685.jpg", "image_width": 640, "image_height": 555, "image_resolution": "640x555", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2939.238, "latencies_ms": [2939.238], "images_per_second": 0.34, "prompt_tokens": 1442, "response_tokens_est": 56, "n_tiles": 1, "output_text": " The image is taken in a dimly lit room with a warm ambiance, and the wooden floor reflects the soft light. The colors in the image are mostly muted, with the exception of the vibrant red wine bottles and the white apron of the man in the foreground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26778.9, "ram_available_mb": 98993.3, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26779.6, "ram_available_mb": 98992.6, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.95, "peak": 40.57, "min": 18.52}, "VIN": {"avg": 69.86, "peak": 117.34, "min": 28.66}, "VIN_SYS_5V0": {"avg": 15.2, "peak": 16.66, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 16.24, "peak": 17.71, "min": 13.79}}, "power_watts_avg": 28.95, "energy_joules_est": 85.1, "sample_count": 29, "duration_seconds": 2.94}, "timestamp": "2026-01-19T14:17:47.856021"}
{"image_index": 34, "image_name": "000000002923.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002923.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1830.647, "latencies_ms": [1830.647], "images_per_second": 0.546, "prompt_tokens": 1099, "response_tokens_est": 29, "n_tiles": 1, "output_text": " In the image, there are two white birds standing in a green field, with a large body of water and a cloudy sky in the background.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 26779.6, "ram_available_mb": 98992.6, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26779.6, "ram_available_mb": 98992.6, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.59, "peak": 40.18, "min": 15.77}, "VIN": {"avg": 71.41, "peak": 104.58, "min": 28.49}, "VIN_SYS_5V0": {"avg": 14.81, "peak": 15.95, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 15.75, "peak": 16.93, "min": 13.0}}, "power_watts_avg": 29.59, "energy_joules_est": 54.18, "sample_count": 18, "duration_seconds": 1.831}, "timestamp": "2026-01-19T14:17:49.737242"}
{"image_index": 34, "image_name": "000000002923.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002923.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2524.193, "latencies_ms": [2524.193], "images_per_second": 0.396, "prompt_tokens": 1113, "response_tokens_est": 56, "n_tiles": 1, "output_text": " 1. birds: 2\n2. grass: 1\n3. water: 1\n4. sky: 1\n5. clouds: 1\n6. boats: 1\n7. dock: 1\n8. crane: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26779.6, "ram_available_mb": 98992.6, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26780.1, "ram_available_mb": 98992.1, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.41, "peak": 40.57, "min": 17.74}, "VIN": {"avg": 71.23, "peak": 114.25, "min": 29.63}, "VIN_SYS_5V0": {"avg": 14.94, "peak": 16.05, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 16.13, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 27.41, "energy_joules_est": 69.2, "sample_count": 25, "duration_seconds": 2.525}, "timestamp": "2026-01-19T14:17:52.332416"}
{"image_index": 34, "image_name": "000000002923.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002923.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2165.672, "latencies_ms": [2165.672], "images_per_second": 0.462, "prompt_tokens": 1117, "response_tokens_est": 42, "n_tiles": 1, "output_text": " The two white birds are in the foreground, with the marina and boats in the background. The sky is above the birds and the marina, and the water is below the birds and the marina.", "error": null, "sys_before": {"cpu_percent": 50.0, "ram_used_mb": 26780.1, "ram_available_mb": 98992.1, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26780.1, "ram_available_mb": 98992.1, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.37, "peak": 40.16, "min": 15.77}, "VIN": {"avg": 69.81, "peak": 101.73, "min": 28.7}, "VIN_SYS_5V0": {"avg": 14.89, "peak": 16.05, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 16.01, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 28.37, "energy_joules_est": 61.45, "sample_count": 21, "duration_seconds": 2.166}, "timestamp": "2026-01-19T14:17:54.527305"}
{"image_index": 34, "image_name": "000000002923.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002923.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1995.523, "latencies_ms": [1995.523], "images_per_second": 0.501, "prompt_tokens": 1111, "response_tokens_est": 36, "n_tiles": 1, "output_text": " In this image, we can see a field with grass and two birds. In the background, there are boats and poles. At the top, there are clouds in the sky.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26780.1, "ram_available_mb": 98992.1, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26780.3, "ram_available_mb": 98991.9, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.18, "peak": 40.57, "min": 17.35}, "VIN": {"avg": 73.25, "peak": 126.66, "min": 27.62}, "VIN_SYS_5V0": {"avg": 15.01, "peak": 16.05, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 16.1, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 29.18, "energy_joules_est": 58.24, "sample_count": 20, "duration_seconds": 1.996}, "timestamp": "2026-01-19T14:17:56.613274"}
{"image_index": 34, "image_name": "000000002923.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002923.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1407.309, "latencies_ms": [1407.309], "images_per_second": 0.711, "prompt_tokens": 1109, "response_tokens_est": 13, "n_tiles": 1, "output_text": " The sky is cloudy and blue, and the grass is green.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26780.3, "ram_available_mb": 98991.9, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 26780.3, "ram_available_mb": 98991.9, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.35, "peak": 40.18, "min": 16.16}, "VIN": {"avg": 67.28, "peak": 102.49, "min": 28.67}, "VIN_SYS_5V0": {"avg": 14.73, "peak": 15.95, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.89, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 31.35, "energy_joules_est": 44.13, "sample_count": 14, "duration_seconds": 1.408}, "timestamp": "2026-01-19T14:17:58.085859"}
{"image_index": 35, "image_name": "000000003156.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003156.jpg", "image_width": 443, "image_height": 640, "image_resolution": "443x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1619.919, "latencies_ms": [1619.919], "images_per_second": 0.617, "prompt_tokens": 1100, "response_tokens_est": 21, "n_tiles": 1, "output_text": " A man is sitting on a toilet in a bathroom with a checkered tile wall and a window.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26780.3, "ram_available_mb": 98991.9, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26780.3, "ram_available_mb": 98991.9, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.59, "peak": 40.16, "min": 20.89}, "VIN": {"avg": 78.86, "peak": 118.0, "min": 30.14}, "VIN_SYS_5V0": {"avg": 14.88, "peak": 16.15, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.97, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 31.59, "energy_joules_est": 51.2, "sample_count": 16, "duration_seconds": 1.621}, "timestamp": "2026-01-19T14:17:59.760059"}
{"image_index": 35, "image_name": "000000003156.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003156.jpg", "image_width": 443, "image_height": 640, "image_resolution": "443x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2958.272, "latencies_ms": [2958.272], "images_per_second": 0.338, "prompt_tokens": 1114, "response_tokens_est": 69, "n_tiles": 1, "output_text": " 1. man: 1\n2. toilet: 1\n3. man's pants: 1\n4. man's shirt: 1\n5. man's shoes: 1\n6. man's socks: 1\n7. man's belt: 1\n8. man's belt buckle: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26780.3, "ram_available_mb": 98991.9, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 26780.0, "ram_available_mb": 98992.1, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.22, "peak": 40.18, "min": 17.34}, "VIN": {"avg": 66.84, "peak": 121.84, "min": 29.39}, "VIN_SYS_5V0": {"avg": 14.81, "peak": 16.05, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.76, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 26.22, "energy_joules_est": 77.57, "sample_count": 29, "duration_seconds": 2.959}, "timestamp": "2026-01-19T14:18:02.785643"}
{"image_index": 35, "image_name": "000000003156.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003156.jpg", "image_width": 443, "image_height": 640, "image_resolution": "443x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2071.064, "latencies_ms": [2071.064], "images_per_second": 0.483, "prompt_tokens": 1118, "response_tokens_est": 38, "n_tiles": 1, "output_text": " The man is sitting on the toilet, which is located in the foreground of the image. The sink is positioned to the left of the toilet, and the window is located above the sink.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26780.0, "ram_available_mb": 98992.1, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26780.0, "ram_available_mb": 98992.1, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.45, "peak": 39.78, "min": 14.98}, "VIN": {"avg": 71.89, "peak": 126.99, "min": 30.14}, "VIN_SYS_5V0": {"avg": 14.85, "peak": 16.05, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.89, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 28.45, "energy_joules_est": 58.93, "sample_count": 20, "duration_seconds": 2.071}, "timestamp": "2026-01-19T14:18:04.872072"}
{"image_index": 35, "image_name": "000000003156.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003156.jpg", "image_width": 443, "image_height": 640, "image_resolution": "443x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1494.689, "latencies_ms": [1494.689], "images_per_second": 0.669, "prompt_tokens": 1112, "response_tokens_est": 17, "n_tiles": 1, "output_text": " A man is sitting on a toilet in a bathroom, wearing a shirt and pants.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26780.0, "ram_available_mb": 98992.1, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26780.0, "ram_available_mb": 98992.1, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.25, "peak": 39.39, "min": 17.73}, "VIN": {"avg": 73.84, "peak": 126.23, "min": 28.77}, "VIN_SYS_5V0": {"avg": 15.14, "peak": 16.26, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 16.36, "peak": 17.71, "min": 14.19}}, "power_watts_avg": 31.25, "energy_joules_est": 46.72, "sample_count": 15, "duration_seconds": 1.495}, "timestamp": "2026-01-19T14:18:06.441500"}
{"image_index": 35, "image_name": "000000003156.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003156.jpg", "image_width": 443, "image_height": 640, "image_resolution": "443x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1568.629, "latencies_ms": [1568.629], "images_per_second": 0.637, "prompt_tokens": 1110, "response_tokens_est": 19, "n_tiles": 1, "output_text": " The image is in black and white, and the man is wearing a black shirt and jeans.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26780.0, "ram_available_mb": 98992.1, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 26780.0, "ram_available_mb": 98992.2, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.24, "peak": 40.56, "min": 19.32}, "VIN": {"avg": 76.94, "peak": 122.83, "min": 27.1}, "VIN_SYS_5V0": {"avg": 14.78, "peak": 16.05, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.97, "peak": 17.32, "min": 13.4}}, "power_watts_avg": 31.24, "energy_joules_est": 49.01, "sample_count": 16, "duration_seconds": 1.569}, "timestamp": "2026-01-19T14:18:08.114021"}
{"image_index": 36, "image_name": "000000003255.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003255.jpg", "image_width": 640, "image_height": 363, "image_resolution": "640x363", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1759.677, "latencies_ms": [1759.677], "images_per_second": 0.568, "prompt_tokens": 1099, "response_tokens_est": 26, "n_tiles": 1, "output_text": " In the image, a group of people are standing on a snow-covered mountain, with a large rock formation in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26780.0, "ram_available_mb": 98992.2, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26779.8, "ram_available_mb": 98992.4, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.48, "peak": 40.18, "min": 17.74}, "VIN": {"avg": 69.25, "peak": 123.08, "min": 28.95}, "VIN_SYS_5V0": {"avg": 14.75, "peak": 15.95, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.77, "peak": 16.93, "min": 13.0}}, "power_watts_avg": 30.48, "energy_joules_est": 53.66, "sample_count": 17, "duration_seconds": 1.76}, "timestamp": "2026-01-19T14:18:09.895006"}
{"image_index": 36, "image_name": "000000003255.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003255.jpg", "image_width": 640, "image_height": 363, "image_resolution": "640x363", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2638.87, "latencies_ms": [2638.87], "images_per_second": 0.379, "prompt_tokens": 1113, "response_tokens_est": 61, "n_tiles": 1, "output_text": " 1. mountain: 1\n2. people: 4\n3. ski tracks: 3\n4. snow: 1\n5. rocks: 1\n6. sky: 1\n7. mountain peak: 1\n8. snow-covered slope: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26779.8, "ram_available_mb": 98992.4, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 26779.3, "ram_available_mb": 98992.9, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.35, "peak": 40.57, "min": 18.13}, "VIN": {"avg": 69.87, "peak": 123.43, "min": 29.52}, "VIN_SYS_5V0": {"avg": 15.12, "peak": 16.15, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 16.04, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 27.35, "energy_joules_est": 72.18, "sample_count": 26, "duration_seconds": 2.639}, "timestamp": "2026-01-19T14:18:12.607799"}
{"image_index": 36, "image_name": "000000003255.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003255.jpg", "image_width": 640, "image_height": 363, "image_resolution": "640x363", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2204.352, "latencies_ms": [2204.352], "images_per_second": 0.454, "prompt_tokens": 1117, "response_tokens_est": 43, "n_tiles": 1, "output_text": " The group of people is positioned in the foreground of the image, with the mountain range in the background. The mountain range is located to the right of the group, and the sky is visible above the mountain range.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26779.3, "ram_available_mb": 98992.9, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26779.3, "ram_available_mb": 98992.9, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.87, "peak": 40.18, "min": 15.38}, "VIN": {"avg": 71.06, "peak": 120.79, "min": 27.44}, "VIN_SYS_5V0": {"avg": 14.83, "peak": 15.95, "min": 12.15}, "VDD_CPU_SOC_MSS": {"avg": 15.82, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 27.87, "energy_joules_est": 61.45, "sample_count": 22, "duration_seconds": 2.205}, "timestamp": "2026-01-19T14:18:14.898092"}
{"image_index": 36, "image_name": "000000003255.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003255.jpg", "image_width": 640, "image_height": 363, "image_resolution": "640x363", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1616.08, "latencies_ms": [1616.08], "images_per_second": 0.619, "prompt_tokens": 1111, "response_tokens_est": 20, "n_tiles": 1, "output_text": " A group of people are standing on a snowy mountain, with a large rock formation in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26779.3, "ram_available_mb": 98992.9, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26779.3, "ram_available_mb": 98992.9, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.24, "peak": 39.78, "min": 15.38}, "VIN": {"avg": 79.08, "peak": 126.41, "min": 30.25}, "VIN_SYS_5V0": {"avg": 14.73, "peak": 16.05, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.82, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 30.24, "energy_joules_est": 48.88, "sample_count": 16, "duration_seconds": 1.616}, "timestamp": "2026-01-19T14:18:16.569506"}
{"image_index": 36, "image_name": "000000003255.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003255.jpg", "image_width": 640, "image_height": 363, "image_resolution": "640x363", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2144.516, "latencies_ms": [2144.516], "images_per_second": 0.466, "prompt_tokens": 1109, "response_tokens_est": 41, "n_tiles": 1, "output_text": " The image features a group of people standing on a snow-covered mountain, with the sky above them being a clear blue. The snow is pristine white, and the mountain's surface is rugged and rocky.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26779.3, "ram_available_mb": 98992.9, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26779.0, "ram_available_mb": 98993.2, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.08, "peak": 40.16, "min": 19.32}, "VIN": {"avg": 69.77, "peak": 121.31, "min": 28.36}, "VIN_SYS_5V0": {"avg": 14.88, "peak": 15.95, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.95, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 29.08, "energy_joules_est": 62.37, "sample_count": 21, "duration_seconds": 2.145}, "timestamp": "2026-01-19T14:18:18.756362"}
{"image_index": 37, "image_name": "000000003501.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003501.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2016.694, "latencies_ms": [2016.694], "images_per_second": 0.496, "prompt_tokens": 1432, "response_tokens_est": 21, "n_tiles": 1, "output_text": " A white bowl filled with rice, broccoli, and a red bean dish is placed on a wooden table.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26779.0, "ram_available_mb": 98993.2, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 26779.0, "ram_available_mb": 98993.2, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.2, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.56, "peak": 39.78, "min": 16.95}, "VIN": {"avg": 76.16, "peak": 123.47, "min": 29.52}, "VIN_SYS_5V0": {"avg": 15.4, "peak": 16.76, "min": 12.56}, "VDD_CPU_SOC_MSS": {"avg": 16.45, "peak": 17.71, "min": 13.79}}, "power_watts_avg": 31.56, "energy_joules_est": 63.67, "sample_count": 20, "duration_seconds": 2.017}, "timestamp": "2026-01-19T14:18:20.847990"}
{"image_index": 37, "image_name": "000000003501.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003501.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1955.3, "latencies_ms": [1955.3], "images_per_second": 0.511, "prompt_tokens": 1446, "response_tokens_est": 19, "n_tiles": 1, "output_text": " broccoli: 2, rice: 1, bean: 1, vegetable: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26779.0, "ram_available_mb": 98993.2, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 26778.7, "ram_available_mb": 98993.5, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.68, "peak": 40.57, "min": 18.53}, "VIN": {"avg": 77.26, "peak": 117.76, "min": 28.66}, "VIN_SYS_5V0": {"avg": 15.25, "peak": 16.56, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 16.37, "peak": 17.71, "min": 13.39}}, "power_watts_avg": 32.68, "energy_joules_est": 63.91, "sample_count": 19, "duration_seconds": 1.956}, "timestamp": "2026-01-19T14:18:22.825885"}
{"image_index": 37, "image_name": "000000003501.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003501.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2597.38, "latencies_ms": [2597.38], "images_per_second": 0.385, "prompt_tokens": 1450, "response_tokens_est": 43, "n_tiles": 1, "output_text": " The broccoli is located to the left of the rice, which is in the center of the bowl. The red bean mixture is on top of the rice, and the white rice is on the bottom of the bowl.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26778.7, "ram_available_mb": 98993.5, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26778.7, "ram_available_mb": 98993.5, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.63, "peak": 40.18, "min": 20.89}, "VIN": {"avg": 73.09, "peak": 117.08, "min": 34.07}, "VIN_SYS_5V0": {"avg": 15.34, "peak": 16.66, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 16.44, "peak": 17.71, "min": 14.18}}, "power_watts_avg": 30.63, "energy_joules_est": 79.57, "sample_count": 25, "duration_seconds": 2.598}, "timestamp": "2026-01-19T14:18:25.429633"}
{"image_index": 37, "image_name": "000000003501.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003501.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1677.737, "latencies_ms": [1677.737], "images_per_second": 0.596, "prompt_tokens": 1444, "response_tokens_est": 9, "n_tiles": 1, "output_text": " A bowl of food is on a table.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26778.7, "ram_available_mb": 98993.5, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.2, "ram_used_mb": 26778.7, "ram_available_mb": 98993.5, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 33.46, "peak": 40.57, "min": 17.73}, "VIN": {"avg": 77.01, "peak": 119.55, "min": 29.54}, "VIN_SYS_5V0": {"avg": 15.48, "peak": 16.76, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 16.6, "peak": 17.71, "min": 14.18}}, "power_watts_avg": 33.46, "energy_joules_est": 56.15, "sample_count": 17, "duration_seconds": 1.678}, "timestamp": "2026-01-19T14:18:27.205721"}
{"image_index": 37, "image_name": "000000003501.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003501.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1728.735, "latencies_ms": [1728.735], "images_per_second": 0.578, "prompt_tokens": 1442, "response_tokens_est": 10, "n_tiles": 1, "output_text": " The bowl is white and the food is colorful.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26778.7, "ram_available_mb": 98993.5, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.3, "ram_used_mb": 26778.7, "ram_available_mb": 98993.5, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 34.0, "peak": 40.57, "min": 20.89}, "VIN": {"avg": 76.44, "peak": 129.66, "min": 29.43}, "VIN_SYS_5V0": {"avg": 15.11, "peak": 16.56, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 16.28, "peak": 17.71, "min": 13.0}}, "power_watts_avg": 34.0, "energy_joules_est": 58.79, "sample_count": 17, "duration_seconds": 1.729}, "timestamp": "2026-01-19T14:18:28.985662"}
{"image_index": 38, "image_name": "000000003553.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003553.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1809.937, "latencies_ms": [1809.937], "images_per_second": 0.553, "prompt_tokens": 1099, "response_tokens_est": 28, "n_tiles": 1, "output_text": " A person is riding a skateboard on a wooden ramp, wearing black and white sneakers with a checkered pattern on the bottom.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26778.7, "ram_available_mb": 98993.5, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26765.1, "ram_available_mb": 99007.1, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.19, "peak": 40.97, "min": 20.49}, "VIN": {"avg": 74.54, "peak": 120.67, "min": 29.23}, "VIN_SYS_5V0": {"avg": 14.92, "peak": 16.05, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 16.1, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 31.19, "energy_joules_est": 56.47, "sample_count": 18, "duration_seconds": 1.811}, "timestamp": "2026-01-19T14:18:30.868981"}
{"image_index": 38, "image_name": "000000003553.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003553.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2872.507, "latencies_ms": [2872.507], "images_per_second": 0.348, "prompt_tokens": 1113, "response_tokens_est": 69, "n_tiles": 1, "output_text": " 1. person: 1\n2. skateboard: 1\n3. wooden plank: 1\n4. grass: 1\n5. wooden board: 1\n6. white letters: 1\n7. black and white checkered pattern: 1\n8. black and white shoes: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26765.1, "ram_available_mb": 99007.1, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 26765.4, "ram_available_mb": 99006.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.51, "peak": 40.18, "min": 17.74}, "VIN": {"avg": 69.61, "peak": 119.49, "min": 29.23}, "VIN_SYS_5V0": {"avg": 14.86, "peak": 16.05, "min": 12.15}, "VDD_CPU_SOC_MSS": {"avg": 15.89, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 26.51, "energy_joules_est": 76.16, "sample_count": 28, "duration_seconds": 2.873}, "timestamp": "2026-01-19T14:18:33.784190"}
{"image_index": 38, "image_name": "000000003553.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003553.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2102.856, "latencies_ms": [2102.856], "images_per_second": 0.476, "prompt_tokens": 1117, "response_tokens_est": 39, "n_tiles": 1, "output_text": " The skateboard is in the foreground, with the person's feet on it. The person is wearing jeans, which are in the foreground. The background is a grassy area with a wooden structure.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26765.4, "ram_available_mb": 99006.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26765.9, "ram_available_mb": 99006.3, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.42, "peak": 40.18, "min": 16.16}, "VIN": {"avg": 70.37, "peak": 120.53, "min": 28.09}, "VIN_SYS_5V0": {"avg": 14.88, "peak": 16.05, "min": 12.46}, "VDD_CPU_SOC_MSS": {"avg": 16.01, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 28.42, "energy_joules_est": 59.78, "sample_count": 21, "duration_seconds": 2.103}, "timestamp": "2026-01-19T14:18:35.970661"}
{"image_index": 38, "image_name": "000000003553.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003553.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1519.772, "latencies_ms": [1519.772], "images_per_second": 0.658, "prompt_tokens": 1111, "response_tokens_est": 16, "n_tiles": 1, "output_text": " A person is riding a skateboard on a wooden ramp in a grassy area.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26765.9, "ram_available_mb": 99006.3, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 26765.9, "ram_available_mb": 99006.3, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.83, "peak": 40.18, "min": 16.16}, "VIN": {"avg": 69.84, "peak": 109.06, "min": 29.22}, "VIN_SYS_5V0": {"avg": 14.71, "peak": 16.05, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.83, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 30.83, "energy_joules_est": 46.87, "sample_count": 15, "duration_seconds": 1.52}, "timestamp": "2026-01-19T14:18:37.539171"}
{"image_index": 38, "image_name": "000000003553.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003553.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2117.518, "latencies_ms": [2117.518], "images_per_second": 0.472, "prompt_tokens": 1109, "response_tokens_est": 40, "n_tiles": 1, "output_text": " The image features a person wearing black and white sneakers with a checkered pattern on the bottom, standing on a wooden ramp. The lighting is natural, and the weather appears to be sunny.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26765.9, "ram_available_mb": 99006.3, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26765.9, "ram_available_mb": 99006.3, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.42, "peak": 40.97, "min": 18.92}, "VIN": {"avg": 72.33, "peak": 123.28, "min": 29.38}, "VIN_SYS_5V0": {"avg": 14.9, "peak": 15.95, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.9, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 29.42, "energy_joules_est": 62.32, "sample_count": 21, "duration_seconds": 2.118}, "timestamp": "2026-01-19T14:18:39.725334"}
{"image_index": 39, "image_name": "000000003661.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003661.jpg", "image_width": 640, "image_height": 384, "image_resolution": "640x384", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1509.278, "latencies_ms": [1509.278], "images_per_second": 0.663, "prompt_tokens": 1099, "response_tokens_est": 16, "n_tiles": 1, "output_text": " A bunch of bananas are on a desk with a computer keyboard in the background.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 26765.9, "ram_available_mb": 99006.3, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26766.3, "ram_available_mb": 99005.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.89, "peak": 40.18, "min": 16.16}, "VIN": {"avg": 71.36, "peak": 125.3, "min": 29.78}, "VIN_SYS_5V0": {"avg": 14.76, "peak": 15.95, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 15.88, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 30.89, "energy_joules_est": 46.65, "sample_count": 15, "duration_seconds": 1.51}, "timestamp": "2026-01-19T14:18:41.296245"}
{"image_index": 39, "image_name": "000000003661.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003661.jpg", "image_width": 640, "image_height": 384, "image_resolution": "640x384", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1196.937, "latencies_ms": [1196.937], "images_per_second": 0.835, "prompt_tokens": 1113, "response_tokens_est": 4, "n_tiles": 1, "output_text": " banana: 4", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26766.3, "ram_available_mb": 99005.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.2, "ram_used_mb": 26766.0, "ram_available_mb": 99006.2, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 33.23, "peak": 40.57, "min": 20.11}, "VIN": {"avg": 78.92, "peak": 130.53, "min": 27.83}, "VIN_SYS_5V0": {"avg": 14.8, "peak": 16.05, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.95, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 33.23, "energy_joules_est": 39.79, "sample_count": 12, "duration_seconds": 1.197}, "timestamp": "2026-01-19T14:18:42.552711"}
{"image_index": 39, "image_name": "000000003661.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003661.jpg", "image_width": 640, "image_height": 384, "image_resolution": "640x384", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2235.381, "latencies_ms": [2235.381], "images_per_second": 0.447, "prompt_tokens": 1117, "response_tokens_est": 44, "n_tiles": 1, "output_text": " The bananas are located in the foreground of the image, with the keyboard and computer monitor in the background. The bananas are positioned to the left of the keyboard, and the computer monitor is to the right of the keyboard.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26766.0, "ram_available_mb": 99006.2, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26766.0, "ram_available_mb": 99006.2, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.05, "peak": 41.36, "min": 19.31}, "VIN": {"avg": 72.63, "peak": 119.75, "min": 29.29}, "VIN_SYS_5V0": {"avg": 14.87, "peak": 16.05, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 16.05, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 30.05, "energy_joules_est": 67.19, "sample_count": 22, "duration_seconds": 2.236}, "timestamp": "2026-01-19T14:18:44.839862"}
{"image_index": 39, "image_name": "000000003661.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003661.jpg", "image_width": 640, "image_height": 384, "image_resolution": "640x384", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1500.023, "latencies_ms": [1500.023], "images_per_second": 0.667, "prompt_tokens": 1111, "response_tokens_est": 16, "n_tiles": 1, "output_text": " A bunch of bananas are on a desk with a computer keyboard in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26766.0, "ram_available_mb": 99006.2, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 26766.0, "ram_available_mb": 99006.2, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.2, "peak": 40.18, "min": 16.56}, "VIN": {"avg": 72.46, "peak": 104.56, "min": 29.22}, "VIN_SYS_5V0": {"avg": 14.9, "peak": 16.05, "min": 12.36}, "VDD_CPU_SOC_MSS": {"avg": 15.98, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 31.2, "energy_joules_est": 46.81, "sample_count": 15, "duration_seconds": 1.5}, "timestamp": "2026-01-19T14:18:46.407239"}
{"image_index": 39, "image_name": "000000003661.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003661.jpg", "image_width": 640, "image_height": 384, "image_resolution": "640x384", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1480.977, "latencies_ms": [1480.977], "images_per_second": 0.675, "prompt_tokens": 1109, "response_tokens_est": 15, "n_tiles": 1, "output_text": " The bananas are yellow and ripe, and the table is made of wood.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26766.0, "ram_available_mb": 99006.2, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 26765.7, "ram_available_mb": 99006.4, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.07, "peak": 40.57, "min": 20.11}, "VIN": {"avg": 80.88, "peak": 124.2, "min": 27.95}, "VIN_SYS_5V0": {"avg": 14.79, "peak": 16.05, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.93, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 32.07, "energy_joules_est": 47.52, "sample_count": 15, "duration_seconds": 1.482}, "timestamp": "2026-01-19T14:18:47.968659"}
{"image_index": 40, "image_name": "000000003845.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003845.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1494.986, "latencies_ms": [1494.986], "images_per_second": 0.669, "prompt_tokens": 1099, "response_tokens_est": 16, "n_tiles": 1, "output_text": " A plate of food with rice, vegetables, and chicken is on a table.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 26765.7, "ram_available_mb": 99006.4, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 26765.7, "ram_available_mb": 99006.4, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.12, "peak": 40.57, "min": 19.71}, "VIN": {"avg": 71.16, "peak": 107.48, "min": 27.81}, "VIN_SYS_5V0": {"avg": 14.78, "peak": 15.95, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.85, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 32.12, "energy_joules_est": 48.04, "sample_count": 15, "duration_seconds": 1.496}, "timestamp": "2026-01-19T14:18:49.536612"}
{"image_index": 40, "image_name": "000000003845.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003845.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2087.485, "latencies_ms": [2087.485], "images_per_second": 0.479, "prompt_tokens": 1113, "response_tokens_est": 39, "n_tiles": 1, "output_text": " plate: 1, fork: 1, knife: 1, glass: 1, rice: 1, carrots: 1, broccoli: 1, cauliflower: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26765.7, "ram_available_mb": 99006.4, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26765.7, "ram_available_mb": 99006.4, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.21, "peak": 40.57, "min": 18.52}, "VIN": {"avg": 72.67, "peak": 125.04, "min": 28.25}, "VIN_SYS_5V0": {"avg": 14.84, "peak": 15.95, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.99, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 29.21, "energy_joules_est": 60.99, "sample_count": 21, "duration_seconds": 2.088}, "timestamp": "2026-01-19T14:18:51.726379"}
{"image_index": 40, "image_name": "000000003845.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003845.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2099.549, "latencies_ms": [2099.549], "images_per_second": 0.476, "prompt_tokens": 1117, "response_tokens_est": 39, "n_tiles": 1, "output_text": " The plate is located in the foreground, with the fork and spoon placed on the left side of the plate. The glass of water is positioned in the background, to the left of the plate.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26765.7, "ram_available_mb": 99006.4, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26765.4, "ram_available_mb": 99006.7, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.31, "peak": 39.78, "min": 15.77}, "VIN": {"avg": 72.67, "peak": 121.6, "min": 28.27}, "VIN_SYS_5V0": {"avg": 14.76, "peak": 15.95, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.75, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 28.31, "energy_joules_est": 59.45, "sample_count": 21, "duration_seconds": 2.1}, "timestamp": "2026-01-19T14:18:53.908666"}
{"image_index": 40, "image_name": "000000003845.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003845.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1454.694, "latencies_ms": [1454.694], "images_per_second": 0.687, "prompt_tokens": 1111, "response_tokens_est": 14, "n_tiles": 1, "output_text": " A plate of food is on a table with a glass of water.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26765.4, "ram_available_mb": 99006.7, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 26765.4, "ram_available_mb": 99006.7, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.49, "peak": 40.18, "min": 16.16}, "VIN": {"avg": 80.89, "peak": 129.96, "min": 31.65}, "VIN_SYS_5V0": {"avg": 14.84, "peak": 16.05, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 15.89, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 31.49, "energy_joules_est": 45.82, "sample_count": 14, "duration_seconds": 1.455}, "timestamp": "2026-01-19T14:18:55.366540"}
{"image_index": 40, "image_name": "000000003845.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003845.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1515.473, "latencies_ms": [1515.473], "images_per_second": 0.66, "prompt_tokens": 1109, "response_tokens_est": 18, "n_tiles": 1, "output_text": " The plate is white and red, and the food is colorful. The lighting is natural.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26765.4, "ram_available_mb": 99006.7, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26765.4, "ram_available_mb": 99006.7, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 33.38, "peak": 40.57, "min": 24.04}, "VIN": {"avg": 74.12, "peak": 102.48, "min": 28.51}, "VIN_SYS_5V0": {"avg": 15.36, "peak": 16.26, "min": 13.69}, "VDD_CPU_SOC_MSS": {"avg": 16.46, "peak": 17.71, "min": 14.57}}, "power_watts_avg": 33.38, "energy_joules_est": 50.61, "sample_count": 15, "duration_seconds": 1.516}, "timestamp": "2026-01-19T14:18:56.934346"}
{"image_index": 41, "image_name": "000000003934.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003934.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1927.327, "latencies_ms": [1927.327], "images_per_second": 0.519, "prompt_tokens": 1100, "response_tokens_est": 33, "n_tiles": 1, "output_text": " A young girl in a colorful dress is playing with a Wii remote in a living room with a couch, a coffee table, and a staircase in the background.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 26765.4, "ram_available_mb": 99006.7, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26765.4, "ram_available_mb": 99006.7, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.11, "peak": 40.57, "min": 20.1}, "VIN": {"avg": 68.51, "peak": 89.07, "min": 30.44}, "VIN_SYS_5V0": {"avg": 14.96, "peak": 16.05, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 16.12, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 30.11, "energy_joules_est": 58.04, "sample_count": 19, "duration_seconds": 1.928}, "timestamp": "2026-01-19T14:18:58.919740"}
{"image_index": 41, "image_name": "000000003934.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003934.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2723.796, "latencies_ms": [2723.796], "images_per_second": 0.367, "prompt_tokens": 1114, "response_tokens_est": 57, "n_tiles": 1, "output_text": " 1. couch: 1\n2. rug: 1\n3. chair: 1\n4. table: 1\n5. sofa: 1\n6. person: 1\n7. person: 1\n8. person: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26765.4, "ram_available_mb": 99006.7, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 26766.2, "ram_available_mb": 99006.0, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.02, "peak": 40.18, "min": 16.95}, "VIN": {"avg": 68.07, "peak": 118.99, "min": 28.73}, "VIN_SYS_5V0": {"avg": 14.66, "peak": 16.15, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.61, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 26.02, "energy_joules_est": 70.88, "sample_count": 27, "duration_seconds": 2.724}, "timestamp": "2026-01-19T14:19:01.735596"}
{"image_index": 41, "image_name": "000000003934.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003934.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2143.075, "latencies_ms": [2143.075], "images_per_second": 0.467, "prompt_tokens": 1118, "response_tokens_est": 41, "n_tiles": 1, "output_text": " The couch is located in the background, with the girl standing in the foreground. The woman in the white dress is standing near the couch, while the man in the green shirt is standing near the bar.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26766.2, "ram_available_mb": 99006.0, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26766.2, "ram_available_mb": 99006.0, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.03, "peak": 39.78, "min": 14.98}, "VIN": {"avg": 74.38, "peak": 124.54, "min": 28.71}, "VIN_SYS_5V0": {"avg": 14.75, "peak": 16.05, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.75, "peak": 17.32, "min": 12.61}}, "power_watts_avg": 28.03, "energy_joules_est": 60.08, "sample_count": 21, "duration_seconds": 2.144}, "timestamp": "2026-01-19T14:19:03.934202"}
{"image_index": 41, "image_name": "000000003934.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003934.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1728.227, "latencies_ms": [1728.227], "images_per_second": 0.579, "prompt_tokens": 1112, "response_tokens_est": 26, "n_tiles": 1, "output_text": " A group of people are gathered in a living room, with a young girl playing with a toy in the center of the room.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26766.2, "ram_available_mb": 99006.0, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26766.2, "ram_available_mb": 99006.0, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.01, "peak": 39.39, "min": 16.56}, "VIN": {"avg": 68.65, "peak": 97.24, "min": 28.91}, "VIN_SYS_5V0": {"avg": 14.98, "peak": 16.15, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 16.07, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 30.01, "energy_joules_est": 51.87, "sample_count": 17, "duration_seconds": 1.729}, "timestamp": "2026-01-19T14:19:05.711003"}
{"image_index": 41, "image_name": "000000003934.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003934.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1624.059, "latencies_ms": [1624.059], "images_per_second": 0.616, "prompt_tokens": 1110, "response_tokens_est": 22, "n_tiles": 1, "output_text": " The room is well-lit with natural light coming from the windows, and the furniture is made of wood.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26766.2, "ram_available_mb": 99006.0, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26766.9, "ram_available_mb": 99005.3, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.25, "peak": 40.18, "min": 18.14}, "VIN": {"avg": 73.87, "peak": 117.77, "min": 28.78}, "VIN_SYS_5V0": {"avg": 15.1, "peak": 16.15, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 16.19, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 31.25, "energy_joules_est": 50.76, "sample_count": 16, "duration_seconds": 1.624}, "timestamp": "2026-01-19T14:19:07.384493"}
{"image_index": 42, "image_name": "000000004134.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000004134.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1443.414, "latencies_ms": [1443.414], "images_per_second": 0.693, "prompt_tokens": 1099, "response_tokens_est": 14, "n_tiles": 1, "output_text": " Two men are shaking hands in a room with other people and tables.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 26766.9, "ram_available_mb": 99005.3, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 26766.9, "ram_available_mb": 99005.3, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.44, "peak": 40.57, "min": 18.91}, "VIN": {"avg": 77.73, "peak": 127.13, "min": 28.98}, "VIN_SYS_5V0": {"avg": 14.96, "peak": 16.05, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 16.09, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 32.44, "energy_joules_est": 46.85, "sample_count": 14, "duration_seconds": 1.444}, "timestamp": "2026-01-19T14:19:08.856484"}
{"image_index": 42, "image_name": "000000004134.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000004134.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2509.121, "latencies_ms": [2509.121], "images_per_second": 0.399, "prompt_tokens": 1113, "response_tokens_est": 56, "n_tiles": 1, "output_text": " 1. man: 2\n2. man: 1\n3. man: 1\n4. man: 1\n5. man: 1\n6. man: 1\n7. man: 1\n8. man: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26766.9, "ram_available_mb": 99005.3, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 26767.4, "ram_available_mb": 99004.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.21, "peak": 40.95, "min": 17.74}, "VIN": {"avg": 63.66, "peak": 114.55, "min": 27.37}, "VIN_SYS_5V0": {"avg": 15.07, "peak": 16.15, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 16.11, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 28.21, "energy_joules_est": 70.79, "sample_count": 25, "duration_seconds": 2.51}, "timestamp": "2026-01-19T14:19:11.454348"}
{"image_index": 42, "image_name": "000000004134.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000004134.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2922.64, "latencies_ms": [2922.64], "images_per_second": 0.342, "prompt_tokens": 1117, "response_tokens_est": 68, "n_tiles": 1, "output_text": " The man in the dark suit is standing to the right of the man in the patterned shirt, and the man in the dark suit is in the foreground of the image. The man in the patterned shirt is standing to the left of the man in the dark suit, and the man in the patterned shirt is in the background of the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26767.4, "ram_available_mb": 99004.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 26767.6, "ram_available_mb": 99004.5, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 25.72, "peak": 40.18, "min": 14.98}, "VIN": {"avg": 66.57, "peak": 119.51, "min": 28.47}, "VIN_SYS_5V0": {"avg": 14.7, "peak": 15.95, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.64, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 25.72, "energy_joules_est": 75.18, "sample_count": 29, "duration_seconds": 2.923}, "timestamp": "2026-01-19T14:19:14.475707"}
{"image_index": 42, "image_name": "000000004134.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000004134.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1469.548, "latencies_ms": [1469.548], "images_per_second": 0.68, "prompt_tokens": 1111, "response_tokens_est": 14, "n_tiles": 1, "output_text": " Two men are shaking hands in a large room with many other people.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26767.6, "ram_available_mb": 99004.5, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 26768.4, "ram_available_mb": 99003.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.31, "peak": 39.78, "min": 14.59}, "VIN": {"avg": 72.35, "peak": 128.87, "min": 28.38}, "VIN_SYS_5V0": {"avg": 14.56, "peak": 15.85, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 15.62, "peak": 16.93, "min": 12.61}}, "power_watts_avg": 30.31, "energy_joules_est": 44.56, "sample_count": 15, "duration_seconds": 1.47}, "timestamp": "2026-01-19T14:19:16.038842"}
{"image_index": 42, "image_name": "000000004134.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000004134.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1718.75, "latencies_ms": [1718.75], "images_per_second": 0.582, "prompt_tokens": 1109, "response_tokens_est": 23, "n_tiles": 1, "output_text": " The image is taken in a well-lit room with warm lighting, and the attendees are dressed in formal attire.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26768.4, "ram_available_mb": 99003.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26768.6, "ram_available_mb": 99003.6, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.75, "peak": 40.18, "min": 19.32}, "VIN": {"avg": 72.84, "peak": 114.91, "min": 28.56}, "VIN_SYS_5V0": {"avg": 14.72, "peak": 16.05, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.75, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 30.75, "energy_joules_est": 52.87, "sample_count": 17, "duration_seconds": 1.719}, "timestamp": "2026-01-19T14:19:17.810091"}
{"image_index": 43, "image_name": "000000004395.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000004395.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1514.068, "latencies_ms": [1514.068], "images_per_second": 0.66, "prompt_tokens": 1100, "response_tokens_est": 17, "n_tiles": 1, "output_text": " A man wearing a white shirt and a striped tie is standing in a dark room.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26768.6, "ram_available_mb": 99003.6, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 26768.6, "ram_available_mb": 99003.6, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.51, "peak": 40.16, "min": 18.53}, "VIN": {"avg": 75.54, "peak": 116.78, "min": 28.56}, "VIN_SYS_5V0": {"avg": 14.92, "peak": 16.15, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 16.01, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 31.51, "energy_joules_est": 47.72, "sample_count": 15, "duration_seconds": 1.514}, "timestamp": "2026-01-19T14:19:19.379472"}
{"image_index": 43, "image_name": "000000004395.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000004395.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2526.484, "latencies_ms": [2526.484], "images_per_second": 0.396, "prompt_tokens": 1114, "response_tokens_est": 56, "n_tiles": 1, "output_text": " 1. tie: 1\n2. shirt: 1\n3. hand: 1\n4. person: 1\n5. tie: 1\n6. shirt: 1\n7. hand: 1\n8. person: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26768.6, "ram_available_mb": 99003.6, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 26769.1, "ram_available_mb": 99003.1, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.72, "peak": 40.57, "min": 18.14}, "VIN": {"avg": 73.59, "peak": 125.32, "min": 29.2}, "VIN_SYS_5V0": {"avg": 14.95, "peak": 16.15, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.94, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 27.72, "energy_joules_est": 70.04, "sample_count": 25, "duration_seconds": 2.527}, "timestamp": "2026-01-19T14:19:21.981097"}
{"image_index": 43, "image_name": "000000004395.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000004395.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2065.286, "latencies_ms": [2065.286], "images_per_second": 0.484, "prompt_tokens": 1118, "response_tokens_est": 38, "n_tiles": 1, "output_text": " The man is standing in the foreground, wearing a white shirt and a striped tie. The background is dark and out of focus, suggesting that the man is in a dimly lit room.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26769.1, "ram_available_mb": 99003.1, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26769.4, "ram_available_mb": 99002.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.53, "peak": 39.78, "min": 15.38}, "VIN": {"avg": 75.4, "peak": 132.56, "min": 28.9}, "VIN_SYS_5V0": {"avg": 14.91, "peak": 16.05, "min": 12.15}, "VDD_CPU_SOC_MSS": {"avg": 15.96, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 28.53, "energy_joules_est": 58.93, "sample_count": 20, "duration_seconds": 2.066}, "timestamp": "2026-01-19T14:19:24.067142"}
{"image_index": 43, "image_name": "000000004395.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000004395.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1432.267, "latencies_ms": [1432.267], "images_per_second": 0.698, "prompt_tokens": 1112, "response_tokens_est": 15, "n_tiles": 1, "output_text": " A man wearing a tie and a shirt is standing in a dark room.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26769.4, "ram_available_mb": 99002.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26769.9, "ram_available_mb": 99002.3, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.83, "peak": 39.39, "min": 17.73}, "VIN": {"avg": 77.51, "peak": 123.91, "min": 29.75}, "VIN_SYS_5V0": {"avg": 15.26, "peak": 16.26, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 16.48, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 31.83, "energy_joules_est": 45.61, "sample_count": 14, "duration_seconds": 1.433}, "timestamp": "2026-01-19T14:19:25.532403"}
{"image_index": 43, "image_name": "000000004395.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000004395.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1628.309, "latencies_ms": [1628.309], "images_per_second": 0.614, "prompt_tokens": 1110, "response_tokens_est": 22, "n_tiles": 1, "output_text": " The man is wearing a white shirt and a striped tie. The lighting is dim and the background is dark.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26769.9, "ram_available_mb": 99002.3, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26770.3, "ram_available_mb": 99001.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.13, "peak": 40.57, "min": 21.28}, "VIN": {"avg": 68.16, "peak": 102.76, "min": 29.24}, "VIN_SYS_5V0": {"avg": 15.17, "peak": 16.15, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 16.29, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 32.13, "energy_joules_est": 52.33, "sample_count": 16, "duration_seconds": 1.629}, "timestamp": "2026-01-19T14:19:27.197522"}
{"image_index": 44, "image_name": "000000004495.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000004495.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1727.741, "latencies_ms": [1727.741], "images_per_second": 0.579, "prompt_tokens": 1099, "response_tokens_est": 26, "n_tiles": 1, "output_text": " The image shows a room with a television on a stand, a chair, and a couch, with a whiteboard on the wall.", "error": null, "sys_before": {"cpu_percent": 14.3, "ram_used_mb": 26770.3, "ram_available_mb": 99001.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26770.3, "ram_available_mb": 99001.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.31, "peak": 40.97, "min": 19.71}, "VIN": {"avg": 78.91, "peak": 131.84, "min": 31.77}, "VIN_SYS_5V0": {"avg": 15.14, "peak": 16.15, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 16.21, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 31.31, "energy_joules_est": 54.12, "sample_count": 17, "duration_seconds": 1.729}, "timestamp": "2026-01-19T14:19:28.969014"}
{"image_index": 44, "image_name": "000000004495.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000004495.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1685.639, "latencies_ms": [1685.639], "images_per_second": 0.593, "prompt_tokens": 1113, "response_tokens_est": 24, "n_tiles": 1, "output_text": " chair: 1, television: 1, couch: 2, poster: 1, whiteboard: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26770.3, "ram_available_mb": 99001.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26771.3, "ram_available_mb": 99000.9, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.94, "peak": 40.57, "min": 18.53}, "VIN": {"avg": 74.39, "peak": 127.08, "min": 27.33}, "VIN_SYS_5V0": {"avg": 15.06, "peak": 16.15, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 16.12, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 30.94, "energy_joules_est": 52.16, "sample_count": 17, "duration_seconds": 1.686}, "timestamp": "2026-01-19T14:19:30.742672"}
{"image_index": 44, "image_name": "000000004495.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000004495.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2576.633, "latencies_ms": [2576.633], "images_per_second": 0.388, "prompt_tokens": 1117, "response_tokens_est": 58, "n_tiles": 1, "output_text": " The red armchair is positioned to the left of the television, which is placed on a wooden stand in the center of the room. The blue and red plaid couch is located to the right of the television, and the whiteboard is mounted on the wall to the left of the television.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.3, "ram_available_mb": 99000.9, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 26771.3, "ram_available_mb": 99000.9, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.39, "peak": 40.57, "min": 17.74}, "VIN": {"avg": 67.81, "peak": 118.0, "min": 28.51}, "VIN_SYS_5V0": {"avg": 14.94, "peak": 16.05, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.86, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 27.39, "energy_joules_est": 70.59, "sample_count": 25, "duration_seconds": 2.577}, "timestamp": "2026-01-19T14:19:33.359345"}
{"image_index": 44, "image_name": "000000004495.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000004495.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1375.857, "latencies_ms": [1375.857], "images_per_second": 0.727, "prompt_tokens": 1111, "response_tokens_est": 12, "n_tiles": 1, "output_text": " A room with a TV, chairs, and a whiteboard.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.3, "ram_available_mb": 99000.9, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 26771.3, "ram_available_mb": 99000.9, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.74, "peak": 40.56, "min": 16.55}, "VIN": {"avg": 80.47, "peak": 125.59, "min": 28.7}, "VIN_SYS_5V0": {"avg": 15.05, "peak": 16.15, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 16.14, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 31.74, "energy_joules_est": 43.68, "sample_count": 14, "duration_seconds": 1.376}, "timestamp": "2026-01-19T14:19:34.820078"}
{"image_index": 44, "image_name": "000000004495.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000004495.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1397.902, "latencies_ms": [1397.902], "images_per_second": 0.715, "prompt_tokens": 1109, "response_tokens_est": 12, "n_tiles": 1, "output_text": " The room is painted yellow and has a plaid couch.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.3, "ram_available_mb": 99000.9, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26771.2, "ram_available_mb": 99001.0, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 33.01, "peak": 40.97, "min": 20.89}, "VIN": {"avg": 75.0, "peak": 124.23, "min": 28.75}, "VIN_SYS_5V0": {"avg": 14.81, "peak": 16.05, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.86, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 33.01, "energy_joules_est": 46.16, "sample_count": 14, "duration_seconds": 1.398}, "timestamp": "2026-01-19T14:19:36.281245"}
{"image_index": 45, "image_name": "000000004765.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000004765.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1936.997, "latencies_ms": [1936.997], "images_per_second": 0.516, "prompt_tokens": 1432, "response_tokens_est": 18, "n_tiles": 1, "output_text": " A man wearing a yellow shirt and black pants is riding a wave on a surfboard.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.2, "ram_available_mb": 99001.0, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.5, "ram_used_mb": 26771.2, "ram_available_mb": 99001.0, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.2, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 33.26, "peak": 40.57, "min": 21.67}, "VIN": {"avg": 78.68, "peak": 125.04, "min": 29.47}, "VIN_SYS_5V0": {"avg": 15.45, "peak": 16.76, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 16.51, "peak": 17.71, "min": 13.39}}, "power_watts_avg": 33.26, "energy_joules_est": 64.43, "sample_count": 19, "duration_seconds": 1.937}, "timestamp": "2026-01-19T14:19:38.254893"}
{"image_index": 45, "image_name": "000000004765.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000004765.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3479.38, "latencies_ms": [3479.38], "images_per_second": 0.287, "prompt_tokens": 1446, "response_tokens_est": 76, "n_tiles": 1, "output_text": " 1. Surfer: 1\n2. Surfboard: 1\n3. Wetsuit: 1\n4. Wetsuit sleeve: 1\n5. Wetsuit leg: 1\n6. Wetsuit arm: 1\n7. Wetsuit chest: 1\n8. Wetsuit back: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.2, "ram_available_mb": 99001.0, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26771.2, "ram_available_mb": 99001.0, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.75, "peak": 40.18, "min": 18.52}, "VIN": {"avg": 69.68, "peak": 123.11, "min": 28.56}, "VIN_SYS_5V0": {"avg": 15.25, "peak": 16.76, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 16.25, "peak": 17.71, "min": 14.18}}, "power_watts_avg": 27.75, "energy_joules_est": 96.57, "sample_count": 34, "duration_seconds": 3.48}, "timestamp": "2026-01-19T14:19:41.788535"}
{"image_index": 45, "image_name": "000000004765.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000004765.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2633.531, "latencies_ms": [2633.531], "images_per_second": 0.38, "prompt_tokens": 1450, "response_tokens_est": 44, "n_tiles": 1, "output_text": " The surfer is positioned in the foreground, riding a wave that is in the middle of the image. The surfer is wearing a yellow shirt and black pants, and is positioned on the right side of the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.2, "ram_available_mb": 99001.0, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26771.2, "ram_available_mb": 99001.0, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.31, "peak": 40.18, "min": 16.16}, "VIN": {"avg": 71.16, "peak": 118.6, "min": 29.12}, "VIN_SYS_5V0": {"avg": 15.16, "peak": 16.56, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 16.19, "peak": 17.71, "min": 13.39}}, "power_watts_avg": 29.31, "energy_joules_est": 77.21, "sample_count": 26, "duration_seconds": 2.634}, "timestamp": "2026-01-19T14:19:44.497526"}
{"image_index": 45, "image_name": "000000004765.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000004765.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2030.87, "latencies_ms": [2030.87], "images_per_second": 0.492, "prompt_tokens": 1444, "response_tokens_est": 21, "n_tiles": 1, "output_text": " A man wearing a yellow shirt and black pants is riding a wave on a surfboard in the ocean.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.2, "ram_available_mb": 99001.0, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 26771.2, "ram_available_mb": 99001.0, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.54, "peak": 40.18, "min": 16.16}, "VIN": {"avg": 74.9, "peak": 117.88, "min": 28.85}, "VIN_SYS_5V0": {"avg": 15.11, "peak": 16.56, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 16.18, "peak": 17.71, "min": 13.0}}, "power_watts_avg": 31.54, "energy_joules_est": 64.06, "sample_count": 20, "duration_seconds": 2.031}, "timestamp": "2026-01-19T14:19:46.595552"}
{"image_index": 45, "image_name": "000000004765.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000004765.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2042.895, "latencies_ms": [2042.895], "images_per_second": 0.49, "prompt_tokens": 1442, "response_tokens_est": 22, "n_tiles": 1, "output_text": " The surfer is wearing a yellow shirt and black pants, and the water is a greenish-blue color.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.2, "ram_available_mb": 99001.0, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 26771.2, "ram_available_mb": 99001.0, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.11, "peak": 40.18, "min": 18.53}, "VIN": {"avg": 74.47, "peak": 123.78, "min": 29.49}, "VIN_SYS_5V0": {"avg": 15.17, "peak": 16.56, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 16.3, "peak": 17.71, "min": 13.0}}, "power_watts_avg": 32.11, "energy_joules_est": 65.61, "sample_count": 20, "duration_seconds": 2.043}, "timestamp": "2026-01-19T14:19:48.681282"}
{"image_index": 46, "image_name": "000000004795.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000004795.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1505.41, "latencies_ms": [1505.41], "images_per_second": 0.664, "prompt_tokens": 1099, "response_tokens_est": 17, "n_tiles": 1, "output_text": " A black cat is sitting in front of a computer monitor, looking at the screen.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 26771.2, "ram_available_mb": 99001.0, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 26771.2, "ram_available_mb": 99001.0, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.04, "peak": 40.97, "min": 19.32}, "VIN": {"avg": 67.82, "peak": 121.61, "min": 28.95}, "VIN_SYS_5V0": {"avg": 15.01, "peak": 16.05, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 16.17, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 32.04, "energy_joules_est": 48.26, "sample_count": 15, "duration_seconds": 1.506}, "timestamp": "2026-01-19T14:19:50.256387"}
{"image_index": 46, "image_name": "000000004795.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000004795.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1977.969, "latencies_ms": [1977.969], "images_per_second": 0.506, "prompt_tokens": 1113, "response_tokens_est": 35, "n_tiles": 1, "output_text": " 1. black cat\n2. laptop\n3. computer monitor\n4. keyboard\n5. mouse\n6. phone\n7. mousepad\n8. desk", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.2, "ram_available_mb": 99001.0, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26771.2, "ram_available_mb": 99001.0, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.8, "peak": 40.97, "min": 18.52}, "VIN": {"avg": 72.93, "peak": 119.82, "min": 27.74}, "VIN_SYS_5V0": {"avg": 14.99, "peak": 16.05, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.94, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 29.8, "energy_joules_est": 58.96, "sample_count": 20, "duration_seconds": 1.978}, "timestamp": "2026-01-19T14:19:52.337523"}
{"image_index": 46, "image_name": "000000004795.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000004795.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2242.35, "latencies_ms": [2242.35], "images_per_second": 0.446, "prompt_tokens": 1117, "response_tokens_est": 45, "n_tiles": 1, "output_text": " The cat is in the foreground, looking at the computer screen. The laptop is on the left side of the cat, and the phone is on the right side. The cat is closer to the computer screen than the laptop.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.2, "ram_available_mb": 99001.0, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26771.2, "ram_available_mb": 99001.0, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.13, "peak": 40.18, "min": 16.16}, "VIN": {"avg": 70.51, "peak": 102.27, "min": 30.32}, "VIN_SYS_5V0": {"avg": 14.86, "peak": 16.05, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.84, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 28.13, "energy_joules_est": 63.09, "sample_count": 22, "duration_seconds": 2.243}, "timestamp": "2026-01-19T14:19:54.623733"}
{"image_index": 46, "image_name": "000000004795.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000004795.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1514.599, "latencies_ms": [1514.599], "images_per_second": 0.66, "prompt_tokens": 1111, "response_tokens_est": 17, "n_tiles": 1, "output_text": " A black cat is sitting in front of a computer monitor, looking at the screen.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.2, "ram_available_mb": 99001.0, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 26771.2, "ram_available_mb": 99001.0, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.31, "peak": 40.18, "min": 16.95}, "VIN": {"avg": 75.35, "peak": 127.31, "min": 30.2}, "VIN_SYS_5V0": {"avg": 15.01, "peak": 16.05, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 16.17, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 31.31, "energy_joules_est": 47.43, "sample_count": 15, "duration_seconds": 1.515}, "timestamp": "2026-01-19T14:19:56.200638"}
{"image_index": 46, "image_name": "000000004795.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000004795.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2719.891, "latencies_ms": [2719.891], "images_per_second": 0.368, "prompt_tokens": 1109, "response_tokens_est": 63, "n_tiles": 1, "output_text": " The image is a close-up of a black cat sitting in front of a computer screen, with the cat's head turned to the side, and the screen displaying a webpage with text. The lighting in the image is bright, and the cat's fur is fluffy and appears to be well-groomed.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 26771.2, "ram_available_mb": 99001.0, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26771.2, "ram_available_mb": 99001.0, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.1, "peak": 40.97, "min": 17.35}, "VIN": {"avg": 69.22, "peak": 89.24, "min": 27.64}, "VIN_SYS_5V0": {"avg": 14.93, "peak": 16.05, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.85, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 27.1, "energy_joules_est": 73.72, "sample_count": 27, "duration_seconds": 2.72}, "timestamp": "2026-01-19T14:19:59.016697"}
{"image_index": 47, "image_name": "000000005001.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005001.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2388.574, "latencies_ms": [2388.574], "images_per_second": 0.419, "prompt_tokens": 1099, "response_tokens_est": 48, "n_tiles": 1, "output_text": " A group of people, including a man in a blue shirt and a man in a blue hat, are cutting a red ribbon with a pair of scissors, while a child in a pink jacket and a woman in a black coat stand nearby.", "error": null, "sys_before": {"cpu_percent": 7.1, "ram_used_mb": 26771.2, "ram_available_mb": 99001.0, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26771.2, "ram_available_mb": 99001.0, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.29, "peak": 40.18, "min": 14.98}, "VIN": {"avg": 68.67, "peak": 121.34, "min": 30.31}, "VIN_SYS_5V0": {"avg": 14.75, "peak": 15.95, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.75, "peak": 16.93, "min": 12.61}}, "power_watts_avg": 27.29, "energy_joules_est": 65.2, "sample_count": 23, "duration_seconds": 2.389}, "timestamp": "2026-01-19T14:20:01.421930"}
{"image_index": 47, "image_name": "000000005001.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005001.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2704.891, "latencies_ms": [2704.891], "images_per_second": 0.37, "prompt_tokens": 1113, "response_tokens_est": 64, "n_tiles": 1, "output_text": " 1. People: 10\n2. Balloons: 2\n3. Helmet: 2\n4. Helmet: 1\n5. Helmet: 1\n6. Helmet: 1\n7. Helmet: 1\n8. Helmet: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.2, "ram_available_mb": 99001.0, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 26771.2, "ram_available_mb": 99001.0, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.81, "peak": 40.18, "min": 17.35}, "VIN": {"avg": 70.75, "peak": 103.47, "min": 27.24}, "VIN_SYS_5V0": {"avg": 15.11, "peak": 16.15, "min": 13.17}, "VDD_CPU_SOC_MSS": {"avg": 16.11, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 26.81, "energy_joules_est": 72.53, "sample_count": 27, "duration_seconds": 2.705}, "timestamp": "2026-01-19T14:20:04.227751"}
{"image_index": 47, "image_name": "000000005001.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005001.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3846.391, "latencies_ms": [3846.391], "images_per_second": 0.26, "prompt_tokens": 1117, "response_tokens_est": 106, "n_tiles": 1, "output_text": " The main objects are located in the foreground of the image, with the man in the blue shirt and the child in the pink jacket being the closest to the camera. The man in the blue shirt is holding the scissors, while the child is holding the ribbon. The man in the blue shirt is standing to the left of the child, and the man in the black jacket is standing to the right of the child. The man in the blue shirt is also holding a blue balloon, which is located to the left of the child.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.2, "ram_available_mb": 99001.0, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 26771.2, "ram_available_mb": 99001.0, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 24.33, "peak": 40.18, "min": 14.98}, "VIN": {"avg": 68.08, "peak": 123.05, "min": 29.2}, "VIN_SYS_5V0": {"avg": 14.8, "peak": 15.95, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.76, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 24.33, "energy_joules_est": 93.59, "sample_count": 38, "duration_seconds": 3.847}, "timestamp": "2026-01-19T14:20:08.167342"}
{"image_index": 47, "image_name": "000000005001.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005001.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1612.672, "latencies_ms": [1612.672], "images_per_second": 0.62, "prompt_tokens": 1111, "response_tokens_est": 20, "n_tiles": 1, "output_text": " A group of people are gathered outside a building, cutting a red ribbon with a pair of scissors.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26771.2, "ram_available_mb": 99001.0, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26771.2, "ram_available_mb": 99001.0, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.19, "peak": 40.18, "min": 14.98}, "VIN": {"avg": 76.81, "peak": 125.2, "min": 28.63}, "VIN_SYS_5V0": {"avg": 14.75, "peak": 15.95, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.77, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 30.19, "energy_joules_est": 48.7, "sample_count": 16, "duration_seconds": 1.613}, "timestamp": "2026-01-19T14:20:09.836313"}
{"image_index": 47, "image_name": "000000005001.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005001.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2450.899, "latencies_ms": [2450.899], "images_per_second": 0.408, "prompt_tokens": 1109, "response_tokens_est": 54, "n_tiles": 1, "output_text": " The image is a vibrant and lively scene, with the colors of the people's clothing and the red ribbon being the most prominent. The lighting is natural, coming from the sun, and the weather appears to be clear, as there are no signs of rain or snow.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26771.2, "ram_available_mb": 99001.0, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 26771.2, "ram_available_mb": 99001.0, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.14, "peak": 40.57, "min": 18.91}, "VIN": {"avg": 71.25, "peak": 123.82, "min": 29.19}, "VIN_SYS_5V0": {"avg": 14.98, "peak": 16.05, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 16.18, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 28.14, "energy_joules_est": 68.98, "sample_count": 24, "duration_seconds": 2.451}, "timestamp": "2026-01-19T14:20:12.340439"}
{"image_index": 48, "image_name": "000000005037.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005037.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1954.142, "latencies_ms": [1954.142], "images_per_second": 0.512, "prompt_tokens": 1099, "response_tokens_est": 34, "n_tiles": 1, "output_text": " A white and pink bus with the number 65745 on the front is driving down a street with a sign that says \"First Group\" on top.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 26771.2, "ram_available_mb": 99001.0, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 26771.2, "ram_available_mb": 99001.0, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.36, "peak": 40.56, "min": 16.16}, "VIN": {"avg": 77.78, "peak": 128.85, "min": 28.92}, "VIN_SYS_5V0": {"avg": 14.92, "peak": 16.05, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 16.17, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 29.36, "energy_joules_est": 57.4, "sample_count": 19, "duration_seconds": 1.955}, "timestamp": "2026-01-19T14:20:14.320036"}
{"image_index": 48, "image_name": "000000005037.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005037.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2511.502, "latencies_ms": [2511.502], "images_per_second": 0.398, "prompt_tokens": 1113, "response_tokens_est": 56, "n_tiles": 1, "output_text": " 1. bus: 1\n2. people: 0\n3. buildings: 1\n4. flowers: 1\n5. road: 1\n6. sky: 1\n7. clouds: 1\n8. text: 1", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26771.2, "ram_available_mb": 99001.0, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 26771.2, "ram_available_mb": 99001.0, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.49, "peak": 40.57, "min": 17.35}, "VIN": {"avg": 67.26, "peak": 114.56, "min": 28.33}, "VIN_SYS_5V0": {"avg": 14.99, "peak": 16.15, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 16.2, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 27.49, "energy_joules_est": 69.05, "sample_count": 25, "duration_seconds": 2.512}, "timestamp": "2026-01-19T14:20:16.929689"}
{"image_index": 48, "image_name": "000000005037.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005037.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2045.587, "latencies_ms": [2045.587], "images_per_second": 0.489, "prompt_tokens": 1117, "response_tokens_est": 37, "n_tiles": 1, "output_text": " The bus is positioned on the left side of the image, with the street and buildings in the background. The bus is in the foreground, with the sidewalk and people in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.2, "ram_available_mb": 99001.0, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 26771.2, "ram_available_mb": 99001.0, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.6, "peak": 39.78, "min": 14.98}, "VIN": {"avg": 70.65, "peak": 116.16, "min": 31.0}, "VIN_SYS_5V0": {"avg": 14.76, "peak": 15.95, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.98, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 28.6, "energy_joules_est": 58.52, "sample_count": 20, "duration_seconds": 2.046}, "timestamp": "2026-01-19T14:20:19.020863"}
{"image_index": 48, "image_name": "000000005037.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005037.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1550.217, "latencies_ms": [1550.217], "images_per_second": 0.645, "prompt_tokens": 1111, "response_tokens_est": 18, "n_tiles": 1, "output_text": " A white and pink bus is driving down a street with buildings and people in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.2, "ram_available_mb": 99001.0, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 26771.2, "ram_available_mb": 99001.0, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.44, "peak": 40.57, "min": 17.34}, "VIN": {"avg": 74.87, "peak": 111.21, "min": 28.81}, "VIN_SYS_5V0": {"avg": 15.02, "peak": 16.15, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 16.22, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 31.44, "energy_joules_est": 48.75, "sample_count": 15, "duration_seconds": 1.55}, "timestamp": "2026-01-19T14:20:20.592723"}
{"image_index": 48, "image_name": "000000005037.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005037.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1481.323, "latencies_ms": [1481.323], "images_per_second": 0.675, "prompt_tokens": 1109, "response_tokens_est": 16, "n_tiles": 1, "output_text": " The bus is white with blue and pink accents, and the sky is cloudy.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.2, "ram_available_mb": 99001.0, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 26771.4, "ram_available_mb": 99000.7, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.52, "peak": 40.97, "min": 20.89}, "VIN": {"avg": 77.11, "peak": 125.7, "min": 27.32}, "VIN_SYS_5V0": {"avg": 15.08, "peak": 16.05, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 16.22, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 32.52, "energy_joules_est": 48.18, "sample_count": 15, "duration_seconds": 1.482}, "timestamp": "2026-01-19T14:20:22.165776"}
{"image_index": 49, "image_name": "000000005060.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005060.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1536.891, "latencies_ms": [1536.891], "images_per_second": 0.651, "prompt_tokens": 1100, "response_tokens_est": 18, "n_tiles": 1, "output_text": " A man is sitting on the floor in front of a mirror, holding a cell phone.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 26771.4, "ram_available_mb": 99000.7, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26771.4, "ram_available_mb": 99000.7, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.99, "peak": 40.57, "min": 19.32}, "VIN": {"avg": 73.7, "peak": 118.99, "min": 30.24}, "VIN_SYS_5V0": {"avg": 14.81, "peak": 16.05, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.88, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 31.99, "energy_joules_est": 49.18, "sample_count": 15, "duration_seconds": 1.537}, "timestamp": "2026-01-19T14:20:23.739182"}
{"image_index": 49, "image_name": "000000005060.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005060.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2496.619, "latencies_ms": [2496.619], "images_per_second": 0.401, "prompt_tokens": 1114, "response_tokens_est": 56, "n_tiles": 1, "output_text": " 1. Man: 1\n2. Mirror: 1\n3. Floor: 1\n4. Wall: 1\n5. Window: 1\n6. Door: 1\n7. Table: 1\n8. Chair: 1", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26771.4, "ram_available_mb": 99000.7, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 26771.4, "ram_available_mb": 99000.7, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.99, "peak": 40.57, "min": 17.74}, "VIN": {"avg": 70.97, "peak": 118.12, "min": 29.09}, "VIN_SYS_5V0": {"avg": 15.09, "peak": 16.15, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 16.08, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 27.99, "energy_joules_est": 69.89, "sample_count": 25, "duration_seconds": 2.497}, "timestamp": "2026-01-19T14:20:26.336155"}
{"image_index": 49, "image_name": "000000005060.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005060.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2474.934, "latencies_ms": [2474.934], "images_per_second": 0.404, "prompt_tokens": 1118, "response_tokens_est": 54, "n_tiles": 1, "output_text": " The man is sitting in front of the mirror, which is positioned on the left side of the image. The mirror is reflecting the room behind him, which is located in the background. The man is sitting on the floor, which is in the foreground of the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.4, "ram_available_mb": 99000.7, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26771.4, "ram_available_mb": 99000.7, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.17, "peak": 40.18, "min": 14.98}, "VIN": {"avg": 76.0, "peak": 126.08, "min": 29.4}, "VIN_SYS_5V0": {"avg": 14.86, "peak": 16.05, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.8, "peak": 17.32, "min": 12.61}}, "power_watts_avg": 27.17, "energy_joules_est": 67.26, "sample_count": 24, "duration_seconds": 2.475}, "timestamp": "2026-01-19T14:20:28.833558"}
{"image_index": 49, "image_name": "000000005060.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005060.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1538.821, "latencies_ms": [1538.821], "images_per_second": 0.65, "prompt_tokens": 1112, "response_tokens_est": 19, "n_tiles": 1, "output_text": " A man is sitting on the floor in front of a mirror, taking a picture of himself.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.4, "ram_available_mb": 99000.7, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 26771.4, "ram_available_mb": 99000.7, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.15, "peak": 39.39, "min": 16.95}, "VIN": {"avg": 69.96, "peak": 127.31, "min": 29.26}, "VIN_SYS_5V0": {"avg": 15.21, "peak": 16.26, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 16.25, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 31.15, "energy_joules_est": 47.94, "sample_count": 15, "duration_seconds": 1.539}, "timestamp": "2026-01-19T14:20:30.406583"}
{"image_index": 49, "image_name": "000000005060.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005060.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1543.198, "latencies_ms": [1543.198], "images_per_second": 0.648, "prompt_tokens": 1110, "response_tokens_est": 19, "n_tiles": 1, "output_text": " The room is well-lit with natural light, and the wooden floor is polished and clean.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.4, "ram_available_mb": 99000.7, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 26771.4, "ram_available_mb": 99000.7, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.41, "peak": 40.18, "min": 20.1}, "VIN": {"avg": 78.12, "peak": 120.8, "min": 29.43}, "VIN_SYS_5V0": {"avg": 15.19, "peak": 16.15, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 16.32, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 32.41, "energy_joules_est": 50.04, "sample_count": 15, "duration_seconds": 1.544}, "timestamp": "2026-01-19T14:20:31.971692"}
{"image_index": 50, "image_name": "000000005193.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005193.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1784.551, "latencies_ms": [1784.551], "images_per_second": 0.56, "prompt_tokens": 1099, "response_tokens_est": 28, "n_tiles": 1, "output_text": " A group of people, including a man holding a surfboard, are posing for a photo in a room with a door and a window.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.4, "ram_available_mb": 99000.7, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.93, "peak": 40.97, "min": 20.1}, "VIN": {"avg": 69.66, "peak": 113.85, "min": 29.06}, "VIN_SYS_5V0": {"avg": 15.12, "peak": 16.15, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 16.12, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 30.93, "energy_joules_est": 55.22, "sample_count": 18, "duration_seconds": 1.785}, "timestamp": "2026-01-19T14:20:33.850291"}
{"image_index": 50, "image_name": "000000005193.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005193.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2560.131, "latencies_ms": [2560.131], "images_per_second": 0.391, "prompt_tokens": 1113, "response_tokens_est": 57, "n_tiles": 1, "output_text": " 1. surfboard: 4\n2. person: 4\n3. person: 1\n4. person: 1\n5. person: 1\n6. person: 1\n7. person: 1\n8. person: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.39, "peak": 40.18, "min": 17.35}, "VIN": {"avg": 69.97, "peak": 123.07, "min": 29.33}, "VIN_SYS_5V0": {"avg": 14.92, "peak": 16.05, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.8, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 27.39, "energy_joules_est": 70.13, "sample_count": 25, "duration_seconds": 2.56}, "timestamp": "2026-01-19T14:20:36.438797"}
{"image_index": 50, "image_name": "000000005193.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005193.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2232.878, "latencies_ms": [2232.878], "images_per_second": 0.448, "prompt_tokens": 1117, "response_tokens_est": 45, "n_tiles": 1, "output_text": " The surfboards are positioned in the foreground, with the group of people standing behind them. The person taking the photo is positioned to the left of the surfboards, while the person holding the flag is positioned to the right.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.26, "peak": 40.57, "min": 16.56}, "VIN": {"avg": 67.45, "peak": 112.23, "min": 28.47}, "VIN_SYS_5V0": {"avg": 15.09, "peak": 16.15, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 16.09, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 28.26, "energy_joules_est": 63.13, "sample_count": 22, "duration_seconds": 2.234}, "timestamp": "2026-01-19T14:20:38.731895"}
{"image_index": 50, "image_name": "000000005193.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005193.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1505.819, "latencies_ms": [1505.819], "images_per_second": 0.664, "prompt_tokens": 1111, "response_tokens_est": 16, "n_tiles": 1, "output_text": " A group of people are posing for a picture in a room with surfboards.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.04, "peak": 40.16, "min": 16.16}, "VIN": {"avg": 73.2, "peak": 125.9, "min": 29.82}, "VIN_SYS_5V0": {"avg": 14.85, "peak": 16.05, "min": 12.25}, "VDD_CPU_SOC_MSS": {"avg": 15.91, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 31.04, "energy_joules_est": 46.75, "sample_count": 15, "duration_seconds": 1.506}, "timestamp": "2026-01-19T14:20:40.298110"}
{"image_index": 50, "image_name": "000000005193.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005193.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1729.769, "latencies_ms": [1729.769], "images_per_second": 0.578, "prompt_tokens": 1109, "response_tokens_est": 25, "n_tiles": 1, "output_text": " The image is taken in a dimly lit room with a warm ambiance, and the surfboards are made of wood.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.26, "peak": 40.57, "min": 20.11}, "VIN": {"avg": 78.28, "peak": 124.19, "min": 29.62}, "VIN_SYS_5V0": {"avg": 14.95, "peak": 16.05, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.98, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 31.26, "energy_joules_est": 54.08, "sample_count": 17, "duration_seconds": 1.73}, "timestamp": "2026-01-19T14:20:42.077771"}
{"image_index": 51, "image_name": "000000005477.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005477.jpg", "image_width": 640, "image_height": 349, "image_resolution": "640x349", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2215.159, "latencies_ms": [2215.159], "images_per_second": 0.451, "prompt_tokens": 1099, "response_tokens_est": 44, "n_tiles": 1, "output_text": " The image features a large, golden-colored airplane with the words \"POLSKIE LOTNIE LOTNICZE\" written on its side, parked on a runway with a cloudy sky in the background.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.56, "peak": 40.57, "min": 18.52}, "VIN": {"avg": 69.41, "peak": 119.24, "min": 28.76}, "VIN_SYS_5V0": {"avg": 14.91, "peak": 16.05, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.87, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 28.56, "energy_joules_est": 63.28, "sample_count": 22, "duration_seconds": 2.216}, "timestamp": "2026-01-19T14:20:44.371114"}
{"image_index": 51, "image_name": "000000005477.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005477.jpg", "image_width": 640, "image_height": 349, "image_resolution": "640x349", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2535.829, "latencies_ms": [2535.829], "images_per_second": 0.394, "prompt_tokens": 1113, "response_tokens_est": 56, "n_tiles": 1, "output_text": " 1. airplane: 1\n2. tail: 1\n3. wing: 2\n4. engine: 2\n5. wheels: 2\n6. logo: 1\n7. clouds: 1\n8. runway: 1", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.01, "peak": 40.18, "min": 15.77}, "VIN": {"avg": 68.46, "peak": 122.05, "min": 28.77}, "VIN_SYS_5V0": {"avg": 14.81, "peak": 16.05, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 15.81, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 27.01, "energy_joules_est": 68.51, "sample_count": 25, "duration_seconds": 2.536}, "timestamp": "2026-01-19T14:20:46.982325"}
{"image_index": 51, "image_name": "000000005477.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005477.jpg", "image_width": 640, "image_height": 349, "image_resolution": "640x349", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2251.785, "latencies_ms": [2251.785], "images_per_second": 0.444, "prompt_tokens": 1117, "response_tokens_est": 45, "n_tiles": 1, "output_text": " The airplane is positioned in the foreground, with the runway and other aircraft in the background. The airplane is facing towards the left side of the image, with the tail of the airplane visible on the left side of the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.03, "peak": 40.18, "min": 15.77}, "VIN": {"avg": 63.93, "peak": 126.25, "min": 28.58}, "VIN_SYS_5V0": {"avg": 14.84, "peak": 16.05, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 15.82, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 28.03, "energy_joules_est": 63.13, "sample_count": 22, "duration_seconds": 2.252}, "timestamp": "2026-01-19T14:20:49.275192"}
{"image_index": 51, "image_name": "000000005477.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005477.jpg", "image_width": 640, "image_height": 349, "image_resolution": "640x349", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1544.877, "latencies_ms": [1544.877], "images_per_second": 0.647, "prompt_tokens": 1111, "response_tokens_est": 18, "n_tiles": 1, "output_text": " A golden-colored airplane is parked on a runway with a cloudy sky in the background.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.39, "peak": 40.57, "min": 16.95}, "VIN": {"avg": 75.82, "peak": 106.27, "min": 29.61}, "VIN_SYS_5V0": {"avg": 15.02, "peak": 16.05, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 16.12, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 31.39, "energy_joules_est": 48.51, "sample_count": 15, "duration_seconds": 1.545}, "timestamp": "2026-01-19T14:20:50.846667"}
{"image_index": 51, "image_name": "000000005477.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005477.jpg", "image_width": 640, "image_height": 349, "image_resolution": "640x349", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1491.288, "latencies_ms": [1491.288], "images_per_second": 0.671, "prompt_tokens": 1109, "response_tokens_est": 16, "n_tiles": 1, "output_text": " The airplane is gold and blue, and the sky is blue with white clouds.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.44, "peak": 40.97, "min": 20.89}, "VIN": {"avg": 75.83, "peak": 118.51, "min": 29.0}, "VIN_SYS_5V0": {"avg": 15.1, "peak": 16.15, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 16.14, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 32.44, "energy_joules_est": 48.39, "sample_count": 15, "duration_seconds": 1.492}, "timestamp": "2026-01-19T14:20:52.418208"}
{"image_index": 52, "image_name": "000000005503.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005503.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1499.132, "latencies_ms": [1499.132], "images_per_second": 0.667, "prompt_tokens": 1100, "response_tokens_est": 16, "n_tiles": 1, "output_text": " A toilet with a lid up and water running in it is in a bathroom.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.89, "peak": 40.57, "min": 19.71}, "VIN": {"avg": 74.75, "peak": 125.01, "min": 27.32}, "VIN_SYS_5V0": {"avg": 14.77, "peak": 16.05, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.85, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 31.89, "energy_joules_est": 47.83, "sample_count": 15, "duration_seconds": 1.5}, "timestamp": "2026-01-19T14:20:53.984686"}
{"image_index": 52, "image_name": "000000005503.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005503.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2141.527, "latencies_ms": [2141.527], "images_per_second": 0.467, "prompt_tokens": 1114, "response_tokens_est": 41, "n_tiles": 1, "output_text": " toilet: 1\ntoilet seat: 1\ntoilet lid: 1\ntoilet bowl: 1\nwater: 1\nwater pipe: 1\nperson: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.14, "peak": 40.57, "min": 19.71}, "VIN": {"avg": 74.6, "peak": 120.54, "min": 28.9}, "VIN_SYS_5V0": {"avg": 14.89, "peak": 16.15, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.93, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 29.14, "energy_joules_est": 62.42, "sample_count": 21, "duration_seconds": 2.142}, "timestamp": "2026-01-19T14:20:56.169915"}
{"image_index": 52, "image_name": "000000005503.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005503.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2240.147, "latencies_ms": [2240.147], "images_per_second": 0.446, "prompt_tokens": 1118, "response_tokens_est": 45, "n_tiles": 1, "output_text": " The toilet is located in the foreground of the image, with the sink and towel rack in the background. The person's legs are visible in the bottom left corner of the image, indicating that they are standing near the toilet.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.21, "peak": 40.16, "min": 16.95}, "VIN": {"avg": 74.41, "peak": 129.92, "min": 28.69}, "VIN_SYS_5V0": {"avg": 14.97, "peak": 16.15, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 16.02, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 28.21, "energy_joules_est": 63.2, "sample_count": 22, "duration_seconds": 2.24}, "timestamp": "2026-01-19T14:20:58.460261"}
{"image_index": 52, "image_name": "000000005503.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005503.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1376.217, "latencies_ms": [1376.217], "images_per_second": 0.727, "prompt_tokens": 1112, "response_tokens_est": 11, "n_tiles": 1, "output_text": " A toilet with a lid up is in a bathroom.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.5, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.15, "peak": 39.39, "min": 16.56}, "VIN": {"avg": 74.8, "peak": 123.67, "min": 28.33}, "VIN_SYS_5V0": {"avg": 14.83, "peak": 16.15, "min": 12.36}, "VDD_CPU_SOC_MSS": {"avg": 15.94, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 31.15, "energy_joules_est": 42.88, "sample_count": 14, "duration_seconds": 1.377}, "timestamp": "2026-01-19T14:20:59.924125"}
{"image_index": 52, "image_name": "000000005503.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005503.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1362.844, "latencies_ms": [1362.844], "images_per_second": 0.734, "prompt_tokens": 1110, "response_tokens_est": 10, "n_tiles": 1, "output_text": " The toilet is white and the water is clear.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.97, "peak": 40.57, "min": 20.5}, "VIN": {"avg": 75.79, "peak": 125.54, "min": 33.82}, "VIN_SYS_5V0": {"avg": 14.78, "peak": 16.05, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.9, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 32.97, "energy_joules_est": 44.94, "sample_count": 13, "duration_seconds": 1.363}, "timestamp": "2026-01-19T14:21:01.292536"}
{"image_index": 53, "image_name": "000000005529.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005529.jpg", "image_width": 444, "image_height": 640, "image_resolution": "444x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1599.729, "latencies_ms": [1599.729], "images_per_second": 0.625, "prompt_tokens": 1100, "response_tokens_est": 20, "n_tiles": 1, "output_text": " A person wearing a blue jacket and a white helmet is skiing down a snowy slope surrounded by trees.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.6, "peak": 40.97, "min": 22.07}, "VIN": {"avg": 68.77, "peak": 109.97, "min": 29.12}, "VIN_SYS_5V0": {"avg": 15.16, "peak": 16.15, "min": 13.29}, "VDD_CPU_SOC_MSS": {"avg": 16.24, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 32.6, "energy_joules_est": 52.17, "sample_count": 16, "duration_seconds": 1.6}, "timestamp": "2026-01-19T14:21:02.969138"}
{"image_index": 53, "image_name": "000000005529.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005529.jpg", "image_width": 444, "image_height": 640, "image_resolution": "444x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2586.52, "latencies_ms": [2586.52], "images_per_second": 0.387, "prompt_tokens": 1114, "response_tokens_est": 58, "n_tiles": 1, "output_text": " 1. person: 1\n2. skis: 2\n3. ski poles: 2\n4. helmet: 1\n5. goggles: 1\n6. backpack: 1\n7. snow: 1\n8. trees: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.45, "peak": 40.57, "min": 18.52}, "VIN": {"avg": 71.06, "peak": 120.67, "min": 31.02}, "VIN_SYS_5V0": {"avg": 14.87, "peak": 16.05, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.92, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 27.45, "energy_joules_est": 71.01, "sample_count": 25, "duration_seconds": 2.587}, "timestamp": "2026-01-19T14:21:05.584685"}
{"image_index": 53, "image_name": "000000005529.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005529.jpg", "image_width": 444, "image_height": 640, "image_resolution": "444x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2185.871, "latencies_ms": [2185.871], "images_per_second": 0.457, "prompt_tokens": 1118, "response_tokens_est": 43, "n_tiles": 1, "output_text": " The skier is positioned in the foreground of the image, with the snowy forest and trees in the background. The skier is skiing towards the left side of the image, with the trees lining the right side.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.54, "peak": 40.18, "min": 16.56}, "VIN": {"avg": 74.11, "peak": 122.72, "min": 33.78}, "VIN_SYS_5V0": {"avg": 15.05, "peak": 16.15, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 16.22, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 28.54, "energy_joules_est": 62.39, "sample_count": 21, "duration_seconds": 2.186}, "timestamp": "2026-01-19T14:21:07.776458"}
{"image_index": 53, "image_name": "000000005529.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005529.jpg", "image_width": 444, "image_height": 640, "image_resolution": "444x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1362.056, "latencies_ms": [1362.056], "images_per_second": 0.734, "prompt_tokens": 1112, "response_tokens_est": 12, "n_tiles": 1, "output_text": " A person is skiing down a snowy slope surrounded by trees.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.71, "peak": 39.38, "min": 17.74}, "VIN": {"avg": 78.75, "peak": 122.52, "min": 27.65}, "VIN_SYS_5V0": {"avg": 15.22, "peak": 16.26, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 16.48, "peak": 17.71, "min": 14.57}}, "power_watts_avg": 31.71, "energy_joules_est": 43.21, "sample_count": 14, "duration_seconds": 1.363}, "timestamp": "2026-01-19T14:21:09.235180"}
{"image_index": 53, "image_name": "000000005529.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005529.jpg", "image_width": 444, "image_height": 640, "image_resolution": "444x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1612.058, "latencies_ms": [1612.058], "images_per_second": 0.62, "prompt_tokens": 1110, "response_tokens_est": 20, "n_tiles": 1, "output_text": " The skier is wearing a blue jacket and black pants, and the snow is white and fluffy.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.44, "peak": 40.16, "min": 20.11}, "VIN": {"avg": 73.71, "peak": 119.38, "min": 28.52}, "VIN_SYS_5V0": {"avg": 14.77, "peak": 16.05, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.87, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 31.44, "energy_joules_est": 50.71, "sample_count": 16, "duration_seconds": 1.613}, "timestamp": "2026-01-19T14:21:10.906230"}
{"image_index": 54, "image_name": "000000005586.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005586.jpg", "image_width": 320, "image_height": 240, "image_resolution": "320x240", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1747.443, "latencies_ms": [1747.443], "images_per_second": 0.572, "prompt_tokens": 1099, "response_tokens_est": 26, "n_tiles": 1, "output_text": " A man in a yellow shirt and black shorts is playing tennis on a blue court with a crowd of people watching in the background.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.03, "peak": 40.57, "min": 19.32}, "VIN": {"avg": 70.56, "peak": 95.28, "min": 31.12}, "VIN_SYS_5V0": {"avg": 15.02, "peak": 16.05, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 16.03, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 31.03, "energy_joules_est": 54.25, "sample_count": 17, "duration_seconds": 1.748}, "timestamp": "2026-01-19T14:21:12.680048"}
{"image_index": 54, "image_name": "000000005586.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005586.jpg", "image_width": 320, "image_height": 240, "image_resolution": "320x240", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2875.193, "latencies_ms": [2875.193], "images_per_second": 0.348, "prompt_tokens": 1113, "response_tokens_est": 70, "n_tiles": 1, "output_text": " 1. man: 1\n2. tennis racket: 1\n3. tennis ball: 1\n4. blue court: 1\n5. white lines: 1\n6. white lines on court: 1\n7. white lines on court: 1\n8. white lines on court: 1", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.88, "peak": 40.18, "min": 18.52}, "VIN": {"avg": 71.75, "peak": 123.59, "min": 29.5}, "VIN_SYS_5V0": {"avg": 15.07, "peak": 16.15, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 16.14, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 26.88, "energy_joules_est": 77.29, "sample_count": 28, "duration_seconds": 2.875}, "timestamp": "2026-01-19T14:21:15.594734"}
{"image_index": 54, "image_name": "000000005586.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005586.jpg", "image_width": 320, "image_height": 240, "image_resolution": "320x240", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1839.311, "latencies_ms": [1839.311], "images_per_second": 0.544, "prompt_tokens": 1117, "response_tokens_est": 30, "n_tiles": 1, "output_text": " The tennis player is in the foreground, with the crowd in the background. The player is near the net, while the ball is in the air.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.92, "peak": 40.56, "min": 16.16}, "VIN": {"avg": 74.82, "peak": 125.21, "min": 29.21}, "VIN_SYS_5V0": {"avg": 15.08, "peak": 16.15, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 16.18, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 29.92, "energy_joules_est": 55.05, "sample_count": 18, "duration_seconds": 1.84}, "timestamp": "2026-01-19T14:21:17.471017"}
{"image_index": 54, "image_name": "000000005586.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005586.jpg", "image_width": 320, "image_height": 240, "image_resolution": "320x240", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1764.657, "latencies_ms": [1764.657], "images_per_second": 0.567, "prompt_tokens": 1111, "response_tokens_est": 27, "n_tiles": 1, "output_text": " A man in a yellow shirt and black shorts is playing tennis on a blue court, with a crowd of people watching in the stands.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.98, "peak": 40.57, "min": 18.13}, "VIN": {"avg": 81.72, "peak": 123.16, "min": 36.26}, "VIN_SYS_5V0": {"avg": 15.12, "peak": 16.15, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 16.3, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 30.98, "energy_joules_est": 54.69, "sample_count": 17, "duration_seconds": 1.765}, "timestamp": "2026-01-19T14:21:19.240383"}
{"image_index": 54, "image_name": "000000005586.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005586.jpg", "image_width": 320, "image_height": 240, "image_resolution": "320x240", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2146.625, "latencies_ms": [2146.625], "images_per_second": 0.466, "prompt_tokens": 1109, "response_tokens_est": 43, "n_tiles": 1, "output_text": " The image features a tennis court with a blue surface, white lines, and a man in a yellow shirt and black shorts playing tennis. The lighting is bright, and the court is surrounded by a crowd of spectators.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.34, "peak": 40.18, "min": 19.7}, "VIN": {"avg": 69.16, "peak": 106.54, "min": 29.12}, "VIN_SYS_5V0": {"avg": 15.27, "peak": 16.26, "min": 13.49}, "VDD_CPU_SOC_MSS": {"avg": 16.39, "peak": 17.32, "min": 14.97}}, "power_watts_avg": 29.34, "energy_joules_est": 63.0, "sample_count": 21, "duration_seconds": 2.147}, "timestamp": "2026-01-19T14:21:21.426281"}
{"image_index": 55, "image_name": "000000005600.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005600.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2100.227, "latencies_ms": [2100.227], "images_per_second": 0.476, "prompt_tokens": 1099, "response_tokens_est": 40, "n_tiles": 1, "output_text": " The image shows a plate with two bowls of food, one containing a reddish-brown dish and the other a bowl of orange and white round objects, placed on a table with a white napkin.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.68, "peak": 40.56, "min": 16.95}, "VIN": {"avg": 69.71, "peak": 104.74, "min": 27.57}, "VIN_SYS_5V0": {"avg": 14.98, "peak": 16.05, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 16.09, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 28.68, "energy_joules_est": 60.26, "sample_count": 21, "duration_seconds": 2.101}, "timestamp": "2026-01-19T14:21:23.614512"}
{"image_index": 55, "image_name": "000000005600.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005600.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2698.262, "latencies_ms": [2698.262], "images_per_second": 0.371, "prompt_tokens": 1113, "response_tokens_est": 62, "n_tiles": 1, "output_text": " 1. plate: 1\n2. bowl: 2\n3. food: 2\n4. tablecloth: 1\n5. glass: 1\n6. napkin: 1\n7. bowl of food: 1\n8. bowl of fruit: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.87, "peak": 40.18, "min": 16.16}, "VIN": {"avg": 69.99, "peak": 124.33, "min": 30.16}, "VIN_SYS_5V0": {"avg": 14.9, "peak": 16.05, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 15.86, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 26.87, "energy_joules_est": 72.52, "sample_count": 26, "duration_seconds": 2.699}, "timestamp": "2026-01-19T14:21:26.322703"}
{"image_index": 55, "image_name": "000000005600.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005600.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2280.802, "latencies_ms": [2280.802], "images_per_second": 0.438, "prompt_tokens": 1117, "response_tokens_est": 47, "n_tiles": 1, "output_text": " The bowl of curry is located to the left of the bowl of fruit, with the curry bowl being closer to the camera than the fruit bowl. The fruit bowl is positioned in the foreground, while the curry bowl is in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.42, "peak": 40.57, "min": 16.95}, "VIN": {"avg": 75.58, "peak": 124.8, "min": 30.79}, "VIN_SYS_5V0": {"avg": 15.18, "peak": 16.26, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 16.19, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 28.42, "energy_joules_est": 64.84, "sample_count": 22, "duration_seconds": 2.282}, "timestamp": "2026-01-19T14:21:28.611700"}
{"image_index": 55, "image_name": "000000005600.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005600.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1626.676, "latencies_ms": [1626.676], "images_per_second": 0.615, "prompt_tokens": 1111, "response_tokens_est": 22, "n_tiles": 1, "output_text": " A plate with two bowls of food on it, one with a red sauce and the other with orange food.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.27, "peak": 40.18, "min": 17.73}, "VIN": {"avg": 76.29, "peak": 126.53, "min": 30.39}, "VIN_SYS_5V0": {"avg": 15.24, "peak": 16.15, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 16.44, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 31.27, "energy_joules_est": 50.88, "sample_count": 16, "duration_seconds": 1.627}, "timestamp": "2026-01-19T14:21:30.281215"}
{"image_index": 55, "image_name": "000000005600.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005600.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2340.061, "latencies_ms": [2340.061], "images_per_second": 0.427, "prompt_tokens": 1109, "response_tokens_est": 49, "n_tiles": 1, "output_text": " The image features a plate with two bowls of food, one containing a reddish-brown dish and the other a bowl of orange and white food. The food is placed on a white napkin, and the background is a dark green tablecloth.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.55, "peak": 40.57, "min": 19.31}, "VIN": {"avg": 67.03, "peak": 99.13, "min": 28.95}, "VIN_SYS_5V0": {"avg": 15.04, "peak": 16.05, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 16.06, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 28.55, "energy_joules_est": 66.82, "sample_count": 23, "duration_seconds": 2.341}, "timestamp": "2026-01-19T14:21:32.662906"}
{"image_index": 56, "image_name": "000000005992.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005992.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1880.147, "latencies_ms": [1880.147], "images_per_second": 0.532, "prompt_tokens": 1099, "response_tokens_est": 31, "n_tiles": 1, "output_text": " In the image, there are several sheep standing in a grassy area, with one sheep looking directly at the camera, and the others facing away from it.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.36, "peak": 40.57, "min": 16.56}, "VIN": {"avg": 75.12, "peak": 131.39, "min": 28.57}, "VIN_SYS_5V0": {"avg": 14.93, "peak": 16.05, "min": 12.46}, "VDD_CPU_SOC_MSS": {"avg": 16.0, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 29.36, "energy_joules_est": 55.22, "sample_count": 19, "duration_seconds": 1.881}, "timestamp": "2026-01-19T14:21:34.647458"}
{"image_index": 56, "image_name": "000000005992.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005992.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1203.051, "latencies_ms": [1203.051], "images_per_second": 0.831, "prompt_tokens": 1113, "response_tokens_est": 4, "n_tiles": 1, "output_text": " sheep: 5", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.2, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.91, "peak": 40.16, "min": 16.56}, "VIN": {"avg": 73.95, "peak": 101.0, "min": 29.7}, "VIN_SYS_5V0": {"avg": 14.57, "peak": 15.95, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.78, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 31.91, "energy_joules_est": 38.4, "sample_count": 12, "duration_seconds": 1.203}, "timestamp": "2026-01-19T14:21:35.900325"}
{"image_index": 56, "image_name": "000000005992.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005992.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2073.737, "latencies_ms": [2073.737], "images_per_second": 0.482, "prompt_tokens": 1117, "response_tokens_est": 38, "n_tiles": 1, "output_text": " The sheep are positioned in the foreground of the image, with the brick wall serving as a backdrop. The sheep are standing close to each other, suggesting a sense of companionship or social interaction.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.6, "peak": 41.36, "min": 18.52}, "VIN": {"avg": 73.23, "peak": 119.13, "min": 29.16}, "VIN_SYS_5V0": {"avg": 14.95, "peak": 16.15, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 16.06, "peak": 17.32, "min": 13.78}}, "power_watts_avg": 30.6, "energy_joules_est": 63.47, "sample_count": 21, "duration_seconds": 2.074}, "timestamp": "2026-01-19T14:21:38.076345"}
{"image_index": 56, "image_name": "000000005992.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005992.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1629.534, "latencies_ms": [1629.534], "images_per_second": 0.614, "prompt_tokens": 1111, "response_tokens_est": 20, "n_tiles": 1, "output_text": " In a sunny day, a group of sheep are standing in a grassy area near a brick wall.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.46, "peak": 40.18, "min": 16.16}, "VIN": {"avg": 71.65, "peak": 115.62, "min": 30.68}, "VIN_SYS_5V0": {"avg": 14.63, "peak": 15.95, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.67, "peak": 16.92, "min": 13.0}}, "power_watts_avg": 30.46, "energy_joules_est": 49.65, "sample_count": 16, "duration_seconds": 1.63}, "timestamp": "2026-01-19T14:21:39.746221"}
{"image_index": 56, "image_name": "000000005992.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005992.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2451.441, "latencies_ms": [2451.441], "images_per_second": 0.408, "prompt_tokens": 1109, "response_tokens_est": 53, "n_tiles": 1, "output_text": " The image features a group of sheep with thick wool coats, standing in a grassy area with a brick wall in the background. The lighting is natural and bright, suggesting it is daytime. The sheep appear to be in a rural setting, possibly a farm or pasture.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.12, "peak": 40.57, "min": 18.91}, "VIN": {"avg": 71.74, "peak": 111.59, "min": 29.38}, "VIN_SYS_5V0": {"avg": 14.99, "peak": 16.05, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 16.11, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 28.12, "energy_joules_est": 68.94, "sample_count": 24, "duration_seconds": 2.452}, "timestamp": "2026-01-19T14:21:42.248105"}
{"image_index": 57, "image_name": "000000006012.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006012.jpg", "image_width": 640, "image_height": 531, "image_resolution": "640x531", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2038.443, "latencies_ms": [2038.443], "images_per_second": 0.491, "prompt_tokens": 1432, "response_tokens_est": 21, "n_tiles": 1, "output_text": " In the image, a bunch of bananas and an apple are placed on a blue and white patterned background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.2, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.34, "peak": 40.18, "min": 16.16}, "VIN": {"avg": 75.66, "peak": 118.02, "min": 29.97}, "VIN_SYS_5V0": {"avg": 15.33, "peak": 16.76, "min": 12.36}, "VDD_CPU_SOC_MSS": {"avg": 16.46, "peak": 17.71, "min": 13.39}}, "power_watts_avg": 31.34, "energy_joules_est": 63.9, "sample_count": 20, "duration_seconds": 2.039}, "timestamp": "2026-01-19T14:21:44.339728"}
{"image_index": 57, "image_name": "000000006012.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006012.jpg", "image_width": 640, "image_height": 531, "image_resolution": "640x531", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1695.359, "latencies_ms": [1695.359], "images_per_second": 0.59, "prompt_tokens": 1446, "response_tokens_est": 9, "n_tiles": 1, "output_text": " banana: 5, apple: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.1, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 33.34, "peak": 40.56, "min": 18.92}, "VIN": {"avg": 77.61, "peak": 118.89, "min": 30.34}, "VIN_SYS_5V0": {"avg": 15.27, "peak": 16.66, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 16.47, "peak": 17.71, "min": 13.79}}, "power_watts_avg": 33.34, "energy_joules_est": 56.54, "sample_count": 17, "duration_seconds": 1.696}, "timestamp": "2026-01-19T14:21:46.119500"}
{"image_index": 57, "image_name": "000000006012.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006012.jpg", "image_width": 640, "image_height": 531, "image_resolution": "640x531", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2975.716, "latencies_ms": [2975.716], "images_per_second": 0.336, "prompt_tokens": 1450, "response_tokens_est": 56, "n_tiles": 1, "output_text": " The red apple is positioned in the center of the image, with the bananas surrounding it. The bananas are arranged in a circular pattern, with the red apple placed in the middle. The background is a blue and white floral pattern, which provides a contrasting backdrop to the main objects.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.28, "peak": 40.57, "min": 19.31}, "VIN": {"avg": 73.11, "peak": 119.61, "min": 28.81}, "VIN_SYS_5V0": {"avg": 15.08, "peak": 16.56, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 16.17, "peak": 17.71, "min": 13.39}}, "power_watts_avg": 29.28, "energy_joules_est": 87.14, "sample_count": 29, "duration_seconds": 2.976}, "timestamp": "2026-01-19T14:21:49.137993"}
{"image_index": 57, "image_name": "000000006012.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006012.jpg", "image_width": 640, "image_height": 531, "image_resolution": "640x531", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3078.126, "latencies_ms": [3078.126], "images_per_second": 0.325, "prompt_tokens": 1444, "response_tokens_est": 55, "n_tiles": 1, "output_text": " In the center of the image, a vibrant red apple sits atop a bunch of bananas, which are arranged in a circular pattern around it. The bananas are a bright yellow color, and the apple's red hue stands out against the backdrop of a blue and white floral pattern.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.76, "peak": 40.18, "min": 16.56}, "VIN": {"avg": 68.49, "peak": 126.09, "min": 29.53}, "VIN_SYS_5V0": {"avg": 14.96, "peak": 16.56, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 16.04, "peak": 17.71, "min": 13.79}}, "power_watts_avg": 27.76, "energy_joules_est": 85.46, "sample_count": 30, "duration_seconds": 3.079}, "timestamp": "2026-01-19T14:21:52.261600"}
{"image_index": 57, "image_name": "000000006012.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006012.jpg", "image_width": 640, "image_height": 531, "image_resolution": "640x531", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3781.871, "latencies_ms": [3781.871], "images_per_second": 0.264, "prompt_tokens": 1442, "response_tokens_est": 87, "n_tiles": 1, "output_text": " The image features a vibrant display of bananas and an apple, with the bananas arranged in a circular pattern around the apple. The bananas are a bright yellow color, while the apple is a striking combination of red and yellow hues. The lighting in the image is bright and even, highlighting the natural colors of the fruits. The background is a cool blue color with a floral pattern, providing a contrasting backdrop to the warm tones of the fruits.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.47, "peak": 40.16, "min": 15.38}, "VIN": {"avg": 67.94, "peak": 119.74, "min": 29.11}, "VIN_SYS_5V0": {"avg": 15.02, "peak": 16.56, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 16.08, "peak": 17.71, "min": 13.0}}, "power_watts_avg": 26.47, "energy_joules_est": 100.12, "sample_count": 37, "duration_seconds": 3.782}, "timestamp": "2026-01-19T14:21:56.114677"}
{"image_index": 58, "image_name": "000000006040.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006040.jpg", "image_width": 640, "image_height": 351, "image_resolution": "640x351", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1647.459, "latencies_ms": [1647.459], "images_per_second": 0.607, "prompt_tokens": 1099, "response_tokens_est": 22, "n_tiles": 1, "output_text": " A blue and white train with red seats is parked at a station with trees and power lines in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.58, "peak": 40.18, "min": 15.77}, "VIN": {"avg": 70.33, "peak": 92.97, "min": 29.14}, "VIN_SYS_5V0": {"avg": 14.93, "peak": 16.05, "min": 12.15}, "VDD_CPU_SOC_MSS": {"avg": 16.02, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 30.58, "energy_joules_est": 50.39, "sample_count": 16, "duration_seconds": 1.648}, "timestamp": "2026-01-19T14:21:57.787275"}
{"image_index": 58, "image_name": "000000006040.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006040.jpg", "image_width": 640, "image_height": 351, "image_resolution": "640x351", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2616.673, "latencies_ms": [2616.673], "images_per_second": 0.382, "prompt_tokens": 1113, "response_tokens_est": 59, "n_tiles": 1, "output_text": " 1. train: 1\n2. windows: 12\n3. doors: 2\n4. seats: 12\n5. trolley: 1\n6. tracks: 2\n7. trees: 1\n8. wires: 1", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26771.6, "ram_available_mb": 99000.6, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.41, "peak": 40.97, "min": 17.35}, "VIN": {"avg": 69.53, "peak": 105.52, "min": 29.56}, "VIN_SYS_5V0": {"avg": 15.04, "peak": 16.15, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 16.11, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 27.41, "energy_joules_est": 71.73, "sample_count": 26, "duration_seconds": 2.617}, "timestamp": "2026-01-19T14:22:00.493389"}
{"image_index": 58, "image_name": "000000006040.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006040.jpg", "image_width": 640, "image_height": 351, "image_resolution": "640x351", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2134.327, "latencies_ms": [2134.327], "images_per_second": 0.469, "prompt_tokens": 1117, "response_tokens_est": 40, "n_tiles": 1, "output_text": " The train is positioned on the left side of the image, with the tracks extending towards the right. The background features a clear blue sky and trees, while the foreground includes a fence and a building.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.6, "ram_available_mb": 99000.6, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26771.6, "ram_available_mb": 99000.6, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.16, "peak": 40.18, "min": 14.98}, "VIN": {"avg": 71.81, "peak": 122.98, "min": 29.31}, "VIN_SYS_5V0": {"avg": 14.71, "peak": 15.95, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.82, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 28.16, "energy_joules_est": 60.11, "sample_count": 21, "duration_seconds": 2.135}, "timestamp": "2026-01-19T14:22:02.682913"}
{"image_index": 58, "image_name": "000000006040.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006040.jpg", "image_width": 640, "image_height": 351, "image_resolution": "640x351", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1607.111, "latencies_ms": [1607.111], "images_per_second": 0.622, "prompt_tokens": 1111, "response_tokens_est": 20, "n_tiles": 1, "output_text": " A blue and white train is parked at a station, with trees and other vehicles in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.6, "ram_available_mb": 99000.6, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 26771.6, "ram_available_mb": 99000.6, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.75, "peak": 40.18, "min": 16.56}, "VIN": {"avg": 75.09, "peak": 122.46, "min": 29.03}, "VIN_SYS_5V0": {"avg": 14.88, "peak": 16.05, "min": 12.36}, "VDD_CPU_SOC_MSS": {"avg": 15.97, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 30.75, "energy_joules_est": 49.44, "sample_count": 16, "duration_seconds": 1.608}, "timestamp": "2026-01-19T14:22:04.356626"}
{"image_index": 58, "image_name": "000000006040.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006040.jpg", "image_width": 640, "image_height": 351, "image_resolution": "640x351", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1452.211, "latencies_ms": [1452.211], "images_per_second": 0.689, "prompt_tokens": 1109, "response_tokens_est": 14, "n_tiles": 1, "output_text": " The train is blue and white, and the sky is clear blue.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.6, "ram_available_mb": 99000.6, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 26771.6, "ram_available_mb": 99000.6, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.5, "peak": 40.56, "min": 18.92}, "VIN": {"avg": 80.39, "peak": 122.27, "min": 29.95}, "VIN_SYS_5V0": {"avg": 14.91, "peak": 16.05, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 16.0, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 32.5, "energy_joules_est": 47.21, "sample_count": 14, "duration_seconds": 1.453}, "timestamp": "2026-01-19T14:22:05.823162"}
{"image_index": 59, "image_name": "000000006213.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006213.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1771.92, "latencies_ms": [1771.92], "images_per_second": 0.564, "prompt_tokens": 1099, "response_tokens_est": 27, "n_tiles": 1, "output_text": " The image depicts a bathroom with a shower area, a bathtub, a double sink vanity, and a red bath mat on the floor.", "error": null, "sys_before": {"cpu_percent": 22.2, "ram_used_mb": 26771.6, "ram_available_mb": 99000.6, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26771.6, "ram_available_mb": 99000.6, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.89, "peak": 40.95, "min": 22.46}, "VIN": {"avg": 76.12, "peak": 124.01, "min": 40.35}, "VIN_SYS_5V0": {"avg": 15.2, "peak": 16.15, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 16.33, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 31.89, "energy_joules_est": 56.53, "sample_count": 17, "duration_seconds": 1.773}, "timestamp": "2026-01-19T14:22:07.602598"}
{"image_index": 59, "image_name": "000000006213.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006213.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2631.614, "latencies_ms": [2631.614], "images_per_second": 0.38, "prompt_tokens": 1113, "response_tokens_est": 61, "n_tiles": 1, "output_text": " 1. Bathtub: 1\n2. Shower: 1\n3. Mirror: 2\n4. Sink: 1\n5. Cabinet: 1\n6. Towel: 1\n7. Rug: 1\n8. Light: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.6, "ram_available_mb": 99000.6, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26771.6, "ram_available_mb": 99000.6, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.49, "peak": 40.57, "min": 17.73}, "VIN": {"avg": 70.68, "peak": 126.27, "min": 28.75}, "VIN_SYS_5V0": {"avg": 15.11, "peak": 16.15, "min": 13.29}, "VDD_CPU_SOC_MSS": {"avg": 16.28, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 27.49, "energy_joules_est": 72.36, "sample_count": 26, "duration_seconds": 2.632}, "timestamp": "2026-01-19T14:22:10.316184"}
{"image_index": 59, "image_name": "000000006213.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006213.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2134.601, "latencies_ms": [2134.601], "images_per_second": 0.468, "prompt_tokens": 1117, "response_tokens_est": 40, "n_tiles": 1, "output_text": " The shower is located to the left of the sink, and the bathtub is situated behind the shower curtain. The sink is positioned in the foreground, with the mirror above it reflecting the bathroom's interior.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26771.6, "ram_available_mb": 99000.6, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26771.6, "ram_available_mb": 99000.6, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.22, "peak": 39.77, "min": 15.38}, "VIN": {"avg": 73.24, "peak": 119.63, "min": 28.84}, "VIN_SYS_5V0": {"avg": 14.84, "peak": 16.05, "min": 12.15}, "VDD_CPU_SOC_MSS": {"avg": 15.84, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 28.22, "energy_joules_est": 60.25, "sample_count": 21, "duration_seconds": 2.135}, "timestamp": "2026-01-19T14:22:12.507541"}
{"image_index": 59, "image_name": "000000006213.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006213.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1506.723, "latencies_ms": [1506.723], "images_per_second": 0.664, "prompt_tokens": 1111, "response_tokens_est": 16, "n_tiles": 1, "output_text": " A bathroom with a shower, bathtub, and sink is shown in the image.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26771.6, "ram_available_mb": 99000.6, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 26771.6, "ram_available_mb": 99000.6, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.2, "peak": 40.18, "min": 16.56}, "VIN": {"avg": 74.61, "peak": 125.84, "min": 28.39}, "VIN_SYS_5V0": {"avg": 14.85, "peak": 16.05, "min": 12.36}, "VDD_CPU_SOC_MSS": {"avg": 15.96, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 31.2, "energy_joules_est": 47.02, "sample_count": 15, "duration_seconds": 1.507}, "timestamp": "2026-01-19T14:22:14.072464"}
{"image_index": 59, "image_name": "000000006213.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006213.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1971.1, "latencies_ms": [1971.1], "images_per_second": 0.507, "prompt_tokens": 1109, "response_tokens_est": 34, "n_tiles": 1, "output_text": " The bathroom has a warm color scheme with beige walls and a red rug on the floor. The lighting is bright and natural, coming from a window in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.6, "ram_available_mb": 99000.6, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26771.6, "ram_available_mb": 99000.6, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.19, "peak": 40.57, "min": 20.1}, "VIN": {"avg": 68.73, "peak": 127.51, "min": 30.34}, "VIN_SYS_5V0": {"avg": 14.94, "peak": 16.05, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 16.04, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 30.19, "energy_joules_est": 59.52, "sample_count": 19, "duration_seconds": 1.971}, "timestamp": "2026-01-19T14:22:16.059940"}
{"image_index": 60, "image_name": "000000006460.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006460.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1808.966, "latencies_ms": [1808.966], "images_per_second": 0.553, "prompt_tokens": 1099, "response_tokens_est": 29, "n_tiles": 1, "output_text": " A black and white photo captures a surfer skillfully riding a wave, with the word \"STAR\" prominently displayed in the top right corner.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 26771.6, "ram_available_mb": 99000.6, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26771.6, "ram_available_mb": 99000.6, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.2, "peak": 40.57, "min": 18.13}, "VIN": {"avg": 73.3, "peak": 115.89, "min": 28.93}, "VIN_SYS_5V0": {"avg": 15.11, "peak": 16.15, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 16.16, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 30.2, "energy_joules_est": 54.64, "sample_count": 18, "duration_seconds": 1.809}, "timestamp": "2026-01-19T14:22:17.942919"}
{"image_index": 60, "image_name": "000000006460.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006460.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2954.442, "latencies_ms": [2954.442], "images_per_second": 0.338, "prompt_tokens": 1113, "response_tokens_est": 72, "n_tiles": 1, "output_text": " 1. Surfer: 1\n2. Surfboard: 1\n3. Wave: 1\n4. Water: 1\n5. Surfboard leash: 1\n6. Surfboard grip: 1\n7. Surfboard fin: 1\n8. Surfboard deck: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.6, "ram_available_mb": 99000.6, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 26771.6, "ram_available_mb": 99000.6, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.27, "peak": 40.18, "min": 17.35}, "VIN": {"avg": 67.81, "peak": 119.83, "min": 28.66}, "VIN_SYS_5V0": {"avg": 14.83, "peak": 16.05, "min": 12.15}, "VDD_CPU_SOC_MSS": {"avg": 15.89, "peak": 17.32, "min": 13.4}}, "power_watts_avg": 26.27, "energy_joules_est": 77.62, "sample_count": 29, "duration_seconds": 2.955}, "timestamp": "2026-01-19T14:22:20.966539"}
{"image_index": 60, "image_name": "000000006460.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006460.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2209.953, "latencies_ms": [2209.953], "images_per_second": 0.452, "prompt_tokens": 1117, "response_tokens_est": 43, "n_tiles": 1, "output_text": " The surfer is positioned in the foreground, riding a wave that dominates the majority of the image. The surfer is in the center of the image, with the wave extending from the left to the right side.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.6, "ram_available_mb": 99000.6, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26771.6, "ram_available_mb": 99000.6, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.83, "peak": 39.78, "min": 15.38}, "VIN": {"avg": 73.04, "peak": 118.57, "min": 28.78}, "VIN_SYS_5V0": {"avg": 14.79, "peak": 16.05, "min": 12.15}, "VDD_CPU_SOC_MSS": {"avg": 15.89, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 27.83, "energy_joules_est": 61.53, "sample_count": 22, "duration_seconds": 2.211}, "timestamp": "2026-01-19T14:22:23.261158"}
{"image_index": 60, "image_name": "000000006460.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006460.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1490.245, "latencies_ms": [1490.245], "images_per_second": 0.671, "prompt_tokens": 1111, "response_tokens_est": 15, "n_tiles": 1, "output_text": " A surfer is riding a wave on a surfboard in the ocean.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.6, "ram_available_mb": 99000.6, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 26771.6, "ram_available_mb": 99000.6, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.76, "peak": 39.78, "min": 15.77}, "VIN": {"avg": 73.52, "peak": 112.58, "min": 28.84}, "VIN_SYS_5V0": {"avg": 14.68, "peak": 16.05, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.8, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 30.76, "energy_joules_est": 45.85, "sample_count": 15, "duration_seconds": 1.491}, "timestamp": "2026-01-19T14:22:24.827576"}
{"image_index": 60, "image_name": "000000006460.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006460.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2183.49, "latencies_ms": [2183.49], "images_per_second": 0.458, "prompt_tokens": 1109, "response_tokens_est": 42, "n_tiles": 1, "output_text": " The image is in black and white, with the surfer's silhouette standing out against the white foam of the wave. The lighting is natural, with the sun reflecting off the water, creating a dramatic contrast.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.6, "ram_available_mb": 99000.6, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26771.6, "ram_available_mb": 99000.6, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.29, "peak": 40.57, "min": 20.11}, "VIN": {"avg": 66.68, "peak": 116.86, "min": 29.75}, "VIN_SYS_5V0": {"avg": 14.88, "peak": 16.05, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.84, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 29.29, "energy_joules_est": 63.96, "sample_count": 21, "duration_seconds": 2.184}, "timestamp": "2026-01-19T14:22:27.021290"}
{"image_index": 61, "image_name": "000000006471.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006471.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2807.538, "latencies_ms": [2807.538], "images_per_second": 0.356, "prompt_tokens": 1099, "response_tokens_est": 68, "n_tiles": 1, "output_text": " A baseball player in a white uniform with the number 10 on it is standing at home plate with a bat over his shoulder, while a catcher in a black uniform and a catcher's mitt is crouched behind him, and an umpire in a blue shirt and black pants is standing behind the catcher.", "error": null, "sys_before": {"cpu_percent": 14.3, "ram_used_mb": 26771.6, "ram_available_mb": 99000.6, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 26771.6, "ram_available_mb": 99000.6, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.89, "peak": 40.57, "min": 17.74}, "VIN": {"avg": 70.98, "peak": 121.76, "min": 32.76}, "VIN_SYS_5V0": {"avg": 15.12, "peak": 16.15, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 16.16, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 26.89, "energy_joules_est": 75.52, "sample_count": 27, "duration_seconds": 2.808}, "timestamp": "2026-01-19T14:22:29.835184"}
{"image_index": 61, "image_name": "000000006471.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006471.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2605.008, "latencies_ms": [2605.008], "images_per_second": 0.384, "prompt_tokens": 1113, "response_tokens_est": 60, "n_tiles": 1, "output_text": " 1. batter: 1\n2. catcher: 1\n3. umpire: 1\n4. pitcher: 1\n5. home plate: 1\n6. baseball: 1\n7. grass: 1\n8. dirt: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26771.6, "ram_available_mb": 99000.6, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26771.6, "ram_available_mb": 99000.6, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.37, "peak": 39.78, "min": 17.35}, "VIN": {"avg": 69.38, "peak": 125.19, "min": 29.84}, "VIN_SYS_5V0": {"avg": 15.2, "peak": 16.15, "min": 13.29}, "VDD_CPU_SOC_MSS": {"avg": 16.27, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 27.37, "energy_joules_est": 71.31, "sample_count": 25, "duration_seconds": 2.606}, "timestamp": "2026-01-19T14:22:32.452414"}
{"image_index": 61, "image_name": "000000006471.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006471.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1998.623, "latencies_ms": [1998.623], "images_per_second": 0.5, "prompt_tokens": 1117, "response_tokens_est": 36, "n_tiles": 1, "output_text": " The batter is positioned in the foreground, with the catcher and umpire in the background. The batter is closer to the camera than the catcher and umpire.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26771.6, "ram_available_mb": 99000.6, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26771.6, "ram_available_mb": 99000.6, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.15, "peak": 40.56, "min": 16.95}, "VIN": {"avg": 73.78, "peak": 122.29, "min": 28.08}, "VIN_SYS_5V0": {"avg": 15.11, "peak": 16.15, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 16.24, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 29.15, "energy_joules_est": 58.27, "sample_count": 20, "duration_seconds": 1.999}, "timestamp": "2026-01-19T14:22:34.536979"}
{"image_index": 61, "image_name": "000000006471.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006471.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1514.199, "latencies_ms": [1514.199], "images_per_second": 0.66, "prompt_tokens": 1111, "response_tokens_est": 16, "n_tiles": 1, "output_text": " A baseball game is taking place in a stadium with a crowd of people watching.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.6, "ram_available_mb": 99000.6, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 26771.6, "ram_available_mb": 99000.6, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.94, "peak": 40.18, "min": 16.16}, "VIN": {"avg": 72.55, "peak": 120.6, "min": 27.36}, "VIN_SYS_5V0": {"avg": 14.69, "peak": 15.95, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.83, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 30.94, "energy_joules_est": 46.86, "sample_count": 15, "duration_seconds": 1.515}, "timestamp": "2026-01-19T14:22:36.108844"}
{"image_index": 61, "image_name": "000000006471.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006471.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2759.325, "latencies_ms": [2759.325], "images_per_second": 0.362, "prompt_tokens": 1109, "response_tokens_est": 65, "n_tiles": 1, "output_text": " The image captures a vibrant scene of a baseball game, with the players dressed in crisp white uniforms and the lush green grass of the field contrasting against the blue sky. The lighting is natural, casting a warm glow on the players and the field, and the weather appears to be clear, with no signs of rain or wind.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.6, "ram_available_mb": 99000.6, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 26771.6, "ram_available_mb": 99000.6, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.19, "peak": 40.56, "min": 18.13}, "VIN": {"avg": 69.79, "peak": 122.62, "min": 28.52}, "VIN_SYS_5V0": {"avg": 14.91, "peak": 16.05, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.91, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 27.19, "energy_joules_est": 75.04, "sample_count": 27, "duration_seconds": 2.76}, "timestamp": "2026-01-19T14:22:38.926601"}
{"image_index": 62, "image_name": "000000006614.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006614.jpg", "image_width": 500, "image_height": 396, "image_resolution": "500x396", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1928.391, "latencies_ms": [1928.391], "images_per_second": 0.519, "prompt_tokens": 1432, "response_tokens_est": 17, "n_tiles": 1, "output_text": " A black and white photo of a bunch of fruits and nuts on a white surface.", "error": null, "sys_before": {"cpu_percent": 14.3, "ram_used_mb": 26771.6, "ram_available_mb": 99000.6, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 26771.6, "ram_available_mb": 99000.6, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.2, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.6, "peak": 40.18, "min": 15.77}, "VIN": {"avg": 74.59, "peak": 118.76, "min": 29.96}, "VIN_SYS_5V0": {"avg": 15.29, "peak": 16.76, "min": 12.15}, "VDD_CPU_SOC_MSS": {"avg": 16.41, "peak": 17.71, "min": 13.39}}, "power_watts_avg": 31.6, "energy_joules_est": 60.96, "sample_count": 19, "duration_seconds": 1.929}, "timestamp": "2026-01-19T14:22:40.916567"}
{"image_index": 62, "image_name": "000000006614.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006614.jpg", "image_width": 500, "image_height": 396, "image_resolution": "500x396", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2502.353, "latencies_ms": [2502.353], "images_per_second": 0.4, "prompt_tokens": 1446, "response_tokens_est": 39, "n_tiles": 1, "output_text": " apple: 2, grapes: 10, orange: 1, grapes: 10, grapes: 10, grapes: 10, grapes: 10", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.6, "ram_available_mb": 99000.6, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26771.6, "ram_available_mb": 99000.6, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.24, "peak": 40.57, "min": 18.52}, "VIN": {"avg": 70.84, "peak": 134.61, "min": 28.97}, "VIN_SYS_5V0": {"avg": 15.2, "peak": 16.66, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 16.3, "peak": 17.71, "min": 13.79}}, "power_watts_avg": 30.24, "energy_joules_est": 75.68, "sample_count": 25, "duration_seconds": 2.503}, "timestamp": "2026-01-19T14:22:43.524164"}
{"image_index": 62, "image_name": "000000006614.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006614.jpg", "image_width": 500, "image_height": 396, "image_resolution": "500x396", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3217.752, "latencies_ms": [3217.752], "images_per_second": 0.311, "prompt_tokens": 1450, "response_tokens_est": 62, "n_tiles": 1, "output_text": " The main objects are arranged in a diagonal line from the top left to the bottom right, with the grapes and oranges placed in the foreground and the apples and pears in the background. The apples and pears are positioned towards the top right of the image, while the grapes and oranges are located towards the bottom left.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.6, "ram_available_mb": 99000.6, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26771.6, "ram_available_mb": 99000.6, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.58, "peak": 40.18, "min": 15.77}, "VIN": {"avg": 69.91, "peak": 119.61, "min": 29.83}, "VIN_SYS_5V0": {"avg": 14.92, "peak": 16.46, "min": 11.56}, "VDD_CPU_SOC_MSS": {"avg": 15.89, "peak": 17.71, "min": 12.61}}, "power_watts_avg": 27.58, "energy_joules_est": 88.76, "sample_count": 31, "duration_seconds": 3.218}, "timestamp": "2026-01-19T14:22:46.759244"}
{"image_index": 62, "image_name": "000000006614.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006614.jpg", "image_width": 500, "image_height": 396, "image_resolution": "500x396", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1897.784, "latencies_ms": [1897.784], "images_per_second": 0.527, "prompt_tokens": 1444, "response_tokens_est": 17, "n_tiles": 1, "output_text": " A black and white photo of a bunch of fruits and nuts on a white surface.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.6, "ram_available_mb": 99000.6, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.4, "ram_used_mb": 26771.6, "ram_available_mb": 99000.6, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.18, "peak": 40.18, "min": 16.56}, "VIN": {"avg": 78.08, "peak": 126.95, "min": 30.28}, "VIN_SYS_5V0": {"avg": 15.37, "peak": 16.66, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 16.47, "peak": 17.71, "min": 13.79}}, "power_watts_avg": 32.18, "energy_joules_est": 61.08, "sample_count": 19, "duration_seconds": 1.898}, "timestamp": "2026-01-19T14:22:48.747064"}
{"image_index": 62, "image_name": "000000006614.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006614.jpg", "image_width": 500, "image_height": 396, "image_resolution": "500x396", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2149.202, "latencies_ms": [2149.202], "images_per_second": 0.465, "prompt_tokens": 1442, "response_tokens_est": 25, "n_tiles": 1, "output_text": " The image is a black and white photo with a white background, and the fruits are all in shades of white and gray.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.6, "ram_available_mb": 99000.6, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 26771.6, "ram_available_mb": 99000.6, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.66, "peak": 40.18, "min": 18.92}, "VIN": {"avg": 73.31, "peak": 125.86, "min": 28.9}, "VIN_SYS_5V0": {"avg": 15.07, "peak": 16.46, "min": 11.66}, "VDD_CPU_SOC_MSS": {"avg": 16.22, "peak": 17.71, "min": 13.0}}, "power_watts_avg": 31.66, "energy_joules_est": 68.06, "sample_count": 21, "duration_seconds": 2.15}, "timestamp": "2026-01-19T14:22:50.939495"}
{"image_index": 63, "image_name": "000000006723.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006723.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 3641.649, "latencies_ms": [3641.649], "images_per_second": 0.275, "prompt_tokens": 1099, "response_tokens_est": 99, "n_tiles": 1, "output_text": " The image captures a bustling urban street scene, where a variety of vehicles, including cars and a bus, are in motion, while pedestrians navigate the sidewalks. The buildings, constructed from brick and concrete, display a mix of architectural styles, with some featuring balconies and others boasting large windows. The street is lined with trees, providing a touch of greenery amidst the urban landscape. The sky overhead is a clear blue, suggesting a pleasant day, and the overall atmosphere is one of everyday city life.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 26771.6, "ram_available_mb": 99000.6, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 26771.6, "ram_available_mb": 99000.6, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 25.06, "peak": 40.57, "min": 16.95}, "VIN": {"avg": 68.12, "peak": 91.41, "min": 28.39}, "VIN_SYS_5V0": {"avg": 14.92, "peak": 16.05, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.99, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 25.06, "energy_joules_est": 91.27, "sample_count": 36, "duration_seconds": 3.642}, "timestamp": "2026-01-19T14:22:54.685067"}
{"image_index": 63, "image_name": "000000006723.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006723.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2591.377, "latencies_ms": [2591.377], "images_per_second": 0.386, "prompt_tokens": 1113, "response_tokens_est": 58, "n_tiles": 1, "output_text": " 1. street: 2 lanes\n2. car: 2\n3. building: 3\n4. sidewalk: 1\n5. streetlight: 2\n6. tree: 1\n7. road: 1\n8. sky: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.6, "ram_available_mb": 99000.6, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26771.6, "ram_available_mb": 99000.6, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.79, "peak": 40.16, "min": 14.59}, "VIN": {"avg": 69.27, "peak": 122.67, "min": 28.92}, "VIN_SYS_5V0": {"avg": 14.82, "peak": 15.95, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.84, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 26.79, "energy_joules_est": 69.43, "sample_count": 25, "duration_seconds": 2.592}, "timestamp": "2026-01-19T14:22:57.293601"}
{"image_index": 63, "image_name": "000000006723.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006723.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2937.21, "latencies_ms": [2937.21], "images_per_second": 0.34, "prompt_tokens": 1117, "response_tokens_est": 72, "n_tiles": 1, "output_text": " The main objects in the image are positioned in a way that the street is in the foreground, with the buildings and trees in the background. The street is on the left side of the image, while the buildings and trees are on the right side. The buildings are located on both sides of the street, with the trees lining the sidewalk on the right side.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.6, "ram_available_mb": 99000.6, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 26771.6, "ram_available_mb": 99000.6, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.26, "peak": 40.56, "min": 16.55}, "VIN": {"avg": 71.06, "peak": 113.24, "min": 30.14}, "VIN_SYS_5V0": {"avg": 15.07, "peak": 16.15, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 16.06, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 26.26, "energy_joules_est": 77.14, "sample_count": 29, "duration_seconds": 2.938}, "timestamp": "2026-01-19T14:23:00.298658"}
{"image_index": 63, "image_name": "000000006723.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006723.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2459.718, "latencies_ms": [2459.718], "images_per_second": 0.407, "prompt_tokens": 1111, "response_tokens_est": 53, "n_tiles": 1, "output_text": " The image captures a bustling urban street scene, where modern buildings of varying heights and designs line the street. The buildings are predominantly white and red, with some featuring balconies and windows. The street is busy with cars and pedestrians, indicating a lively and active environment.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.6, "ram_available_mb": 99000.6, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26771.6, "ram_available_mb": 99000.6, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.2, "peak": 40.16, "min": 15.38}, "VIN": {"avg": 68.41, "peak": 120.89, "min": 28.76}, "VIN_SYS_5V0": {"avg": 14.84, "peak": 16.05, "min": 12.15}, "VDD_CPU_SOC_MSS": {"avg": 15.8, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 27.2, "energy_joules_est": 66.93, "sample_count": 24, "duration_seconds": 2.461}, "timestamp": "2026-01-19T14:23:02.803617"}
{"image_index": 63, "image_name": "000000006723.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006723.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1907.807, "latencies_ms": [1907.807], "images_per_second": 0.524, "prompt_tokens": 1109, "response_tokens_est": 32, "n_tiles": 1, "output_text": " The image features a street with a clear blue sky and a few clouds. The buildings are made of brick and concrete, and the road is paved with asphalt.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.6, "ram_available_mb": 99000.6, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26771.6, "ram_available_mb": 99000.6, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.36, "peak": 40.16, "min": 16.16}, "VIN": {"avg": 66.01, "peak": 123.41, "min": 28.42}, "VIN_SYS_5V0": {"avg": 14.92, "peak": 16.05, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 16.04, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 29.36, "energy_joules_est": 56.03, "sample_count": 19, "duration_seconds": 1.908}, "timestamp": "2026-01-19T14:23:04.777889"}
{"image_index": 64, "image_name": "000000006763.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006763.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1998.954, "latencies_ms": [1998.954], "images_per_second": 0.5, "prompt_tokens": 1100, "response_tokens_est": 36, "n_tiles": 1, "output_text": " In the image, there are two people standing close to each other, with one person wearing a blue shirt and the other wearing a white shirt, both smiling and embracing each other.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.6, "ram_available_mb": 99000.6, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26771.6, "ram_available_mb": 99000.6, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.06, "peak": 40.57, "min": 16.95}, "VIN": {"avg": 72.21, "peak": 126.61, "min": 27.53}, "VIN_SYS_5V0": {"avg": 14.89, "peak": 16.05, "min": 12.25}, "VDD_CPU_SOC_MSS": {"avg": 15.91, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 29.06, "energy_joules_est": 58.11, "sample_count": 20, "duration_seconds": 2.0}, "timestamp": "2026-01-19T14:23:06.863441"}
{"image_index": 64, "image_name": "000000006763.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006763.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2543.65, "latencies_ms": [2543.65], "images_per_second": 0.393, "prompt_tokens": 1114, "response_tokens_est": 57, "n_tiles": 1, "output_text": " 1. person: 2\n2. tie: 1\n3. television: 1\n4. wall: 1\n5. chair: 1\n6. table: 1\n7. wall clock: 1\n8. picture: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.6, "ram_available_mb": 99000.6, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26771.6, "ram_available_mb": 99000.6, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.15, "peak": 40.16, "min": 16.55}, "VIN": {"avg": 72.07, "peak": 118.02, "min": 30.26}, "VIN_SYS_5V0": {"avg": 14.89, "peak": 16.05, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 15.92, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 27.15, "energy_joules_est": 69.07, "sample_count": 25, "duration_seconds": 2.544}, "timestamp": "2026-01-19T14:23:09.453795"}
{"image_index": 64, "image_name": "000000006763.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006763.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1990.081, "latencies_ms": [1990.081], "images_per_second": 0.502, "prompt_tokens": 1118, "response_tokens_est": 36, "n_tiles": 1, "output_text": " The woman is standing to the right of the man, with her arm around his shoulder. The man is standing in front of a bar counter, with the woman standing behind him.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.6, "ram_available_mb": 99000.6, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26771.6, "ram_available_mb": 99000.6, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.78, "peak": 40.18, "min": 16.16}, "VIN": {"avg": 70.59, "peak": 119.7, "min": 27.19}, "VIN_SYS_5V0": {"avg": 15.02, "peak": 16.15, "min": 12.56}, "VDD_CPU_SOC_MSS": {"avg": 16.02, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 28.78, "energy_joules_est": 57.28, "sample_count": 20, "duration_seconds": 1.99}, "timestamp": "2026-01-19T14:23:11.537838"}
{"image_index": 64, "image_name": "000000006763.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006763.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1476.125, "latencies_ms": [1476.125], "images_per_second": 0.677, "prompt_tokens": 1112, "response_tokens_est": 15, "n_tiles": 1, "output_text": " Two people are standing close together in a bar, smiling at the camera.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.6, "ram_available_mb": 99000.6, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 26771.6, "ram_available_mb": 99000.6, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.6, "peak": 39.78, "min": 15.77}, "VIN": {"avg": 74.68, "peak": 125.67, "min": 28.99}, "VIN_SYS_5V0": {"avg": 14.72, "peak": 16.15, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.77, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 30.6, "energy_joules_est": 45.18, "sample_count": 15, "duration_seconds": 1.477}, "timestamp": "2026-01-19T14:23:13.103633"}
{"image_index": 64, "image_name": "000000006763.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006763.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1728.647, "latencies_ms": [1728.647], "images_per_second": 0.578, "prompt_tokens": 1110, "response_tokens_est": 24, "n_tiles": 1, "output_text": " The image is taken in a dimly lit room with a warm ambiance, and the subjects are wearing formal attire.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26771.6, "ram_available_mb": 99000.6, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26771.6, "ram_available_mb": 99000.6, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.8, "peak": 40.18, "min": 18.92}, "VIN": {"avg": 75.92, "peak": 126.85, "min": 28.64}, "VIN_SYS_5V0": {"avg": 14.78, "peak": 16.05, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.79, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 30.8, "energy_joules_est": 53.26, "sample_count": 17, "duration_seconds": 1.729}, "timestamp": "2026-01-19T14:23:14.871584"}
{"image_index": 65, "image_name": "000000006771.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006771.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1549.141, "latencies_ms": [1549.141], "images_per_second": 0.646, "prompt_tokens": 1099, "response_tokens_est": 18, "n_tiles": 1, "output_text": " A woman in a costume is smiling at the camera while standing in a crowd of people.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 26771.6, "ram_available_mb": 99000.6, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 26771.6, "ram_available_mb": 99000.6, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.78, "peak": 40.57, "min": 18.53}, "VIN": {"avg": 78.36, "peak": 121.88, "min": 30.56}, "VIN_SYS_5V0": {"avg": 14.98, "peak": 16.05, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.98, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 31.78, "energy_joules_est": 49.24, "sample_count": 15, "duration_seconds": 1.549}, "timestamp": "2026-01-19T14:23:16.442839"}
{"image_index": 65, "image_name": "000000006771.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006771.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2587.311, "latencies_ms": [2587.311], "images_per_second": 0.387, "prompt_tokens": 1113, "response_tokens_est": 59, "n_tiles": 1, "output_text": " 1. person: 1\n2. hat: 1\n3. headband: 1\n4. earring: 1\n5. necklace: 1\n6. bracelet: 1\n7. shirt: 1\n8. pants: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.6, "ram_available_mb": 99000.6, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 26771.6, "ram_available_mb": 99000.6, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.1, "peak": 40.95, "min": 19.7}, "VIN": {"avg": 71.46, "peak": 120.96, "min": 29.15}, "VIN_SYS_5V0": {"avg": 15.13, "peak": 16.15, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 16.13, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 28.1, "energy_joules_est": 72.73, "sample_count": 25, "duration_seconds": 2.588}, "timestamp": "2026-01-19T14:23:19.049675"}
{"image_index": 65, "image_name": "000000006771.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006771.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2054.663, "latencies_ms": [2054.663], "images_per_second": 0.487, "prompt_tokens": 1117, "response_tokens_est": 38, "n_tiles": 1, "output_text": " The woman is in the foreground, wearing a black and gold costume, while the man is in the background, wearing a white hat. The woman is closer to the camera than the man.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.6, "ram_available_mb": 99000.6, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26771.6, "ram_available_mb": 99000.6, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.11, "peak": 40.56, "min": 16.55}, "VIN": {"avg": 71.88, "peak": 111.1, "min": 28.84}, "VIN_SYS_5V0": {"avg": 15.13, "peak": 16.15, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 16.2, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 29.11, "energy_joules_est": 59.82, "sample_count": 20, "duration_seconds": 2.055}, "timestamp": "2026-01-19T14:23:21.133514"}
{"image_index": 65, "image_name": "000000006771.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006771.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1599.197, "latencies_ms": [1599.197], "images_per_second": 0.625, "prompt_tokens": 1111, "response_tokens_est": 20, "n_tiles": 1, "output_text": " A group of people are gathered in an outdoor setting, dressed in elaborate costumes and engaging in conversation.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.6, "ram_available_mb": 99000.6, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 26771.6, "ram_available_mb": 99000.6, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.05, "peak": 40.56, "min": 17.35}, "VIN": {"avg": 72.47, "peak": 107.93, "min": 28.54}, "VIN_SYS_5V0": {"avg": 15.02, "peak": 16.15, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 16.19, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 31.05, "energy_joules_est": 49.67, "sample_count": 16, "duration_seconds": 1.6}, "timestamp": "2026-01-19T14:23:22.804197"}
{"image_index": 65, "image_name": "000000006771.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006771.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2000.479, "latencies_ms": [2000.479], "images_per_second": 0.5, "prompt_tokens": 1109, "response_tokens_est": 35, "n_tiles": 1, "output_text": " The image is taken during the day with natural lighting, and the colors are vibrant and bright. The woman is wearing a black and gold costume, which is shiny and reflective.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26771.6, "ram_available_mb": 99000.6, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26771.6, "ram_available_mb": 99000.6, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.39, "peak": 40.57, "min": 18.91}, "VIN": {"avg": 73.84, "peak": 121.29, "min": 29.58}, "VIN_SYS_5V0": {"avg": 14.8, "peak": 16.05, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.83, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 29.39, "energy_joules_est": 58.81, "sample_count": 20, "duration_seconds": 2.001}, "timestamp": "2026-01-19T14:23:24.891438"}
{"image_index": 66, "image_name": "000000006818.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006818.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2733.869, "latencies_ms": [2733.869], "images_per_second": 0.366, "prompt_tokens": 1100, "response_tokens_est": 64, "n_tiles": 1, "output_text": " The image depicts a small bathroom with white tiled walls and floor, a white toilet, a white sink, a white shower head, a white pipe, a white toilet tank, a white soap dish, a white toilet paper holder, a green bucket, a red bucket, a white drain, and a white window.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 26771.6, "ram_available_mb": 99000.6, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26771.6, "ram_available_mb": 99000.6, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.36, "peak": 39.78, "min": 16.16}, "VIN": {"avg": 72.12, "peak": 128.3, "min": 28.48}, "VIN_SYS_5V0": {"avg": 14.84, "peak": 16.05, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.91, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 26.36, "energy_joules_est": 72.09, "sample_count": 27, "duration_seconds": 2.735}, "timestamp": "2026-01-19T14:23:27.703845"}
{"image_index": 66, "image_name": "000000006818.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006818.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2174.285, "latencies_ms": [2174.285], "images_per_second": 0.46, "prompt_tokens": 1114, "response_tokens_est": 42, "n_tiles": 1, "output_text": " 1. white pipe\n2. white toilet\n3. white shower head\n4. white sink\n5. white toilet tank\n6. white bathtub\n7. white window\n8. white bucket", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.6, "ram_available_mb": 99000.6, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26771.6, "ram_available_mb": 99000.6, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.14, "peak": 39.78, "min": 15.38}, "VIN": {"avg": 69.09, "peak": 125.44, "min": 29.56}, "VIN_SYS_5V0": {"avg": 14.91, "peak": 16.05, "min": 12.15}, "VDD_CPU_SOC_MSS": {"avg": 15.99, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 28.14, "energy_joules_est": 61.2, "sample_count": 21, "duration_seconds": 2.175}, "timestamp": "2026-01-19T14:23:29.895169"}
{"image_index": 66, "image_name": "000000006818.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006818.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2380.55, "latencies_ms": [2380.55], "images_per_second": 0.42, "prompt_tokens": 1118, "response_tokens_est": 51, "n_tiles": 1, "output_text": " The green bucket is located in the foreground, near the wall, while the red bucket is positioned further back, near the sink. The white pipe runs vertically from the left side of the image to the right, creating a vertical line that divides the space.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 26771.6, "ram_available_mb": 99000.6, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26771.6, "ram_available_mb": 99000.6, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.94, "peak": 40.18, "min": 17.35}, "VIN": {"avg": 62.95, "peak": 114.74, "min": 28.82}, "VIN_SYS_5V0": {"avg": 15.14, "peak": 16.26, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 16.14, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 27.94, "energy_joules_est": 66.53, "sample_count": 23, "duration_seconds": 2.381}, "timestamp": "2026-01-19T14:23:32.290081"}
{"image_index": 66, "image_name": "000000006818.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006818.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1594.639, "latencies_ms": [1594.639], "images_per_second": 0.627, "prompt_tokens": 1112, "response_tokens_est": 21, "n_tiles": 1, "output_text": " A small bathroom with white walls and a white floor has a shower, a toilet, and a sink.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.6, "ram_available_mb": 99000.6, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 26771.6, "ram_available_mb": 99000.6, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.65, "peak": 39.38, "min": 16.95}, "VIN": {"avg": 76.82, "peak": 126.17, "min": 28.62}, "VIN_SYS_5V0": {"avg": 15.2, "peak": 16.26, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 16.27, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 30.65, "energy_joules_est": 48.88, "sample_count": 16, "duration_seconds": 1.595}, "timestamp": "2026-01-19T14:23:33.960440"}
{"image_index": 66, "image_name": "000000006818.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006818.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1579.49, "latencies_ms": [1579.49], "images_per_second": 0.633, "prompt_tokens": 1110, "response_tokens_est": 19, "n_tiles": 1, "output_text": " The bathroom is white with a blue and green bucket, and a window with a black frame.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26771.6, "ram_available_mb": 99000.6, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 26771.5, "ram_available_mb": 99000.6, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.97, "peak": 40.18, "min": 18.52}, "VIN": {"avg": 70.36, "peak": 119.8, "min": 28.6}, "VIN_SYS_5V0": {"avg": 14.79, "peak": 16.15, "min": 12.25}, "VDD_CPU_SOC_MSS": {"avg": 15.92, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 30.97, "energy_joules_est": 48.93, "sample_count": 16, "duration_seconds": 1.58}, "timestamp": "2026-01-19T14:23:35.628978"}
{"image_index": 67, "image_name": "000000006894.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006894.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1750.317, "latencies_ms": [1750.317], "images_per_second": 0.571, "prompt_tokens": 1099, "response_tokens_est": 26, "n_tiles": 1, "output_text": " A man wearing glasses and a green shirt is standing next to a large gray elephant in a natural setting with trees in the background.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 26771.5, "ram_available_mb": 99000.6, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26771.5, "ram_available_mb": 99000.6, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.85, "peak": 40.57, "min": 18.53}, "VIN": {"avg": 72.91, "peak": 112.06, "min": 30.2}, "VIN_SYS_5V0": {"avg": 14.82, "peak": 15.95, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.86, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 30.85, "energy_joules_est": 54.02, "sample_count": 17, "duration_seconds": 1.751}, "timestamp": "2026-01-19T14:23:37.406636"}
{"image_index": 67, "image_name": "000000006894.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006894.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2092.136, "latencies_ms": [2092.136], "images_per_second": 0.478, "prompt_tokens": 1113, "response_tokens_est": 40, "n_tiles": 1, "output_text": " elephant: 1, man: 1, glasses: 1, shirt: 1, elephant's trunk: 1, elephant's ear: 1, elephant's eye: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.5, "ram_available_mb": 99000.6, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26771.5, "ram_available_mb": 99000.6, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.28, "peak": 40.97, "min": 18.52}, "VIN": {"avg": 76.64, "peak": 124.44, "min": 27.3}, "VIN_SYS_5V0": {"avg": 15.13, "peak": 16.15, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 16.1, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 29.28, "energy_joules_est": 61.27, "sample_count": 21, "duration_seconds": 2.093}, "timestamp": "2026-01-19T14:23:39.598095"}
{"image_index": 67, "image_name": "000000006894.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006894.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2019.749, "latencies_ms": [2019.749], "images_per_second": 0.495, "prompt_tokens": 1117, "response_tokens_est": 36, "n_tiles": 1, "output_text": " The man is standing to the right of the elephant, with the elephant's trunk reaching towards his face. The man is in the foreground, while the elephant is in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.5, "ram_available_mb": 99000.6, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26771.5, "ram_available_mb": 99000.6, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.86, "peak": 40.18, "min": 15.77}, "VIN": {"avg": 69.63, "peak": 122.57, "min": 30.5}, "VIN_SYS_5V0": {"avg": 14.78, "peak": 15.95, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.85, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 28.86, "energy_joules_est": 58.3, "sample_count": 20, "duration_seconds": 2.02}, "timestamp": "2026-01-19T14:23:41.683837"}
{"image_index": 67, "image_name": "000000006894.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006894.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1603.562, "latencies_ms": [1603.562], "images_per_second": 0.624, "prompt_tokens": 1111, "response_tokens_est": 20, "n_tiles": 1, "output_text": " A man wearing glasses is standing next to an elephant in a natural setting with trees in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.5, "ram_available_mb": 99000.6, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 26771.5, "ram_available_mb": 99000.6, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.8, "peak": 40.16, "min": 16.95}, "VIN": {"avg": 74.02, "peak": 119.7, "min": 30.26}, "VIN_SYS_5V0": {"avg": 14.88, "peak": 16.05, "min": 12.25}, "VDD_CPU_SOC_MSS": {"avg": 15.94, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 30.8, "energy_joules_est": 49.4, "sample_count": 16, "duration_seconds": 1.604}, "timestamp": "2026-01-19T14:23:43.353564"}
{"image_index": 67, "image_name": "000000006894.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006894.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2765.334, "latencies_ms": [2765.334], "images_per_second": 0.362, "prompt_tokens": 1109, "response_tokens_est": 62, "n_tiles": 1, "output_text": " The image features a man wearing glasses and a green shirt standing next to a large gray elephant. The elephant's skin is wrinkled and rough, and the man's face is covered in sweat, indicating that he is hot. The background is a lush green forest, and the lighting suggests that it is daytime.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.5, "ram_available_mb": 99000.6, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26771.5, "ram_available_mb": 99000.6, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.88, "peak": 40.57, "min": 18.13}, "VIN": {"avg": 66.33, "peak": 105.34, "min": 29.59}, "VIN_SYS_5V0": {"avg": 14.87, "peak": 16.05, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.82, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 26.88, "energy_joules_est": 74.34, "sample_count": 27, "duration_seconds": 2.766}, "timestamp": "2026-01-19T14:23:46.164458"}
{"image_index": 68, "image_name": "000000006954.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006954.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1589.845, "latencies_ms": [1589.845], "images_per_second": 0.629, "prompt_tokens": 1099, "response_tokens_est": 20, "n_tiles": 1, "output_text": " Five children are sitting on the grass holding frisbees, with trees and bushes in the background.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 26771.5, "ram_available_mb": 99000.6, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 26771.8, "ram_available_mb": 99000.4, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.46, "peak": 40.57, "min": 15.38}, "VIN": {"avg": 73.66, "peak": 107.89, "min": 29.87}, "VIN_SYS_5V0": {"avg": 14.94, "peak": 16.05, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.92, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 30.46, "energy_joules_est": 48.44, "sample_count": 16, "duration_seconds": 1.59}, "timestamp": "2026-01-19T14:23:47.832683"}
{"image_index": 68, "image_name": "000000006954.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006954.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1268.008, "latencies_ms": [1268.008], "images_per_second": 0.789, "prompt_tokens": 1113, "response_tokens_est": 7, "n_tiles": 1, "output_text": " 1. boy: 5", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.8, "ram_available_mb": 99000.4, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.2, "ram_used_mb": 26771.8, "ram_available_mb": 99000.4, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.67, "peak": 40.57, "min": 18.53}, "VIN": {"avg": 75.72, "peak": 109.74, "min": 28.89}, "VIN_SYS_5V0": {"avg": 14.79, "peak": 16.05, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.9, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 32.67, "energy_joules_est": 41.44, "sample_count": 13, "duration_seconds": 1.268}, "timestamp": "2026-01-19T14:23:49.190353"}
{"image_index": 68, "image_name": "000000006954.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006954.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3760.278, "latencies_ms": [3760.278], "images_per_second": 0.266, "prompt_tokens": 1117, "response_tokens_est": 103, "n_tiles": 1, "output_text": " The frisbees are positioned in the foreground of the image, with the boy in the blue shirt holding the \"Ultimate\" frisbee closest to the camera. The boy in the red shirt is sitting to the left of the boy in the blue shirt, and the boy in the brown shirt is sitting to the right of the boy in the blue shirt. The frisbees are positioned in the middle of the image, with the boy in the brown shirt holding the frisbee closest to the camera.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.8, "ram_available_mb": 99000.4, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 26771.8, "ram_available_mb": 99000.4, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 25.46, "peak": 40.97, "min": 17.74}, "VIN": {"avg": 60.48, "peak": 119.13, "min": 28.8}, "VIN_SYS_5V0": {"avg": 14.82, "peak": 16.05, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.87, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 25.46, "energy_joules_est": 95.75, "sample_count": 37, "duration_seconds": 3.761}, "timestamp": "2026-01-19T14:23:53.027431"}
{"image_index": 68, "image_name": "000000006954.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006954.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1424.726, "latencies_ms": [1424.726], "images_per_second": 0.702, "prompt_tokens": 1111, "response_tokens_est": 13, "n_tiles": 1, "output_text": " Five children are sitting on the grass playing with frisbees.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.8, "ram_available_mb": 99000.4, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 26771.8, "ram_available_mb": 99000.4, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.24, "peak": 40.18, "min": 15.38}, "VIN": {"avg": 76.19, "peak": 128.75, "min": 30.23}, "VIN_SYS_5V0": {"avg": 14.77, "peak": 16.05, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.83, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 31.24, "energy_joules_est": 44.52, "sample_count": 14, "duration_seconds": 1.425}, "timestamp": "2026-01-19T14:23:54.484046"}
{"image_index": 68, "image_name": "000000006954.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006954.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1844.264, "latencies_ms": [1844.264], "images_per_second": 0.542, "prompt_tokens": 1109, "response_tokens_est": 29, "n_tiles": 1, "output_text": " The image is taken in a bright, sunny day with clear blue sky. The children are sitting on the grass, which is green and lush.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.8, "ram_available_mb": 99000.4, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26771.8, "ram_available_mb": 99000.4, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.34, "peak": 40.97, "min": 21.28}, "VIN": {"avg": 73.29, "peak": 122.31, "min": 27.6}, "VIN_SYS_5V0": {"avg": 15.09, "peak": 16.15, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 16.16, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 31.34, "energy_joules_est": 57.81, "sample_count": 18, "duration_seconds": 1.845}, "timestamp": "2026-01-19T14:23:56.353496"}
{"image_index": 69, "image_name": "000000007088.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007088.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1763.011, "latencies_ms": [1763.011], "images_per_second": 0.567, "prompt_tokens": 1100, "response_tokens_est": 26, "n_tiles": 1, "output_text": " A young girl wearing a red coat and holding a black umbrella with pink designs stands on a wet sidewalk in front of a bush.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 26771.8, "ram_available_mb": 99000.4, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26771.8, "ram_available_mb": 99000.4, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.5, "peak": 40.18, "min": 17.73}, "VIN": {"avg": 69.53, "peak": 103.56, "min": 30.94}, "VIN_SYS_5V0": {"avg": 15.07, "peak": 16.05, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 16.12, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 30.5, "energy_joules_est": 53.8, "sample_count": 17, "duration_seconds": 1.764}, "timestamp": "2026-01-19T14:23:58.138694"}
{"image_index": 69, "image_name": "000000007088.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007088.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1942.366, "latencies_ms": [1942.366], "images_per_second": 0.515, "prompt_tokens": 1114, "response_tokens_est": 34, "n_tiles": 1, "output_text": " umbrella: 1, child: 1, sidewalk: 1, house: 1, bush: 1, car: 1, tree: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.8, "ram_available_mb": 99000.4, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26771.8, "ram_available_mb": 99000.4, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.9, "peak": 40.56, "min": 18.92}, "VIN": {"avg": 68.55, "peak": 118.69, "min": 29.92}, "VIN_SYS_5V0": {"avg": 15.15, "peak": 16.26, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 16.27, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 29.9, "energy_joules_est": 58.09, "sample_count": 19, "duration_seconds": 1.943}, "timestamp": "2026-01-19T14:24:00.119568"}
{"image_index": 69, "image_name": "000000007088.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007088.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2554.127, "latencies_ms": [2554.127], "images_per_second": 0.392, "prompt_tokens": 1118, "response_tokens_est": 57, "n_tiles": 1, "output_text": " The child is standing in the foreground of the image, holding the umbrella in front of her. The umbrella is positioned above her head, and she is standing on a wet sidewalk. The background of the image features a house and trees, indicating that the child is in a residential area.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26771.8, "ram_available_mb": 99000.4, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26771.7, "ram_available_mb": 99000.4, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.37, "peak": 40.57, "min": 17.35}, "VIN": {"avg": 69.8, "peak": 101.89, "min": 29.29}, "VIN_SYS_5V0": {"avg": 15.01, "peak": 16.15, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 16.14, "peak": 17.71, "min": 13.79}}, "power_watts_avg": 27.37, "energy_joules_est": 69.91, "sample_count": 25, "duration_seconds": 2.554}, "timestamp": "2026-01-19T14:24:02.723056"}
{"image_index": 69, "image_name": "000000007088.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007088.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1383.636, "latencies_ms": [1383.636], "images_per_second": 0.723, "prompt_tokens": 1112, "response_tokens_est": 12, "n_tiles": 1, "output_text": " A little girl is standing on the sidewalk with an umbrella.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.7, "ram_available_mb": 99000.4, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 26771.7, "ram_available_mb": 99000.4, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.18, "peak": 39.78, "min": 16.16}, "VIN": {"avg": 75.26, "peak": 119.37, "min": 30.47}, "VIN_SYS_5V0": {"avg": 14.92, "peak": 16.26, "min": 12.46}, "VDD_CPU_SOC_MSS": {"avg": 16.06, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 31.18, "energy_joules_est": 43.17, "sample_count": 14, "duration_seconds": 1.384}, "timestamp": "2026-01-19T14:24:04.185940"}
{"image_index": 69, "image_name": "000000007088.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007088.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1982.039, "latencies_ms": [1982.039], "images_per_second": 0.505, "prompt_tokens": 1110, "response_tokens_est": 35, "n_tiles": 1, "output_text": " The image features a young girl wearing a red coat and holding a black umbrella with pink patterns. The scene is set on a rainy day with wet pavement and a gray sky.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.7, "ram_available_mb": 99000.4, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26771.7, "ram_available_mb": 99000.4, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.82, "peak": 40.57, "min": 18.53}, "VIN": {"avg": 71.72, "peak": 124.73, "min": 28.09}, "VIN_SYS_5V0": {"avg": 14.89, "peak": 16.15, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.91, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 29.82, "energy_joules_est": 59.11, "sample_count": 20, "duration_seconds": 1.982}, "timestamp": "2026-01-19T14:24:06.270782"}
{"image_index": 70, "image_name": "000000007108.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007108.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2016.848, "latencies_ms": [2016.848], "images_per_second": 0.496, "prompt_tokens": 1099, "response_tokens_est": 36, "n_tiles": 1, "output_text": " In the image, a group of elephants, including a young one, are walking through a muddy area near a body of water, with the young elephant in the foreground prominently displayed.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 26771.7, "ram_available_mb": 99000.4, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26771.7, "ram_available_mb": 99000.4, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.82, "peak": 40.57, "min": 16.16}, "VIN": {"avg": 69.47, "peak": 114.39, "min": 28.8}, "VIN_SYS_5V0": {"avg": 14.74, "peak": 15.95, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.75, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 28.82, "energy_joules_est": 58.13, "sample_count": 20, "duration_seconds": 2.017}, "timestamp": "2026-01-19T14:24:08.359333"}
{"image_index": 70, "image_name": "000000007108.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007108.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2284.018, "latencies_ms": [2284.018], "images_per_second": 0.438, "prompt_tokens": 1113, "response_tokens_est": 46, "n_tiles": 1, "output_text": " elephant: 1\nelephant: 1\nelephant: 1\nelephant: 1\nelephant: 1\nelephant: 1\nelephant: 1\nelephant: 1", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26771.7, "ram_available_mb": 99000.4, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26771.7, "ram_available_mb": 99000.4, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.19, "peak": 40.18, "min": 16.56}, "VIN": {"avg": 73.19, "peak": 122.76, "min": 29.76}, "VIN_SYS_5V0": {"avg": 14.91, "peak": 16.05, "min": 12.15}, "VDD_CPU_SOC_MSS": {"avg": 16.03, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 28.19, "energy_joules_est": 64.4, "sample_count": 22, "duration_seconds": 2.284}, "timestamp": "2026-01-19T14:24:10.662494"}
{"image_index": 70, "image_name": "000000007108.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007108.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2463.205, "latencies_ms": [2463.205], "images_per_second": 0.406, "prompt_tokens": 1117, "response_tokens_est": 54, "n_tiles": 1, "output_text": " The main elephant is in the foreground, with its trunk extended towards the camera. The other elephants are in the background, with one of them partially obscured by the main elephant. The elephants are walking on a dirt path, with a body of water visible in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.7, "ram_available_mb": 99000.4, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26771.7, "ram_available_mb": 99000.4, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.71, "peak": 40.56, "min": 16.95}, "VIN": {"avg": 65.58, "peak": 122.07, "min": 29.35}, "VIN_SYS_5V0": {"avg": 15.12, "peak": 16.15, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 16.11, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 27.71, "energy_joules_est": 68.27, "sample_count": 24, "duration_seconds": 2.464}, "timestamp": "2026-01-19T14:24:13.163586"}
{"image_index": 70, "image_name": "000000007108.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007108.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1619.679, "latencies_ms": [1619.679], "images_per_second": 0.617, "prompt_tokens": 1111, "response_tokens_est": 21, "n_tiles": 1, "output_text": " A group of elephants, including a baby, are walking through a muddy area near a body of water.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.7, "ram_available_mb": 99000.4, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 26771.7, "ram_available_mb": 99000.4, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.75, "peak": 40.57, "min": 16.55}, "VIN": {"avg": 70.11, "peak": 112.7, "min": 28.63}, "VIN_SYS_5V0": {"avg": 15.07, "peak": 16.15, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 16.12, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 30.75, "energy_joules_est": 49.82, "sample_count": 16, "duration_seconds": 1.62}, "timestamp": "2026-01-19T14:24:14.841053"}
{"image_index": 70, "image_name": "000000007108.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007108.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1883.907, "latencies_ms": [1883.907], "images_per_second": 0.531, "prompt_tokens": 1109, "response_tokens_est": 31, "n_tiles": 1, "output_text": " The elephants are in a muddy environment, with a mix of brown and red tones. The lighting is natural, and the elephants are in a natural setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.7, "ram_available_mb": 99000.4, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26771.7, "ram_available_mb": 99000.4, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.9, "peak": 40.56, "min": 18.92}, "VIN": {"avg": 72.66, "peak": 124.61, "min": 30.16}, "VIN_SYS_5V0": {"avg": 14.89, "peak": 16.05, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.85, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 29.9, "energy_joules_est": 56.34, "sample_count": 19, "duration_seconds": 1.884}, "timestamp": "2026-01-19T14:24:16.824460"}
{"image_index": 71, "image_name": "000000007278.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007278.jpg", "image_width": 640, "image_height": 482, "image_resolution": "640x482", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1982.505, "latencies_ms": [1982.505], "images_per_second": 0.504, "prompt_tokens": 1432, "response_tokens_est": 19, "n_tiles": 1, "output_text": " A surfer is riding a wave on a surfboard, with the ocean in the background.", "error": null, "sys_before": {"cpu_percent": 5.9, "ram_used_mb": 26771.7, "ram_available_mb": 99000.4, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 26771.7, "ram_available_mb": 99000.4, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.2, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.77, "peak": 40.18, "min": 16.16}, "VIN": {"avg": 73.5, "peak": 116.46, "min": 32.65}, "VIN_SYS_5V0": {"avg": 15.22, "peak": 16.66, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 16.29, "peak": 17.71, "min": 12.61}}, "power_watts_avg": 31.77, "energy_joules_est": 63.0, "sample_count": 19, "duration_seconds": 1.983}, "timestamp": "2026-01-19T14:24:18.820111"}
{"image_index": 71, "image_name": "000000007278.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007278.jpg", "image_width": 640, "image_height": 482, "image_resolution": "640x482", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3109.089, "latencies_ms": [3109.089], "images_per_second": 0.322, "prompt_tokens": 1446, "response_tokens_est": 63, "n_tiles": 1, "output_text": " 1. surfer: 1\n2. wave: 1\n3. surfboard: 1\n4. water: 1\n5. sky: 0\n6. surfboard deck: 1\n7. surfboard leash: 1\n8. logo: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.7, "ram_available_mb": 99000.4, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26771.5, "ram_available_mb": 99000.7, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.06, "peak": 40.57, "min": 20.1}, "VIN": {"avg": 73.63, "peak": 121.19, "min": 29.76}, "VIN_SYS_5V0": {"avg": 15.39, "peak": 16.76, "min": 13.49}, "VDD_CPU_SOC_MSS": {"avg": 16.39, "peak": 18.11, "min": 14.57}}, "power_watts_avg": 29.06, "energy_joules_est": 90.36, "sample_count": 30, "duration_seconds": 3.11}, "timestamp": "2026-01-19T14:24:21.943608"}
{"image_index": 71, "image_name": "000000007278.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007278.jpg", "image_width": 640, "image_height": 482, "image_resolution": "640x482", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2509.024, "latencies_ms": [2509.024], "images_per_second": 0.399, "prompt_tokens": 1450, "response_tokens_est": 40, "n_tiles": 1, "output_text": " The surfer is positioned in the foreground, riding a wave that dominates the majority of the image. The wave is in the background, with the surfer's body and surfboard occupying the foreground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.5, "ram_available_mb": 99000.7, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26771.5, "ram_available_mb": 99000.7, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.99, "peak": 40.18, "min": 16.95}, "VIN": {"avg": 70.61, "peak": 116.79, "min": 28.53}, "VIN_SYS_5V0": {"avg": 15.31, "peak": 16.66, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 16.39, "peak": 17.71, "min": 14.18}}, "power_watts_avg": 29.99, "energy_joules_est": 75.26, "sample_count": 25, "duration_seconds": 2.51}, "timestamp": "2026-01-19T14:24:24.548331"}
{"image_index": 71, "image_name": "000000007278.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007278.jpg", "image_width": 640, "image_height": 482, "image_resolution": "640x482", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1793.216, "latencies_ms": [1793.216], "images_per_second": 0.558, "prompt_tokens": 1444, "response_tokens_est": 12, "n_tiles": 1, "output_text": " A surfer is riding a wave on a surfboard.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.5, "ram_available_mb": 99000.7, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 26771.5, "ram_available_mb": 99000.7, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.22, "peak": 40.18, "min": 16.16}, "VIN": {"avg": 76.06, "peak": 132.58, "min": 28.84}, "VIN_SYS_5V0": {"avg": 14.99, "peak": 16.46, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 16.16, "peak": 17.71, "min": 12.61}}, "power_watts_avg": 32.22, "energy_joules_est": 57.79, "sample_count": 18, "duration_seconds": 1.794}, "timestamp": "2026-01-19T14:24:26.429284"}
{"image_index": 71, "image_name": "000000007278.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007278.jpg", "image_width": 640, "image_height": 482, "image_resolution": "640x482", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2039.598, "latencies_ms": [2039.598], "images_per_second": 0.49, "prompt_tokens": 1442, "response_tokens_est": 22, "n_tiles": 1, "output_text": " The surfer is wearing a red and green wetsuit, and the wave is a deep green color.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.5, "ram_available_mb": 99000.7, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26771.5, "ram_available_mb": 99000.7, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.52, "peak": 40.57, "min": 20.11}, "VIN": {"avg": 75.8, "peak": 116.53, "min": 27.83}, "VIN_SYS_5V0": {"avg": 15.17, "peak": 16.56, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 16.34, "peak": 17.71, "min": 13.0}}, "power_watts_avg": 32.52, "energy_joules_est": 66.34, "sample_count": 20, "duration_seconds": 2.04}, "timestamp": "2026-01-19T14:24:28.518676"}
{"image_index": 72, "image_name": "000000007281.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007281.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1599.98, "latencies_ms": [1599.98], "images_per_second": 0.625, "prompt_tokens": 1099, "response_tokens_est": 21, "n_tiles": 1, "output_text": " Two men on horseback are riding on a beach, with the ocean and a few people in the background.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 26771.5, "ram_available_mb": 99000.7, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 26771.5, "ram_available_mb": 99000.7, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.44, "peak": 40.56, "min": 18.91}, "VIN": {"avg": 72.19, "peak": 107.95, "min": 29.02}, "VIN_SYS_5V0": {"avg": 15.01, "peak": 16.05, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 16.24, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 31.44, "energy_joules_est": 50.33, "sample_count": 16, "duration_seconds": 1.601}, "timestamp": "2026-01-19T14:24:30.191627"}
{"image_index": 72, "image_name": "000000007281.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007281.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2523.993, "latencies_ms": [2523.993], "images_per_second": 0.396, "prompt_tokens": 1113, "response_tokens_est": 56, "n_tiles": 1, "output_text": " 1. horse: 2\n2. rider: 2\n3. rider: 1\n4. rider: 1\n5. rider: 1\n6. rider: 1\n7. rider: 1\n8. rider: 1", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26771.5, "ram_available_mb": 99000.7, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 26771.5, "ram_available_mb": 99000.7, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.67, "peak": 40.57, "min": 18.13}, "VIN": {"avg": 68.5, "peak": 122.29, "min": 29.42}, "VIN_SYS_5V0": {"avg": 14.94, "peak": 16.05, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 16.03, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 27.67, "energy_joules_est": 69.85, "sample_count": 25, "duration_seconds": 2.524}, "timestamp": "2026-01-19T14:24:32.787191"}
{"image_index": 72, "image_name": "000000007281.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007281.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2637.565, "latencies_ms": [2637.565], "images_per_second": 0.379, "prompt_tokens": 1117, "response_tokens_est": 60, "n_tiles": 1, "output_text": " The two men on horseback are positioned in the foreground, with the ocean and beach extending into the background. The man on the left is closer to the camera, while the man on the right is farther away. The man on the left is also closer to the camera than the man on the right.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26771.5, "ram_available_mb": 99000.7, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 26771.5, "ram_available_mb": 99000.7, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.75, "peak": 40.18, "min": 15.77}, "VIN": {"avg": 70.63, "peak": 129.59, "min": 29.76}, "VIN_SYS_5V0": {"avg": 14.89, "peak": 16.05, "min": 12.15}, "VDD_CPU_SOC_MSS": {"avg": 15.89, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 26.75, "energy_joules_est": 70.56, "sample_count": 26, "duration_seconds": 2.638}, "timestamp": "2026-01-19T14:24:35.496842"}
{"image_index": 72, "image_name": "000000007281.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007281.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1521.142, "latencies_ms": [1521.142], "images_per_second": 0.657, "prompt_tokens": 1111, "response_tokens_est": 17, "n_tiles": 1, "output_text": " Two men on horseback are riding on a beach, with the ocean in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.5, "ram_available_mb": 99000.7, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 26771.5, "ram_available_mb": 99000.7, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.89, "peak": 40.18, "min": 15.38}, "VIN": {"avg": 74.92, "peak": 102.67, "min": 28.64}, "VIN_SYS_5V0": {"avg": 14.85, "peak": 16.05, "min": 12.15}, "VDD_CPU_SOC_MSS": {"avg": 15.93, "peak": 17.32, "min": 13.4}}, "power_watts_avg": 30.89, "energy_joules_est": 47.0, "sample_count": 15, "duration_seconds": 1.522}, "timestamp": "2026-01-19T14:24:37.065251"}
{"image_index": 72, "image_name": "000000007281.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007281.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2145.295, "latencies_ms": [2145.295], "images_per_second": 0.466, "prompt_tokens": 1109, "response_tokens_est": 36, "n_tiles": 1, "output_text": " The image features a sandy beach with a clear blue sky and ocean in the background. The two men are dressed in white traditional clothing and are riding horses, which are also white.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.5, "ram_available_mb": 99000.7, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26771.5, "ram_available_mb": 99000.7, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.78, "peak": 40.56, "min": 18.52}, "VIN": {"avg": 69.23, "peak": 121.92, "min": 28.62}, "VIN_SYS_5V0": {"avg": 14.73, "peak": 16.05, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 15.73, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 28.78, "energy_joules_est": 61.75, "sample_count": 21, "duration_seconds": 2.146}, "timestamp": "2026-01-19T14:24:39.257863"}
{"image_index": 73, "image_name": "000000007386.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007386.jpg", "image_width": 600, "image_height": 400, "image_resolution": "600x400", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1630.935, "latencies_ms": [1630.935], "images_per_second": 0.613, "prompt_tokens": 1099, "response_tokens_est": 21, "n_tiles": 1, "output_text": " A motorcycle with a sidecar is parked in front of a garage, with a small dog standing nearby.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 26771.5, "ram_available_mb": 99000.7, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26771.5, "ram_available_mb": 99000.7, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.48, "peak": 40.18, "min": 15.77}, "VIN": {"avg": 73.67, "peak": 126.09, "min": 29.53}, "VIN_SYS_5V0": {"avg": 14.78, "peak": 16.05, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.92, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 30.48, "energy_joules_est": 49.73, "sample_count": 16, "duration_seconds": 1.632}, "timestamp": "2026-01-19T14:24:40.936348"}
{"image_index": 73, "image_name": "000000007386.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007386.jpg", "image_width": 600, "image_height": 400, "image_resolution": "600x400", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2633.292, "latencies_ms": [2633.292], "images_per_second": 0.38, "prompt_tokens": 1113, "response_tokens_est": 60, "n_tiles": 1, "output_text": " 1. motorcycle: 1\n2. dog: 1\n3. tire: 1\n4. handlebar: 1\n5. seat: 1\n6. front wheel: 1\n7. rear wheel: 1\n8. garage door: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.5, "ram_available_mb": 99000.7, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26771.5, "ram_available_mb": 99000.7, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.41, "peak": 40.97, "min": 17.73}, "VIN": {"avg": 64.83, "peak": 107.17, "min": 27.83}, "VIN_SYS_5V0": {"avg": 14.96, "peak": 16.15, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 16.01, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 27.41, "energy_joules_est": 72.19, "sample_count": 26, "duration_seconds": 2.634}, "timestamp": "2026-01-19T14:24:43.656022"}
{"image_index": 73, "image_name": "000000007386.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007386.jpg", "image_width": 600, "image_height": 400, "image_resolution": "600x400", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2054.776, "latencies_ms": [2054.776], "images_per_second": 0.487, "prompt_tokens": 1117, "response_tokens_est": 37, "n_tiles": 1, "output_text": " The motorcycle is positioned to the left of the garage door, with the dog standing in front of it. The motorcycle is in the foreground, while the garage door is in the background.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26771.5, "ram_available_mb": 99000.7, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26771.5, "ram_available_mb": 99000.7, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.64, "peak": 40.18, "min": 15.38}, "VIN": {"avg": 72.83, "peak": 119.68, "min": 29.95}, "VIN_SYS_5V0": {"avg": 14.8, "peak": 16.05, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.91, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 28.64, "energy_joules_est": 58.86, "sample_count": 20, "duration_seconds": 2.055}, "timestamp": "2026-01-19T14:24:45.740571"}
{"image_index": 73, "image_name": "000000007386.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007386.jpg", "image_width": 600, "image_height": 400, "image_resolution": "600x400", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1463.947, "latencies_ms": [1463.947], "images_per_second": 0.683, "prompt_tokens": 1111, "response_tokens_est": 15, "n_tiles": 1, "output_text": " A motorcycle is parked outside a garage with a dog standing next to it.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26771.5, "ram_available_mb": 99000.7, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.5, "ram_used_mb": 26771.5, "ram_available_mb": 99000.7, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.49, "peak": 40.57, "min": 17.35}, "VIN": {"avg": 74.98, "peak": 110.43, "min": 31.08}, "VIN_SYS_5V0": {"avg": 15.0, "peak": 16.15, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 16.14, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 31.49, "energy_joules_est": 46.11, "sample_count": 15, "duration_seconds": 1.464}, "timestamp": "2026-01-19T14:24:47.310347"}
{"image_index": 73, "image_name": "000000007386.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007386.jpg", "image_width": 600, "image_height": 400, "image_resolution": "600x400", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1706.248, "latencies_ms": [1706.248], "images_per_second": 0.586, "prompt_tokens": 1109, "response_tokens_est": 24, "n_tiles": 1, "output_text": " The motorcycle is red and silver, and the garage is beige. The sky is blue and the grass is green.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.5, "ram_available_mb": 99000.7, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26771.5, "ram_available_mb": 99000.7, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.91, "peak": 40.56, "min": 19.32}, "VIN": {"avg": 76.15, "peak": 123.99, "min": 29.1}, "VIN_SYS_5V0": {"avg": 14.75, "peak": 16.05, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.82, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 30.91, "energy_joules_est": 52.75, "sample_count": 17, "duration_seconds": 1.707}, "timestamp": "2026-01-19T14:24:49.101589"}
{"image_index": 74, "image_name": "000000007511.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007511.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1828.847, "latencies_ms": [1828.847], "images_per_second": 0.547, "prompt_tokens": 1099, "response_tokens_est": 29, "n_tiles": 1, "output_text": " A man is flying a kite on the beach, with a blue and white kite in the sky, and people enjoying the beach in the background.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 26771.5, "ram_available_mb": 99000.7, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26771.5, "ram_available_mb": 99000.7, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.2, "peak": 40.57, "min": 17.74}, "VIN": {"avg": 74.76, "peak": 125.04, "min": 28.94}, "VIN_SYS_5V0": {"avg": 14.83, "peak": 16.05, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.81, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 30.2, "energy_joules_est": 55.25, "sample_count": 18, "duration_seconds": 1.829}, "timestamp": "2026-01-19T14:24:50.982259"}
{"image_index": 74, "image_name": "000000007511.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007511.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2520.55, "latencies_ms": [2520.55], "images_per_second": 0.397, "prompt_tokens": 1113, "response_tokens_est": 56, "n_tiles": 1, "output_text": " 1. person: 1\n2. kite: 1\n3. sand: 1\n4. water: 1\n5. trees: 1\n6. buildings: 1\n7. clouds: 1\n8. sky: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.5, "ram_available_mb": 99000.7, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26771.5, "ram_available_mb": 99000.7, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.42, "peak": 40.57, "min": 17.74}, "VIN": {"avg": 69.06, "peak": 114.18, "min": 28.92}, "VIN_SYS_5V0": {"avg": 14.94, "peak": 16.15, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.95, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 27.42, "energy_joules_est": 69.12, "sample_count": 25, "duration_seconds": 2.521}, "timestamp": "2026-01-19T14:24:53.596268"}
{"image_index": 74, "image_name": "000000007511.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007511.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2229.428, "latencies_ms": [2229.428], "images_per_second": 0.449, "prompt_tokens": 1117, "response_tokens_est": 44, "n_tiles": 1, "output_text": " The man is in the foreground, flying the kite in the middle of the beach. The kite is in the background, flying high in the sky. The beach is in the background, with people and trees around it.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26771.5, "ram_available_mb": 99000.7, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26771.5, "ram_available_mb": 99000.7, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.96, "peak": 40.18, "min": 15.38}, "VIN": {"avg": 63.83, "peak": 121.07, "min": 28.7}, "VIN_SYS_5V0": {"avg": 14.74, "peak": 16.05, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.84, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 27.96, "energy_joules_est": 62.35, "sample_count": 22, "duration_seconds": 2.23}, "timestamp": "2026-01-19T14:24:55.887793"}
{"image_index": 74, "image_name": "000000007511.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007511.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1488.19, "latencies_ms": [1488.19], "images_per_second": 0.672, "prompt_tokens": 1111, "response_tokens_est": 16, "n_tiles": 1, "output_text": " A man is flying a kite on a beach with a lake in the background.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26771.5, "ram_available_mb": 99000.7, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 26771.5, "ram_available_mb": 99000.7, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.15, "peak": 40.57, "min": 16.16}, "VIN": {"avg": 72.7, "peak": 115.28, "min": 30.33}, "VIN_SYS_5V0": {"avg": 14.91, "peak": 16.05, "min": 12.25}, "VDD_CPU_SOC_MSS": {"avg": 16.01, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 31.15, "energy_joules_est": 46.37, "sample_count": 15, "duration_seconds": 1.489}, "timestamp": "2026-01-19T14:24:57.452896"}
{"image_index": 74, "image_name": "000000007511.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007511.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1699.749, "latencies_ms": [1699.749], "images_per_second": 0.588, "prompt_tokens": 1109, "response_tokens_est": 24, "n_tiles": 1, "output_text": " The sky is blue and the kite is blue and white. The beach is sandy and the people are wearing casual clothes.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.5, "ram_available_mb": 99000.7, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26771.5, "ram_available_mb": 99000.7, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.15, "peak": 40.57, "min": 19.71}, "VIN": {"avg": 70.27, "peak": 102.34, "min": 28.83}, "VIN_SYS_5V0": {"avg": 14.9, "peak": 16.05, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.96, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 31.15, "energy_joules_est": 52.95, "sample_count": 17, "duration_seconds": 1.7}, "timestamp": "2026-01-19T14:24:59.223497"}
{"image_index": 75, "image_name": "000000007574.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007574.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2240.741, "latencies_ms": [2240.741], "images_per_second": 0.446, "prompt_tokens": 1099, "response_tokens_est": 43, "n_tiles": 1, "output_text": " The image depicts a kitchen with wooden cabinets, a stainless steel refrigerator, and a black countertop, with various items such as a green bottle, a blue bottle, and a red bow placed on the countertop.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.5, "ram_available_mb": 99000.7, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26771.5, "ram_available_mb": 99000.7, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.3, "peak": 40.57, "min": 17.74}, "VIN": {"avg": 66.03, "peak": 105.94, "min": 28.92}, "VIN_SYS_5V0": {"avg": 14.8, "peak": 15.95, "min": 12.15}, "VDD_CPU_SOC_MSS": {"avg": 15.82, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 28.3, "energy_joules_est": 63.44, "sample_count": 22, "duration_seconds": 2.242}, "timestamp": "2026-01-19T14:25:01.520055"}
{"image_index": 75, "image_name": "000000007574.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007574.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1989.009, "latencies_ms": [1989.009], "images_per_second": 0.503, "prompt_tokens": 1113, "response_tokens_est": 34, "n_tiles": 1, "output_text": " sink: 1, bottle: 2, bowl: 1, cup: 1, dish: 1, bottle: 1, bottle: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.5, "ram_available_mb": 99000.7, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26771.5, "ram_available_mb": 99000.7, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.78, "peak": 40.56, "min": 16.16}, "VIN": {"avg": 72.03, "peak": 119.0, "min": 27.07}, "VIN_SYS_5V0": {"avg": 14.88, "peak": 16.05, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.91, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 28.78, "energy_joules_est": 57.26, "sample_count": 20, "duration_seconds": 1.989}, "timestamp": "2026-01-19T14:25:03.605929"}
{"image_index": 75, "image_name": "000000007574.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007574.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2103.151, "latencies_ms": [2103.151], "images_per_second": 0.475, "prompt_tokens": 1117, "response_tokens_est": 39, "n_tiles": 1, "output_text": " The green dish soap bottle is located near the sink, while the blue dish soap bottle is placed further back on the counter. The red bow is positioned in the foreground, close to the camera.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.5, "ram_available_mb": 99000.7, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26771.5, "ram_available_mb": 99000.7, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.42, "peak": 40.18, "min": 15.77}, "VIN": {"avg": 70.09, "peak": 120.7, "min": 27.5}, "VIN_SYS_5V0": {"avg": 14.7, "peak": 15.95, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 15.79, "peak": 17.32, "min": 12.61}}, "power_watts_avg": 28.42, "energy_joules_est": 59.78, "sample_count": 21, "duration_seconds": 2.103}, "timestamp": "2026-01-19T14:25:05.790490"}
{"image_index": 75, "image_name": "000000007574.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007574.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1518.059, "latencies_ms": [1518.059], "images_per_second": 0.659, "prompt_tokens": 1111, "response_tokens_est": 17, "n_tiles": 1, "output_text": " A kitchen with wooden cabinets, a black countertop, and a stainless steel refrigerator.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.5, "ram_available_mb": 99000.7, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26771.5, "ram_available_mb": 99000.7, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.15, "peak": 40.57, "min": 15.77}, "VIN": {"avg": 73.26, "peak": 105.0, "min": 30.3}, "VIN_SYS_5V0": {"avg": 14.81, "peak": 16.05, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.91, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 31.15, "energy_joules_est": 47.3, "sample_count": 15, "duration_seconds": 1.518}, "timestamp": "2026-01-19T14:25:07.361473"}
{"image_index": 75, "image_name": "000000007574.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007574.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1536.038, "latencies_ms": [1536.038], "images_per_second": 0.651, "prompt_tokens": 1109, "response_tokens_est": 18, "n_tiles": 1, "output_text": " The kitchen is well-lit with natural light, and the cabinets are made of wood.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.5, "ram_available_mb": 99000.7, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 26771.5, "ram_available_mb": 99000.7, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.52, "peak": 40.97, "min": 20.11}, "VIN": {"avg": 75.63, "peak": 117.4, "min": 30.39}, "VIN_SYS_5V0": {"avg": 15.07, "peak": 16.15, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 16.17, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 32.52, "energy_joules_est": 49.96, "sample_count": 15, "duration_seconds": 1.536}, "timestamp": "2026-01-19T14:25:08.927979"}
{"image_index": 76, "image_name": "000000007784.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007784.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1421.436, "latencies_ms": [1421.436], "images_per_second": 0.704, "prompt_tokens": 1099, "response_tokens_est": 14, "n_tiles": 1, "output_text": " A kite with a red and white design is flying in the sky.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.5, "ram_available_mb": 99000.7, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 26771.5, "ram_available_mb": 99000.7, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 33.17, "peak": 40.56, "min": 20.89}, "VIN": {"avg": 78.22, "peak": 121.01, "min": 29.39}, "VIN_SYS_5V0": {"avg": 15.14, "peak": 16.15, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 16.28, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 33.17, "energy_joules_est": 47.16, "sample_count": 14, "duration_seconds": 1.422}, "timestamp": "2026-01-19T14:25:10.395222"}
{"image_index": 76, "image_name": "000000007784.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007784.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1175.517, "latencies_ms": [1175.517], "images_per_second": 0.851, "prompt_tokens": 1113, "response_tokens_est": 4, "n_tiles": 1, "output_text": " kite: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26771.5, "ram_available_mb": 99000.7, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.2, "ram_used_mb": 26771.4, "ram_available_mb": 99000.7, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 34.14, "peak": 40.97, "min": 21.67}, "VIN": {"avg": 84.89, "peak": 124.85, "min": 27.13}, "VIN_SYS_5V0": {"avg": 15.04, "peak": 16.15, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 16.17, "peak": 17.32, "min": 13.78}}, "power_watts_avg": 34.14, "energy_joules_est": 40.15, "sample_count": 12, "duration_seconds": 1.176}, "timestamp": "2026-01-19T14:25:11.656625"}
{"image_index": 76, "image_name": "000000007784.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007784.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2186.192, "latencies_ms": [2186.192], "images_per_second": 0.457, "prompt_tokens": 1117, "response_tokens_est": 43, "n_tiles": 1, "output_text": " The kite is in the foreground, flying high in the sky, and is positioned to the left of the frame. The kite is also relatively close to the camera, as it is the main focus of the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.4, "ram_available_mb": 99000.7, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26771.4, "ram_available_mb": 99000.7, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.5, "peak": 41.36, "min": 20.48}, "VIN": {"avg": 73.71, "peak": 114.42, "min": 37.95}, "VIN_SYS_5V0": {"avg": 15.0, "peak": 16.05, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 16.09, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 30.5, "energy_joules_est": 66.69, "sample_count": 21, "duration_seconds": 2.186}, "timestamp": "2026-01-19T14:25:13.848103"}
{"image_index": 76, "image_name": "000000007784.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007784.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1412.849, "latencies_ms": [1412.849], "images_per_second": 0.708, "prompt_tokens": 1111, "response_tokens_est": 14, "n_tiles": 1, "output_text": " A kite with a red and white design is flying in the sky.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26771.4, "ram_available_mb": 99000.7, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 26771.4, "ram_available_mb": 99000.7, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.41, "peak": 40.18, "min": 18.91}, "VIN": {"avg": 77.18, "peak": 106.64, "min": 29.06}, "VIN_SYS_5V0": {"avg": 15.36, "peak": 16.26, "min": 13.49}, "VDD_CPU_SOC_MSS": {"avg": 16.45, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 32.41, "energy_joules_est": 45.8, "sample_count": 14, "duration_seconds": 1.413}, "timestamp": "2026-01-19T14:25:15.312082"}
{"image_index": 76, "image_name": "000000007784.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007784.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1515.028, "latencies_ms": [1515.028], "images_per_second": 0.66, "prompt_tokens": 1109, "response_tokens_est": 17, "n_tiles": 1, "output_text": " The kite is white with red and black patterns, and the sky is clear blue.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.4, "ram_available_mb": 99000.7, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 26771.4, "ram_available_mb": 99000.7, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.67, "peak": 40.97, "min": 21.29}, "VIN": {"avg": 73.82, "peak": 99.66, "min": 29.1}, "VIN_SYS_5V0": {"avg": 15.01, "peak": 16.15, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 16.17, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 32.67, "energy_joules_est": 49.51, "sample_count": 15, "duration_seconds": 1.516}, "timestamp": "2026-01-19T14:25:16.876515"}
{"image_index": 77, "image_name": "000000007795.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007795.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2322.549, "latencies_ms": [2322.549], "images_per_second": 0.431, "prompt_tokens": 1099, "response_tokens_est": 48, "n_tiles": 1, "output_text": " The image depicts a spacious hotel room with two queen-sized beds, each adorned with white linens and a dark-colored bedspread, positioned side by side, with a window in the background allowing natural light to filter into the room.", "error": null, "sys_before": {"cpu_percent": 18.2, "ram_used_mb": 26771.4, "ram_available_mb": 99000.7, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26771.4, "ram_available_mb": 99000.7, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.57, "peak": 40.56, "min": 18.53}, "VIN": {"avg": 70.02, "peak": 126.72, "min": 29.24}, "VIN_SYS_5V0": {"avg": 14.93, "peak": 16.05, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.88, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 28.57, "energy_joules_est": 66.37, "sample_count": 23, "duration_seconds": 2.323}, "timestamp": "2026-01-19T14:25:19.275531"}
{"image_index": 77, "image_name": "000000007795.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007795.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2502.671, "latencies_ms": [2502.671], "images_per_second": 0.4, "prompt_tokens": 1113, "response_tokens_est": 55, "n_tiles": 1, "output_text": " 1. beds: 2\n2. pillows: 12\n3. lamps: 2\n4. paintings: 2\n5. window: 1\n6. door: 1\n7. floor: wooden\n8. ceiling: white", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.4, "ram_available_mb": 99000.7, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26771.4, "ram_available_mb": 99000.7, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.08, "peak": 40.18, "min": 15.77}, "VIN": {"avg": 68.92, "peak": 124.58, "min": 29.43}, "VIN_SYS_5V0": {"avg": 14.9, "peak": 16.05, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.84, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 27.08, "energy_joules_est": 67.78, "sample_count": 25, "duration_seconds": 2.503}, "timestamp": "2026-01-19T14:25:21.871763"}
{"image_index": 77, "image_name": "000000007795.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007795.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2680.764, "latencies_ms": [2680.764], "images_per_second": 0.373, "prompt_tokens": 1117, "response_tokens_est": 61, "n_tiles": 1, "output_text": " The beds are positioned on the left side of the room, with the window and door located on the right side. The lamp on the left bed is closer to the camera than the lamp on the right bed. The artwork is positioned above the beds, and the towels are placed on the bedspreads.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.4, "ram_available_mb": 99000.7, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 26771.4, "ram_available_mb": 99000.7, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.67, "peak": 40.18, "min": 15.38}, "VIN": {"avg": 72.81, "peak": 125.42, "min": 28.89}, "VIN_SYS_5V0": {"avg": 14.87, "peak": 16.05, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.88, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 26.67, "energy_joules_est": 71.51, "sample_count": 26, "duration_seconds": 2.681}, "timestamp": "2026-01-19T14:25:24.584162"}
{"image_index": 77, "image_name": "000000007795.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007795.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2702.125, "latencies_ms": [2702.125], "images_per_second": 0.37, "prompt_tokens": 1111, "response_tokens_est": 63, "n_tiles": 1, "output_text": " The image captures a serene hotel room bathed in soft light, where two queen-sized beds rest against a wall adorned with a vibrant abstract painting. The room exudes a sense of tranquility, with each bed neatly made and a lamp perched on the nightstand, ready to bathe the room in a warm glow.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26771.4, "ram_available_mb": 99000.7, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 26771.4, "ram_available_mb": 99000.7, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.99, "peak": 40.56, "min": 16.55}, "VIN": {"avg": 73.11, "peak": 118.35, "min": 38.94}, "VIN_SYS_5V0": {"avg": 15.06, "peak": 16.15, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 16.26, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 26.99, "energy_joules_est": 72.94, "sample_count": 26, "duration_seconds": 2.703}, "timestamp": "2026-01-19T14:25:27.290908"}
{"image_index": 77, "image_name": "000000007795.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007795.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2171.336, "latencies_ms": [2171.336], "images_per_second": 0.461, "prompt_tokens": 1109, "response_tokens_est": 43, "n_tiles": 1, "output_text": " The room is bathed in soft light from the large window, and the walls are painted a soothing shade of green. The beds are made with crisp white linens and are adorned with dark brown and black pillows.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26771.4, "ram_available_mb": 99000.7, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26771.4, "ram_available_mb": 99000.7, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.87, "peak": 40.16, "min": 17.34}, "VIN": {"avg": 72.58, "peak": 125.5, "min": 29.5}, "VIN_SYS_5V0": {"avg": 15.27, "peak": 16.15, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 16.38, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 28.87, "energy_joules_est": 62.7, "sample_count": 21, "duration_seconds": 2.172}, "timestamp": "2026-01-19T14:25:29.477313"}
{"image_index": 78, "image_name": "000000007816.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007816.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1892.305, "latencies_ms": [1892.305], "images_per_second": 0.528, "prompt_tokens": 1099, "response_tokens_est": 32, "n_tiles": 1, "output_text": " A man wearing a white helmet and a white and green racing suit is riding a white motorcycle on a road with a crowd of people watching from behind a fence.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.4, "ram_available_mb": 99000.7, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26771.4, "ram_available_mb": 99000.7, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.63, "peak": 40.57, "min": 17.34}, "VIN": {"avg": 73.71, "peak": 107.0, "min": 28.49}, "VIN_SYS_5V0": {"avg": 15.12, "peak": 16.15, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 16.27, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 29.63, "energy_joules_est": 56.09, "sample_count": 19, "duration_seconds": 1.893}, "timestamp": "2026-01-19T14:25:31.448723"}
{"image_index": 78, "image_name": "000000007816.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007816.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2001.431, "latencies_ms": [2001.431], "images_per_second": 0.5, "prompt_tokens": 1113, "response_tokens_est": 35, "n_tiles": 1, "output_text": " 1. motorcycle: 1\n2. rider: 1\n3. helmet: 1\n4. fence: 2\n5. people: 8", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.4, "ram_available_mb": 99000.7, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26771.4, "ram_available_mb": 99000.7, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.88, "peak": 40.16, "min": 16.56}, "VIN": {"avg": 74.31, "peak": 125.95, "min": 28.34}, "VIN_SYS_5V0": {"avg": 14.81, "peak": 16.05, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 15.91, "peak": 17.32, "min": 13.4}}, "power_watts_avg": 28.88, "energy_joules_est": 57.81, "sample_count": 20, "duration_seconds": 2.002}, "timestamp": "2026-01-19T14:25:33.534834"}
{"image_index": 78, "image_name": "000000007816.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007816.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2217.968, "latencies_ms": [2217.968], "images_per_second": 0.451, "prompt_tokens": 1117, "response_tokens_est": 43, "n_tiles": 1, "output_text": " The motorcycle is positioned on the left side of the image, with the rider leaning into a turn. The spectators are located in the background, on the right side of the image, and are positioned behind a fence.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.4, "ram_available_mb": 99000.7, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26771.4, "ram_available_mb": 99000.7, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.03, "peak": 40.18, "min": 16.16}, "VIN": {"avg": 69.83, "peak": 97.88, "min": 29.3}, "VIN_SYS_5V0": {"avg": 14.77, "peak": 15.95, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.8, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 28.03, "energy_joules_est": 62.18, "sample_count": 22, "duration_seconds": 2.218}, "timestamp": "2026-01-19T14:25:35.833548"}
{"image_index": 78, "image_name": "000000007816.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007816.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1926.636, "latencies_ms": [1926.636], "images_per_second": 0.519, "prompt_tokens": 1111, "response_tokens_est": 32, "n_tiles": 1, "output_text": " A man wearing a white helmet and a white and green racing suit is riding a white motorcycle on a road. There are people standing behind a fence watching him.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.4, "ram_available_mb": 99000.7, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26771.4, "ram_available_mb": 99000.7, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.13, "peak": 40.18, "min": 15.77}, "VIN": {"avg": 71.11, "peak": 125.87, "min": 28.8}, "VIN_SYS_5V0": {"avg": 14.83, "peak": 16.05, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 15.89, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 29.13, "energy_joules_est": 56.14, "sample_count": 19, "duration_seconds": 1.927}, "timestamp": "2026-01-19T14:25:37.825254"}
{"image_index": 78, "image_name": "000000007816.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007816.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1462.637, "latencies_ms": [1462.637], "images_per_second": 0.684, "prompt_tokens": 1109, "response_tokens_est": 14, "n_tiles": 1, "output_text": " The motorcycle is white and green, and the sky is overcast.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.4, "ram_available_mb": 99000.7, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 26771.4, "ram_available_mb": 99000.7, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.17, "peak": 40.18, "min": 16.95}, "VIN": {"avg": 73.64, "peak": 123.22, "min": 28.06}, "VIN_SYS_5V0": {"avg": 14.84, "peak": 16.15, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.91, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 31.17, "energy_joules_est": 45.6, "sample_count": 15, "duration_seconds": 1.463}, "timestamp": "2026-01-19T14:25:39.386690"}
{"image_index": 79, "image_name": "000000007818.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007818.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1581.907, "latencies_ms": [1581.907], "images_per_second": 0.632, "prompt_tokens": 1099, "response_tokens_est": 19, "n_tiles": 1, "output_text": " A table set with a vase of white flowers and wine glasses is illuminated by a soft light.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.4, "ram_available_mb": 99000.7, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 26771.4, "ram_available_mb": 99000.7, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.42, "peak": 40.57, "min": 19.71}, "VIN": {"avg": 71.95, "peak": 119.69, "min": 29.78}, "VIN_SYS_5V0": {"avg": 14.71, "peak": 16.05, "min": 11.87}, "VDD_CPU_SOC_MSS": {"avg": 15.8, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 31.42, "energy_joules_est": 49.72, "sample_count": 16, "duration_seconds": 1.582}, "timestamp": "2026-01-19T14:25:41.059860"}
{"image_index": 79, "image_name": "000000007818.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007818.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2234.729, "latencies_ms": [2234.729], "images_per_second": 0.447, "prompt_tokens": 1113, "response_tokens_est": 44, "n_tiles": 1, "output_text": " tablecloth: 1\nglass: 4\nflower: 1\nstem: 1\nstems: 1\nstems: 1\nstems: 1\nstems: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.4, "ram_available_mb": 99000.7, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.67, "peak": 40.57, "min": 18.92}, "VIN": {"avg": 69.38, "peak": 111.49, "min": 29.49}, "VIN_SYS_5V0": {"avg": 14.82, "peak": 16.05, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.86, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 28.67, "energy_joules_est": 64.08, "sample_count": 22, "duration_seconds": 2.235}, "timestamp": "2026-01-19T14:25:43.351499"}
{"image_index": 79, "image_name": "000000007818.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007818.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2098.711, "latencies_ms": [2098.711], "images_per_second": 0.476, "prompt_tokens": 1117, "response_tokens_est": 39, "n_tiles": 1, "output_text": " The vase with flowers is in the center of the table, with the glasses and silverware arranged around it. The background is blurred, suggesting that the focus is on the table and its contents.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.42, "peak": 40.16, "min": 16.56}, "VIN": {"avg": 66.59, "peak": 119.51, "min": 29.92}, "VIN_SYS_5V0": {"avg": 14.87, "peak": 16.05, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.92, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 28.42, "energy_joules_est": 59.66, "sample_count": 21, "duration_seconds": 2.099}, "timestamp": "2026-01-19T14:25:45.547786"}
{"image_index": 79, "image_name": "000000007818.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007818.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1595.112, "latencies_ms": [1595.112], "images_per_second": 0.627, "prompt_tokens": 1111, "response_tokens_est": 18, "n_tiles": 1, "output_text": " A table set for a dinner party with a centerpiece of white flowers and wine glasses.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.19, "peak": 40.18, "min": 15.77}, "VIN": {"avg": 70.99, "peak": 105.62, "min": 26.98}, "VIN_SYS_5V0": {"avg": 14.6, "peak": 15.95, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.72, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 30.19, "energy_joules_est": 48.17, "sample_count": 16, "duration_seconds": 1.596}, "timestamp": "2026-01-19T14:25:47.220296"}
{"image_index": 79, "image_name": "000000007818.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007818.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1927.492, "latencies_ms": [1927.492], "images_per_second": 0.519, "prompt_tokens": 1109, "response_tokens_est": 31, "n_tiles": 1, "output_text": " The table is covered with a white tablecloth, and the glasses are clear. The lighting is dim, and the table is set for a formal dinner.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.67, "peak": 40.18, "min": 18.53}, "VIN": {"avg": 73.98, "peak": 124.58, "min": 30.44}, "VIN_SYS_5V0": {"avg": 14.7, "peak": 15.85, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.67, "peak": 16.92, "min": 13.0}}, "power_watts_avg": 29.67, "energy_joules_est": 57.2, "sample_count": 19, "duration_seconds": 1.928}, "timestamp": "2026-01-19T14:25:49.203225"}
{"image_index": 80, "image_name": "000000007888.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007888.jpg", "image_width": 638, "image_height": 640, "image_resolution": "638x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1976.037, "latencies_ms": [1976.037], "images_per_second": 0.506, "prompt_tokens": 1432, "response_tokens_est": 19, "n_tiles": 1, "output_text": " A black and white photo of a clock on a pole with a cloudy sky in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.2, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.03, "peak": 40.57, "min": 16.95}, "VIN": {"avg": 78.45, "peak": 122.27, "min": 31.58}, "VIN_SYS_5V0": {"avg": 15.37, "peak": 16.66, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 16.43, "peak": 17.71, "min": 13.39}}, "power_watts_avg": 32.03, "energy_joules_est": 63.32, "sample_count": 19, "duration_seconds": 1.977}, "timestamp": "2026-01-19T14:25:51.188118"}
{"image_index": 80, "image_name": "000000007888.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007888.jpg", "image_width": 638, "image_height": 640, "image_resolution": "638x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2371.736, "latencies_ms": [2371.736], "images_per_second": 0.422, "prompt_tokens": 1446, "response_tokens_est": 35, "n_tiles": 1, "output_text": " 1. clock: 2\n2. pole: 1\n3. sky: 1\n4. clouds: 1\n5. bird: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.48, "peak": 40.56, "min": 20.89}, "VIN": {"avg": 71.99, "peak": 117.53, "min": 30.61}, "VIN_SYS_5V0": {"avg": 15.46, "peak": 16.76, "min": 13.39}, "VDD_CPU_SOC_MSS": {"avg": 16.53, "peak": 17.71, "min": 14.57}}, "power_watts_avg": 31.48, "energy_joules_est": 74.68, "sample_count": 23, "duration_seconds": 2.372}, "timestamp": "2026-01-19T14:25:53.594659"}
{"image_index": 80, "image_name": "000000007888.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007888.jpg", "image_width": 638, "image_height": 640, "image_resolution": "638x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2508.968, "latencies_ms": [2508.968], "images_per_second": 0.399, "prompt_tokens": 1450, "response_tokens_est": 39, "n_tiles": 1, "output_text": " The clock is positioned in the center of the image, with the pole extending upwards and the sky occupying the background. The clock is relatively close to the camera, while the sky is farther away.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.92, "peak": 40.18, "min": 17.74}, "VIN": {"avg": 70.51, "peak": 103.47, "min": 27.41}, "VIN_SYS_5V0": {"avg": 15.18, "peak": 16.66, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 16.28, "peak": 17.71, "min": 13.79}}, "power_watts_avg": 29.92, "energy_joules_est": 75.08, "sample_count": 25, "duration_seconds": 2.509}, "timestamp": "2026-01-19T14:25:56.193070"}
{"image_index": 80, "image_name": "000000007888.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007888.jpg", "image_width": 638, "image_height": 640, "image_resolution": "638x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1973.289, "latencies_ms": [1973.289], "images_per_second": 0.507, "prompt_tokens": 1444, "response_tokens_est": 19, "n_tiles": 1, "output_text": " A black and white photo of a clock on a pole with a cloudy sky in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.91, "peak": 39.78, "min": 16.16}, "VIN": {"avg": 77.24, "peak": 131.65, "min": 31.61}, "VIN_SYS_5V0": {"avg": 15.11, "peak": 16.46, "min": 11.77}, "VDD_CPU_SOC_MSS": {"avg": 16.27, "peak": 17.71, "min": 13.0}}, "power_watts_avg": 31.91, "energy_joules_est": 62.98, "sample_count": 19, "duration_seconds": 1.974}, "timestamp": "2026-01-19T14:25:58.174337"}
{"image_index": 80, "image_name": "000000007888.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007888.jpg", "image_width": 638, "image_height": 640, "image_resolution": "638x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1783.417, "latencies_ms": [1783.417], "images_per_second": 0.561, "prompt_tokens": 1442, "response_tokens_est": 13, "n_tiles": 1, "output_text": " The clock is black and white, and the sky is cloudy.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.3, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 33.81, "peak": 40.57, "min": 21.28}, "VIN": {"avg": 76.81, "peak": 125.51, "min": 30.04}, "VIN_SYS_5V0": {"avg": 15.51, "peak": 16.76, "min": 13.29}, "VDD_CPU_SOC_MSS": {"avg": 16.64, "peak": 17.71, "min": 14.57}}, "power_watts_avg": 33.81, "energy_joules_est": 60.31, "sample_count": 18, "duration_seconds": 1.784}, "timestamp": "2026-01-19T14:26:00.045374"}
{"image_index": 81, "image_name": "000000007977.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007977.jpg", "image_width": 429, "image_height": 640, "image_resolution": "429x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1721.361, "latencies_ms": [1721.361], "images_per_second": 0.581, "prompt_tokens": 1100, "response_tokens_est": 25, "n_tiles": 1, "output_text": " A young man wearing a black t-shirt and a blue cap is performing a trick on his skateboard in a park.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.19, "peak": 40.57, "min": 20.5}, "VIN": {"avg": 77.23, "peak": 127.11, "min": 28.89}, "VIN_SYS_5V0": {"avg": 14.87, "peak": 16.15, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.84, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 31.19, "energy_joules_est": 53.72, "sample_count": 17, "duration_seconds": 1.722}, "timestamp": "2026-01-19T14:26:01.824181"}
{"image_index": 81, "image_name": "000000007977.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007977.jpg", "image_width": 429, "image_height": 640, "image_resolution": "429x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2536.244, "latencies_ms": [2536.244], "images_per_second": 0.394, "prompt_tokens": 1114, "response_tokens_est": 57, "n_tiles": 1, "output_text": " 1. person: 1\n2. skateboard: 1\n3. cap: 1\n4. pants: 1\n5. shoes: 1\n6. trees: 1\n7. ground: 1\n8. sky: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.49, "peak": 40.57, "min": 18.14}, "VIN": {"avg": 65.93, "peak": 98.79, "min": 29.09}, "VIN_SYS_5V0": {"avg": 14.98, "peak": 16.15, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 16.02, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 27.49, "energy_joules_est": 69.73, "sample_count": 25, "duration_seconds": 2.536}, "timestamp": "2026-01-19T14:26:04.432709"}
{"image_index": 81, "image_name": "000000007977.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007977.jpg", "image_width": 429, "image_height": 640, "image_resolution": "429x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1840.747, "latencies_ms": [1840.747], "images_per_second": 0.543, "prompt_tokens": 1118, "response_tokens_est": 28, "n_tiles": 1, "output_text": " The skateboarder is in the foreground, performing a trick on the skateboard. The background features a park with trees and a building.", "error": null, "sys_before": {"cpu_percent": 16.7, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.42, "peak": 40.16, "min": 15.77}, "VIN": {"avg": 74.47, "peak": 121.62, "min": 28.17}, "VIN_SYS_5V0": {"avg": 14.86, "peak": 16.15, "min": 12.15}, "VDD_CPU_SOC_MSS": {"avg": 15.81, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 29.42, "energy_joules_est": 54.17, "sample_count": 18, "duration_seconds": 1.841}, "timestamp": "2026-01-19T14:26:06.313876"}
{"image_index": 81, "image_name": "000000007977.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007977.jpg", "image_width": 429, "image_height": 640, "image_resolution": "429x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1817.751, "latencies_ms": [1817.751], "images_per_second": 0.55, "prompt_tokens": 1112, "response_tokens_est": 28, "n_tiles": 1, "output_text": " A young man wearing a black t-shirt and a blue baseball cap is skateboarding on a red and gray concrete surface in a park.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.83, "peak": 39.78, "min": 17.74}, "VIN": {"avg": 69.32, "peak": 110.68, "min": 28.66}, "VIN_SYS_5V0": {"avg": 14.99, "peak": 16.15, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.97, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 29.83, "energy_joules_est": 54.24, "sample_count": 18, "duration_seconds": 1.818}, "timestamp": "2026-01-19T14:26:08.192018"}
{"image_index": 81, "image_name": "000000007977.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007977.jpg", "image_width": 429, "image_height": 640, "image_resolution": "429x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2243.139, "latencies_ms": [2243.139], "images_per_second": 0.446, "prompt_tokens": 1110, "response_tokens_est": 45, "n_tiles": 1, "output_text": " The image features a skateboarder performing a trick on a concrete surface, with a backdrop of trees and a cloudy sky. The skateboarder is wearing a black t-shirt, gray pants, and a blue cap.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.24, "peak": 40.18, "min": 17.35}, "VIN": {"avg": 73.42, "peak": 125.71, "min": 29.34}, "VIN_SYS_5V0": {"avg": 14.95, "peak": 16.15, "min": 12.25}, "VDD_CPU_SOC_MSS": {"avg": 16.0, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 28.24, "energy_joules_est": 63.36, "sample_count": 22, "duration_seconds": 2.244}, "timestamp": "2026-01-19T14:26:10.487676"}
{"image_index": 82, "image_name": "000000007991.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007991.jpg", "image_width": 640, "image_height": 359, "image_resolution": "640x359", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1522.528, "latencies_ms": [1522.528], "images_per_second": 0.657, "prompt_tokens": 1099, "response_tokens_est": 17, "n_tiles": 1, "output_text": " A white plate filled with orange carrots and green beans is placed on a kitchen counter.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.22, "peak": 40.56, "min": 16.55}, "VIN": {"avg": 81.95, "peak": 124.34, "min": 28.21}, "VIN_SYS_5V0": {"avg": 14.94, "peak": 16.05, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 16.09, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 31.22, "energy_joules_est": 47.54, "sample_count": 15, "duration_seconds": 1.523}, "timestamp": "2026-01-19T14:26:12.060185"}
{"image_index": 82, "image_name": "000000007991.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007991.jpg", "image_width": 640, "image_height": 359, "image_resolution": "640x359", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2341.444, "latencies_ms": [2341.444], "images_per_second": 0.427, "prompt_tokens": 1113, "response_tokens_est": 47, "n_tiles": 1, "output_text": " carrot: 10, carrot: 10, carrot: 10, carrot: 10, carrot: 10, carrot: 10, carrot: 10, carrot: 10", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.49, "peak": 40.57, "min": 18.52}, "VIN": {"avg": 66.36, "peak": 103.64, "min": 29.58}, "VIN_SYS_5V0": {"avg": 14.99, "peak": 16.05, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 15.97, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 28.49, "energy_joules_est": 66.72, "sample_count": 23, "duration_seconds": 2.342}, "timestamp": "2026-01-19T14:26:14.460606"}
{"image_index": 82, "image_name": "000000007991.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007991.jpg", "image_width": 640, "image_height": 359, "image_resolution": "640x359", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2395.17, "latencies_ms": [2395.17], "images_per_second": 0.418, "prompt_tokens": 1117, "response_tokens_est": 48, "n_tiles": 1, "output_text": " The carrots are placed on the left side of the plate, with the peeled carrots and beetroots on the right side. The peeled carrots are closer to the camera than the beetroots. The peeled carrots are in front of the beetroots.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.25, "peak": 40.18, "min": 15.77}, "VIN": {"avg": 69.76, "peak": 130.16, "min": 28.52}, "VIN_SYS_5V0": {"avg": 14.82, "peak": 16.05, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.77, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 27.25, "energy_joules_est": 65.28, "sample_count": 24, "duration_seconds": 2.395}, "timestamp": "2026-01-19T14:26:16.961953"}
{"image_index": 82, "image_name": "000000007991.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007991.jpg", "image_width": 640, "image_height": 359, "image_resolution": "640x359", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1749.717, "latencies_ms": [1749.717], "images_per_second": 0.572, "prompt_tokens": 1111, "response_tokens_est": 25, "n_tiles": 1, "output_text": " In a kitchen, a white plate is filled with fresh carrots and green beans, while a blue carrot peeler sits nearby.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.76, "peak": 39.78, "min": 14.98}, "VIN": {"avg": 73.89, "peak": 128.08, "min": 30.72}, "VIN_SYS_5V0": {"avg": 14.72, "peak": 15.95, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 15.77, "peak": 17.32, "min": 12.61}}, "power_watts_avg": 29.76, "energy_joules_est": 52.09, "sample_count": 17, "duration_seconds": 1.75}, "timestamp": "2026-01-19T14:26:18.739071"}
{"image_index": 82, "image_name": "000000007991.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007991.jpg", "image_width": 640, "image_height": 359, "image_resolution": "640x359", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1989.658, "latencies_ms": [1989.658], "images_per_second": 0.503, "prompt_tokens": 1109, "response_tokens_est": 35, "n_tiles": 1, "output_text": " The image features a white plate filled with orange carrots and green beans, placed on a kitchen counter. The lighting is bright and natural, illuminating the vibrant colors of the vegetables.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.51, "peak": 40.95, "min": 18.91}, "VIN": {"avg": 73.56, "peak": 107.72, "min": 28.01}, "VIN_SYS_5V0": {"avg": 15.01, "peak": 16.15, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 16.16, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 29.51, "energy_joules_est": 58.72, "sample_count": 20, "duration_seconds": 1.99}, "timestamp": "2026-01-19T14:26:20.825727"}
{"image_index": 83, "image_name": "000000008021.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008021.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1632.647, "latencies_ms": [1632.647], "images_per_second": 0.613, "prompt_tokens": 1099, "response_tokens_est": 21, "n_tiles": 1, "output_text": " A man in a suit and tie is giving a presentation on a large screen in front of an audience.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.56, "peak": 40.18, "min": 16.16}, "VIN": {"avg": 72.93, "peak": 120.71, "min": 29.06}, "VIN_SYS_5V0": {"avg": 14.71, "peak": 15.95, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.85, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 30.56, "energy_joules_est": 49.91, "sample_count": 16, "duration_seconds": 1.633}, "timestamp": "2026-01-19T14:26:22.499948"}
{"image_index": 83, "image_name": "000000008021.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008021.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2514.53, "latencies_ms": [2514.53], "images_per_second": 0.398, "prompt_tokens": 1113, "response_tokens_est": 56, "n_tiles": 1, "output_text": " 1. man: 1\n2. woman: 1\n3. head: 2\n4. hair: 1\n5. neck: 1\n6. suit: 1\n7. tie: 1\n8. shirt: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.78, "peak": 40.57, "min": 18.13}, "VIN": {"avg": 68.25, "peak": 92.26, "min": 29.47}, "VIN_SYS_5V0": {"avg": 15.01, "peak": 16.15, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 15.99, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 27.78, "energy_joules_est": 69.86, "sample_count": 25, "duration_seconds": 2.515}, "timestamp": "2026-01-19T14:26:25.088278"}
{"image_index": 83, "image_name": "000000008021.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008021.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2358.262, "latencies_ms": [2358.262], "images_per_second": 0.424, "prompt_tokens": 1117, "response_tokens_est": 49, "n_tiles": 1, "output_text": " The man is standing on the left side of the image, in front of a large screen that is positioned in the middle of the image. The audience is located in the foreground, with their heads visible in the bottom left corner of the image.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.72, "peak": 40.18, "min": 15.77}, "VIN": {"avg": 68.0, "peak": 128.58, "min": 30.1}, "VIN_SYS_5V0": {"avg": 14.93, "peak": 16.05, "min": 12.15}, "VDD_CPU_SOC_MSS": {"avg": 15.83, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 27.72, "energy_joules_est": 65.38, "sample_count": 23, "duration_seconds": 2.359}, "timestamp": "2026-01-19T14:26:27.483759"}
{"image_index": 83, "image_name": "000000008021.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008021.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1566.135, "latencies_ms": [1566.135], "images_per_second": 0.639, "prompt_tokens": 1111, "response_tokens_est": 19, "n_tiles": 1, "output_text": " A man in a suit is giving a presentation on a large screen in front of an audience.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.78, "peak": 40.57, "min": 16.56}, "VIN": {"avg": 68.54, "peak": 82.92, "min": 27.01}, "VIN_SYS_5V0": {"avg": 15.02, "peak": 16.15, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 16.02, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 30.78, "energy_joules_est": 48.22, "sample_count": 16, "duration_seconds": 1.566}, "timestamp": "2026-01-19T14:26:29.145962"}
{"image_index": 83, "image_name": "000000008021.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008021.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2486.69, "latencies_ms": [2486.69], "images_per_second": 0.402, "prompt_tokens": 1109, "response_tokens_est": 54, "n_tiles": 1, "output_text": " The image features a man in a suit and tie standing on a stage, with a large screen behind him displaying a presentation. The lighting is bright and focused on the speaker, while the audience is seated in front of the stage, with their heads visible in the foreground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.76, "peak": 40.56, "min": 18.14}, "VIN": {"avg": 64.5, "peak": 119.46, "min": 28.92}, "VIN_SYS_5V0": {"avg": 14.87, "peak": 15.95, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.83, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 27.76, "energy_joules_est": 69.04, "sample_count": 24, "duration_seconds": 2.487}, "timestamp": "2026-01-19T14:26:31.650579"}
{"image_index": 84, "image_name": "000000008211.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008211.jpg", "image_width": 640, "image_height": 459, "image_resolution": "640x459", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1813.655, "latencies_ms": [1813.655], "images_per_second": 0.551, "prompt_tokens": 1099, "response_tokens_est": 29, "n_tiles": 1, "output_text": " Three workers in blue uniforms are standing on a street corner, with a motorcycle parked nearby, and a billboard with Chinese text in the background.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.94, "peak": 40.57, "min": 16.56}, "VIN": {"avg": 72.48, "peak": 120.84, "min": 27.33}, "VIN_SYS_5V0": {"avg": 15.07, "peak": 16.15, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 16.21, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 29.94, "energy_joules_est": 54.32, "sample_count": 18, "duration_seconds": 1.814}, "timestamp": "2026-01-19T14:26:33.537245"}
{"image_index": 84, "image_name": "000000008211.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008211.jpg", "image_width": 640, "image_height": 459, "image_resolution": "640x459", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2692.686, "latencies_ms": [2692.686], "images_per_second": 0.371, "prompt_tokens": 1113, "response_tokens_est": 62, "n_tiles": 1, "output_text": " 1. Scooters: 3\n2. Motorcycles: 2\n3. Banners: 2\n4. Posters: 1\n5. People: 3\n6. Banners: 1\n7. Sign: 1\n8. Building: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.05, "peak": 40.57, "min": 17.35}, "VIN": {"avg": 66.48, "peak": 130.91, "min": 28.95}, "VIN_SYS_5V0": {"avg": 14.94, "peak": 16.05, "min": 12.15}, "VDD_CPU_SOC_MSS": {"avg": 15.9, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 27.05, "energy_joules_est": 72.85, "sample_count": 26, "duration_seconds": 2.693}, "timestamp": "2026-01-19T14:26:36.251327"}
{"image_index": 84, "image_name": "000000008211.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008211.jpg", "image_width": 640, "image_height": 459, "image_resolution": "640x459", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2287.481, "latencies_ms": [2287.481], "images_per_second": 0.437, "prompt_tokens": 1117, "response_tokens_est": 47, "n_tiles": 1, "output_text": " The three men are standing on the sidewalk, with the motorcycles parked on the left side of the image. The motorcycles are positioned closer to the camera than the men, and the men are standing in front of a building with a sign.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.33, "peak": 40.16, "min": 16.55}, "VIN": {"avg": 70.96, "peak": 96.42, "min": 37.61}, "VIN_SYS_5V0": {"avg": 15.12, "peak": 16.15, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 16.2, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 28.33, "energy_joules_est": 64.82, "sample_count": 22, "duration_seconds": 2.288}, "timestamp": "2026-01-19T14:26:38.542227"}
{"image_index": 84, "image_name": "000000008211.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008211.jpg", "image_width": 640, "image_height": 459, "image_resolution": "640x459", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1506.431, "latencies_ms": [1506.431], "images_per_second": 0.664, "prompt_tokens": 1111, "response_tokens_est": 17, "n_tiles": 1, "output_text": " Three workers in blue uniforms are standing on a street corner, talking to each other.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.7, "peak": 40.57, "min": 17.74}, "VIN": {"avg": 81.31, "peak": 128.13, "min": 29.89}, "VIN_SYS_5V0": {"avg": 15.24, "peak": 16.15, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 16.4, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 31.7, "energy_joules_est": 47.78, "sample_count": 15, "duration_seconds": 1.507}, "timestamp": "2026-01-19T14:26:40.108900"}
{"image_index": 84, "image_name": "000000008211.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008211.jpg", "image_width": 640, "image_height": 459, "image_resolution": "640x459", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2056.264, "latencies_ms": [2056.264], "images_per_second": 0.486, "prompt_tokens": 1109, "response_tokens_est": 36, "n_tiles": 1, "output_text": " The image is taken during the day with natural light, and the colors are vibrant with a mix of blue, red, and white. The ground is wet, indicating recent rain.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.65, "peak": 40.56, "min": 20.1}, "VIN": {"avg": 72.47, "peak": 122.01, "min": 28.9}, "VIN_SYS_5V0": {"avg": 14.9, "peak": 16.05, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.95, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 29.65, "energy_joules_est": 60.98, "sample_count": 20, "duration_seconds": 2.057}, "timestamp": "2026-01-19T14:26:42.196702"}
{"image_index": 85, "image_name": "000000008277.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008277.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2231.198, "latencies_ms": [2231.198], "images_per_second": 0.448, "prompt_tokens": 1432, "response_tokens_est": 29, "n_tiles": 1, "output_text": " A black plastic container filled with a green and yellow mixture of vegetables and meat is placed on a white paper plate, which also contains shredded chicken.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.2, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.87, "peak": 40.18, "min": 16.95}, "VIN": {"avg": 73.61, "peak": 116.0, "min": 28.79}, "VIN_SYS_5V0": {"avg": 15.4, "peak": 16.76, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 16.48, "peak": 18.11, "min": 13.79}}, "power_watts_avg": 30.87, "energy_joules_est": 68.89, "sample_count": 22, "duration_seconds": 2.232}, "timestamp": "2026-01-19T14:26:44.495094"}
{"image_index": 85, "image_name": "000000008277.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008277.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3067.78, "latencies_ms": [3067.78], "images_per_second": 0.326, "prompt_tokens": 1446, "response_tokens_est": 58, "n_tiles": 1, "output_text": " 1. plate: 1\n2. bowl: 1\n3. fork: 1\n4. shredded chicken: 1\n5. broccoli: 1\n6. mashed potatoes: 1\n7. sauce: 1\n8. carpet: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.31, "peak": 40.57, "min": 17.74}, "VIN": {"avg": 74.21, "peak": 118.4, "min": 30.77}, "VIN_SYS_5V0": {"avg": 15.09, "peak": 16.56, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 16.09, "peak": 17.71, "min": 13.39}}, "power_watts_avg": 28.31, "energy_joules_est": 86.87, "sample_count": 30, "duration_seconds": 3.069}, "timestamp": "2026-01-19T14:26:47.610601"}
{"image_index": 85, "image_name": "000000008277.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008277.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2863.823, "latencies_ms": [2863.823], "images_per_second": 0.349, "prompt_tokens": 1450, "response_tokens_est": 53, "n_tiles": 1, "output_text": " The black bowl of food is located in the upper left corner of the image, while the white plate with shredded chicken is situated in the lower right corner. The chicken is placed near the edge of the plate, and the bowl of food is positioned slightly above it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.75, "peak": 40.18, "min": 16.16}, "VIN": {"avg": 73.03, "peak": 121.83, "min": 29.26}, "VIN_SYS_5V0": {"avg": 15.18, "peak": 16.56, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 16.18, "peak": 17.71, "min": 13.39}}, "power_watts_avg": 28.75, "energy_joules_est": 82.35, "sample_count": 28, "duration_seconds": 2.864}, "timestamp": "2026-01-19T14:26:50.522671"}
{"image_index": 85, "image_name": "000000008277.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008277.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1942.895, "latencies_ms": [1942.895], "images_per_second": 0.515, "prompt_tokens": 1444, "response_tokens_est": 18, "n_tiles": 1, "output_text": " A plate with a bowl of food and a fork on it is sitting on a table.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.5, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.18, "peak": 40.18, "min": 16.56}, "VIN": {"avg": 72.37, "peak": 117.37, "min": 27.58}, "VIN_SYS_5V0": {"avg": 15.29, "peak": 16.56, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 16.37, "peak": 17.71, "min": 13.39}}, "power_watts_avg": 32.18, "energy_joules_est": 62.54, "sample_count": 19, "duration_seconds": 1.943}, "timestamp": "2026-01-19T14:26:52.504084"}
{"image_index": 85, "image_name": "000000008277.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008277.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2950.64, "latencies_ms": [2950.64], "images_per_second": 0.339, "prompt_tokens": 1442, "response_tokens_est": 54, "n_tiles": 1, "output_text": " The image features a black plastic container filled with a green and yellow mixture of vegetables and meat, placed on a white paper plate. The plate is placed on a beige carpeted floor, and the lighting in the room is dim, creating a warm and inviting atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.94, "peak": 40.57, "min": 18.14}, "VIN": {"avg": 69.81, "peak": 116.38, "min": 28.68}, "VIN_SYS_5V0": {"avg": 15.15, "peak": 16.66, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 16.12, "peak": 17.71, "min": 13.79}}, "power_watts_avg": 28.94, "energy_joules_est": 85.4, "sample_count": 29, "duration_seconds": 2.951}, "timestamp": "2026-01-19T14:26:55.525015"}
{"image_index": 86, "image_name": "000000008532.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008532.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1560.781, "latencies_ms": [1560.781], "images_per_second": 0.641, "prompt_tokens": 1099, "response_tokens_est": 17, "n_tiles": 1, "output_text": " A man with glasses and a plaid hat is smiling in front of a building.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.24, "peak": 39.78, "min": 15.77}, "VIN": {"avg": 68.23, "peak": 104.66, "min": 28.67}, "VIN_SYS_5V0": {"avg": 14.63, "peak": 15.95, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.58, "peak": 16.92, "min": 13.0}}, "power_watts_avg": 30.24, "energy_joules_est": 47.22, "sample_count": 16, "duration_seconds": 1.562}, "timestamp": "2026-01-19T14:26:57.192156"}
{"image_index": 86, "image_name": "000000008532.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008532.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2607.484, "latencies_ms": [2607.484], "images_per_second": 0.384, "prompt_tokens": 1113, "response_tokens_est": 56, "n_tiles": 1, "output_text": " 1. man: 1\n2. glasses: 1\n3. tie: 1\n4. shirt: 1\n5. cap: 1\n6. building: 1\n7. window: 1\n8. ground: 1", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.91, "peak": 40.18, "min": 17.35}, "VIN": {"avg": 68.81, "peak": 118.17, "min": 29.89}, "VIN_SYS_5V0": {"avg": 14.64, "peak": 15.85, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.51, "peak": 16.92, "min": 12.61}}, "power_watts_avg": 26.91, "energy_joules_est": 70.18, "sample_count": 26, "duration_seconds": 2.608}, "timestamp": "2026-01-19T14:26:59.896363"}
{"image_index": 86, "image_name": "000000008532.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008532.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2233.171, "latencies_ms": [2233.171], "images_per_second": 0.448, "prompt_tokens": 1117, "response_tokens_est": 44, "n_tiles": 1, "output_text": " The man is positioned in the foreground of the image, with the background consisting of a building and a pool. The man is wearing a blue shirt and a red tie, and he is also wearing a plaid hat.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.83, "peak": 39.78, "min": 14.59}, "VIN": {"avg": 71.0, "peak": 118.79, "min": 28.68}, "VIN_SYS_5V0": {"avg": 14.81, "peak": 15.95, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.71, "peak": 17.32, "min": 12.61}}, "power_watts_avg": 27.83, "energy_joules_est": 62.16, "sample_count": 22, "duration_seconds": 2.234}, "timestamp": "2026-01-19T14:27:02.188297"}
{"image_index": 86, "image_name": "000000008532.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008532.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1528.383, "latencies_ms": [1528.383], "images_per_second": 0.654, "prompt_tokens": 1111, "response_tokens_est": 17, "n_tiles": 1, "output_text": " A man wearing a plaid hat and glasses is standing in front of a building.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.17, "peak": 40.16, "min": 16.55}, "VIN": {"avg": 71.66, "peak": 123.38, "min": 28.68}, "VIN_SYS_5V0": {"avg": 14.89, "peak": 16.05, "min": 12.36}, "VDD_CPU_SOC_MSS": {"avg": 15.96, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 31.17, "energy_joules_est": 47.65, "sample_count": 15, "duration_seconds": 1.529}, "timestamp": "2026-01-19T14:27:03.751244"}
{"image_index": 86, "image_name": "000000008532.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008532.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1618.505, "latencies_ms": [1618.505], "images_per_second": 0.618, "prompt_tokens": 1109, "response_tokens_est": 21, "n_tiles": 1, "output_text": " The man is wearing a blue shirt and a red tie, and the picture was taken in bright daylight.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.06, "peak": 40.97, "min": 20.89}, "VIN": {"avg": 71.72, "peak": 104.27, "min": 29.53}, "VIN_SYS_5V0": {"avg": 15.07, "peak": 16.15, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 16.19, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 32.06, "energy_joules_est": 51.91, "sample_count": 16, "duration_seconds": 1.619}, "timestamp": "2026-01-19T14:27:05.418107"}
{"image_index": 87, "image_name": "000000008629.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008629.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2370.571, "latencies_ms": [2370.571], "images_per_second": 0.422, "prompt_tokens": 1432, "response_tokens_est": 33, "n_tiles": 1, "output_text": " The image is a collage of six photos showing different slices of pizza, with the top left photo being the most detailed and the bottom right photo being the least detailed.", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.2, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.97, "peak": 40.57, "min": 19.31}, "VIN": {"avg": 72.33, "peak": 123.57, "min": 29.8}, "VIN_SYS_5V0": {"avg": 15.35, "peak": 16.66, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 16.38, "peak": 17.71, "min": 13.79}}, "power_watts_avg": 30.97, "energy_joules_est": 73.44, "sample_count": 23, "duration_seconds": 2.371}, "timestamp": "2026-01-19T14:27:07.805825"}
{"image_index": 87, "image_name": "000000008629.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008629.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1527.787, "latencies_ms": [1527.787], "images_per_second": 0.655, "prompt_tokens": 1446, "response_tokens_est": 4, "n_tiles": 1, "output_text": " pizza: 6", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 1.7, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 34.27, "peak": 40.57, "min": 18.14}, "VIN": {"avg": 76.6, "peak": 125.13, "min": 30.3}, "VIN_SYS_5V0": {"avg": 15.6, "peak": 16.76, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 16.72, "peak": 17.71, "min": 14.18}}, "power_watts_avg": 34.27, "energy_joules_est": 52.37, "sample_count": 15, "duration_seconds": 1.528}, "timestamp": "2026-01-19T14:27:09.371722"}
{"image_index": 87, "image_name": "000000008629.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008629.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2982.106, "latencies_ms": [2982.106], "images_per_second": 0.335, "prompt_tokens": 1450, "response_tokens_est": 57, "n_tiles": 1, "output_text": " The main objects are the pizza slices, which are arranged in a grid pattern. The pizza slices are in the foreground, with the background being the plate they are on. The pizza slices are positioned to the left of the plate, with the plate being in the center of the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.24, "peak": 41.36, "min": 19.7}, "VIN": {"avg": 70.68, "peak": 129.2, "min": 29.07}, "VIN_SYS_5V0": {"avg": 15.31, "peak": 16.66, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 16.33, "peak": 17.71, "min": 14.18}}, "power_watts_avg": 30.24, "energy_joules_est": 90.19, "sample_count": 29, "duration_seconds": 2.982}, "timestamp": "2026-01-19T14:27:12.385671"}
{"image_index": 87, "image_name": "000000008629.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008629.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1808.04, "latencies_ms": [1808.04], "images_per_second": 0.553, "prompt_tokens": 1444, "response_tokens_est": 14, "n_tiles": 1, "output_text": " The image is a collage of six photos showing different slices of pizza.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.3, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.77, "peak": 40.18, "min": 16.95}, "VIN": {"avg": 78.91, "peak": 130.2, "min": 29.13}, "VIN_SYS_5V0": {"avg": 15.4, "peak": 16.66, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 16.53, "peak": 17.71, "min": 13.79}}, "power_watts_avg": 32.77, "energy_joules_est": 59.26, "sample_count": 18, "duration_seconds": 1.809}, "timestamp": "2026-01-19T14:27:14.259582"}
{"image_index": 87, "image_name": "000000008629.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008629.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1849.016, "latencies_ms": [1849.016], "images_per_second": 0.541, "prompt_tokens": 1442, "response_tokens_est": 15, "n_tiles": 1, "output_text": " The pizza has a yellow and orange color, and the lighting is bright.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.5, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 33.63, "peak": 40.56, "min": 20.89}, "VIN": {"avg": 73.74, "peak": 123.73, "min": 28.94}, "VIN_SYS_5V0": {"avg": 15.29, "peak": 16.66, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 16.4, "peak": 17.71, "min": 13.39}}, "power_watts_avg": 33.63, "energy_joules_est": 62.2, "sample_count": 18, "duration_seconds": 1.849}, "timestamp": "2026-01-19T14:27:16.142054"}
{"image_index": 88, "image_name": "000000008690.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008690.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1502.817, "latencies_ms": [1502.817], "images_per_second": 0.665, "prompt_tokens": 1099, "response_tokens_est": 17, "n_tiles": 1, "output_text": " Two young girls are petting a goat and a sheep in a fenced area.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.93, "peak": 41.34, "min": 21.67}, "VIN": {"avg": 78.05, "peak": 120.67, "min": 29.96}, "VIN_SYS_5V0": {"avg": 15.15, "peak": 16.15, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 16.24, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 32.93, "energy_joules_est": 49.52, "sample_count": 15, "duration_seconds": 1.504}, "timestamp": "2026-01-19T14:27:17.708410"}
{"image_index": 88, "image_name": "000000008690.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008690.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1585.87, "latencies_ms": [1585.87], "images_per_second": 0.631, "prompt_tokens": 1113, "response_tokens_est": 19, "n_tiles": 1, "output_text": " goat: 2, girl: 2, fence: 1, person: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.79, "peak": 40.57, "min": 20.1}, "VIN": {"avg": 75.6, "peak": 128.04, "min": 27.53}, "VIN_SYS_5V0": {"avg": 14.89, "peak": 16.05, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.94, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 31.79, "energy_joules_est": 50.43, "sample_count": 16, "duration_seconds": 1.586}, "timestamp": "2026-01-19T14:27:19.375565"}
{"image_index": 88, "image_name": "000000008690.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008690.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2667.019, "latencies_ms": [2667.019], "images_per_second": 0.375, "prompt_tokens": 1117, "response_tokens_est": 61, "n_tiles": 1, "output_text": " The two girls are standing on the left side of the image, with the goat in the middle and the other goat on the right side. The goat on the left is being petted by the girl in the pink dress, while the girl in the blue dress is petting the goat on the right.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.44, "peak": 40.57, "min": 18.91}, "VIN": {"avg": 70.77, "peak": 112.77, "min": 29.38}, "VIN_SYS_5V0": {"avg": 14.91, "peak": 16.05, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.89, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 27.44, "energy_joules_est": 73.2, "sample_count": 26, "duration_seconds": 2.667}, "timestamp": "2026-01-19T14:27:22.088244"}
{"image_index": 88, "image_name": "000000008690.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008690.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1434.38, "latencies_ms": [1434.38], "images_per_second": 0.697, "prompt_tokens": 1111, "response_tokens_est": 14, "n_tiles": 1, "output_text": " Two young girls are petting a goat in a fenced area.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.69, "peak": 40.57, "min": 16.16}, "VIN": {"avg": 73.96, "peak": 106.33, "min": 31.08}, "VIN_SYS_5V0": {"avg": 15.1, "peak": 16.15, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 16.2, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 31.69, "energy_joules_est": 45.47, "sample_count": 14, "duration_seconds": 1.435}, "timestamp": "2026-01-19T14:27:23.547776"}
{"image_index": 88, "image_name": "000000008690.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008690.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1861.923, "latencies_ms": [1861.923], "images_per_second": 0.537, "prompt_tokens": 1109, "response_tokens_est": 31, "n_tiles": 1, "output_text": " The image is taken in bright daylight with a clear blue sky in the background. The girls are wearing colorful dresses, and the goats are black and white.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.54, "peak": 40.95, "min": 22.06}, "VIN": {"avg": 75.53, "peak": 127.42, "min": 29.3}, "VIN_SYS_5V0": {"avg": 15.2, "peak": 16.15, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 16.27, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 31.54, "energy_joules_est": 58.74, "sample_count": 18, "duration_seconds": 1.862}, "timestamp": "2026-01-19T14:27:25.423295"}
{"image_index": 89, "image_name": "000000008762.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008762.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2062.006, "latencies_ms": [2062.006], "images_per_second": 0.485, "prompt_tokens": 1099, "response_tokens_est": 39, "n_tiles": 1, "output_text": " The image captures a nighttime scene at a street intersection, where a traffic light is illuminated by a green light, and a street sign with the name \"MAGNOLIA\" is visible.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26771.4, "ram_available_mb": 99000.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26775.4, "ram_available_mb": 98996.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.59, "peak": 40.57, "min": 18.52}, "VIN": {"avg": 71.11, "peak": 121.12, "min": 29.08}, "VIN_SYS_5V0": {"avg": 15.19, "peak": 16.15, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 16.24, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 29.59, "energy_joules_est": 61.03, "sample_count": 20, "duration_seconds": 2.062}, "timestamp": "2026-01-19T14:27:27.510406"}
{"image_index": 89, "image_name": "000000008762.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008762.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2622.939, "latencies_ms": [2622.939], "images_per_second": 0.381, "prompt_tokens": 1113, "response_tokens_est": 60, "n_tiles": 1, "output_text": " 1. traffic light: 2\n2. street sign: 1\n3. street light: 1\n4. car: 1\n5. mountain: 1\n6. road: 1\n7. sky: 1\n8. road sign: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26775.4, "ram_available_mb": 98996.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26775.4, "ram_available_mb": 98996.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.09, "peak": 40.57, "min": 17.35}, "VIN": {"avg": 68.69, "peak": 114.1, "min": 27.41}, "VIN_SYS_5V0": {"avg": 15.08, "peak": 16.15, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 16.1, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 27.09, "energy_joules_est": 71.06, "sample_count": 26, "duration_seconds": 2.623}, "timestamp": "2026-01-19T14:27:30.215262"}
{"image_index": 89, "image_name": "000000008762.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008762.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2537.307, "latencies_ms": [2537.307], "images_per_second": 0.394, "prompt_tokens": 1117, "response_tokens_est": 56, "n_tiles": 1, "output_text": " The traffic light is on the left side of the image, while the street sign is on the right side. The traffic light is closer to the viewer, while the street sign is farther away. The traffic light is in the foreground, while the street sign is in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26775.4, "ram_available_mb": 98996.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26775.3, "ram_available_mb": 98996.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.95, "peak": 40.16, "min": 15.38}, "VIN": {"avg": 71.75, "peak": 104.18, "min": 29.27}, "VIN_SYS_5V0": {"avg": 14.9, "peak": 16.05, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 15.92, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 26.95, "energy_joules_est": 68.39, "sample_count": 25, "duration_seconds": 2.538}, "timestamp": "2026-01-19T14:27:32.824204"}
{"image_index": 89, "image_name": "000000008762.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008762.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2512.707, "latencies_ms": [2512.707], "images_per_second": 0.398, "prompt_tokens": 1111, "response_tokens_est": 55, "n_tiles": 1, "output_text": " The image captures a night scene at a street intersection, where a traffic light is illuminated by a green light, signaling vehicles to proceed. The sky is dark, suggesting it is either early morning or late evening, and the street is lined with buildings and mountains in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26775.3, "ram_available_mb": 98996.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26775.3, "ram_available_mb": 98996.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.01, "peak": 40.16, "min": 15.77}, "VIN": {"avg": 72.67, "peak": 119.95, "min": 29.35}, "VIN_SYS_5V0": {"avg": 14.88, "peak": 16.05, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.89, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 27.01, "energy_joules_est": 67.88, "sample_count": 25, "duration_seconds": 2.513}, "timestamp": "2026-01-19T14:27:35.432497"}
{"image_index": 89, "image_name": "000000008762.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008762.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3414.282, "latencies_ms": [3414.282], "images_per_second": 0.293, "prompt_tokens": 1109, "response_tokens_est": 82, "n_tiles": 1, "output_text": " The image captures a tranquil night scene at a street intersection, where the sky is painted in hues of deep blue, and the moon casts a soft glow. The traffic lights, glowing in a soothing shade of green, stand as silent sentinels guiding the flow of vehicles. The street, lined with buildings and trees, is bathed in the warm light of street lamps, creating a serene and peaceful atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26775.3, "ram_available_mb": 98996.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 26775.3, "ram_available_mb": 98996.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 24.55, "peak": 40.16, "min": 15.38}, "VIN": {"avg": 64.49, "peak": 110.9, "min": 29.76}, "VIN_SYS_5V0": {"avg": 14.58, "peak": 15.95, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.49, "peak": 16.92, "min": 13.0}}, "power_watts_avg": 24.55, "energy_joules_est": 83.83, "sample_count": 33, "duration_seconds": 3.415}, "timestamp": "2026-01-19T14:27:38.877244"}
{"image_index": 90, "image_name": "000000008844.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008844.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1642.371, "latencies_ms": [1642.371], "images_per_second": 0.609, "prompt_tokens": 1099, "response_tokens_est": 22, "n_tiles": 1, "output_text": " A woman wearing a patterned dress stands in front of a building with a bunch of bananas in front of her.", "error": null, "sys_before": {"cpu_percent": 14.3, "ram_used_mb": 26775.3, "ram_available_mb": 98996.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26775.3, "ram_available_mb": 98996.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.53, "peak": 40.57, "min": 15.77}, "VIN": {"avg": 68.32, "peak": 85.42, "min": 29.09}, "VIN_SYS_5V0": {"avg": 15.05, "peak": 16.05, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 16.12, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 30.53, "energy_joules_est": 50.15, "sample_count": 16, "duration_seconds": 1.643}, "timestamp": "2026-01-19T14:27:40.556063"}
{"image_index": 90, "image_name": "000000008844.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008844.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1954.947, "latencies_ms": [1954.947], "images_per_second": 0.512, "prompt_tokens": 1113, "response_tokens_est": 34, "n_tiles": 1, "output_text": " banana: 2, woman: 1, building: 1, door: 1, wall: 1, sign: 1, window: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26775.3, "ram_available_mb": 98996.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26775.3, "ram_available_mb": 98996.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.25, "peak": 40.95, "min": 19.7}, "VIN": {"avg": 70.54, "peak": 91.02, "min": 29.05}, "VIN_SYS_5V0": {"avg": 15.07, "peak": 16.15, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 16.22, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 30.25, "energy_joules_est": 59.15, "sample_count": 19, "duration_seconds": 1.955}, "timestamp": "2026-01-19T14:27:42.536156"}
{"image_index": 90, "image_name": "000000008844.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008844.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1934.713, "latencies_ms": [1934.713], "images_per_second": 0.517, "prompt_tokens": 1117, "response_tokens_est": 33, "n_tiles": 1, "output_text": " The bananas are in the foreground, with the woman standing behind them. The woman is positioned to the right of the bananas, and the building is in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26775.3, "ram_available_mb": 98996.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26775.3, "ram_available_mb": 98996.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.84, "peak": 40.57, "min": 17.73}, "VIN": {"avg": 69.76, "peak": 91.2, "min": 29.32}, "VIN_SYS_5V0": {"avg": 15.09, "peak": 16.15, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 16.18, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 29.84, "energy_joules_est": 57.74, "sample_count": 19, "duration_seconds": 1.935}, "timestamp": "2026-01-19T14:27:44.521930"}
{"image_index": 90, "image_name": "000000008844.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008844.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1479.246, "latencies_ms": [1479.246], "images_per_second": 0.676, "prompt_tokens": 1111, "response_tokens_est": 15, "n_tiles": 1, "output_text": " A woman is standing in front of a building with bananas on a table.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26775.3, "ram_available_mb": 98996.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 26775.3, "ram_available_mb": 98996.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.41, "peak": 40.57, "min": 17.35}, "VIN": {"avg": 78.83, "peak": 125.37, "min": 28.85}, "VIN_SYS_5V0": {"avg": 14.87, "peak": 16.05, "min": 12.46}, "VDD_CPU_SOC_MSS": {"avg": 15.96, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 31.41, "energy_joules_est": 46.47, "sample_count": 15, "duration_seconds": 1.479}, "timestamp": "2026-01-19T14:27:46.092320"}
{"image_index": 90, "image_name": "000000008844.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008844.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1786.98, "latencies_ms": [1786.98], "images_per_second": 0.56, "prompt_tokens": 1109, "response_tokens_est": 27, "n_tiles": 1, "output_text": " The image is taken in a sunny day with a warm color tone. The woman is wearing a colorful dress and the bananas are yellow.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26775.3, "ram_available_mb": 98996.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26775.3, "ram_available_mb": 98996.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.51, "peak": 40.57, "min": 19.32}, "VIN": {"avg": 66.27, "peak": 126.62, "min": 28.63}, "VIN_SYS_5V0": {"avg": 14.78, "peak": 16.05, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.79, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 30.51, "energy_joules_est": 54.53, "sample_count": 18, "duration_seconds": 1.787}, "timestamp": "2026-01-19T14:27:47.967990"}
{"image_index": 91, "image_name": "000000008899.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008899.jpg", "image_width": 640, "image_height": 539, "image_resolution": "640x539", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 3381.898, "latencies_ms": [3381.898], "images_per_second": 0.296, "prompt_tokens": 1432, "response_tokens_est": 71, "n_tiles": 1, "output_text": " The image captures a vibrant urban scene, featuring a red brick building adorned with a green awning and a green fire escape. The building's facade is a canvas of graffiti, with various tags and markings adorning the shuttered windows and doors. A fire hydrant stands guard on the sidewalk, adding a touch of urban grit to the scene.", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 26775.3, "ram_available_mb": 98996.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26775.3, "ram_available_mb": 98996.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.2, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.34, "peak": 40.18, "min": 16.95}, "VIN": {"avg": 69.02, "peak": 123.87, "min": 30.19}, "VIN_SYS_5V0": {"avg": 15.11, "peak": 16.76, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 16.1, "peak": 18.11, "min": 13.0}}, "power_watts_avg": 27.34, "energy_joules_est": 92.47, "sample_count": 33, "duration_seconds": 3.382}, "timestamp": "2026-01-19T14:27:51.403196"}
{"image_index": 91, "image_name": "000000008899.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008899.jpg", "image_width": 640, "image_height": 539, "image_resolution": "640x539", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1979.605, "latencies_ms": [1979.605], "images_per_second": 0.505, "prompt_tokens": 1446, "response_tokens_est": 20, "n_tiles": 1, "output_text": " fire hydrant: 1\nbike rack: 1\ngraffiti: 10", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26775.3, "ram_available_mb": 98996.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 26775.3, "ram_available_mb": 98996.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.61, "peak": 40.18, "min": 16.16}, "VIN": {"avg": 75.58, "peak": 116.07, "min": 27.43}, "VIN_SYS_5V0": {"avg": 15.24, "peak": 16.66, "min": 12.46}, "VDD_CPU_SOC_MSS": {"avg": 16.4, "peak": 17.71, "min": 13.39}}, "power_watts_avg": 31.61, "energy_joules_est": 62.58, "sample_count": 20, "duration_seconds": 1.98}, "timestamp": "2026-01-19T14:27:53.485501"}
{"image_index": 91, "image_name": "000000008899.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008899.jpg", "image_width": 640, "image_height": 539, "image_resolution": "640x539", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3022.721, "latencies_ms": [3022.721], "images_per_second": 0.331, "prompt_tokens": 1450, "response_tokens_est": 55, "n_tiles": 1, "output_text": " The fire hydrant is located on the left side of the image, in the foreground. The building is in the background, with the fire hydrant in front of it. The green door is in the middle of the image, between the fire hydrant and the building.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26775.3, "ram_available_mb": 98996.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26775.3, "ram_available_mb": 98996.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.2, "peak": 40.18, "min": 17.73}, "VIN": {"avg": 71.0, "peak": 125.61, "min": 28.67}, "VIN_SYS_5V0": {"avg": 14.91, "peak": 16.46, "min": 11.46}, "VDD_CPU_SOC_MSS": {"avg": 15.91, "peak": 17.71, "min": 13.0}}, "power_watts_avg": 28.2, "energy_joules_est": 85.25, "sample_count": 30, "duration_seconds": 3.023}, "timestamp": "2026-01-19T14:27:56.610701"}
{"image_index": 91, "image_name": "000000008899.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008899.jpg", "image_width": 640, "image_height": 539, "image_resolution": "640x539", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3504.845, "latencies_ms": [3504.845], "images_per_second": 0.285, "prompt_tokens": 1444, "response_tokens_est": 76, "n_tiles": 1, "output_text": " The image captures a vibrant urban scene, where a red brick building stands proudly on a city street corner. The building, adorned with a green awning, features two shuttered storefronts, each covered in a riot of graffiti. A fire hydrant, a silent sentinel, stands guard on the sidewalk, while a tree adds a touch of nature to the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26775.3, "ram_available_mb": 98996.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26775.3, "ram_available_mb": 98996.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.12, "peak": 40.18, "min": 14.98}, "VIN": {"avg": 72.01, "peak": 125.94, "min": 29.31}, "VIN_SYS_5V0": {"avg": 15.01, "peak": 16.46, "min": 11.46}, "VDD_CPU_SOC_MSS": {"avg": 15.99, "peak": 17.71, "min": 12.61}}, "power_watts_avg": 27.12, "energy_joules_est": 95.07, "sample_count": 34, "duration_seconds": 3.505}, "timestamp": "2026-01-19T14:28:00.128127"}
{"image_index": 91, "image_name": "000000008899.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008899.jpg", "image_width": 640, "image_height": 539, "image_resolution": "640x539", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2665.017, "latencies_ms": [2665.017], "images_per_second": 0.375, "prompt_tokens": 1442, "response_tokens_est": 46, "n_tiles": 1, "output_text": " The image depicts a red brick building with green shutters and a fire escape, and a fire hydrant is visible in the foreground. The weather appears to be overcast, and the lighting is natural, likely from the sun.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26775.3, "ram_available_mb": 98996.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26775.3, "ram_available_mb": 98996.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.59, "peak": 40.16, "min": 16.95}, "VIN": {"avg": 71.91, "peak": 114.23, "min": 28.68}, "VIN_SYS_5V0": {"avg": 15.33, "peak": 16.66, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 16.44, "peak": 17.71, "min": 14.18}}, "power_watts_avg": 29.59, "energy_joules_est": 78.87, "sample_count": 26, "duration_seconds": 2.665}, "timestamp": "2026-01-19T14:28:02.836786"}
{"image_index": 92, "image_name": "000000009378.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009378.jpg", "image_width": 600, "image_height": 400, "image_resolution": "600x400", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1824.359, "latencies_ms": [1824.359], "images_per_second": 0.548, "prompt_tokens": 1099, "response_tokens_est": 29, "n_tiles": 1, "output_text": " A man wearing a blue and white striped beanie is holding a yellow frisbee in his hand and appears to be preparing to throw it.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 26775.3, "ram_available_mb": 98996.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26775.3, "ram_available_mb": 98996.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.07, "peak": 40.56, "min": 16.95}, "VIN": {"avg": 77.56, "peak": 123.43, "min": 28.68}, "VIN_SYS_5V0": {"avg": 14.96, "peak": 16.05, "min": 12.56}, "VDD_CPU_SOC_MSS": {"avg": 16.12, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 30.07, "energy_joules_est": 54.88, "sample_count": 18, "duration_seconds": 1.825}, "timestamp": "2026-01-19T14:28:04.713465"}
{"image_index": 92, "image_name": "000000009378.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009378.jpg", "image_width": 600, "image_height": 400, "image_resolution": "600x400", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2582.588, "latencies_ms": [2582.588], "images_per_second": 0.387, "prompt_tokens": 1113, "response_tokens_est": 58, "n_tiles": 1, "output_text": " 1. person: 1\n2. head: 1\n3. hair: 1\n4. shirt: 1\n5. hand: 1\n6. frisbee: 1\n7. ceiling: 1\n8. wall: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26775.3, "ram_available_mb": 98996.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 26775.3, "ram_available_mb": 98996.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.48, "peak": 40.57, "min": 17.74}, "VIN": {"avg": 71.68, "peak": 122.53, "min": 29.08}, "VIN_SYS_5V0": {"avg": 15.0, "peak": 16.05, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 16.06, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 27.48, "energy_joules_est": 70.98, "sample_count": 25, "duration_seconds": 2.583}, "timestamp": "2026-01-19T14:28:07.321993"}
{"image_index": 92, "image_name": "000000009378.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009378.jpg", "image_width": 600, "image_height": 400, "image_resolution": "600x400", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2260.025, "latencies_ms": [2260.025], "images_per_second": 0.442, "prompt_tokens": 1117, "response_tokens_est": 46, "n_tiles": 1, "output_text": " The man is in the foreground, holding a frisbee in his hand. The frisbee is in front of him, and he is looking at it. The background is dark, and there are people in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26775.3, "ram_available_mb": 98996.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26775.3, "ram_available_mb": 98996.9, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.33, "peak": 40.16, "min": 16.55}, "VIN": {"avg": 70.7, "peak": 106.48, "min": 31.21}, "VIN_SYS_5V0": {"avg": 15.11, "peak": 16.15, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 16.21, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 28.33, "energy_joules_est": 64.03, "sample_count": 22, "duration_seconds": 2.26}, "timestamp": "2026-01-19T14:28:09.610078"}
{"image_index": 92, "image_name": "000000009378.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009378.jpg", "image_width": 600, "image_height": 400, "image_resolution": "600x400", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1602.672, "latencies_ms": [1602.672], "images_per_second": 0.624, "prompt_tokens": 1111, "response_tokens_est": 20, "n_tiles": 1, "output_text": " A man wearing a blue hat and a black shirt is playing frisbee in a dark room.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26775.3, "ram_available_mb": 98996.9, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26775.3, "ram_available_mb": 98996.9, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.85, "peak": 40.56, "min": 16.95}, "VIN": {"avg": 73.34, "peak": 115.93, "min": 29.6}, "VIN_SYS_5V0": {"avg": 15.05, "peak": 16.15, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 16.17, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 30.85, "energy_joules_est": 49.45, "sample_count": 16, "duration_seconds": 1.603}, "timestamp": "2026-01-19T14:28:11.283187"}
{"image_index": 92, "image_name": "000000009378.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009378.jpg", "image_width": 600, "image_height": 400, "image_resolution": "600x400", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1841.899, "latencies_ms": [1841.899], "images_per_second": 0.543, "prompt_tokens": 1109, "response_tokens_est": 29, "n_tiles": 1, "output_text": " The image is a black and white photo with a blue and white striped hat. The lighting is dim, and the subject is in the foreground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26775.3, "ram_available_mb": 98996.9, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26775.5, "ram_available_mb": 98996.6, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.46, "peak": 40.57, "min": 18.92}, "VIN": {"avg": 75.49, "peak": 127.13, "min": 29.36}, "VIN_SYS_5V0": {"avg": 14.91, "peak": 16.05, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 16.03, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 30.46, "energy_joules_est": 56.13, "sample_count": 18, "duration_seconds": 1.843}, "timestamp": "2026-01-19T14:28:13.159247"}
{"image_index": 93, "image_name": "000000009400.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009400.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1651.407, "latencies_ms": [1651.407], "images_per_second": 0.606, "prompt_tokens": 1099, "response_tokens_est": 23, "n_tiles": 1, "output_text": " A group of people are sitting around a table with laptops and books, and they are all looking at their screens.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 26775.5, "ram_available_mb": 98996.6, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26775.5, "ram_available_mb": 98996.6, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.37, "peak": 40.57, "min": 18.13}, "VIN": {"avg": 75.41, "peak": 120.9, "min": 29.89}, "VIN_SYS_5V0": {"avg": 15.16, "peak": 16.15, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 16.29, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 31.37, "energy_joules_est": 51.82, "sample_count": 16, "duration_seconds": 1.652}, "timestamp": "2026-01-19T14:28:14.828783"}
{"image_index": 93, "image_name": "000000009400.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009400.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1932.501, "latencies_ms": [1932.501], "images_per_second": 0.517, "prompt_tokens": 1113, "response_tokens_est": 34, "n_tiles": 1, "output_text": " laptop: 3, keyboard: 1, mouse: 1, cup: 1, bottle: 1, book: 1, person: 4", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26775.5, "ram_available_mb": 98996.6, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26775.5, "ram_available_mb": 98996.6, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.5, "peak": 40.57, "min": 20.49}, "VIN": {"avg": 70.46, "peak": 107.04, "min": 28.56}, "VIN_SYS_5V0": {"avg": 15.23, "peak": 16.26, "min": 13.29}, "VDD_CPU_SOC_MSS": {"avg": 16.33, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 30.5, "energy_joules_est": 58.96, "sample_count": 19, "duration_seconds": 1.933}, "timestamp": "2026-01-19T14:28:16.807997"}
{"image_index": 93, "image_name": "000000009400.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009400.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2490.685, "latencies_ms": [2490.685], "images_per_second": 0.401, "prompt_tokens": 1117, "response_tokens_est": 55, "n_tiles": 1, "output_text": " The laptop is on the left side of the table, the woman is on the right side of the table, and the man is in the middle of the table. The laptop is in the foreground, the woman is in the middle, and the man is in the background.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26775.5, "ram_available_mb": 98996.6, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26775.5, "ram_available_mb": 98996.6, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.82, "peak": 40.56, "min": 17.34}, "VIN": {"avg": 68.08, "peak": 110.5, "min": 34.96}, "VIN_SYS_5V0": {"avg": 15.03, "peak": 16.15, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 16.13, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 27.82, "energy_joules_est": 69.3, "sample_count": 24, "duration_seconds": 2.491}, "timestamp": "2026-01-19T14:28:19.308279"}
{"image_index": 93, "image_name": "000000009400.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009400.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1517.497, "latencies_ms": [1517.497], "images_per_second": 0.659, "prompt_tokens": 1111, "response_tokens_est": 18, "n_tiles": 1, "output_text": " A group of people are sitting at a table in a restaurant, working on their laptops.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26775.5, "ram_available_mb": 98996.6, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26777.7, "ram_available_mb": 98994.4, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.65, "peak": 39.78, "min": 17.73}, "VIN": {"avg": 72.28, "peak": 98.45, "min": 29.02}, "VIN_SYS_5V0": {"avg": 15.28, "peak": 16.26, "min": 13.29}, "VDD_CPU_SOC_MSS": {"avg": 16.56, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 31.65, "energy_joules_est": 48.04, "sample_count": 15, "duration_seconds": 1.518}, "timestamp": "2026-01-19T14:28:20.875422"}
{"image_index": 93, "image_name": "000000009400.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009400.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2354.023, "latencies_ms": [2354.023], "images_per_second": 0.425, "prompt_tokens": 1109, "response_tokens_est": 50, "n_tiles": 1, "output_text": " The image is taken in a dimly lit room with a warm yellow light. The people are using laptops and computers, and there are many objects on the table, including a glass of water, a cup, and a bottle of ketchup.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26777.7, "ram_available_mb": 98994.4, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 26778.7, "ram_available_mb": 98993.5, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.61, "peak": 40.97, "min": 19.31}, "VIN": {"avg": 66.45, "peak": 116.35, "min": 28.79}, "VIN_SYS_5V0": {"avg": 15.07, "peak": 16.15, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 16.3, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 28.61, "energy_joules_est": 67.36, "sample_count": 23, "duration_seconds": 2.354}, "timestamp": "2026-01-19T14:28:23.276366"}
{"image_index": 94, "image_name": "000000009448.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009448.jpg", "image_width": 551, "image_height": 640, "image_resolution": "551x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2027.515, "latencies_ms": [2027.515], "images_per_second": 0.493, "prompt_tokens": 1432, "response_tokens_est": 21, "n_tiles": 1, "output_text": " A young girl wearing a pink jacket and blue jeans is holding a brown teddy bear and a blue umbrella.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 26778.7, "ram_available_mb": 98993.5, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 26778.7, "ram_available_mb": 98993.5, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.2, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.42, "peak": 40.18, "min": 16.16}, "VIN": {"avg": 73.83, "peak": 119.39, "min": 28.6}, "VIN_SYS_5V0": {"avg": 15.38, "peak": 16.76, "min": 12.46}, "VDD_CPU_SOC_MSS": {"avg": 16.56, "peak": 18.11, "min": 13.79}}, "power_watts_avg": 31.42, "energy_joules_est": 63.73, "sample_count": 20, "duration_seconds": 2.028}, "timestamp": "2026-01-19T14:28:25.364311"}
{"image_index": 94, "image_name": "000000009448.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009448.jpg", "image_width": 551, "image_height": 640, "image_resolution": "551x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3297.268, "latencies_ms": [3297.268], "images_per_second": 0.303, "prompt_tokens": 1446, "response_tokens_est": 69, "n_tiles": 1, "output_text": " 1. girl: 1\n2. blue umbrella: 1\n3. girl's hand: 1\n4. girl's leg: 1\n5. girl's foot: 1\n6. girl's hand: 1\n7. girl's leg: 1\n8. girl's foot: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26778.7, "ram_available_mb": 98993.5, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26778.7, "ram_available_mb": 98993.5, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.13, "peak": 40.57, "min": 18.91}, "VIN": {"avg": 70.34, "peak": 119.31, "min": 29.95}, "VIN_SYS_5V0": {"avg": 15.16, "peak": 16.66, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 16.25, "peak": 17.71, "min": 13.79}}, "power_watts_avg": 28.13, "energy_joules_est": 92.76, "sample_count": 32, "duration_seconds": 3.298}, "timestamp": "2026-01-19T14:28:28.696450"}
{"image_index": 94, "image_name": "000000009448.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009448.jpg", "image_width": 551, "image_height": 640, "image_resolution": "551x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2229.074, "latencies_ms": [2229.074], "images_per_second": 0.449, "prompt_tokens": 1450, "response_tokens_est": 29, "n_tiles": 1, "output_text": " The girl is standing in the foreground of the image, holding the umbrella in her right hand. The umbrella is positioned above her, providing shade.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26778.7, "ram_available_mb": 98993.5, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26778.7, "ram_available_mb": 98993.5, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.96, "peak": 40.18, "min": 16.56}, "VIN": {"avg": 74.51, "peak": 121.78, "min": 29.18}, "VIN_SYS_5V0": {"avg": 15.29, "peak": 16.66, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 16.45, "peak": 17.71, "min": 13.79}}, "power_watts_avg": 30.96, "energy_joules_est": 69.03, "sample_count": 22, "duration_seconds": 2.23}, "timestamp": "2026-01-19T14:28:30.990222"}
{"image_index": 94, "image_name": "000000009448.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009448.jpg", "image_width": 551, "image_height": 640, "image_resolution": "551x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2013.814, "latencies_ms": [2013.814], "images_per_second": 0.497, "prompt_tokens": 1444, "response_tokens_est": 21, "n_tiles": 1, "output_text": " A young girl wearing a pink jacket and blue jeans is standing on a gravel surface holding a blue umbrella.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26778.7, "ram_available_mb": 98993.5, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 26778.7, "ram_available_mb": 98993.5, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.95, "peak": 40.18, "min": 17.74}, "VIN": {"avg": 75.06, "peak": 124.88, "min": 29.2}, "VIN_SYS_5V0": {"avg": 15.2, "peak": 16.56, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 16.34, "peak": 17.71, "min": 13.39}}, "power_watts_avg": 31.95, "energy_joules_est": 64.36, "sample_count": 20, "duration_seconds": 2.014}, "timestamp": "2026-01-19T14:28:33.073366"}
{"image_index": 94, "image_name": "000000009448.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009448.jpg", "image_width": 551, "image_height": 640, "image_resolution": "551x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2181.1, "latencies_ms": [2181.1], "images_per_second": 0.458, "prompt_tokens": 1442, "response_tokens_est": 27, "n_tiles": 1, "output_text": " The girl is wearing a pink jacket and blue jeans, and the umbrella is blue. The lighting is natural and the weather is sunny.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26778.7, "ram_available_mb": 98993.5, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26778.7, "ram_available_mb": 98993.5, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.89, "peak": 40.57, "min": 18.92}, "VIN": {"avg": 74.33, "peak": 119.93, "min": 30.53}, "VIN_SYS_5V0": {"avg": 15.21, "peak": 16.56, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 16.38, "peak": 17.71, "min": 13.39}}, "power_watts_avg": 31.89, "energy_joules_est": 69.57, "sample_count": 21, "duration_seconds": 2.182}, "timestamp": "2026-01-19T14:28:35.268153"}
{"image_index": 95, "image_name": "000000009483.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009483.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1976.44, "latencies_ms": [1976.44], "images_per_second": 0.506, "prompt_tokens": 1099, "response_tokens_est": 36, "n_tiles": 1, "output_text": " A man in a suit stands in front of a window with a reflection of another man in it, with a desk in front of him that has a computer monitor and a keyboard.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26778.7, "ram_available_mb": 98993.5, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 26778.7, "ram_available_mb": 98993.5, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.29, "peak": 40.18, "min": 19.7}, "VIN": {"avg": 74.08, "peak": 118.74, "min": 42.86}, "VIN_SYS_5V0": {"avg": 15.26, "peak": 16.15, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 16.39, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 30.29, "energy_joules_est": 59.89, "sample_count": 19, "duration_seconds": 1.977}, "timestamp": "2026-01-19T14:28:37.255418"}
{"image_index": 95, "image_name": "000000009483.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009483.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2082.659, "latencies_ms": [2082.659], "images_per_second": 0.48, "prompt_tokens": 1113, "response_tokens_est": 40, "n_tiles": 1, "output_text": " man: 1, chair: 1, computer: 2, monitor: 1, keyboard: 1, mouse: 1, tv: 1, tv screen: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26778.7, "ram_available_mb": 98993.5, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 26778.7, "ram_available_mb": 98993.5, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.67, "peak": 40.18, "min": 19.31}, "VIN": {"avg": 72.13, "peak": 100.61, "min": 57.01}, "VIN_SYS_5V0": {"avg": 15.34, "peak": 16.26, "min": 13.39}, "VDD_CPU_SOC_MSS": {"avg": 16.42, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 29.67, "energy_joules_est": 61.81, "sample_count": 20, "duration_seconds": 2.083}, "timestamp": "2026-01-19T14:28:39.343458"}
{"image_index": 95, "image_name": "000000009483.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009483.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2600.614, "latencies_ms": [2600.614], "images_per_second": 0.385, "prompt_tokens": 1117, "response_tokens_est": 60, "n_tiles": 1, "output_text": " The man is standing to the left of the desk, which is in the foreground of the image. The desk is located in the middle of the image, with the computer monitors placed on it. The window is located behind the desk, and the reflection of the man can be seen in the window.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26778.7, "ram_available_mb": 98993.5, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 26778.7, "ram_available_mb": 98993.5, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.72, "peak": 40.16, "min": 18.91}, "VIN": {"avg": 68.97, "peak": 103.38, "min": 35.14}, "VIN_SYS_5V0": {"avg": 15.27, "peak": 16.26, "min": 13.49}, "VDD_CPU_SOC_MSS": {"avg": 16.36, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 27.72, "energy_joules_est": 72.1, "sample_count": 25, "duration_seconds": 2.601}, "timestamp": "2026-01-19T14:28:41.949665"}
{"image_index": 95, "image_name": "000000009483.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009483.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1542.92, "latencies_ms": [1542.92], "images_per_second": 0.648, "prompt_tokens": 1111, "response_tokens_est": 19, "n_tiles": 1, "output_text": " A man in a suit stands in front of a window with a computer setup on a desk.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26778.7, "ram_available_mb": 98993.5, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 26778.9, "ram_available_mb": 98993.3, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.67, "peak": 39.78, "min": 18.13}, "VIN": {"avg": 72.58, "peak": 100.13, "min": 29.03}, "VIN_SYS_5V0": {"avg": 15.34, "peak": 16.26, "min": 13.39}, "VDD_CPU_SOC_MSS": {"avg": 16.56, "peak": 17.32, "min": 14.97}}, "power_watts_avg": 31.67, "energy_joules_est": 48.87, "sample_count": 15, "duration_seconds": 1.543}, "timestamp": "2026-01-19T14:28:43.517043"}
{"image_index": 95, "image_name": "000000009483.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009483.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1605.375, "latencies_ms": [1605.375], "images_per_second": 0.623, "prompt_tokens": 1109, "response_tokens_est": 21, "n_tiles": 1, "output_text": " The room is well-lit with natural light coming from a window, and the walls are painted white.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26778.9, "ram_available_mb": 98993.3, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26778.9, "ram_available_mb": 98993.3, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.13, "peak": 40.95, "min": 20.49}, "VIN": {"avg": 82.11, "peak": 126.01, "min": 27.33}, "VIN_SYS_5V0": {"avg": 15.19, "peak": 16.15, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 16.34, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 32.13, "energy_joules_est": 51.6, "sample_count": 16, "duration_seconds": 1.606}, "timestamp": "2026-01-19T14:28:45.179372"}
{"image_index": 96, "image_name": "000000009590.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009590.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1772.418, "latencies_ms": [1772.418], "images_per_second": 0.564, "prompt_tokens": 1099, "response_tokens_est": 27, "n_tiles": 1, "output_text": " A group of people are sitting around a table in a room with wooden walls and a clock on the wall, enjoying a meal together.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 26778.9, "ram_available_mb": 98993.3, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26778.9, "ram_available_mb": 98993.3, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.01, "peak": 40.56, "min": 18.92}, "VIN": {"avg": 76.3, "peak": 116.67, "min": 32.93}, "VIN_SYS_5V0": {"avg": 15.0, "peak": 16.05, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 16.14, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 31.01, "energy_joules_est": 54.98, "sample_count": 17, "duration_seconds": 1.773}, "timestamp": "2026-01-19T14:28:46.963294"}
{"image_index": 96, "image_name": "000000009590.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009590.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2144.574, "latencies_ms": [2144.574], "images_per_second": 0.466, "prompt_tokens": 1113, "response_tokens_est": 42, "n_tiles": 1, "output_text": " table: 1\nclock: 1\nwindow: 1\ncurtain: 1\ncup: 1\nbottle: 1\nbowl: 1\nplate: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26778.9, "ram_available_mb": 98993.3, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26778.9, "ram_available_mb": 98993.3, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.34, "peak": 40.95, "min": 19.32}, "VIN": {"avg": 70.4, "peak": 126.81, "min": 28.78}, "VIN_SYS_5V0": {"avg": 15.18, "peak": 16.26, "min": 13.29}, "VDD_CPU_SOC_MSS": {"avg": 16.35, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 29.34, "energy_joules_est": 62.93, "sample_count": 21, "duration_seconds": 2.145}, "timestamp": "2026-01-19T14:28:49.151951"}
{"image_index": 96, "image_name": "000000009590.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009590.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2145.587, "latencies_ms": [2145.587], "images_per_second": 0.466, "prompt_tokens": 1117, "response_tokens_est": 41, "n_tiles": 1, "output_text": " The main objects are arranged in a way that the table is in the foreground, with the people sitting around it. The window is in the background, and the clock is on the wall above the table.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26778.9, "ram_available_mb": 98993.3, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26778.9, "ram_available_mb": 98993.3, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.68, "peak": 40.56, "min": 16.95}, "VIN": {"avg": 73.33, "peak": 127.81, "min": 28.67}, "VIN_SYS_5V0": {"avg": 14.96, "peak": 16.15, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 16.05, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 28.68, "energy_joules_est": 61.55, "sample_count": 21, "duration_seconds": 2.146}, "timestamp": "2026-01-19T14:28:51.343857"}
{"image_index": 96, "image_name": "000000009590.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009590.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2010.265, "latencies_ms": [2010.265], "images_per_second": 0.497, "prompt_tokens": 1111, "response_tokens_est": 36, "n_tiles": 1, "output_text": " A group of friends are gathered around a wooden table in a cozy room with a large window. They are enjoying a meal together, with plates of food and drinks on the table.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26778.9, "ram_available_mb": 98993.3, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26778.9, "ram_available_mb": 98993.3, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.07, "peak": 40.57, "min": 16.95}, "VIN": {"avg": 75.03, "peak": 127.57, "min": 29.27}, "VIN_SYS_5V0": {"avg": 14.92, "peak": 16.05, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 16.12, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 29.07, "energy_joules_est": 58.45, "sample_count": 20, "duration_seconds": 2.011}, "timestamp": "2026-01-19T14:28:53.432713"}
{"image_index": 96, "image_name": "000000009590.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009590.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1767.657, "latencies_ms": [1767.657], "images_per_second": 0.566, "prompt_tokens": 1109, "response_tokens_est": 26, "n_tiles": 1, "output_text": " The room is dimly lit with natural light coming through the window, and the wooden table is polished and has a dark finish.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26778.9, "ram_available_mb": 98993.3, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26778.5, "ram_available_mb": 98993.7, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.2, "peak": 40.18, "min": 16.56}, "VIN": {"avg": 74.23, "peak": 118.94, "min": 33.82}, "VIN_SYS_5V0": {"avg": 14.88, "peak": 16.05, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 16.05, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 30.2, "energy_joules_est": 53.39, "sample_count": 17, "duration_seconds": 1.768}, "timestamp": "2026-01-19T14:28:55.203200"}
{"image_index": 97, "image_name": "000000009769.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009769.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1662.524, "latencies_ms": [1662.524], "images_per_second": 0.601, "prompt_tokens": 1099, "response_tokens_est": 24, "n_tiles": 1, "output_text": " A red truck with a snow plow attached to the front is driving down a snowy street, with a person standing nearby.", "error": null, "sys_before": {"cpu_percent": 15.4, "ram_used_mb": 26778.5, "ram_available_mb": 98993.7, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 26778.4, "ram_available_mb": 98993.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.33, "peak": 40.18, "min": 20.49}, "VIN": {"avg": 72.84, "peak": 105.52, "min": 27.47}, "VIN_SYS_5V0": {"avg": 15.23, "peak": 16.26, "min": 13.29}, "VDD_CPU_SOC_MSS": {"avg": 16.35, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 31.33, "energy_joules_est": 52.1, "sample_count": 17, "duration_seconds": 1.663}, "timestamp": "2026-01-19T14:28:56.973440"}
{"image_index": 97, "image_name": "000000009769.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009769.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2604.0, "latencies_ms": [2604.0], "images_per_second": 0.384, "prompt_tokens": 1113, "response_tokens_est": 59, "n_tiles": 1, "output_text": " 1. red truck: 1\n2. snowplow: 1\n3. snow: 1\n4. house: 1\n5. person: 1\n6. tree: 1\n7. road: 1\n8. sky: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26778.4, "ram_available_mb": 98993.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26778.4, "ram_available_mb": 98993.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.12, "peak": 40.18, "min": 17.34}, "VIN": {"avg": 68.32, "peak": 124.16, "min": 30.96}, "VIN_SYS_5V0": {"avg": 14.83, "peak": 16.05, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.9, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 27.12, "energy_joules_est": 70.63, "sample_count": 26, "duration_seconds": 2.604}, "timestamp": "2026-01-19T14:28:59.683643"}
{"image_index": 97, "image_name": "000000009769.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009769.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2601.745, "latencies_ms": [2601.745], "images_per_second": 0.384, "prompt_tokens": 1117, "response_tokens_est": 56, "n_tiles": 1, "output_text": " The red truck is in the foreground, driving down the street. The snowplow is attached to the front of the truck, and it is positioned in the middle of the image. The houses are in the background, and they are located on the right side of the image.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26778.4, "ram_available_mb": 98993.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 26778.4, "ram_available_mb": 98993.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.78, "peak": 39.78, "min": 14.98}, "VIN": {"avg": 69.27, "peak": 119.59, "min": 32.24}, "VIN_SYS_5V0": {"avg": 14.7, "peak": 15.85, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 15.69, "peak": 16.92, "min": 12.61}}, "power_watts_avg": 26.78, "energy_joules_est": 69.69, "sample_count": 25, "duration_seconds": 2.602}, "timestamp": "2026-01-19T14:29:02.292772"}
{"image_index": 97, "image_name": "000000009769.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009769.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1543.089, "latencies_ms": [1543.089], "images_per_second": 0.648, "prompt_tokens": 1111, "response_tokens_est": 19, "n_tiles": 1, "output_text": " A red truck is driving down a snowy street with a snowplow attached to the front.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26778.4, "ram_available_mb": 98993.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 26778.4, "ram_available_mb": 98993.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.6, "peak": 39.78, "min": 17.35}, "VIN": {"avg": 70.46, "peak": 101.61, "min": 30.3}, "VIN_SYS_5V0": {"avg": 15.27, "peak": 16.15, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 16.4, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 31.6, "energy_joules_est": 48.78, "sample_count": 15, "duration_seconds": 1.544}, "timestamp": "2026-01-19T14:29:03.860186"}
{"image_index": 97, "image_name": "000000009769.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009769.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2119.63, "latencies_ms": [2119.63], "images_per_second": 0.472, "prompt_tokens": 1109, "response_tokens_est": 41, "n_tiles": 1, "output_text": " The image features a red truck with a snowplow attached to the front, driving down a snowy street. The sky is overcast, and the snow is piled up on the sides of the road.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26778.4, "ram_available_mb": 98993.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26778.4, "ram_available_mb": 98993.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.59, "peak": 40.97, "min": 19.31}, "VIN": {"avg": 78.43, "peak": 124.31, "min": 30.23}, "VIN_SYS_5V0": {"avg": 15.16, "peak": 16.15, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 16.2, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 29.59, "energy_joules_est": 62.73, "sample_count": 21, "duration_seconds": 2.12}, "timestamp": "2026-01-19T14:29:06.038924"}
{"image_index": 98, "image_name": "000000009772.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009772.jpg", "image_width": 550, "image_height": 640, "image_resolution": "550x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1816.099, "latencies_ms": [1816.099], "images_per_second": 0.551, "prompt_tokens": 1432, "response_tokens_est": 13, "n_tiles": 1, "output_text": " A man is taking a picture of himself in a bathroom mirror.", "error": null, "sys_before": {"cpu_percent": 16.7, "ram_used_mb": 26778.4, "ram_available_mb": 98993.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.3, "ram_used_mb": 26778.4, "ram_available_mb": 98993.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.2, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.19, "peak": 40.18, "min": 16.56}, "VIN": {"avg": 75.9, "peak": 118.67, "min": 29.82}, "VIN_SYS_5V0": {"avg": 15.33, "peak": 16.76, "min": 12.25}, "VDD_CPU_SOC_MSS": {"avg": 16.49, "peak": 18.11, "min": 13.39}}, "power_watts_avg": 32.19, "energy_joules_est": 58.49, "sample_count": 18, "duration_seconds": 1.817}, "timestamp": "2026-01-19T14:29:07.923113"}
{"image_index": 98, "image_name": "000000009772.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009772.jpg", "image_width": 550, "image_height": 640, "image_resolution": "550x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2979.458, "latencies_ms": [2979.458], "images_per_second": 0.336, "prompt_tokens": 1446, "response_tokens_est": 57, "n_tiles": 1, "output_text": " 1. mirror: 2\n2. television: 1\n3. towel: 3\n4. bath tub: 1\n5. sink: 2\n6. counter: 1\n7. wall: 1\n8. floor: 1", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26778.4, "ram_available_mb": 98993.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26778.4, "ram_available_mb": 98993.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.14, "peak": 40.57, "min": 19.31}, "VIN": {"avg": 71.52, "peak": 119.14, "min": 29.73}, "VIN_SYS_5V0": {"avg": 15.19, "peak": 16.66, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 16.28, "peak": 17.71, "min": 13.79}}, "power_watts_avg": 29.14, "energy_joules_est": 86.83, "sample_count": 29, "duration_seconds": 2.98}, "timestamp": "2026-01-19T14:29:10.946176"}
{"image_index": 98, "image_name": "000000009772.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009772.jpg", "image_width": 550, "image_height": 640, "image_resolution": "550x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3135.441, "latencies_ms": [3135.441], "images_per_second": 0.319, "prompt_tokens": 1450, "response_tokens_est": 63, "n_tiles": 1, "output_text": " The mirror is positioned in the center of the bathroom, reflecting the person taking the photo. The sink is located to the left of the mirror, while the television is situated to the right. The towels are placed on the counter in front of the sink, and the bath mat is placed in front of the sink.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26778.4, "ram_available_mb": 98993.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26778.4, "ram_available_mb": 98993.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.96, "peak": 40.18, "min": 16.56}, "VIN": {"avg": 72.91, "peak": 121.29, "min": 29.32}, "VIN_SYS_5V0": {"avg": 15.15, "peak": 16.66, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 16.24, "peak": 17.71, "min": 13.79}}, "power_watts_avg": 27.96, "energy_joules_est": 87.68, "sample_count": 31, "duration_seconds": 3.136}, "timestamp": "2026-01-19T14:29:14.176355"}
{"image_index": 98, "image_name": "000000009772.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009772.jpg", "image_width": 550, "image_height": 640, "image_resolution": "550x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1808.706, "latencies_ms": [1808.706], "images_per_second": 0.553, "prompt_tokens": 1444, "response_tokens_est": 13, "n_tiles": 1, "output_text": " A man is taking a picture of himself in a bathroom mirror.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26778.4, "ram_available_mb": 98993.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.4, "ram_used_mb": 26778.4, "ram_available_mb": 98993.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.13, "peak": 40.18, "min": 15.38}, "VIN": {"avg": 78.1, "peak": 120.22, "min": 28.87}, "VIN_SYS_5V0": {"avg": 15.05, "peak": 16.46, "min": 11.66}, "VDD_CPU_SOC_MSS": {"avg": 16.27, "peak": 17.71, "min": 13.0}}, "power_watts_avg": 32.13, "energy_joules_est": 58.13, "sample_count": 18, "duration_seconds": 1.809}, "timestamp": "2026-01-19T14:29:16.053947"}
{"image_index": 98, "image_name": "000000009772.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009772.jpg", "image_width": 550, "image_height": 640, "image_resolution": "550x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2736.753, "latencies_ms": [2736.753], "images_per_second": 0.365, "prompt_tokens": 1442, "response_tokens_est": 48, "n_tiles": 1, "output_text": " The bathroom is well-lit with warm lighting, and the walls are adorned with a mix of beige and white tiles. The countertop is made of dark green marble, and the floor is covered with a white and red patterned rug.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26778.4, "ram_available_mb": 98993.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26778.4, "ram_available_mb": 98993.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.9, "peak": 40.57, "min": 18.92}, "VIN": {"avg": 73.97, "peak": 115.73, "min": 28.82}, "VIN_SYS_5V0": {"avg": 15.14, "peak": 16.56, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 16.24, "peak": 17.71, "min": 13.79}}, "power_watts_avg": 29.9, "energy_joules_est": 81.84, "sample_count": 27, "duration_seconds": 2.737}, "timestamp": "2026-01-19T14:29:18.864781"}
{"image_index": 99, "image_name": "000000009891.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009891.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1469.758, "latencies_ms": [1469.758], "images_per_second": 0.68, "prompt_tokens": 1099, "response_tokens_est": 15, "n_tiles": 1, "output_text": " Two men are unloading luggage from a cart in an airport parking garage.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 26778.4, "ram_available_mb": 98993.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 26778.4, "ram_available_mb": 98993.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.99, "peak": 40.18, "min": 16.16}, "VIN": {"avg": 79.65, "peak": 123.01, "min": 30.45}, "VIN_SYS_5V0": {"avg": 14.74, "peak": 16.05, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 15.83, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 30.99, "energy_joules_est": 45.57, "sample_count": 15, "duration_seconds": 1.471}, "timestamp": "2026-01-19T14:29:20.438878"}
{"image_index": 99, "image_name": "000000009891.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009891.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2598.071, "latencies_ms": [2598.071], "images_per_second": 0.385, "prompt_tokens": 1113, "response_tokens_est": 56, "n_tiles": 1, "output_text": " 1. luggage: 4\n2. luggage: 1\n3. luggage: 1\n4. luggage: 1\n5. luggage: 1\n6. luggage: 1\n7. luggage: 1\n8. luggage: 1", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26778.4, "ram_available_mb": 98993.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 26778.4, "ram_available_mb": 98993.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.66, "peak": 40.57, "min": 19.31}, "VIN": {"avg": 72.44, "peak": 131.05, "min": 31.97}, "VIN_SYS_5V0": {"avg": 14.83, "peak": 16.05, "min": 11.87}, "VDD_CPU_SOC_MSS": {"avg": 15.72, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 27.66, "energy_joules_est": 71.88, "sample_count": 25, "duration_seconds": 2.599}, "timestamp": "2026-01-19T14:29:23.043992"}
{"image_index": 99, "image_name": "000000009891.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009891.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2139.353, "latencies_ms": [2139.353], "images_per_second": 0.467, "prompt_tokens": 1117, "response_tokens_est": 42, "n_tiles": 1, "output_text": " The man with the luggage is positioned in the foreground, with the car and other luggage items in the background. The luggage cart is located to the left of the man, and the car is to the right.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26778.4, "ram_available_mb": 98993.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26778.4, "ram_available_mb": 98993.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.91, "peak": 40.18, "min": 17.34}, "VIN": {"avg": 66.91, "peak": 102.24, "min": 28.97}, "VIN_SYS_5V0": {"avg": 15.18, "peak": 16.15, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 16.11, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 28.91, "energy_joules_est": 61.86, "sample_count": 21, "duration_seconds": 2.14}, "timestamp": "2026-01-19T14:29:25.228154"}
{"image_index": 99, "image_name": "000000009891.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009891.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1434.189, "latencies_ms": [1434.189], "images_per_second": 0.697, "prompt_tokens": 1111, "response_tokens_est": 14, "n_tiles": 1, "output_text": " Two men are unloading luggage from a car in a parking garage.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26778.4, "ram_available_mb": 98993.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 26778.4, "ram_available_mb": 98993.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.91, "peak": 40.56, "min": 16.95}, "VIN": {"avg": 74.67, "peak": 112.92, "min": 29.09}, "VIN_SYS_5V0": {"avg": 15.08, "peak": 16.15, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 16.14, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 31.91, "energy_joules_est": 45.78, "sample_count": 14, "duration_seconds": 1.435}, "timestamp": "2026-01-19T14:29:26.688893"}
{"image_index": 99, "image_name": "000000009891.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009891.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1904.953, "latencies_ms": [1904.953], "images_per_second": 0.525, "prompt_tokens": 1109, "response_tokens_est": 33, "n_tiles": 1, "output_text": " The image depicts a man in a parking garage with a white car and a black suitcase. The lighting is dim, and the man is wearing a brown shirt.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26778.4, "ram_available_mb": 98993.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26778.4, "ram_available_mb": 98993.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.95, "peak": 40.97, "min": 19.7}, "VIN": {"avg": 77.78, "peak": 119.77, "min": 29.31}, "VIN_SYS_5V0": {"avg": 15.16, "peak": 16.15, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 16.22, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 30.95, "energy_joules_est": 58.97, "sample_count": 19, "duration_seconds": 1.905}, "timestamp": "2026-01-19T14:29:28.672161"}
{"image_index": 100, "image_name": "000000009914.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009914.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1544.464, "latencies_ms": [1544.464], "images_per_second": 0.647, "prompt_tokens": 1099, "response_tokens_est": 18, "n_tiles": 1, "output_text": " A plate with a sandwich, fries, and a side of ketchup and onions.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 26778.4, "ram_available_mb": 98993.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 26778.4, "ram_available_mb": 98993.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.46, "peak": 40.56, "min": 16.95}, "VIN": {"avg": 80.06, "peak": 128.24, "min": 29.75}, "VIN_SYS_5V0": {"avg": 14.9, "peak": 16.05, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 15.93, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 31.46, "energy_joules_est": 48.6, "sample_count": 15, "duration_seconds": 1.545}, "timestamp": "2026-01-19T14:29:30.241255"}
{"image_index": 100, "image_name": "000000009914.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009914.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1865.389, "latencies_ms": [1865.389], "images_per_second": 0.536, "prompt_tokens": 1113, "response_tokens_est": 30, "n_tiles": 1, "output_text": " chicken sandwich: 2\nfries: 1\nlettuce: 1\nonion: 1\nketchup: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26778.4, "ram_available_mb": 98993.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26778.4, "ram_available_mb": 98993.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.21, "peak": 40.95, "min": 21.67}, "VIN": {"avg": 71.98, "peak": 121.22, "min": 35.16}, "VIN_SYS_5V0": {"avg": 15.24, "peak": 16.15, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 16.25, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 31.21, "energy_joules_est": 58.23, "sample_count": 18, "duration_seconds": 1.866}, "timestamp": "2026-01-19T14:29:32.116698"}
{"image_index": 100, "image_name": "000000009914.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009914.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2273.477, "latencies_ms": [2273.477], "images_per_second": 0.44, "prompt_tokens": 1117, "response_tokens_est": 47, "n_tiles": 1, "output_text": " The sandwich is located in the foreground of the plate, with the fries and ketchup in the background. The lettuce and onion are placed on the left side of the plate, while the ketchup is on the right side.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26778.4, "ram_available_mb": 98993.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26778.4, "ram_available_mb": 98993.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.92, "peak": 40.18, "min": 19.31}, "VIN": {"avg": 69.84, "peak": 103.67, "min": 29.4}, "VIN_SYS_5V0": {"avg": 15.21, "peak": 16.15, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 16.28, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 28.92, "energy_joules_est": 65.76, "sample_count": 22, "duration_seconds": 2.274}, "timestamp": "2026-01-19T14:29:34.405788"}
{"image_index": 100, "image_name": "000000009914.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009914.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1549.34, "latencies_ms": [1549.34], "images_per_second": 0.645, "prompt_tokens": 1111, "response_tokens_est": 19, "n_tiles": 1, "output_text": " A plate of food with a sandwich, fries, and ketchup is on a table.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26778.4, "ram_available_mb": 98993.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 26778.4, "ram_available_mb": 98993.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.9, "peak": 40.56, "min": 17.73}, "VIN": {"avg": 79.04, "peak": 124.86, "min": 30.04}, "VIN_SYS_5V0": {"avg": 15.32, "peak": 16.15, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 16.4, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 31.9, "energy_joules_est": 49.43, "sample_count": 15, "duration_seconds": 1.55}, "timestamp": "2026-01-19T14:29:35.965527"}
{"image_index": 100, "image_name": "000000009914.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009914.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1619.676, "latencies_ms": [1619.676], "images_per_second": 0.617, "prompt_tokens": 1109, "response_tokens_est": 22, "n_tiles": 1, "output_text": " The plate is white and the food is colorful. The lighting is bright and the food is well-lit.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26778.4, "ram_available_mb": 98993.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26770.2, "ram_available_mb": 99002.0, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.4, "peak": 40.56, "min": 22.07}, "VIN": {"avg": 69.55, "peak": 90.2, "min": 29.9}, "VIN_SYS_5V0": {"avg": 15.32, "peak": 16.26, "min": 13.39}, "VDD_CPU_SOC_MSS": {"avg": 16.44, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 32.4, "energy_joules_est": 52.49, "sample_count": 16, "duration_seconds": 1.62}, "timestamp": "2026-01-19T14:29:37.635501"}
{"image_index": 101, "image_name": "000000010092.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010092.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1848.922, "latencies_ms": [1848.922], "images_per_second": 0.541, "prompt_tokens": 1099, "response_tokens_est": 30, "n_tiles": 1, "output_text": " The image depicts a cozy bedroom with a large bed covered by a green mosquito net, a wooden table with a candle, and a window with curtains.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 26770.2, "ram_available_mb": 99002.0, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26770.5, "ram_available_mb": 99001.7, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.59, "peak": 40.57, "min": 18.92}, "VIN": {"avg": 74.52, "peak": 122.72, "min": 29.35}, "VIN_SYS_5V0": {"avg": 15.04, "peak": 16.05, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 16.17, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 30.59, "energy_joules_est": 56.58, "sample_count": 18, "duration_seconds": 1.85}, "timestamp": "2026-01-19T14:29:39.514837"}
{"image_index": 101, "image_name": "000000010092.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010092.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2590.443, "latencies_ms": [2590.443], "images_per_second": 0.386, "prompt_tokens": 1113, "response_tokens_est": 56, "n_tiles": 1, "output_text": " 1. bed: 1\n2. curtains: 2\n3. table: 1\n4. chair: 1\n5. floor: 1\n6. wall: 1\n7. window: 2\n8. candle: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26770.5, "ram_available_mb": 99001.7, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26762.3, "ram_available_mb": 99009.9, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.5, "peak": 40.56, "min": 18.13}, "VIN": {"avg": 66.52, "peak": 105.23, "min": 30.16}, "VIN_SYS_5V0": {"avg": 15.04, "peak": 16.15, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 16.02, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 27.5, "energy_joules_est": 71.26, "sample_count": 25, "duration_seconds": 2.591}, "timestamp": "2026-01-19T14:29:42.119974"}
{"image_index": 101, "image_name": "000000010092.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010092.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2905.288, "latencies_ms": [2905.288], "images_per_second": 0.344, "prompt_tokens": 1117, "response_tokens_est": 71, "n_tiles": 1, "output_text": " The bed is positioned in the center of the room, with the mosquito net draped over it, creating a sense of intimacy and seclusion. The table and chairs are situated in the foreground, providing a comfortable seating area for guests. The windows and paintings are located in the background, allowing natural light to flood the room and creating a serene atmosphere.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26762.3, "ram_available_mb": 99009.9, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 26761.3, "ram_available_mb": 99010.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.24, "peak": 40.57, "min": 16.56}, "VIN": {"avg": 71.54, "peak": 120.04, "min": 29.21}, "VIN_SYS_5V0": {"avg": 15.0, "peak": 16.15, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 15.98, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 26.24, "energy_joules_est": 76.25, "sample_count": 29, "duration_seconds": 2.906}, "timestamp": "2026-01-19T14:29:45.130145"}
{"image_index": 101, "image_name": "000000010092.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010092.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2871.262, "latencies_ms": [2871.262], "images_per_second": 0.348, "prompt_tokens": 1111, "response_tokens_est": 69, "n_tiles": 1, "output_text": " The image captures a serene bedroom bathed in soft light, with a large bed draped in a green mosquito net, creating a sense of privacy and tranquility. The room is adorned with yellow walls and a thatched roof, adding to the tropical ambiance. A wooden table and chairs are positioned in the room, inviting relaxation and conversation.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26761.3, "ram_available_mb": 99010.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 26776.6, "ram_available_mb": 98995.6, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.12, "peak": 39.77, "min": 14.98}, "VIN": {"avg": 67.94, "peak": 122.23, "min": 29.02}, "VIN_SYS_5V0": {"avg": 14.84, "peak": 15.95, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 15.89, "peak": 17.32, "min": 12.61}}, "power_watts_avg": 26.12, "energy_joules_est": 75.01, "sample_count": 28, "duration_seconds": 2.872}, "timestamp": "2026-01-19T14:29:48.046630"}
{"image_index": 101, "image_name": "000000010092.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010092.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1858.5, "latencies_ms": [1858.5], "images_per_second": 0.538, "prompt_tokens": 1109, "response_tokens_est": 30, "n_tiles": 1, "output_text": " The room is bathed in warm yellow light, with a canopy of green fabric draped over the bed, and the floor is tiled.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26776.6, "ram_available_mb": 98995.6, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 26831.0, "ram_available_mb": 98941.2, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.92, "peak": 40.18, "min": 16.16}, "VIN": {"avg": 73.07, "peak": 110.77, "min": 30.09}, "VIN_SYS_5V0": {"avg": 15.07, "peak": 16.05, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 16.36, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 29.92, "energy_joules_est": 55.62, "sample_count": 18, "duration_seconds": 1.859}, "timestamp": "2026-01-19T14:29:49.923841"}
{"image_index": 102, "image_name": "000000010363.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010363.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1628.788, "latencies_ms": [1628.788], "images_per_second": 0.614, "prompt_tokens": 1099, "response_tokens_est": 22, "n_tiles": 1, "output_text": " A gray and white cat with a blue collar is standing on the hood of a black car in a garage.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 26831.0, "ram_available_mb": 98941.2, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26828.2, "ram_available_mb": 98943.9, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.46, "peak": 40.57, "min": 18.52}, "VIN": {"avg": 73.15, "peak": 121.67, "min": 29.16}, "VIN_SYS_5V0": {"avg": 15.2, "peak": 16.15, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 16.29, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 31.46, "energy_joules_est": 51.26, "sample_count": 16, "duration_seconds": 1.629}, "timestamp": "2026-01-19T14:29:51.593714"}
{"image_index": 102, "image_name": "000000010363.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010363.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2547.117, "latencies_ms": [2547.117], "images_per_second": 0.393, "prompt_tokens": 1113, "response_tokens_est": 57, "n_tiles": 1, "output_text": " 1. cat: 1\n2. car: 1\n3. lamp: 1\n4. box: 1\n5. bicycle: 1\n6. gas cylinder: 1\n7. floor: 1\n8. wall: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26828.2, "ram_available_mb": 98943.9, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26828.2, "ram_available_mb": 98943.9, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.81, "peak": 40.56, "min": 18.52}, "VIN": {"avg": 70.68, "peak": 104.07, "min": 28.4}, "VIN_SYS_5V0": {"avg": 15.02, "peak": 16.15, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 16.19, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 27.81, "energy_joules_est": 70.85, "sample_count": 25, "duration_seconds": 2.548}, "timestamp": "2026-01-19T14:29:54.192375"}
{"image_index": 102, "image_name": "000000010363.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010363.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2244.154, "latencies_ms": [2244.154], "images_per_second": 0.446, "prompt_tokens": 1117, "response_tokens_est": 45, "n_tiles": 1, "output_text": " The cat is positioned in the foreground, near the center of the image, on top of the car. The lamp is located to the left of the cat, while the box is situated in the background, behind the cat.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26828.2, "ram_available_mb": 98943.9, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26828.2, "ram_available_mb": 98944.0, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.15, "peak": 40.16, "min": 16.16}, "VIN": {"avg": 71.84, "peak": 124.38, "min": 28.96}, "VIN_SYS_5V0": {"avg": 14.97, "peak": 16.05, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.94, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 28.15, "energy_joules_est": 63.19, "sample_count": 22, "duration_seconds": 2.245}, "timestamp": "2026-01-19T14:29:56.484378"}
{"image_index": 102, "image_name": "000000010363.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010363.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1523.902, "latencies_ms": [1523.902], "images_per_second": 0.656, "prompt_tokens": 1111, "response_tokens_est": 17, "n_tiles": 1, "output_text": " A cat with a collar is standing on the hood of a car in a garage.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26828.2, "ram_available_mb": 98944.0, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26826.3, "ram_available_mb": 98945.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.33, "peak": 40.57, "min": 16.56}, "VIN": {"avg": 73.87, "peak": 122.33, "min": 27.25}, "VIN_SYS_5V0": {"avg": 14.94, "peak": 16.05, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 16.09, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 31.33, "energy_joules_est": 47.76, "sample_count": 15, "duration_seconds": 1.524}, "timestamp": "2026-01-19T14:29:58.047304"}
{"image_index": 102, "image_name": "000000010363.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010363.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1905.813, "latencies_ms": [1905.813], "images_per_second": 0.525, "prompt_tokens": 1109, "response_tokens_est": 32, "n_tiles": 1, "output_text": " The cat is gray and white, and the car is black. The room is well-lit, and the cat is standing on the hood of the car.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26826.3, "ram_available_mb": 98945.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26826.3, "ram_available_mb": 98945.9, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.37, "peak": 40.95, "min": 20.1}, "VIN": {"avg": 70.21, "peak": 112.17, "min": 29.84}, "VIN_SYS_5V0": {"avg": 15.01, "peak": 16.15, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 16.1, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 30.37, "energy_joules_est": 57.89, "sample_count": 19, "duration_seconds": 1.906}, "timestamp": "2026-01-19T14:30:00.028585"}
{"image_index": 103, "image_name": "000000010583.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010583.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1839.793, "latencies_ms": [1839.793], "images_per_second": 0.544, "prompt_tokens": 1432, "response_tokens_est": 14, "n_tiles": 1, "output_text": " A plate of food with a knife on it is on a table.", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 26826.3, "ram_available_mb": 98945.9, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.5, "ram_used_mb": 26822.1, "ram_available_mb": 98950.1, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.2, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.39, "peak": 40.16, "min": 16.95}, "VIN": {"avg": 76.9, "peak": 127.7, "min": 29.45}, "VIN_SYS_5V0": {"avg": 15.31, "peak": 16.76, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 16.43, "peak": 18.11, "min": 13.0}}, "power_watts_avg": 32.39, "energy_joules_est": 59.61, "sample_count": 18, "duration_seconds": 1.84}, "timestamp": "2026-01-19T14:30:01.910548"}
{"image_index": 103, "image_name": "000000010583.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010583.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2977.077, "latencies_ms": [2977.077], "images_per_second": 0.336, "prompt_tokens": 1446, "response_tokens_est": 57, "n_tiles": 1, "output_text": " 1. plate: 1\n2. knife: 1\n3. food: 1\n4. table: 1\n5. fork: 1\n6. bread: 1\n7. sauce: 1\n8. tablecloth: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26822.1, "ram_available_mb": 98950.1, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26823.1, "ram_available_mb": 98949.1, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.33, "peak": 40.56, "min": 19.31}, "VIN": {"avg": 69.2, "peak": 131.38, "min": 29.29}, "VIN_SYS_5V0": {"avg": 15.31, "peak": 16.76, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 16.32, "peak": 17.71, "min": 14.18}}, "power_watts_avg": 29.33, "energy_joules_est": 87.33, "sample_count": 29, "duration_seconds": 2.978}, "timestamp": "2026-01-19T14:30:04.929939"}
{"image_index": 103, "image_name": "000000010583.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010583.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2620.765, "latencies_ms": [2620.765], "images_per_second": 0.382, "prompt_tokens": 1450, "response_tokens_est": 44, "n_tiles": 1, "output_text": " The main object, the plate, is in the foreground, and the background is a table with a chair. The plate is to the left of the table, and the knife is on the right side of the plate.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26823.1, "ram_available_mb": 98949.1, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26823.3, "ram_available_mb": 98948.9, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.49, "peak": 40.18, "min": 16.56}, "VIN": {"avg": 75.21, "peak": 134.65, "min": 28.72}, "VIN_SYS_5V0": {"avg": 15.25, "peak": 16.66, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 16.23, "peak": 17.71, "min": 13.79}}, "power_watts_avg": 29.49, "energy_joules_est": 77.3, "sample_count": 26, "duration_seconds": 2.621}, "timestamp": "2026-01-19T14:30:07.640252"}
{"image_index": 103, "image_name": "000000010583.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010583.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1768.356, "latencies_ms": [1768.356], "images_per_second": 0.565, "prompt_tokens": 1444, "response_tokens_est": 12, "n_tiles": 1, "output_text": " A delicious meal is served on a plate with a knife.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 26823.3, "ram_available_mb": 98948.9, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.2, "ram_used_mb": 26823.3, "ram_available_mb": 98948.9, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.35, "peak": 40.57, "min": 16.16}, "VIN": {"avg": 76.33, "peak": 125.75, "min": 30.82}, "VIN_SYS_5V0": {"avg": 15.11, "peak": 16.56, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 16.3, "peak": 17.71, "min": 13.0}}, "power_watts_avg": 32.35, "energy_joules_est": 57.22, "sample_count": 18, "duration_seconds": 1.769}, "timestamp": "2026-01-19T14:30:09.513592"}
{"image_index": 103, "image_name": "000000010583.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010583.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2163.262, "latencies_ms": [2163.262], "images_per_second": 0.462, "prompt_tokens": 1442, "response_tokens_est": 26, "n_tiles": 1, "output_text": " The image has a warm and inviting atmosphere with natural lighting and a metal table. The colors are vibrant and the textures are smooth.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26823.3, "ram_available_mb": 98948.9, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 26823.3, "ram_available_mb": 98948.9, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.17, "peak": 40.57, "min": 20.11}, "VIN": {"avg": 74.29, "peak": 122.77, "min": 29.85}, "VIN_SYS_5V0": {"avg": 15.19, "peak": 16.56, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 16.24, "peak": 17.71, "min": 13.0}}, "power_watts_avg": 32.17, "energy_joules_est": 69.61, "sample_count": 21, "duration_seconds": 2.164}, "timestamp": "2026-01-19T14:30:11.704525"}
{"image_index": 104, "image_name": "000000010707.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010707.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1846.761, "latencies_ms": [1846.761], "images_per_second": 0.541, "prompt_tokens": 1099, "response_tokens_est": 31, "n_tiles": 1, "output_text": " In the image, there are three men in a living room, with one of them holding a camera, and a table with a few items on it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26823.3, "ram_available_mb": 98948.9, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26823.3, "ram_available_mb": 98948.9, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.71, "peak": 40.18, "min": 19.31}, "VIN": {"avg": 73.93, "peak": 131.02, "min": 29.74}, "VIN_SYS_5V0": {"avg": 15.16, "peak": 16.15, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 16.25, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 30.71, "energy_joules_est": 56.72, "sample_count": 18, "duration_seconds": 1.847}, "timestamp": "2026-01-19T14:30:13.588663"}
{"image_index": 104, "image_name": "000000010707.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010707.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2014.751, "latencies_ms": [2014.751], "images_per_second": 0.496, "prompt_tokens": 1113, "response_tokens_est": 37, "n_tiles": 1, "output_text": " table: 1\ncouch: 1\nwindow: 1\nlamp: 1\ntable: 1\nsofa: 1\nperson: 3", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26823.3, "ram_available_mb": 98948.9, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26823.3, "ram_available_mb": 98948.9, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.61, "peak": 40.57, "min": 18.52}, "VIN": {"avg": 75.93, "peak": 122.92, "min": 29.79}, "VIN_SYS_5V0": {"avg": 15.12, "peak": 16.15, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 16.2, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 29.61, "energy_joules_est": 59.67, "sample_count": 20, "duration_seconds": 2.015}, "timestamp": "2026-01-19T14:30:15.679001"}
{"image_index": 104, "image_name": "000000010707.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010707.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2964.945, "latencies_ms": [2964.945], "images_per_second": 0.337, "prompt_tokens": 1117, "response_tokens_est": 73, "n_tiles": 1, "output_text": " The man in the white shirt is standing near the couch, while the man in the blue shirt is sitting on the couch. The man in the black shirt is sitting on the floor, and the man in the red shirt is standing near the couch. The table is located in the middle of the room, and the window is on the left side of the room.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26823.3, "ram_available_mb": 98948.9, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 26823.3, "ram_available_mb": 98948.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.23, "peak": 40.56, "min": 16.56}, "VIN": {"avg": 67.03, "peak": 131.82, "min": 30.6}, "VIN_SYS_5V0": {"avg": 14.95, "peak": 16.05, "min": 12.15}, "VDD_CPU_SOC_MSS": {"avg": 15.87, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 26.23, "energy_joules_est": 77.79, "sample_count": 29, "duration_seconds": 2.966}, "timestamp": "2026-01-19T14:30:18.681222"}
{"image_index": 104, "image_name": "000000010707.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010707.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2104.768, "latencies_ms": [2104.768], "images_per_second": 0.475, "prompt_tokens": 1111, "response_tokens_est": 40, "n_tiles": 1, "output_text": " A group of friends are gathered in a living room, enjoying a casual get-together. The room is furnished with a comfortable couch, a coffee table, and a lamp, creating a cozy atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26823.3, "ram_available_mb": 98948.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26823.3, "ram_available_mb": 98948.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.63, "peak": 40.57, "min": 16.16}, "VIN": {"avg": 73.36, "peak": 121.33, "min": 29.41}, "VIN_SYS_5V0": {"avg": 15.08, "peak": 16.15, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 16.09, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 28.63, "energy_joules_est": 60.27, "sample_count": 21, "duration_seconds": 2.105}, "timestamp": "2026-01-19T14:30:20.869712"}
{"image_index": 104, "image_name": "000000010707.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010707.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1725.329, "latencies_ms": [1725.329], "images_per_second": 0.58, "prompt_tokens": 1109, "response_tokens_est": 25, "n_tiles": 1, "output_text": " The room is lit by a lamp and has a warm yellow light. The walls are painted white and the floor is wooden.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26823.3, "ram_available_mb": 98948.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26823.3, "ram_available_mb": 98948.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.22, "peak": 40.57, "min": 16.16}, "VIN": {"avg": 74.48, "peak": 114.52, "min": 29.8}, "VIN_SYS_5V0": {"avg": 14.85, "peak": 16.05, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.93, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 30.22, "energy_joules_est": 52.15, "sample_count": 17, "duration_seconds": 1.726}, "timestamp": "2026-01-19T14:30:22.647763"}
{"image_index": 105, "image_name": "000000010764.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010764.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2114.24, "latencies_ms": [2114.24], "images_per_second": 0.473, "prompt_tokens": 1099, "response_tokens_est": 40, "n_tiles": 1, "output_text": " The image captures a baseball catcher in a crouched position, wearing a black helmet, black and white uniform, and a black and yellow glove, poised to catch a ball during a game.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26823.3, "ram_available_mb": 98948.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26823.3, "ram_available_mb": 98948.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.93, "peak": 40.57, "min": 18.14}, "VIN": {"avg": 69.69, "peak": 128.37, "min": 29.86}, "VIN_SYS_5V0": {"avg": 14.93, "peak": 16.05, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.99, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 28.93, "energy_joules_est": 61.18, "sample_count": 21, "duration_seconds": 2.115}, "timestamp": "2026-01-19T14:30:24.831458"}
{"image_index": 105, "image_name": "000000010764.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010764.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3484.117, "latencies_ms": [3484.117], "images_per_second": 0.287, "prompt_tokens": 1113, "response_tokens_est": 86, "n_tiles": 1, "output_text": " 1. catcher's mitt: 1\n2. catcher's helmet: 1\n3. catcher's mask: 1\n4. catcher's chest protector: 1\n5. catcher's leg guards: 1\n6. catcher's leg pads: 1\n7. catcher's leg brace: 1\n8. catcher's leg support: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26823.3, "ram_available_mb": 98948.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 26823.4, "ram_available_mb": 98948.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 24.69, "peak": 40.16, "min": 16.56}, "VIN": {"avg": 66.57, "peak": 97.95, "min": 29.08}, "VIN_SYS_5V0": {"avg": 14.74, "peak": 16.15, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.77, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 24.69, "energy_joules_est": 86.03, "sample_count": 34, "duration_seconds": 3.485}, "timestamp": "2026-01-19T14:30:28.342537"}
{"image_index": 105, "image_name": "000000010764.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010764.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2549.613, "latencies_ms": [2549.613], "images_per_second": 0.392, "prompt_tokens": 1117, "response_tokens_est": 54, "n_tiles": 1, "output_text": " The catcher is positioned in the foreground, squatting behind home plate, while the batter is in the background, standing in the batter's box. The catcher's glove is near the catcher, and the batter's bat is far away from the catcher.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26823.4, "ram_available_mb": 98948.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26823.4, "ram_available_mb": 98948.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.93, "peak": 40.18, "min": 15.77}, "VIN": {"avg": 66.95, "peak": 102.01, "min": 28.83}, "VIN_SYS_5V0": {"avg": 14.94, "peak": 16.05, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 15.86, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 26.93, "energy_joules_est": 68.67, "sample_count": 25, "duration_seconds": 2.55}, "timestamp": "2026-01-19T14:30:30.944384"}
{"image_index": 105, "image_name": "000000010764.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010764.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2038.008, "latencies_ms": [2038.008], "images_per_second": 0.491, "prompt_tokens": 1111, "response_tokens_est": 37, "n_tiles": 1, "output_text": " A baseball player is crouched in the batter's box, ready to catch the ball. The catcher is wearing a black helmet and a white uniform with black and yellow stripes.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26823.4, "ram_available_mb": 98948.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26823.4, "ram_available_mb": 98948.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.82, "peak": 40.16, "min": 15.77}, "VIN": {"avg": 61.67, "peak": 102.41, "min": 29.16}, "VIN_SYS_5V0": {"avg": 14.96, "peak": 16.05, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.91, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 28.82, "energy_joules_est": 58.75, "sample_count": 20, "duration_seconds": 2.038}, "timestamp": "2026-01-19T14:30:33.016853"}
{"image_index": 105, "image_name": "000000010764.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010764.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3108.91, "latencies_ms": [3108.91], "images_per_second": 0.322, "prompt_tokens": 1109, "response_tokens_est": 78, "n_tiles": 1, "output_text": " The image captures a moment of intense focus and concentration, with the catcher crouched in the batter's box, his body poised and ready for action. The warm glow of the stadium lights illuminates the scene, casting long shadows and highlighting the vibrant colors of the catcher's uniform. The grass surrounding the field is a lush green, contrasting with the brown dirt of the infield.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26823.4, "ram_available_mb": 98948.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 26823.4, "ram_available_mb": 98948.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 25.94, "peak": 40.56, "min": 17.35}, "VIN": {"avg": 70.5, "peak": 105.1, "min": 29.99}, "VIN_SYS_5V0": {"avg": 15.02, "peak": 16.15, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 16.0, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 25.94, "energy_joules_est": 80.66, "sample_count": 30, "duration_seconds": 3.109}, "timestamp": "2026-01-19T14:30:36.145312"}
{"image_index": 106, "image_name": "000000010977.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010977.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1831.125, "latencies_ms": [1831.125], "images_per_second": 0.546, "prompt_tokens": 1099, "response_tokens_est": 30, "n_tiles": 1, "output_text": " The image depicts a bathroom with a pink and white color scheme, featuring a bathtub, a toilet, and a wooden vanity with a mirror above it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26823.4, "ram_available_mb": 98948.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26823.4, "ram_available_mb": 98948.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.03, "peak": 40.18, "min": 16.95}, "VIN": {"avg": 68.74, "peak": 120.05, "min": 29.32}, "VIN_SYS_5V0": {"avg": 15.2, "peak": 16.15, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 16.25, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 30.03, "energy_joules_est": 55.0, "sample_count": 18, "duration_seconds": 1.832}, "timestamp": "2026-01-19T14:30:38.012408"}
{"image_index": 106, "image_name": "000000010977.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010977.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2014.667, "latencies_ms": [2014.667], "images_per_second": 0.496, "prompt_tokens": 1113, "response_tokens_est": 37, "n_tiles": 1, "output_text": " 1. Bathtub\n2. Toilet\n3. Window\n4. Shower curtain\n5. Sink\n6. Counter\n7. Cabinet\n8. Door", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26823.4, "ram_available_mb": 98948.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26823.6, "ram_available_mb": 98948.6, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.49, "peak": 40.57, "min": 18.53}, "VIN": {"avg": 76.76, "peak": 121.34, "min": 29.04}, "VIN_SYS_5V0": {"avg": 15.14, "peak": 16.15, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 16.24, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 29.49, "energy_joules_est": 59.42, "sample_count": 20, "duration_seconds": 2.015}, "timestamp": "2026-01-19T14:30:40.099422"}
{"image_index": 106, "image_name": "000000010977.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010977.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2304.778, "latencies_ms": [2304.778], "images_per_second": 0.434, "prompt_tokens": 1117, "response_tokens_est": 47, "n_tiles": 1, "output_text": " The shower curtain is located in the middle of the bathroom, with the sink and toilet situated to the right of it. The bathtub is positioned on the left side of the room, while the door is located on the far left side.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26823.6, "ram_available_mb": 98948.6, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26823.9, "ram_available_mb": 98948.3, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.79, "peak": 40.18, "min": 16.56}, "VIN": {"avg": 61.14, "peak": 102.98, "min": 28.75}, "VIN_SYS_5V0": {"avg": 14.91, "peak": 16.05, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.9, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 27.79, "energy_joules_est": 64.06, "sample_count": 23, "duration_seconds": 2.305}, "timestamp": "2026-01-19T14:30:42.489890"}
{"image_index": 106, "image_name": "000000010977.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010977.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1460.975, "latencies_ms": [1460.975], "images_per_second": 0.684, "prompt_tokens": 1111, "response_tokens_est": 15, "n_tiles": 1, "output_text": " A bathroom with pink tiles and a white toilet is shown in the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26823.9, "ram_available_mb": 98948.3, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 26823.9, "ram_available_mb": 98948.3, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.57, "peak": 40.57, "min": 15.77}, "VIN": {"avg": 78.41, "peak": 118.61, "min": 38.22}, "VIN_SYS_5V0": {"avg": 14.94, "peak": 16.05, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 16.0, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 31.57, "energy_joules_est": 46.14, "sample_count": 14, "duration_seconds": 1.461}, "timestamp": "2026-01-19T14:30:43.956120"}
{"image_index": 106, "image_name": "000000010977.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010977.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1907.692, "latencies_ms": [1907.692], "images_per_second": 0.524, "prompt_tokens": 1109, "response_tokens_est": 33, "n_tiles": 1, "output_text": " The bathroom is painted in a light blue color with pink tiles on the walls and floor. The lighting is bright and natural, coming from a window above the bathtub.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26823.9, "ram_available_mb": 98948.3, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26823.9, "ram_available_mb": 98948.3, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.93, "peak": 40.56, "min": 19.7}, "VIN": {"avg": 70.78, "peak": 89.52, "min": 28.99}, "VIN_SYS_5V0": {"avg": 15.29, "peak": 16.36, "min": 13.59}, "VDD_CPU_SOC_MSS": {"avg": 16.39, "peak": 17.32, "min": 14.97}}, "power_watts_avg": 30.93, "energy_joules_est": 59.02, "sample_count": 19, "duration_seconds": 1.908}, "timestamp": "2026-01-19T14:30:45.934384"}
{"image_index": 107, "image_name": "000000010995.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010995.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2279.362, "latencies_ms": [2279.362], "images_per_second": 0.439, "prompt_tokens": 1099, "response_tokens_est": 47, "n_tiles": 1, "output_text": " The image depicts a bedroom with a bed covered in a yellow and white plaid bedspread, positioned in the center of the room, and a window with a white curtain on the left side, allowing natural light to enter the room.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26823.9, "ram_available_mb": 98948.3, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26823.9, "ram_available_mb": 98948.3, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.31, "peak": 40.16, "min": 16.95}, "VIN": {"avg": 65.33, "peak": 118.08, "min": 29.11}, "VIN_SYS_5V0": {"avg": 15.01, "peak": 16.05, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.96, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 28.31, "energy_joules_est": 64.54, "sample_count": 22, "duration_seconds": 2.28}, "timestamp": "2026-01-19T14:30:48.229806"}
{"image_index": 107, "image_name": "000000010995.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010995.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2631.637, "latencies_ms": [2631.637], "images_per_second": 0.38, "prompt_tokens": 1113, "response_tokens_est": 61, "n_tiles": 1, "output_text": " 1. bed: 1\n2. window: 1\n3. curtain: 2\n4. wall: 1\n5. lamp: 1\n6. bedside table: 1\n7. bed sheet: 1\n8. bed headboard: 1", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26823.9, "ram_available_mb": 98948.3, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26823.9, "ram_available_mb": 98948.3, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.08, "peak": 39.78, "min": 17.74}, "VIN": {"avg": 64.3, "peak": 118.06, "min": 28.92}, "VIN_SYS_5V0": {"avg": 15.12, "peak": 16.15, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 16.1, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 27.08, "energy_joules_est": 71.28, "sample_count": 26, "duration_seconds": 2.632}, "timestamp": "2026-01-19T14:30:50.950221"}
{"image_index": 107, "image_name": "000000010995.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010995.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2148.386, "latencies_ms": [2148.386], "images_per_second": 0.465, "prompt_tokens": 1117, "response_tokens_est": 41, "n_tiles": 1, "output_text": " The bed is located in the foreground of the image, with the window and curtains in the background. The lamp is positioned on the right side of the bed, while the window is on the left side.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26823.9, "ram_available_mb": 98948.3, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26823.9, "ram_available_mb": 98948.3, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.37, "peak": 40.18, "min": 15.38}, "VIN": {"avg": 74.14, "peak": 118.97, "min": 29.27}, "VIN_SYS_5V0": {"avg": 14.85, "peak": 16.05, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.86, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 28.37, "energy_joules_est": 60.97, "sample_count": 21, "duration_seconds": 2.149}, "timestamp": "2026-01-19T14:30:53.125584"}
{"image_index": 107, "image_name": "000000010995.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010995.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2557.118, "latencies_ms": [2557.118], "images_per_second": 0.391, "prompt_tokens": 1111, "response_tokens_est": 58, "n_tiles": 1, "output_text": " The image depicts a bedroom with a bed covered in a yellow and white plaid bedspread, positioned in the center of the room. The room is illuminated by natural light coming through a window with white curtains, and there is a nightstand next to the bed with a lamp on it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26823.9, "ram_available_mb": 98948.3, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26823.8, "ram_available_mb": 98948.3, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.47, "peak": 40.56, "min": 17.34}, "VIN": {"avg": 72.4, "peak": 123.89, "min": 28.84}, "VIN_SYS_5V0": {"avg": 15.06, "peak": 16.15, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 16.19, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 27.47, "energy_joules_est": 70.26, "sample_count": 25, "duration_seconds": 2.558}, "timestamp": "2026-01-19T14:30:55.725457"}
{"image_index": 107, "image_name": "000000010995.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010995.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1693.26, "latencies_ms": [1693.26], "images_per_second": 0.591, "prompt_tokens": 1109, "response_tokens_est": 24, "n_tiles": 1, "output_text": " The room is dimly lit with a yellow wall and a bed with a yellow and white plaid comforter.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26823.8, "ram_available_mb": 98948.3, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26823.8, "ram_available_mb": 98948.3, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.17, "peak": 40.18, "min": 16.16}, "VIN": {"avg": 70.66, "peak": 103.86, "min": 29.4}, "VIN_SYS_5V0": {"avg": 15.02, "peak": 16.15, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 16.14, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 30.17, "energy_joules_est": 51.09, "sample_count": 17, "duration_seconds": 1.694}, "timestamp": "2026-01-19T14:30:57.501523"}
{"image_index": 108, "image_name": "000000011051.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011051.jpg", "image_width": 640, "image_height": 536, "image_resolution": "640x536", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2009.085, "latencies_ms": [2009.085], "images_per_second": 0.498, "prompt_tokens": 1432, "response_tokens_est": 19, "n_tiles": 1, "output_text": " A woman in a black dress is pinning a white flower to a man's suit jacket.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26823.8, "ram_available_mb": 98948.3, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26824.0, "ram_available_mb": 98948.2, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.2, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.5, "peak": 40.18, "min": 17.74}, "VIN": {"avg": 72.5, "peak": 117.79, "min": 28.27}, "VIN_SYS_5V0": {"avg": 15.13, "peak": 16.56, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 16.28, "peak": 17.71, "min": 13.0}}, "power_watts_avg": 31.5, "energy_joules_est": 63.31, "sample_count": 20, "duration_seconds": 2.01}, "timestamp": "2026-01-19T14:30:59.595399"}
{"image_index": 108, "image_name": "000000011051.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011051.jpg", "image_width": 640, "image_height": 536, "image_resolution": "640x536", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3314.297, "latencies_ms": [3314.297], "images_per_second": 0.302, "prompt_tokens": 1446, "response_tokens_est": 66, "n_tiles": 1, "output_text": " 1. woman: 1\n2. man: 1\n3. tie: 1\n4. flower: 1\n5. woman's dress: 1\n6. woman's earring: 1\n7. woman's bracelet: 1\n8. woman's necklace: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26824.0, "ram_available_mb": 98948.2, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26824.0, "ram_available_mb": 98948.2, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.77, "peak": 40.18, "min": 18.14}, "VIN": {"avg": 69.11, "peak": 124.6, "min": 28.93}, "VIN_SYS_5V0": {"avg": 14.98, "peak": 16.46, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 16.02, "peak": 17.71, "min": 13.0}}, "power_watts_avg": 27.77, "energy_joules_est": 92.05, "sample_count": 32, "duration_seconds": 3.315}, "timestamp": "2026-01-19T14:31:02.925114"}
{"image_index": 108, "image_name": "000000011051.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011051.jpg", "image_width": 640, "image_height": 536, "image_resolution": "640x536", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2605.649, "latencies_ms": [2605.649], "images_per_second": 0.384, "prompt_tokens": 1450, "response_tokens_est": 43, "n_tiles": 1, "output_text": " The woman is standing to the right of the man, and she is closer to the camera than the man. The woman is holding the man's tie, and the tie is positioned in the center of the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26824.0, "ram_available_mb": 98948.2, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26824.0, "ram_available_mb": 98948.2, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.46, "peak": 40.18, "min": 16.55}, "VIN": {"avg": 71.58, "peak": 131.42, "min": 28.11}, "VIN_SYS_5V0": {"avg": 15.27, "peak": 16.66, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 16.29, "peak": 17.71, "min": 13.79}}, "power_watts_avg": 29.46, "energy_joules_est": 76.78, "sample_count": 26, "duration_seconds": 2.606}, "timestamp": "2026-01-19T14:31:05.632484"}
{"image_index": 108, "image_name": "000000011051.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011051.jpg", "image_width": 640, "image_height": 536, "image_resolution": "640x536", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2224.375, "latencies_ms": [2224.375], "images_per_second": 0.45, "prompt_tokens": 1444, "response_tokens_est": 28, "n_tiles": 1, "output_text": " A man in a suit and tie is getting a boutonniere pinned to his lapel by a woman in a black dress.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26824.0, "ram_available_mb": 98948.2, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 26824.3, "ram_available_mb": 98947.9, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.59, "peak": 39.78, "min": 15.77}, "VIN": {"avg": 76.27, "peak": 118.98, "min": 29.33}, "VIN_SYS_5V0": {"avg": 15.0, "peak": 16.46, "min": 11.56}, "VDD_CPU_SOC_MSS": {"avg": 16.18, "peak": 17.71, "min": 12.61}}, "power_watts_avg": 30.59, "energy_joules_est": 68.06, "sample_count": 22, "duration_seconds": 2.225}, "timestamp": "2026-01-19T14:31:07.925296"}
{"image_index": 108, "image_name": "000000011051.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011051.jpg", "image_width": 640, "image_height": 536, "image_resolution": "640x536", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2823.887, "latencies_ms": [2823.887], "images_per_second": 0.354, "prompt_tokens": 1442, "response_tokens_est": 51, "n_tiles": 1, "output_text": " The image features a man and a woman, both dressed in formal attire, with the man wearing a black suit and the woman wearing a black dress with a beaded bodice. The lighting in the image is soft and warm, creating a cozy atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26824.3, "ram_available_mb": 98947.9, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26824.3, "ram_available_mb": 98947.9, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.9, "peak": 40.57, "min": 17.74}, "VIN": {"avg": 72.37, "peak": 130.12, "min": 27.76}, "VIN_SYS_5V0": {"avg": 15.12, "peak": 16.56, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 16.17, "peak": 17.71, "min": 13.39}}, "power_watts_avg": 28.9, "energy_joules_est": 81.62, "sample_count": 28, "duration_seconds": 2.824}, "timestamp": "2026-01-19T14:31:10.845111"}
{"image_index": 109, "image_name": "000000011122.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011122.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1892.069, "latencies_ms": [1892.069], "images_per_second": 0.529, "prompt_tokens": 1099, "response_tokens_est": 31, "n_tiles": 1, "output_text": " The image shows a chain-link fence with a stop sign attached to it, situated in a grassy area with palm trees and a building in the background.", "error": null, "sys_before": {"cpu_percent": 6.3, "ram_used_mb": 26824.3, "ram_available_mb": 98947.9, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26828.4, "ram_available_mb": 98943.7, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.05, "peak": 40.16, "min": 15.38}, "VIN": {"avg": 72.0, "peak": 124.8, "min": 27.61}, "VIN_SYS_5V0": {"avg": 14.76, "peak": 16.05, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.77, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 29.05, "energy_joules_est": 54.98, "sample_count": 19, "duration_seconds": 1.892}, "timestamp": "2026-01-19T14:31:12.838976"}
{"image_index": 109, "image_name": "000000011122.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011122.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2033.082, "latencies_ms": [2033.082], "images_per_second": 0.492, "prompt_tokens": 1113, "response_tokens_est": 37, "n_tiles": 1, "output_text": " 1. chain link fence\n2. stop sign\n3. palm tree\n4. building\n5. trash\n6. grass\n7. sidewalk\n8. fence post", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26828.4, "ram_available_mb": 98943.7, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26828.4, "ram_available_mb": 98943.7, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.99, "peak": 40.57, "min": 16.16}, "VIN": {"avg": 65.02, "peak": 121.17, "min": 30.88}, "VIN_SYS_5V0": {"avg": 14.92, "peak": 16.05, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.88, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 28.99, "energy_joules_est": 58.95, "sample_count": 20, "duration_seconds": 2.033}, "timestamp": "2026-01-19T14:31:14.932709"}
{"image_index": 109, "image_name": "000000011122.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011122.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2176.544, "latencies_ms": [2176.544], "images_per_second": 0.459, "prompt_tokens": 1117, "response_tokens_est": 42, "n_tiles": 1, "output_text": " The stop sign is located in the foreground of the image, on the left side, and is positioned near a chain link fence. The background of the image features a grassy area with palm trees and a building.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26828.4, "ram_available_mb": 98943.7, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26828.4, "ram_available_mb": 98943.7, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.68, "peak": 40.56, "min": 16.56}, "VIN": {"avg": 70.28, "peak": 128.52, "min": 29.49}, "VIN_SYS_5V0": {"avg": 14.95, "peak": 16.05, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.99, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 28.68, "energy_joules_est": 62.44, "sample_count": 21, "duration_seconds": 2.177}, "timestamp": "2026-01-19T14:31:17.121835"}
{"image_index": 109, "image_name": "000000011122.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011122.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1859.545, "latencies_ms": [1859.545], "images_per_second": 0.538, "prompt_tokens": 1111, "response_tokens_est": 31, "n_tiles": 1, "output_text": " The image captures a scene of a chain-link fence with a stop sign attached to it, situated in a grassy area with palm trees in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26828.4, "ram_available_mb": 98943.7, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26828.4, "ram_available_mb": 98943.7, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.33, "peak": 39.78, "min": 18.53}, "VIN": {"avg": 70.52, "peak": 102.04, "min": 30.14}, "VIN_SYS_5V0": {"avg": 15.24, "peak": 16.26, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 16.4, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 30.33, "energy_joules_est": 56.41, "sample_count": 18, "duration_seconds": 1.86}, "timestamp": "2026-01-19T14:31:19.004198"}
{"image_index": 109, "image_name": "000000011122.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011122.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1964.193, "latencies_ms": [1964.193], "images_per_second": 0.509, "prompt_tokens": 1109, "response_tokens_est": 35, "n_tiles": 1, "output_text": " The image features a red and white stop sign, a chain link fence, and a grassy area with palm trees. The sky is clear and blue, indicating a sunny day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26828.4, "ram_available_mb": 98943.7, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26828.4, "ram_available_mb": 98943.7, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.11, "peak": 40.18, "min": 18.91}, "VIN": {"avg": 74.72, "peak": 123.87, "min": 29.06}, "VIN_SYS_5V0": {"avg": 15.21, "peak": 16.15, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 16.31, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 30.11, "energy_joules_est": 59.15, "sample_count": 19, "duration_seconds": 1.964}, "timestamp": "2026-01-19T14:31:20.983173"}
{"image_index": 110, "image_name": "000000011149.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011149.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1954.723, "latencies_ms": [1954.723], "images_per_second": 0.512, "prompt_tokens": 1099, "response_tokens_est": 35, "n_tiles": 1, "output_text": " A man wearing a helmet is riding a motorcycle next to a black bicycle with a basket on the back, while a man in a gray shirt and khaki shorts stands nearby.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 26828.4, "ram_available_mb": 98943.7, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26830.0, "ram_available_mb": 98942.2, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.96, "peak": 40.18, "min": 18.91}, "VIN": {"avg": 72.61, "peak": 104.67, "min": 29.22}, "VIN_SYS_5V0": {"avg": 15.25, "peak": 16.26, "min": 13.29}, "VDD_CPU_SOC_MSS": {"avg": 16.39, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 29.96, "energy_joules_est": 58.57, "sample_count": 19, "duration_seconds": 1.955}, "timestamp": "2026-01-19T14:31:22.968687"}
{"image_index": 110, "image_name": "000000011149.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011149.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2146.937, "latencies_ms": [2146.937], "images_per_second": 0.466, "prompt_tokens": 1113, "response_tokens_est": 42, "n_tiles": 1, "output_text": " 1. yellow bicycle\n2. black bicycle\n3. motorcycle\n4. person\n5. person's hand\n6. person's leg\n7. person's foot\n8. person's shoe", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26830.0, "ram_available_mb": 98942.2, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26829.3, "ram_available_mb": 98942.9, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.06, "peak": 40.57, "min": 18.14}, "VIN": {"avg": 75.08, "peak": 125.1, "min": 29.39}, "VIN_SYS_5V0": {"avg": 15.21, "peak": 16.15, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 16.16, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 29.06, "energy_joules_est": 62.4, "sample_count": 21, "duration_seconds": 2.147}, "timestamp": "2026-01-19T14:31:25.149285"}
{"image_index": 110, "image_name": "000000011149.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011149.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2324.655, "latencies_ms": [2324.655], "images_per_second": 0.43, "prompt_tokens": 1117, "response_tokens_est": 49, "n_tiles": 1, "output_text": " The yellow bicycle is positioned to the left of the black bicycle, which is in the foreground of the image. The person standing next to the black bicycle is positioned in the background, while the motorcycle is positioned to the left of the yellow bicycle.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26829.3, "ram_available_mb": 98942.9, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26827.4, "ram_available_mb": 98944.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.19, "peak": 40.16, "min": 17.34}, "VIN": {"avg": 72.46, "peak": 124.32, "min": 28.99}, "VIN_SYS_5V0": {"avg": 15.15, "peak": 16.15, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 16.19, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 28.19, "energy_joules_est": 65.55, "sample_count": 23, "duration_seconds": 2.325}, "timestamp": "2026-01-19T14:31:27.541358"}
{"image_index": 110, "image_name": "000000011149.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011149.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1389.109, "latencies_ms": [1389.109], "images_per_second": 0.72, "prompt_tokens": 1111, "response_tokens_est": 12, "n_tiles": 1, "output_text": " A man is standing next to a bike and a motorcycle.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26827.4, "ram_available_mb": 98944.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26827.6, "ram_available_mb": 98944.6, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.6, "peak": 40.56, "min": 16.16}, "VIN": {"avg": 71.1, "peak": 107.96, "min": 28.48}, "VIN_SYS_5V0": {"avg": 14.93, "peak": 16.05, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 16.0, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 31.6, "energy_joules_est": 43.91, "sample_count": 14, "duration_seconds": 1.39}, "timestamp": "2026-01-19T14:31:28.998591"}
{"image_index": 110, "image_name": "000000011149.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011149.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2555.558, "latencies_ms": [2555.558], "images_per_second": 0.391, "prompt_tokens": 1109, "response_tokens_est": 57, "n_tiles": 1, "output_text": " The image features a man wearing a helmet and a black leather jacket, standing next to a black bicycle with a brown basket on the back. The scene is set in a park with trees and a fence in the background. The lighting is natural, and the weather appears to be clear.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26827.6, "ram_available_mb": 98944.6, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26827.6, "ram_available_mb": 98944.6, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.05, "peak": 40.97, "min": 18.52}, "VIN": {"avg": 70.12, "peak": 127.19, "min": 29.47}, "VIN_SYS_5V0": {"avg": 14.91, "peak": 16.05, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 16.05, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 28.05, "energy_joules_est": 71.69, "sample_count": 25, "duration_seconds": 2.556}, "timestamp": "2026-01-19T14:31:31.609309"}
{"image_index": 111, "image_name": "000000011197.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011197.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1731.11, "latencies_ms": [1731.11], "images_per_second": 0.578, "prompt_tokens": 1099, "response_tokens_est": 25, "n_tiles": 1, "output_text": " In the image, a man and a woman are standing on a sidewalk, with a traffic light and a street sign nearby.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 26827.6, "ram_available_mb": 98944.6, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26829.2, "ram_available_mb": 98942.9, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.08, "peak": 40.16, "min": 15.76}, "VIN": {"avg": 66.68, "peak": 107.92, "min": 29.19}, "VIN_SYS_5V0": {"avg": 14.87, "peak": 16.05, "min": 12.25}, "VDD_CPU_SOC_MSS": {"avg": 16.1, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 30.08, "energy_joules_est": 52.1, "sample_count": 17, "duration_seconds": 1.732}, "timestamp": "2026-01-19T14:31:33.391195"}
{"image_index": 111, "image_name": "000000011197.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011197.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2243.64, "latencies_ms": [2243.64], "images_per_second": 0.446, "prompt_tokens": 1113, "response_tokens_est": 45, "n_tiles": 1, "output_text": " 1. black pole\n2. yellow sign\n3. black trash can\n4. traffic light\n5. street sign\n6. pedestrian crossing\n7. man in blue jeans\n8. man in black shirt", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26829.2, "ram_available_mb": 98942.9, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26829.2, "ram_available_mb": 98943.0, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.69, "peak": 40.57, "min": 18.52}, "VIN": {"avg": 72.44, "peak": 121.31, "min": 29.07}, "VIN_SYS_5V0": {"avg": 14.97, "peak": 16.15, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 16.14, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 28.69, "energy_joules_est": 64.38, "sample_count": 22, "duration_seconds": 2.244}, "timestamp": "2026-01-19T14:31:35.690508"}
{"image_index": 111, "image_name": "000000011197.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011197.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2381.246, "latencies_ms": [2381.246], "images_per_second": 0.42, "prompt_tokens": 1117, "response_tokens_est": 50, "n_tiles": 1, "output_text": " The two individuals are standing on the sidewalk, which is located in the foreground of the image. The traffic light is positioned in the background, indicating the presence of a crosswalk. The street sign is also in the background, providing direction for pedestrians.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26829.2, "ram_available_mb": 98943.0, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26829.2, "ram_available_mb": 98943.0, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.9, "peak": 40.18, "min": 16.56}, "VIN": {"avg": 69.55, "peak": 124.14, "min": 30.56}, "VIN_SYS_5V0": {"avg": 14.95, "peak": 16.05, "min": 12.36}, "VDD_CPU_SOC_MSS": {"avg": 15.97, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 27.9, "energy_joules_est": 66.45, "sample_count": 23, "duration_seconds": 2.382}, "timestamp": "2026-01-19T14:31:38.087241"}
{"image_index": 111, "image_name": "000000011197.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011197.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2565.007, "latencies_ms": [2565.007], "images_per_second": 0.39, "prompt_tokens": 1111, "response_tokens_est": 58, "n_tiles": 1, "output_text": " The image captures a bustling city street corner with a pedestrian crossing, where two individuals are standing on the sidewalk, seemingly engaged in a conversation. The street is lined with buildings, and a red car is parked on the side of the road, adding a pop of color to the urban landscape.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26829.2, "ram_available_mb": 98943.0, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26829.2, "ram_available_mb": 98943.0, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.4, "peak": 40.56, "min": 16.95}, "VIN": {"avg": 71.39, "peak": 118.29, "min": 30.63}, "VIN_SYS_5V0": {"avg": 15.12, "peak": 16.15, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 16.28, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 27.4, "energy_joules_est": 70.29, "sample_count": 25, "duration_seconds": 2.565}, "timestamp": "2026-01-19T14:31:40.682195"}
{"image_index": 111, "image_name": "000000011197.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011197.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2704.202, "latencies_ms": [2704.202], "images_per_second": 0.37, "prompt_tokens": 1109, "response_tokens_est": 63, "n_tiles": 1, "output_text": " The image depicts a city street with a mix of natural and artificial lighting. The sky is overcast, casting a soft light over the scene. The colors in the image are muted, with the red of the car and the blue of the sky standing out against the more neutral tones of the buildings and pavement.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26829.2, "ram_available_mb": 98943.0, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 26828.7, "ram_available_mb": 98943.5, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.93, "peak": 40.18, "min": 16.56}, "VIN": {"avg": 61.88, "peak": 94.59, "min": 33.64}, "VIN_SYS_5V0": {"avg": 15.08, "peak": 16.15, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 16.1, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 26.93, "energy_joules_est": 72.84, "sample_count": 26, "duration_seconds": 2.705}, "timestamp": "2026-01-19T14:31:43.396094"}
{"image_index": 112, "image_name": "000000011511.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011511.jpg", "image_width": 640, "image_height": 464, "image_resolution": "640x464", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1911.405, "latencies_ms": [1911.405], "images_per_second": 0.523, "prompt_tokens": 1099, "response_tokens_est": 33, "n_tiles": 1, "output_text": " The image features a bronze statue of two people sitting on a bench, with a handbag placed next to them, and a group of people standing in the background.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 26828.7, "ram_available_mb": 98943.5, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26828.7, "ram_available_mb": 98943.4, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.5, "peak": 40.56, "min": 16.55}, "VIN": {"avg": 71.88, "peak": 118.42, "min": 29.27}, "VIN_SYS_5V0": {"avg": 15.22, "peak": 16.15, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 16.26, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 29.5, "energy_joules_est": 56.39, "sample_count": 19, "duration_seconds": 1.912}, "timestamp": "2026-01-19T14:31:45.376577"}
{"image_index": 112, "image_name": "000000011511.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011511.jpg", "image_width": 640, "image_height": 464, "image_resolution": "640x464", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2530.986, "latencies_ms": [2530.986], "images_per_second": 0.395, "prompt_tokens": 1113, "response_tokens_est": 56, "n_tiles": 1, "output_text": " 1. statue: 2\n2. bench: 1\n3. bag: 1\n4. person: 1\n5. person: 1\n6. person: 1\n7. person: 1\n8. person: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26828.7, "ram_available_mb": 98943.4, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26828.7, "ram_available_mb": 98943.4, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.31, "peak": 40.18, "min": 16.95}, "VIN": {"avg": 67.66, "peak": 125.15, "min": 28.68}, "VIN_SYS_5V0": {"avg": 14.93, "peak": 16.05, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 16.06, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 27.31, "energy_joules_est": 69.13, "sample_count": 25, "duration_seconds": 2.531}, "timestamp": "2026-01-19T14:31:47.979847"}
{"image_index": 112, "image_name": "000000011511.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011511.jpg", "image_width": 640, "image_height": 464, "image_resolution": "640x464", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2327.908, "latencies_ms": [2327.908], "images_per_second": 0.43, "prompt_tokens": 1117, "response_tokens_est": 48, "n_tiles": 1, "output_text": " The statue of two people is positioned on the left side of the image, with the bench and handbag placed in the foreground. The background features a group of people standing on the sidewalk, with a closed shop and a bicycle parked nearby.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26828.7, "ram_available_mb": 98943.4, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26828.7, "ram_available_mb": 98943.4, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.56, "peak": 40.18, "min": 15.38}, "VIN": {"avg": 70.2, "peak": 124.88, "min": 28.79}, "VIN_SYS_5V0": {"avg": 14.89, "peak": 16.05, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.99, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 27.56, "energy_joules_est": 64.17, "sample_count": 23, "duration_seconds": 2.328}, "timestamp": "2026-01-19T14:31:50.380428"}
{"image_index": 112, "image_name": "000000011511.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011511.jpg", "image_width": 640, "image_height": 464, "image_resolution": "640x464", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1667.424, "latencies_ms": [1667.424], "images_per_second": 0.6, "prompt_tokens": 1111, "response_tokens_est": 22, "n_tiles": 1, "output_text": " A bronze statue of two people sitting on a bench, with a handbag on the ground next to them.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26828.7, "ram_available_mb": 98943.4, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26828.7, "ram_available_mb": 98943.4, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.01, "peak": 40.18, "min": 15.77}, "VIN": {"avg": 72.01, "peak": 124.28, "min": 29.25}, "VIN_SYS_5V0": {"avg": 14.77, "peak": 16.05, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 15.93, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 30.01, "energy_joules_est": 50.05, "sample_count": 17, "duration_seconds": 1.668}, "timestamp": "2026-01-19T14:31:52.152493"}
{"image_index": 112, "image_name": "000000011511.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011511.jpg", "image_width": 640, "image_height": 464, "image_resolution": "640x464", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1750.585, "latencies_ms": [1750.585], "images_per_second": 0.571, "prompt_tokens": 1109, "response_tokens_est": 25, "n_tiles": 1, "output_text": " The statue is made of bronze and is located in a public square. The lighting is natural and the weather is overcast.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26828.7, "ram_available_mb": 98943.4, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26828.8, "ram_available_mb": 98943.4, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.4, "peak": 40.18, "min": 17.35}, "VIN": {"avg": 70.25, "peak": 91.32, "min": 28.93}, "VIN_SYS_5V0": {"avg": 14.73, "peak": 15.95, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.91, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 30.4, "energy_joules_est": 53.23, "sample_count": 17, "duration_seconds": 1.751}, "timestamp": "2026-01-19T14:31:53.924037"}
{"image_index": 113, "image_name": "000000011615.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011615.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2358.02, "latencies_ms": [2358.02], "images_per_second": 0.424, "prompt_tokens": 1100, "response_tokens_est": 51, "n_tiles": 1, "output_text": " The image features a set of road signs with various symbols and text, including a blue sign with an airplane and a bus, a green sign with a truck, a blue sign with a parking symbol, and a red and blue sign with a truck symbol.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 26828.8, "ram_available_mb": 98943.4, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26828.8, "ram_available_mb": 98943.4, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.47, "peak": 40.18, "min": 19.31}, "VIN": {"avg": 76.54, "peak": 123.84, "min": 29.1}, "VIN_SYS_5V0": {"avg": 15.19, "peak": 16.26, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 16.21, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 28.47, "energy_joules_est": 67.15, "sample_count": 23, "duration_seconds": 2.359}, "timestamp": "2026-01-19T14:31:56.314833"}
{"image_index": 113, "image_name": "000000011615.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011615.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2140.516, "latencies_ms": [2140.516], "images_per_second": 0.467, "prompt_tokens": 1114, "response_tokens_est": 42, "n_tiles": 1, "output_text": " 1. signpost\n2. blue sign\n3. green sign\n4. white sign\n5. red and blue sign\n6. white arrow\n7. blue arrow\n8. white text", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26828.8, "ram_available_mb": 98943.4, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26828.8, "ram_available_mb": 98943.4, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.63, "peak": 40.18, "min": 16.56}, "VIN": {"avg": 70.16, "peak": 117.83, "min": 28.98}, "VIN_SYS_5V0": {"avg": 15.11, "peak": 16.15, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 16.12, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 28.63, "energy_joules_est": 61.3, "sample_count": 21, "duration_seconds": 2.141}, "timestamp": "2026-01-19T14:31:58.498038"}
{"image_index": 113, "image_name": "000000011615.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011615.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2561.788, "latencies_ms": [2561.788], "images_per_second": 0.39, "prompt_tokens": 1118, "response_tokens_est": 56, "n_tiles": 1, "output_text": " The blue sign is on the left, the green sign is in the middle, and the red and blue signs are on the right. The blue sign is in the foreground, while the green sign is in the middle ground, and the red and blue signs are in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26828.8, "ram_available_mb": 98943.4, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26828.8, "ram_available_mb": 98943.4, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.19, "peak": 40.18, "min": 16.95}, "VIN": {"avg": 64.3, "peak": 88.96, "min": 28.92}, "VIN_SYS_5V0": {"avg": 14.99, "peak": 16.15, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 15.97, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 27.19, "energy_joules_est": 69.67, "sample_count": 25, "duration_seconds": 2.562}, "timestamp": "2026-01-19T14:32:01.089944"}
{"image_index": 113, "image_name": "000000011615.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011615.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3130.685, "latencies_ms": [3130.685], "images_per_second": 0.319, "prompt_tokens": 1112, "response_tokens_est": 80, "n_tiles": 1, "output_text": " The image captures a scene of a road signpost, standing tall against the backdrop of a cloudy sky. The signpost, adorned with four distinct signs, each with its own unique color and symbol, guides travelers on their journey. The signs, in German, read \"Severins-br\u00fccke\" and \"Koelnnesse,\" indicating the names of nearby bridges and roads.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26828.8, "ram_available_mb": 98943.4, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 26828.8, "ram_available_mb": 98943.4, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 25.44, "peak": 39.0, "min": 16.16}, "VIN": {"avg": 71.57, "peak": 121.02, "min": 28.9}, "VIN_SYS_5V0": {"avg": 15.05, "peak": 16.26, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 15.94, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 25.44, "energy_joules_est": 79.66, "sample_count": 31, "duration_seconds": 3.131}, "timestamp": "2026-01-19T14:32:04.310484"}
{"image_index": 113, "image_name": "000000011615.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011615.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2612.884, "latencies_ms": [2612.884], "images_per_second": 0.383, "prompt_tokens": 1110, "response_tokens_est": 59, "n_tiles": 1, "output_text": " The image features a blue and white sign with a yellow arrow pointing to the right, a green and white sign with a black arrow pointing to the right, and a red and white sign with a black arrow pointing to the right. The signs are set against a cloudy sky and surrounded by trees.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26828.8, "ram_available_mb": 98943.4, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26828.8, "ram_available_mb": 98943.4, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.5, "peak": 40.16, "min": 14.98}, "VIN": {"avg": 66.48, "peak": 122.46, "min": 28.22}, "VIN_SYS_5V0": {"avg": 14.86, "peak": 16.15, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.84, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 26.5, "energy_joules_est": 69.25, "sample_count": 26, "duration_seconds": 2.613}, "timestamp": "2026-01-19T14:32:07.010351"}
{"image_index": 114, "image_name": "000000011699.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011699.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1916.421, "latencies_ms": [1916.421], "images_per_second": 0.522, "prompt_tokens": 1100, "response_tokens_est": 33, "n_tiles": 1, "output_text": " Two women, one in a red shirt and the other in a blue shirt, are standing on a train platform with their luggage, with a woman in the background.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 26828.8, "ram_available_mb": 98943.4, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26828.8, "ram_available_mb": 98943.4, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.99, "peak": 39.77, "min": 14.98}, "VIN": {"avg": 68.73, "peak": 91.76, "min": 28.56}, "VIN_SYS_5V0": {"avg": 14.9, "peak": 16.05, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.93, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 28.99, "energy_joules_est": 55.57, "sample_count": 19, "duration_seconds": 1.917}, "timestamp": "2026-01-19T14:32:08.986337"}
{"image_index": 114, "image_name": "000000011699.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011699.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2619.559, "latencies_ms": [2619.559], "images_per_second": 0.382, "prompt_tokens": 1114, "response_tokens_est": 60, "n_tiles": 1, "output_text": " 1. suitcase: 1\n2. woman: 2\n3. girl: 1\n4. backpack: 1\n5. handbag: 1\n6. suitcase tag: 1\n7. platform: 1\n8. train: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26828.8, "ram_available_mb": 98943.4, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 26828.8, "ram_available_mb": 98943.4, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.94, "peak": 40.18, "min": 17.35}, "VIN": {"avg": 69.77, "peak": 106.77, "min": 28.14}, "VIN_SYS_5V0": {"avg": 14.99, "peak": 16.15, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 16.03, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 26.94, "energy_joules_est": 70.58, "sample_count": 26, "duration_seconds": 2.62}, "timestamp": "2026-01-19T14:32:11.701156"}
{"image_index": 114, "image_name": "000000011699.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011699.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2716.884, "latencies_ms": [2716.884], "images_per_second": 0.368, "prompt_tokens": 1118, "response_tokens_est": 63, "n_tiles": 1, "output_text": " The woman in the red shirt is standing to the left of the woman in the blue shirt, with the woman in the blue shirt being closer to the camera. The woman in the blue shirt is standing in front of the woman in the red shirt, with the woman in the blue shirt being closer to the camera.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26828.8, "ram_available_mb": 98943.4, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 26828.8, "ram_available_mb": 98943.4, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.22, "peak": 40.16, "min": 14.98}, "VIN": {"avg": 67.7, "peak": 124.48, "min": 27.36}, "VIN_SYS_5V0": {"avg": 14.82, "peak": 16.05, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.89, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 26.22, "energy_joules_est": 71.25, "sample_count": 27, "duration_seconds": 2.717}, "timestamp": "2026-01-19T14:32:14.511817"}
{"image_index": 114, "image_name": "000000011699.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011699.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1391.176, "latencies_ms": [1391.176], "images_per_second": 0.719, "prompt_tokens": 1112, "response_tokens_est": 12, "n_tiles": 1, "output_text": " Two people are standing on a train platform with their luggage.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26828.8, "ram_available_mb": 98943.4, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 26828.8, "ram_available_mb": 98943.4, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.76, "peak": 39.38, "min": 14.98}, "VIN": {"avg": 76.87, "peak": 119.95, "min": 28.15}, "VIN_SYS_5V0": {"avg": 14.73, "peak": 16.15, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.8, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 30.76, "energy_joules_est": 42.81, "sample_count": 14, "duration_seconds": 1.392}, "timestamp": "2026-01-19T14:32:15.974766"}
{"image_index": 114, "image_name": "000000011699.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011699.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2152.356, "latencies_ms": [2152.356], "images_per_second": 0.465, "prompt_tokens": 1110, "response_tokens_est": 42, "n_tiles": 1, "output_text": " The image is taken during daytime with natural light illuminating the scene. The colors in the image are vibrant, with the red of the woman's shirt standing out against the blue of the girl's t-shirt.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26828.8, "ram_available_mb": 98943.4, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26828.8, "ram_available_mb": 98943.4, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.34, "peak": 40.16, "min": 19.7}, "VIN": {"avg": 71.75, "peak": 117.33, "min": 29.36}, "VIN_SYS_5V0": {"avg": 15.02, "peak": 16.05, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 16.09, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 29.34, "energy_joules_est": 63.16, "sample_count": 21, "duration_seconds": 2.153}, "timestamp": "2026-01-19T14:32:18.169632"}
{"image_index": 115, "image_name": "000000011760.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011760.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1621.993, "latencies_ms": [1621.993], "images_per_second": 0.617, "prompt_tokens": 1099, "response_tokens_est": 21, "n_tiles": 1, "output_text": " Three zebras with black and white stripes are walking on a dirt path in a forest with purple flowers.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 26828.8, "ram_available_mb": 98943.4, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26828.8, "ram_available_mb": 98943.4, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.85, "peak": 40.56, "min": 16.56}, "VIN": {"avg": 69.57, "peak": 86.79, "min": 28.63}, "VIN_SYS_5V0": {"avg": 14.98, "peak": 16.05, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 16.19, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 30.85, "energy_joules_est": 50.06, "sample_count": 16, "duration_seconds": 1.623}, "timestamp": "2026-01-19T14:32:19.844524"}
{"image_index": 115, "image_name": "000000011760.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011760.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1189.565, "latencies_ms": [1189.565], "images_per_second": 0.841, "prompt_tokens": 1113, "response_tokens_est": 4, "n_tiles": 1, "output_text": " zebra: 3", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26828.8, "ram_available_mb": 98943.4, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.2, "ram_used_mb": 26828.8, "ram_available_mb": 98943.4, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 33.16, "peak": 40.57, "min": 19.32}, "VIN": {"avg": 73.04, "peak": 98.71, "min": 29.76}, "VIN_SYS_5V0": {"avg": 14.93, "peak": 16.15, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 16.17, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 33.16, "energy_joules_est": 39.46, "sample_count": 12, "duration_seconds": 1.19}, "timestamp": "2026-01-19T14:32:21.101408"}
{"image_index": 115, "image_name": "000000011760.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011760.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2334.981, "latencies_ms": [2334.981], "images_per_second": 0.428, "prompt_tokens": 1117, "response_tokens_est": 48, "n_tiles": 1, "output_text": " The zebras are positioned in the foreground of the image, with the trees and bushes serving as a backdrop in the background. The zebras are facing the camera, with the tree with purple flowers located to the right of the zebras.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26828.8, "ram_available_mb": 98943.4, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26828.8, "ram_available_mb": 98943.4, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.58, "peak": 40.95, "min": 18.91}, "VIN": {"avg": 59.68, "peak": 115.94, "min": 30.08}, "VIN_SYS_5V0": {"avg": 14.95, "peak": 16.05, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.94, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 29.58, "energy_joules_est": 69.08, "sample_count": 23, "duration_seconds": 2.335}, "timestamp": "2026-01-19T14:32:23.496160"}
{"image_index": 115, "image_name": "000000011760.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011760.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1560.381, "latencies_ms": [1560.381], "images_per_second": 0.641, "prompt_tokens": 1111, "response_tokens_est": 18, "n_tiles": 1, "output_text": " Three zebras are walking in a line on a dirt path surrounded by trees and bushes.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26828.8, "ram_available_mb": 98943.4, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26828.8, "ram_available_mb": 98943.4, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.07, "peak": 40.18, "min": 16.16}, "VIN": {"avg": 81.16, "peak": 121.17, "min": 31.59}, "VIN_SYS_5V0": {"avg": 14.89, "peak": 16.05, "min": 12.25}, "VDD_CPU_SOC_MSS": {"avg": 16.04, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 31.07, "energy_joules_est": 48.49, "sample_count": 15, "duration_seconds": 1.561}, "timestamp": "2026-01-19T14:32:25.060881"}
{"image_index": 115, "image_name": "000000011760.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011760.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1989.686, "latencies_ms": [1989.686], "images_per_second": 0.503, "prompt_tokens": 1109, "response_tokens_est": 36, "n_tiles": 1, "output_text": " The image features three zebras with black and white stripes walking on a dirt path. The zebras are surrounded by trees with purple flowers, and the lighting is bright and sunny.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26828.8, "ram_available_mb": 98943.4, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26828.8, "ram_available_mb": 98943.4, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.25, "peak": 40.95, "min": 19.31}, "VIN": {"avg": 75.57, "peak": 125.49, "min": 29.02}, "VIN_SYS_5V0": {"avg": 15.22, "peak": 16.15, "min": 13.29}, "VDD_CPU_SOC_MSS": {"avg": 16.32, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 30.25, "energy_joules_est": 60.2, "sample_count": 20, "duration_seconds": 1.99}, "timestamp": "2026-01-19T14:32:27.131238"}
{"image_index": 116, "image_name": "000000011813.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011813.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1676.8, "latencies_ms": [1676.8], "images_per_second": 0.596, "prompt_tokens": 1100, "response_tokens_est": 23, "n_tiles": 1, "output_text": " A camera is mounted on a tripod in a room with a vending machine and a laptop on a table.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26828.8, "ram_available_mb": 98943.4, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26828.8, "ram_available_mb": 98943.4, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.01, "peak": 40.16, "min": 16.56}, "VIN": {"avg": 74.1, "peak": 126.57, "min": 29.01}, "VIN_SYS_5V0": {"avg": 14.75, "peak": 16.05, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 15.91, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 30.01, "energy_joules_est": 50.34, "sample_count": 17, "duration_seconds": 1.677}, "timestamp": "2026-01-19T14:32:28.906517"}
{"image_index": 116, "image_name": "000000011813.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011813.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1219.733, "latencies_ms": [1219.733], "images_per_second": 0.82, "prompt_tokens": 1114, "response_tokens_est": 5, "n_tiles": 1, "output_text": " tripod: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26828.8, "ram_available_mb": 98943.4, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.4, "ram_used_mb": 26829.0, "ram_available_mb": 98943.2, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.24, "peak": 40.16, "min": 17.74}, "VIN": {"avg": 72.58, "peak": 103.32, "min": 29.12}, "VIN_SYS_5V0": {"avg": 14.65, "peak": 16.05, "min": 11.87}, "VDD_CPU_SOC_MSS": {"avg": 15.85, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 32.24, "energy_joules_est": 39.35, "sample_count": 12, "duration_seconds": 1.22}, "timestamp": "2026-01-19T14:32:30.158078"}
{"image_index": 116, "image_name": "000000011813.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011813.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2285.644, "latencies_ms": [2285.644], "images_per_second": 0.438, "prompt_tokens": 1118, "response_tokens_est": 47, "n_tiles": 1, "output_text": " The camera is positioned to the right of the tripod, which is in the foreground of the image. The laptop is placed on the tripod, and the vending machine is located in the background, far away from the camera.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26829.0, "ram_available_mb": 98943.2, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26829.0, "ram_available_mb": 98943.2, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.29, "peak": 41.34, "min": 20.48}, "VIN": {"avg": 71.73, "peak": 112.58, "min": 33.47}, "VIN_SYS_5V0": {"avg": 15.15, "peak": 16.26, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 16.25, "peak": 17.71, "min": 14.18}}, "power_watts_avg": 30.29, "energy_joules_est": 69.24, "sample_count": 22, "duration_seconds": 2.286}, "timestamp": "2026-01-19T14:32:32.448378"}
{"image_index": 116, "image_name": "000000011813.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011813.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1849.104, "latencies_ms": [1849.104], "images_per_second": 0.541, "prompt_tokens": 1112, "response_tokens_est": 31, "n_tiles": 1, "output_text": " In a room with a vending machine, a tripod with a camera on it is set up, and a laptop is placed on a table nearby.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26829.0, "ram_available_mb": 98943.2, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26829.0, "ram_available_mb": 98943.2, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.03, "peak": 39.39, "min": 17.73}, "VIN": {"avg": 79.91, "peak": 126.28, "min": 28.07}, "VIN_SYS_5V0": {"avg": 15.31, "peak": 16.36, "min": 13.29}, "VDD_CPU_SOC_MSS": {"avg": 16.49, "peak": 17.71, "min": 14.57}}, "power_watts_avg": 30.03, "energy_joules_est": 55.54, "sample_count": 18, "duration_seconds": 1.849}, "timestamp": "2026-01-19T14:32:34.321397"}
{"image_index": 116, "image_name": "000000011813.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011813.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1865.202, "latencies_ms": [1865.202], "images_per_second": 0.536, "prompt_tokens": 1110, "response_tokens_est": 31, "n_tiles": 1, "output_text": " The image is taken in a room with a blue and white vending machine in the background. The lighting is natural, and the colors are muted.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26829.0, "ram_available_mb": 98943.2, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26829.0, "ram_available_mb": 98943.2, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.73, "peak": 40.57, "min": 18.13}, "VIN": {"avg": 70.79, "peak": 102.54, "min": 27.47}, "VIN_SYS_5V0": {"avg": 15.07, "peak": 16.26, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 16.18, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 29.73, "energy_joules_est": 55.47, "sample_count": 19, "duration_seconds": 1.866}, "timestamp": "2026-01-19T14:32:36.290416"}
{"image_index": 117, "image_name": "000000012062.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012062.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1612.408, "latencies_ms": [1612.408], "images_per_second": 0.62, "prompt_tokens": 1099, "response_tokens_est": 20, "n_tiles": 1, "output_text": " A sheep with a white wool coat is standing in a pen with a pile of wool around it.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 26829.0, "ram_available_mb": 98943.2, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26829.0, "ram_available_mb": 98943.2, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.51, "peak": 40.57, "min": 16.16}, "VIN": {"avg": 75.23, "peak": 115.49, "min": 29.26}, "VIN_SYS_5V0": {"avg": 14.69, "peak": 15.95, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 15.77, "peak": 17.32, "min": 12.61}}, "power_watts_avg": 30.51, "energy_joules_est": 49.21, "sample_count": 16, "duration_seconds": 1.613}, "timestamp": "2026-01-19T14:32:37.966036"}
{"image_index": 117, "image_name": "000000012062.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012062.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2048.261, "latencies_ms": [2048.261], "images_per_second": 0.488, "prompt_tokens": 1113, "response_tokens_est": 37, "n_tiles": 1, "output_text": " sheep: 1, fence: 1, ball: 1, metal: 1, ground: 1, sheep's wool: 1, fence post: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26829.0, "ram_available_mb": 98943.2, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26829.0, "ram_available_mb": 98943.2, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.7, "peak": 40.56, "min": 19.32}, "VIN": {"avg": 73.91, "peak": 120.46, "min": 30.45}, "VIN_SYS_5V0": {"avg": 14.97, "peak": 16.05, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 16.02, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 29.7, "energy_joules_est": 60.84, "sample_count": 20, "duration_seconds": 2.049}, "timestamp": "2026-01-19T14:32:40.042791"}
{"image_index": 117, "image_name": "000000012062.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012062.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2010.248, "latencies_ms": [2010.248], "images_per_second": 0.497, "prompt_tokens": 1117, "response_tokens_est": 36, "n_tiles": 1, "output_text": " The sheep is in the foreground, standing in front of a metal fence. The sheep is eating the wool, which is in the middle ground. The background is a concrete surface.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26829.0, "ram_available_mb": 98943.2, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26829.0, "ram_available_mb": 98943.2, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.25, "peak": 40.56, "min": 17.35}, "VIN": {"avg": 69.57, "peak": 106.86, "min": 28.63}, "VIN_SYS_5V0": {"avg": 15.03, "peak": 16.15, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 16.14, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 29.25, "energy_joules_est": 58.82, "sample_count": 20, "duration_seconds": 2.011}, "timestamp": "2026-01-19T14:32:42.124867"}
{"image_index": 117, "image_name": "000000012062.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012062.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1483.733, "latencies_ms": [1483.733], "images_per_second": 0.674, "prompt_tokens": 1111, "response_tokens_est": 15, "n_tiles": 1, "output_text": " A sheep is standing in a pen with a pile of wool around it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26829.0, "ram_available_mb": 98943.2, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26829.0, "ram_available_mb": 98943.2, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.15, "peak": 40.18, "min": 16.56}, "VIN": {"avg": 75.1, "peak": 115.98, "min": 28.7}, "VIN_SYS_5V0": {"avg": 14.81, "peak": 16.05, "min": 12.15}, "VDD_CPU_SOC_MSS": {"avg": 15.9, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 31.15, "energy_joules_est": 46.23, "sample_count": 15, "duration_seconds": 1.484}, "timestamp": "2026-01-19T14:32:43.690147"}
{"image_index": 117, "image_name": "000000012062.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012062.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1613.712, "latencies_ms": [1613.712], "images_per_second": 0.62, "prompt_tokens": 1109, "response_tokens_est": 20, "n_tiles": 1, "output_text": " The sheep is white and the wool is gray. The lighting is natural and the weather is cloudy.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26829.0, "ram_available_mb": 98943.2, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26829.0, "ram_available_mb": 98943.2, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.64, "peak": 40.57, "min": 20.11}, "VIN": {"avg": 73.9, "peak": 111.77, "min": 29.06}, "VIN_SYS_5V0": {"avg": 14.83, "peak": 16.05, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.89, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 31.64, "energy_joules_est": 51.07, "sample_count": 16, "duration_seconds": 1.614}, "timestamp": "2026-01-19T14:32:45.358837"}
{"image_index": 118, "image_name": "000000012120.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012120.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 3010.382, "latencies_ms": [3010.382], "images_per_second": 0.332, "prompt_tokens": 1099, "response_tokens_est": 75, "n_tiles": 1, "output_text": " In the image, a tennis match is taking place on a blue court, with a player in a pink outfit preparing to serve the ball, while the opposing team, wearing red, is positioned near the net. The court is surrounded by a crowd of spectators, and various advertisements are visible in the background, including those for Lexus, Mirvac, and Sony Ericsson.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 26829.0, "ram_available_mb": 98943.2, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 26829.0, "ram_available_mb": 98943.2, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.61, "peak": 40.56, "min": 19.31}, "VIN": {"avg": 69.45, "peak": 128.13, "min": 33.13}, "VIN_SYS_5V0": {"avg": 15.03, "peak": 16.05, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.96, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 26.61, "energy_joules_est": 80.13, "sample_count": 29, "duration_seconds": 3.011}, "timestamp": "2026-01-19T14:32:48.381692"}
{"image_index": 118, "image_name": "000000012120.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012120.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2095.776, "latencies_ms": [2095.776], "images_per_second": 0.477, "prompt_tokens": 1113, "response_tokens_est": 40, "n_tiles": 1, "output_text": " 1. tennis court\n2. net\n3. ball\n4. racket\n5. player\n6. player's outfit\n7. player's shoes\n8. player's hair", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26829.0, "ram_available_mb": 98943.2, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26829.0, "ram_available_mb": 98943.2, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.7, "peak": 40.56, "min": 16.56}, "VIN": {"avg": 75.28, "peak": 122.54, "min": 29.65}, "VIN_SYS_5V0": {"avg": 15.19, "peak": 16.15, "min": 13.29}, "VDD_CPU_SOC_MSS": {"avg": 16.2, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 28.7, "energy_joules_est": 60.16, "sample_count": 21, "duration_seconds": 2.096}, "timestamp": "2026-01-19T14:32:50.558357"}
{"image_index": 118, "image_name": "000000012120.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012120.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2512.703, "latencies_ms": [2512.703], "images_per_second": 0.398, "prompt_tokens": 1117, "response_tokens_est": 52, "n_tiles": 1, "output_text": " The tennis player is positioned in the foreground, with the net and the ball in the middle ground, and the crowd in the background. The player is near the net, while the ball is in the air, and the crowd is far away from the camera.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26829.0, "ram_available_mb": 98943.2, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26829.0, "ram_available_mb": 98943.2, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.95, "peak": 40.18, "min": 16.16}, "VIN": {"avg": 65.12, "peak": 90.19, "min": 28.8}, "VIN_SYS_5V0": {"avg": 14.74, "peak": 16.05, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 15.72, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 26.95, "energy_joules_est": 67.73, "sample_count": 25, "duration_seconds": 2.513}, "timestamp": "2026-01-19T14:32:53.151205"}
{"image_index": 118, "image_name": "000000012120.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012120.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1641.247, "latencies_ms": [1641.247], "images_per_second": 0.609, "prompt_tokens": 1111, "response_tokens_est": 21, "n_tiles": 1, "output_text": " A tennis match is taking place in a large stadium with a blue court, surrounded by spectators and advertisements.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26829.0, "ram_available_mb": 98943.2, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26829.0, "ram_available_mb": 98943.2, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.33, "peak": 40.18, "min": 15.38}, "VIN": {"avg": 74.15, "peak": 119.9, "min": 29.64}, "VIN_SYS_5V0": {"avg": 14.8, "peak": 16.05, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.95, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 30.33, "energy_joules_est": 49.8, "sample_count": 16, "duration_seconds": 1.642}, "timestamp": "2026-01-19T14:32:54.819164"}
{"image_index": 118, "image_name": "000000012120.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012120.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1594.188, "latencies_ms": [1594.188], "images_per_second": 0.627, "prompt_tokens": 1109, "response_tokens_est": 20, "n_tiles": 1, "output_text": " The court is blue, the players are wearing red and pink, and the crowd is watching intently.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26829.0, "ram_available_mb": 98943.2, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 26829.0, "ram_available_mb": 98943.2, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.76, "peak": 40.57, "min": 20.1}, "VIN": {"avg": 69.02, "peak": 93.8, "min": 28.75}, "VIN_SYS_5V0": {"avg": 15.08, "peak": 16.15, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 16.26, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 31.76, "energy_joules_est": 50.65, "sample_count": 16, "duration_seconds": 1.595}, "timestamp": "2026-01-19T14:32:56.479927"}
{"image_index": 119, "image_name": "000000012280.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012280.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1719.488, "latencies_ms": [1719.488], "images_per_second": 0.582, "prompt_tokens": 1100, "response_tokens_est": 25, "n_tiles": 1, "output_text": " A person is walking through a modern airport terminal with a suitcase, and the terminal has a glass door and a staircase.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 26829.0, "ram_available_mb": 98943.2, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26829.0, "ram_available_mb": 98943.2, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.84, "peak": 40.57, "min": 18.92}, "VIN": {"avg": 72.77, "peak": 117.93, "min": 29.71}, "VIN_SYS_5V0": {"avg": 14.92, "peak": 16.15, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 16.0, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 30.84, "energy_joules_est": 53.06, "sample_count": 17, "duration_seconds": 1.72}, "timestamp": "2026-01-19T14:32:58.258083"}
{"image_index": 119, "image_name": "000000012280.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012280.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2605.566, "latencies_ms": [2605.566], "images_per_second": 0.384, "prompt_tokens": 1114, "response_tokens_est": 58, "n_tiles": 1, "output_text": " 1. person: 1\n2. suitcase: 1\n3. glass door: 1\n4. pillar: 1\n5. sign: 1\n6. wall: 1\n7. floor: 1\n8. ceiling: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26829.0, "ram_available_mb": 98943.2, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26829.0, "ram_available_mb": 98943.2, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.08, "peak": 40.18, "min": 17.73}, "VIN": {"avg": 69.22, "peak": 122.02, "min": 28.23}, "VIN_SYS_5V0": {"avg": 14.96, "peak": 16.15, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.98, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 27.08, "energy_joules_est": 70.57, "sample_count": 26, "duration_seconds": 2.606}, "timestamp": "2026-01-19T14:33:00.944332"}
{"image_index": 119, "image_name": "000000012280.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012280.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2423.854, "latencies_ms": [2423.854], "images_per_second": 0.413, "prompt_tokens": 1118, "response_tokens_est": 52, "n_tiles": 1, "output_text": " The main object, a person, is positioned in the foreground, walking towards the camera. The glass door is located in the middle ground, slightly to the right of the person. The escalator is situated in the background, to the left of the person.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26829.0, "ram_available_mb": 98943.2, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26829.2, "ram_available_mb": 98942.9, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.17, "peak": 40.18, "min": 14.98}, "VIN": {"avg": 72.86, "peak": 124.53, "min": 30.06}, "VIN_SYS_5V0": {"avg": 14.87, "peak": 16.05, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.88, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 27.17, "energy_joules_est": 65.87, "sample_count": 24, "duration_seconds": 2.424}, "timestamp": "2026-01-19T14:33:03.441843"}
{"image_index": 119, "image_name": "000000012280.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012280.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1431.693, "latencies_ms": [1431.693], "images_per_second": 0.698, "prompt_tokens": 1112, "response_tokens_est": 14, "n_tiles": 1, "output_text": " A person is walking through a modern airport terminal with a suitcase.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26829.2, "ram_available_mb": 98942.9, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 26829.2, "ram_available_mb": 98942.9, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.18, "peak": 39.78, "min": 15.77}, "VIN": {"avg": 67.69, "peak": 100.88, "min": 28.65}, "VIN_SYS_5V0": {"avg": 14.91, "peak": 16.26, "min": 12.15}, "VDD_CPU_SOC_MSS": {"avg": 16.0, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 31.18, "energy_joules_est": 44.65, "sample_count": 14, "duration_seconds": 1.432}, "timestamp": "2026-01-19T14:33:04.905084"}
{"image_index": 119, "image_name": "000000012280.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012280.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1892.289, "latencies_ms": [1892.289], "images_per_second": 0.528, "prompt_tokens": 1110, "response_tokens_est": 32, "n_tiles": 1, "output_text": " The image is taken in a well-lit indoor space with a grey floor and white walls. The lighting is bright and natural, coming from the ceiling lights.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26829.2, "ram_available_mb": 98942.9, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26829.2, "ram_available_mb": 98942.9, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.46, "peak": 40.56, "min": 19.7}, "VIN": {"avg": 69.7, "peak": 99.71, "min": 29.87}, "VIN_SYS_5V0": {"avg": 15.09, "peak": 16.26, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 16.16, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 30.46, "energy_joules_est": 57.65, "sample_count": 19, "duration_seconds": 1.893}, "timestamp": "2026-01-19T14:33:06.877527"}
{"image_index": 120, "image_name": "000000012576.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012576.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1721.312, "latencies_ms": [1721.312], "images_per_second": 0.581, "prompt_tokens": 1100, "response_tokens_est": 25, "n_tiles": 1, "output_text": " A dining table is set with two large pizzas, a few glasses of water, and a few forks and knives.", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 26829.2, "ram_available_mb": 98942.9, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26829.2, "ram_available_mb": 98942.9, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.26, "peak": 40.16, "min": 16.56}, "VIN": {"avg": 77.85, "peak": 132.48, "min": 29.12}, "VIN_SYS_5V0": {"avg": 14.84, "peak": 16.15, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.96, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 30.26, "energy_joules_est": 52.11, "sample_count": 17, "duration_seconds": 1.722}, "timestamp": "2026-01-19T14:33:08.645409"}
{"image_index": 120, "image_name": "000000012576.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012576.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2076.616, "latencies_ms": [2076.616], "images_per_second": 0.482, "prompt_tokens": 1114, "response_tokens_est": 34, "n_tiles": 1, "output_text": " pizza: 2, glasses: 2, fork: 1, knife: 1, spoon: 1, television: 1, person: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26829.2, "ram_available_mb": 98942.9, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26829.2, "ram_available_mb": 98942.9, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.38, "peak": 40.56, "min": 17.35}, "VIN": {"avg": 65.14, "peak": 103.67, "min": 27.18}, "VIN_SYS_5V0": {"avg": 14.71, "peak": 16.26, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 15.79, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 28.38, "energy_joules_est": 58.95, "sample_count": 21, "duration_seconds": 2.077}, "timestamp": "2026-01-19T14:33:10.826930"}
{"image_index": 120, "image_name": "000000012576.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012576.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2969.676, "latencies_ms": [2969.676], "images_per_second": 0.337, "prompt_tokens": 1118, "response_tokens_est": 70, "n_tiles": 1, "output_text": " The large pizza boxes are placed on the table, with one box in the foreground and the other in the background. The glasses of water are positioned near the pizza boxes, with one glass closer to the foreground and the other further back. The fork and knife are placed on the table, with the fork closer to the foreground and the knife further back.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26829.2, "ram_available_mb": 98942.9, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 26829.2, "ram_available_mb": 98942.9, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 25.71, "peak": 39.77, "min": 14.98}, "VIN": {"avg": 72.43, "peak": 121.56, "min": 30.27}, "VIN_SYS_5V0": {"avg": 14.72, "peak": 15.95, "min": 11.44}, "VDD_CPU_SOC_MSS": {"avg": 15.69, "peak": 17.32, "min": 12.61}}, "power_watts_avg": 25.71, "energy_joules_est": 76.37, "sample_count": 29, "duration_seconds": 2.971}, "timestamp": "2026-01-19T14:33:13.822466"}
{"image_index": 120, "image_name": "000000012576.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012576.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1698.606, "latencies_ms": [1698.606], "images_per_second": 0.589, "prompt_tokens": 1112, "response_tokens_est": 25, "n_tiles": 1, "output_text": " A dining room with a table set for a pizza party, with two large pizzas and a few glasses of water.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26829.2, "ram_available_mb": 98942.9, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26829.2, "ram_available_mb": 98942.9, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.89, "peak": 38.98, "min": 16.16}, "VIN": {"avg": 72.08, "peak": 115.83, "min": 29.08}, "VIN_SYS_5V0": {"avg": 15.08, "peak": 16.26, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 16.21, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 29.89, "energy_joules_est": 50.79, "sample_count": 17, "duration_seconds": 1.699}, "timestamp": "2026-01-19T14:33:15.592740"}
{"image_index": 120, "image_name": "000000012576.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012576.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2284.262, "latencies_ms": [2284.262], "images_per_second": 0.438, "prompt_tokens": 1110, "response_tokens_est": 47, "n_tiles": 1, "output_text": " The image is taken in a dimly lit room with a warm and cozy atmosphere. The colors in the image are vibrant and inviting, with the red of the pizza boxes standing out against the blue of the television screen in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26829.2, "ram_available_mb": 98942.9, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26829.2, "ram_available_mb": 98942.9, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.46, "peak": 40.57, "min": 17.74}, "VIN": {"avg": 67.27, "peak": 106.43, "min": 30.19}, "VIN_SYS_5V0": {"avg": 14.96, "peak": 16.15, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 16.02, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 28.46, "energy_joules_est": 65.02, "sample_count": 22, "duration_seconds": 2.285}, "timestamp": "2026-01-19T14:33:17.885788"}
{"image_index": 121, "image_name": "000000012639.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012639.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1787.017, "latencies_ms": [1787.017], "images_per_second": 0.56, "prompt_tokens": 1100, "response_tokens_est": 29, "n_tiles": 1, "output_text": " A young boy wearing a helmet and a gray shirt is standing at home plate in a baseball game, holding a bat and getting ready to swing.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 26829.2, "ram_available_mb": 98942.9, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26829.2, "ram_available_mb": 98942.9, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.19, "peak": 40.18, "min": 18.13}, "VIN": {"avg": 67.04, "peak": 106.83, "min": 27.28}, "VIN_SYS_5V0": {"avg": 15.2, "peak": 16.26, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 16.29, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 30.19, "energy_joules_est": 53.97, "sample_count": 18, "duration_seconds": 1.788}, "timestamp": "2026-01-19T14:33:19.764272"}
{"image_index": 121, "image_name": "000000012639.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012639.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2706.99, "latencies_ms": [2706.99], "images_per_second": 0.369, "prompt_tokens": 1114, "response_tokens_est": 63, "n_tiles": 1, "output_text": " 1. baseball bat: 1\n2. baseball glove: 1\n3. baseball: 1\n4. baseball player: 1\n5. catcher: 1\n6. umpire: 1\n7. spectator: 1\n8. tree: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26829.2, "ram_available_mb": 98942.9, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 26829.2, "ram_available_mb": 98942.9, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.6, "peak": 40.18, "min": 16.95}, "VIN": {"avg": 72.91, "peak": 131.93, "min": 28.7}, "VIN_SYS_5V0": {"avg": 14.85, "peak": 16.05, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 15.85, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 26.6, "energy_joules_est": 72.01, "sample_count": 27, "duration_seconds": 2.707}, "timestamp": "2026-01-19T14:33:22.576054"}
{"image_index": 121, "image_name": "000000012639.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012639.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2047.717, "latencies_ms": [2047.717], "images_per_second": 0.488, "prompt_tokens": 1118, "response_tokens_est": 36, "n_tiles": 1, "output_text": " The batter is positioned in the foreground, with the catcher and umpire in the background. The batter is closer to the camera than the catcher and umpire.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26829.2, "ram_available_mb": 98942.9, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26829.2, "ram_available_mb": 98942.9, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.44, "peak": 40.16, "min": 14.98}, "VIN": {"avg": 73.21, "peak": 123.94, "min": 28.89}, "VIN_SYS_5V0": {"avg": 14.76, "peak": 16.05, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 15.77, "peak": 17.32, "min": 12.61}}, "power_watts_avg": 28.44, "energy_joules_est": 58.25, "sample_count": 20, "duration_seconds": 2.048}, "timestamp": "2026-01-19T14:33:24.660260"}
{"image_index": 121, "image_name": "000000012639.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012639.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1472.136, "latencies_ms": [1472.136], "images_per_second": 0.679, "prompt_tokens": 1112, "response_tokens_est": 16, "n_tiles": 1, "output_text": " A young boy is playing baseball in a park with a group of people watching.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26829.2, "ram_available_mb": 98942.9, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 26829.2, "ram_available_mb": 98942.9, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.99, "peak": 39.78, "min": 16.95}, "VIN": {"avg": 64.82, "peak": 95.94, "min": 27.5}, "VIN_SYS_5V0": {"avg": 15.03, "peak": 16.15, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 16.14, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 30.99, "energy_joules_est": 45.64, "sample_count": 15, "duration_seconds": 1.473}, "timestamp": "2026-01-19T14:33:26.224108"}
{"image_index": 121, "image_name": "000000012639.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012639.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2028.372, "latencies_ms": [2028.372], "images_per_second": 0.493, "prompt_tokens": 1110, "response_tokens_est": 37, "n_tiles": 1, "output_text": " The image is taken during a sunny day with clear blue skies. The colors in the image are vibrant, with the green of the grass and trees contrasting against the blue of the sky.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26829.2, "ram_available_mb": 98942.9, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26829.2, "ram_available_mb": 98942.9, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.55, "peak": 40.57, "min": 19.32}, "VIN": {"avg": 76.31, "peak": 123.54, "min": 30.08}, "VIN_SYS_5V0": {"avg": 14.89, "peak": 16.15, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.89, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 29.55, "energy_joules_est": 59.95, "sample_count": 20, "duration_seconds": 2.029}, "timestamp": "2026-01-19T14:33:28.304363"}
{"image_index": 122, "image_name": "000000012667.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012667.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1408.374, "latencies_ms": [1408.374], "images_per_second": 0.71, "prompt_tokens": 1099, "response_tokens_est": 13, "n_tiles": 1, "output_text": " A banana is on top of a black phone on a desk.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 26829.2, "ram_available_mb": 98942.9, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.5, "ram_used_mb": 26829.2, "ram_available_mb": 98942.9, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.83, "peak": 40.57, "min": 16.95}, "VIN": {"avg": 75.4, "peak": 105.64, "min": 28.74}, "VIN_SYS_5V0": {"avg": 14.96, "peak": 16.05, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 16.0, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 31.83, "energy_joules_est": 44.85, "sample_count": 14, "duration_seconds": 1.409}, "timestamp": "2026-01-19T14:33:29.768250"}
{"image_index": 122, "image_name": "000000012667.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012667.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1180.403, "latencies_ms": [1180.403], "images_per_second": 0.847, "prompt_tokens": 1113, "response_tokens_est": 4, "n_tiles": 1, "output_text": " banana: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26829.2, "ram_available_mb": 98942.9, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.1, "ram_used_mb": 26829.2, "ram_available_mb": 98942.9, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 34.11, "peak": 40.95, "min": 21.68}, "VIN": {"avg": 72.96, "peak": 97.63, "min": 28.79}, "VIN_SYS_5V0": {"avg": 14.95, "peak": 16.15, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 16.04, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 34.11, "energy_joules_est": 40.28, "sample_count": 12, "duration_seconds": 1.181}, "timestamp": "2026-01-19T14:33:31.022797"}
{"image_index": 122, "image_name": "000000012667.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012667.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2074.676, "latencies_ms": [2074.676], "images_per_second": 0.482, "prompt_tokens": 1117, "response_tokens_est": 38, "n_tiles": 1, "output_text": " The banana is located in the foreground, to the right of the phone, and the phone is on the desk. The banana is near the phone, and the desk is in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26829.2, "ram_available_mb": 98942.9, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26829.2, "ram_available_mb": 98942.9, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.04, "peak": 41.36, "min": 20.89}, "VIN": {"avg": 71.28, "peak": 117.5, "min": 28.81}, "VIN_SYS_5V0": {"avg": 14.94, "peak": 16.05, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.98, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 31.04, "energy_joules_est": 64.42, "sample_count": 20, "duration_seconds": 2.076}, "timestamp": "2026-01-19T14:33:33.113326"}
{"image_index": 122, "image_name": "000000012667.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012667.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1471.122, "latencies_ms": [1471.122], "images_per_second": 0.68, "prompt_tokens": 1111, "response_tokens_est": 16, "n_tiles": 1, "output_text": " A banana is on top of a phone and a computer is on a desk.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26829.2, "ram_available_mb": 98942.9, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.5, "ram_used_mb": 26829.2, "ram_available_mb": 98943.0, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.86, "peak": 40.18, "min": 18.14}, "VIN": {"avg": 78.38, "peak": 128.84, "min": 28.51}, "VIN_SYS_5V0": {"avg": 15.17, "peak": 16.15, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 16.22, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 31.86, "energy_joules_est": 46.88, "sample_count": 15, "duration_seconds": 1.471}, "timestamp": "2026-01-19T14:33:34.678862"}
{"image_index": 122, "image_name": "000000012667.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012667.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1782.634, "latencies_ms": [1782.634], "images_per_second": 0.561, "prompt_tokens": 1109, "response_tokens_est": 27, "n_tiles": 1, "output_text": " The image has a white desk with a black phone and a yellow banana on it. The lighting is natural and the banana is ripe.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26829.2, "ram_available_mb": 98943.0, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26829.2, "ram_available_mb": 98943.0, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.51, "peak": 40.56, "min": 19.32}, "VIN": {"avg": 69.39, "peak": 119.14, "min": 28.8}, "VIN_SYS_5V0": {"avg": 14.77, "peak": 16.05, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.79, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 30.51, "energy_joules_est": 54.41, "sample_count": 18, "duration_seconds": 1.783}, "timestamp": "2026-01-19T14:33:36.554047"}
{"image_index": 123, "image_name": "000000012670.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012670.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2808.874, "latencies_ms": [2808.874], "images_per_second": 0.356, "prompt_tokens": 1099, "response_tokens_est": 67, "n_tiles": 1, "output_text": " The image captures a bustling scene of a crowd of people gathered in a public space, with individuals of various ages and attires, including a woman in a green jacket and a man in a red shirt, among others, all engaged in different activities, with a prominent presence of a woman with red hair and a man with a beard.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 26829.2, "ram_available_mb": 98943.0, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 26829.2, "ram_available_mb": 98943.0, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.78, "peak": 40.57, "min": 16.95}, "VIN": {"avg": 69.65, "peak": 118.92, "min": 33.64}, "VIN_SYS_5V0": {"avg": 14.92, "peak": 15.95, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.99, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 26.78, "energy_joules_est": 75.24, "sample_count": 27, "duration_seconds": 2.81}, "timestamp": "2026-01-19T14:33:39.374591"}
{"image_index": 123, "image_name": "000000012670.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012670.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2738.845, "latencies_ms": [2738.845], "images_per_second": 0.365, "prompt_tokens": 1113, "response_tokens_est": 65, "n_tiles": 1, "output_text": " 1. People: 10\n2. Green bag: 1\n3. Red bag: 1\n4. Blue scarf: 1\n5. Red shirt: 1\n6. Green jacket: 1\n7. Black jacket: 1\n8. Gray jacket: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26829.2, "ram_available_mb": 98943.0, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26829.5, "ram_available_mb": 98942.7, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.86, "peak": 40.56, "min": 16.95}, "VIN": {"avg": 71.34, "peak": 124.13, "min": 29.81}, "VIN_SYS_5V0": {"avg": 15.1, "peak": 16.15, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 16.17, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 26.86, "energy_joules_est": 73.58, "sample_count": 27, "duration_seconds": 2.739}, "timestamp": "2026-01-19T14:33:42.185154"}
{"image_index": 123, "image_name": "000000012670.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012670.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2740.511, "latencies_ms": [2740.511], "images_per_second": 0.365, "prompt_tokens": 1117, "response_tokens_est": 64, "n_tiles": 1, "output_text": " The main objects are the crowd of people, the bus stop sign, and the trees in the background. The crowd of people is in the foreground, with the bus stop sign and trees in the background. The crowd of people is near the bus stop sign, and the trees are far away from the bus stop sign.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26829.5, "ram_available_mb": 98942.7, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 26829.5, "ram_available_mb": 98942.7, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.51, "peak": 40.16, "min": 15.77}, "VIN": {"avg": 68.18, "peak": 118.31, "min": 28.53}, "VIN_SYS_5V0": {"avg": 14.86, "peak": 16.05, "min": 12.15}, "VDD_CPU_SOC_MSS": {"avg": 15.82, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 26.51, "energy_joules_est": 72.66, "sample_count": 27, "duration_seconds": 2.741}, "timestamp": "2026-01-19T14:33:44.997834"}
{"image_index": 123, "image_name": "000000012670.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012670.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1844.61, "latencies_ms": [1844.61], "images_per_second": 0.542, "prompt_tokens": 1111, "response_tokens_est": 29, "n_tiles": 1, "output_text": " A large crowd of people are gathered in a public space, possibly a park or plaza, and they are all looking in the same direction.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26829.5, "ram_available_mb": 98942.7, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26829.5, "ram_available_mb": 98942.7, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.59, "peak": 40.16, "min": 15.77}, "VIN": {"avg": 71.11, "peak": 115.79, "min": 30.64}, "VIN_SYS_5V0": {"avg": 14.86, "peak": 16.05, "min": 12.15}, "VDD_CPU_SOC_MSS": {"avg": 15.92, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 29.59, "energy_joules_est": 54.6, "sample_count": 18, "duration_seconds": 1.845}, "timestamp": "2026-01-19T14:33:46.874695"}
{"image_index": 123, "image_name": "000000012670.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012670.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1566.338, "latencies_ms": [1566.338], "images_per_second": 0.638, "prompt_tokens": 1109, "response_tokens_est": 19, "n_tiles": 1, "output_text": " The image is taken during the day with natural lighting, and the colors are vibrant and varied.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26829.5, "ram_available_mb": 98942.7, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 26829.5, "ram_available_mb": 98942.7, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.24, "peak": 40.57, "min": 18.13}, "VIN": {"avg": 72.54, "peak": 117.51, "min": 28.04}, "VIN_SYS_5V0": {"avg": 15.0, "peak": 16.15, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 16.09, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 31.24, "energy_joules_est": 48.95, "sample_count": 16, "duration_seconds": 1.567}, "timestamp": "2026-01-19T14:33:48.541500"}
{"image_index": 124, "image_name": "000000012748.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012748.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1668.909, "latencies_ms": [1668.909], "images_per_second": 0.599, "prompt_tokens": 1100, "response_tokens_est": 23, "n_tiles": 1, "output_text": " A man wearing a red shirt is holding a baby in his arms while a horse nuzzles the baby's hand.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 26829.5, "ram_available_mb": 98942.7, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26829.5, "ram_available_mb": 98942.7, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.52, "peak": 40.57, "min": 18.14}, "VIN": {"avg": 77.77, "peak": 121.79, "min": 27.56}, "VIN_SYS_5V0": {"avg": 14.74, "peak": 16.05, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.84, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 30.52, "energy_joules_est": 50.96, "sample_count": 17, "duration_seconds": 1.67}, "timestamp": "2026-01-19T14:33:50.319643"}
{"image_index": 124, "image_name": "000000012748.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012748.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2773.005, "latencies_ms": [2773.005], "images_per_second": 0.361, "prompt_tokens": 1114, "response_tokens_est": 66, "n_tiles": 1, "output_text": " 1. horse: 1\n2. man: 1\n3. baby: 1\n4. man's shirt: 1\n5. man's pants: 1\n6. man's shoes: 1\n7. man's hair: 1\n8. man's face: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26829.5, "ram_available_mb": 98942.7, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 26829.5, "ram_available_mb": 98942.7, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.75, "peak": 40.16, "min": 17.35}, "VIN": {"avg": 69.37, "peak": 127.55, "min": 29.78}, "VIN_SYS_5V0": {"avg": 14.84, "peak": 16.05, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 16.0, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 26.75, "energy_joules_est": 74.19, "sample_count": 27, "duration_seconds": 2.773}, "timestamp": "2026-01-19T14:33:53.120051"}
{"image_index": 124, "image_name": "000000012748.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012748.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2136.887, "latencies_ms": [2136.887], "images_per_second": 0.468, "prompt_tokens": 1118, "response_tokens_est": 42, "n_tiles": 1, "output_text": " The man is standing to the left of the horse, with the baby in his arms positioned in the foreground. The horse is in the background, with the man and baby being the closest objects to the camera.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26829.5, "ram_available_mb": 98942.7, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26829.5, "ram_available_mb": 98942.7, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.62, "peak": 39.77, "min": 16.55}, "VIN": {"avg": 74.86, "peak": 125.1, "min": 29.32}, "VIN_SYS_5V0": {"avg": 15.13, "peak": 16.15, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 16.23, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 28.62, "energy_joules_est": 61.17, "sample_count": 21, "duration_seconds": 2.137}, "timestamp": "2026-01-19T14:33:55.307959"}
{"image_index": 124, "image_name": "000000012748.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012748.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1456.896, "latencies_ms": [1456.896], "images_per_second": 0.686, "prompt_tokens": 1112, "response_tokens_est": 15, "n_tiles": 1, "output_text": " A man in a red shirt is holding a baby and feeding a horse.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26829.5, "ram_available_mb": 98942.7, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26830.9, "ram_available_mb": 98941.2, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.54, "peak": 39.78, "min": 16.55}, "VIN": {"avg": 80.84, "peak": 121.27, "min": 37.19}, "VIN_SYS_5V0": {"avg": 15.08, "peak": 16.26, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 16.31, "peak": 17.71, "min": 13.79}}, "power_watts_avg": 31.54, "energy_joules_est": 45.97, "sample_count": 14, "duration_seconds": 1.457}, "timestamp": "2026-01-19T14:33:56.767932"}
{"image_index": 124, "image_name": "000000012748.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012748.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2587.103, "latencies_ms": [2587.103], "images_per_second": 0.387, "prompt_tokens": 1110, "response_tokens_est": 60, "n_tiles": 1, "output_text": " The image features a man wearing a red shirt and a baby in his arms. The man is holding the baby in front of a brown horse. The horse is standing in a stable with a brick wall in the background. The lighting in the image is natural, and the weather appears to be sunny.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26830.9, "ram_available_mb": 98941.2, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 26830.9, "ram_available_mb": 98941.2, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.38, "peak": 40.56, "min": 19.7}, "VIN": {"avg": 65.77, "peak": 104.52, "min": 29.74}, "VIN_SYS_5V0": {"avg": 15.25, "peak": 16.26, "min": 13.59}, "VDD_CPU_SOC_MSS": {"avg": 16.47, "peak": 17.71, "min": 14.96}}, "power_watts_avg": 28.38, "energy_joules_est": 73.43, "sample_count": 25, "duration_seconds": 2.587}, "timestamp": "2026-01-19T14:33:59.369546"}
{"image_index": 125, "image_name": "000000013004.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013004.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1368.692, "latencies_ms": [1368.692], "images_per_second": 0.731, "prompt_tokens": 1100, "response_tokens_est": 12, "n_tiles": 1, "output_text": " A white plate with a banana and peanut butter on it.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 26830.9, "ram_available_mb": 98941.2, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.5, "ram_used_mb": 26830.9, "ram_available_mb": 98941.2, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.71, "peak": 39.78, "min": 17.35}, "VIN": {"avg": 73.88, "peak": 121.5, "min": 30.47}, "VIN_SYS_5V0": {"avg": 15.17, "peak": 16.15, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 16.25, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 31.71, "energy_joules_est": 43.42, "sample_count": 14, "duration_seconds": 1.369}, "timestamp": "2026-01-19T14:34:00.845426"}
{"image_index": 125, "image_name": "000000013004.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013004.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1473.903, "latencies_ms": [1473.903], "images_per_second": 0.678, "prompt_tokens": 1114, "response_tokens_est": 15, "n_tiles": 1, "output_text": " banana: 1, plate: 1, peanut butter: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26830.9, "ram_available_mb": 98941.2, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26830.9, "ram_available_mb": 98941.2, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.96, "peak": 40.57, "min": 20.11}, "VIN": {"avg": 76.37, "peak": 127.19, "min": 28.38}, "VIN_SYS_5V0": {"avg": 14.69, "peak": 16.05, "min": 11.87}, "VDD_CPU_SOC_MSS": {"avg": 15.83, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 31.96, "energy_joules_est": 47.12, "sample_count": 15, "duration_seconds": 1.474}, "timestamp": "2026-01-19T14:34:02.413022"}
{"image_index": 125, "image_name": "000000013004.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013004.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1988.239, "latencies_ms": [1988.239], "images_per_second": 0.503, "prompt_tokens": 1118, "response_tokens_est": 35, "n_tiles": 1, "output_text": " The banana is on the left side of the plate, and the peanut butter is in the center. The plate is in the foreground, and the table is in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26830.9, "ram_available_mb": 98941.2, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26830.9, "ram_available_mb": 98941.2, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.52, "peak": 40.56, "min": 18.52}, "VIN": {"avg": 74.42, "peak": 128.05, "min": 27.29}, "VIN_SYS_5V0": {"avg": 14.82, "peak": 16.15, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.96, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 29.52, "energy_joules_est": 58.7, "sample_count": 20, "duration_seconds": 1.989}, "timestamp": "2026-01-19T14:34:04.496529"}
{"image_index": 125, "image_name": "000000013004.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013004.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1369.455, "latencies_ms": [1369.455], "images_per_second": 0.73, "prompt_tokens": 1112, "response_tokens_est": 13, "n_tiles": 1, "output_text": " A banana and peanut butter are on a plate on a table.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26830.9, "ram_available_mb": 98941.2, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 26830.9, "ram_available_mb": 98941.2, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.38, "peak": 39.39, "min": 17.35}, "VIN": {"avg": 69.41, "peak": 103.07, "min": 27.49}, "VIN_SYS_5V0": {"avg": 14.92, "peak": 16.15, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 16.14, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 31.38, "energy_joules_est": 43.0, "sample_count": 14, "duration_seconds": 1.37}, "timestamp": "2026-01-19T14:34:05.955372"}
{"image_index": 125, "image_name": "000000013004.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013004.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1733.619, "latencies_ms": [1733.619], "images_per_second": 0.577, "prompt_tokens": 1110, "response_tokens_est": 24, "n_tiles": 1, "output_text": " The image shows a white plate with a banana and peanut butter on it. The plate is placed on a wooden table.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26830.9, "ram_available_mb": 98941.2, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26830.9, "ram_available_mb": 98941.3, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.07, "peak": 40.56, "min": 20.11}, "VIN": {"avg": 78.21, "peak": 127.71, "min": 30.4}, "VIN_SYS_5V0": {"avg": 14.85, "peak": 16.05, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.93, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 31.07, "energy_joules_est": 53.87, "sample_count": 17, "duration_seconds": 1.734}, "timestamp": "2026-01-19T14:34:07.726028"}
{"image_index": 126, "image_name": "000000013177.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013177.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1648.291, "latencies_ms": [1648.291], "images_per_second": 0.607, "prompt_tokens": 1099, "response_tokens_est": 22, "n_tiles": 1, "output_text": " A man in a green shirt and glasses is working on a bicycle wheel on the ground next to a motorcycle.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 26830.9, "ram_available_mb": 98941.3, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26831.7, "ram_available_mb": 98940.5, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.37, "peak": 40.56, "min": 18.52}, "VIN": {"avg": 72.33, "peak": 102.6, "min": 28.93}, "VIN_SYS_5V0": {"avg": 15.03, "peak": 16.05, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 16.22, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 31.37, "energy_joules_est": 51.72, "sample_count": 16, "duration_seconds": 1.649}, "timestamp": "2026-01-19T14:34:09.409823"}
{"image_index": 126, "image_name": "000000013177.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013177.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2547.487, "latencies_ms": [2547.487], "images_per_second": 0.393, "prompt_tokens": 1113, "response_tokens_est": 57, "n_tiles": 1, "output_text": " 1. man: 1\n2. motorcycle: 1\n3. bicycle: 1\n4. wheel: 1\n5. tool: 1\n6. tire: 1\n7. chain: 1\n8. chain link: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26831.7, "ram_available_mb": 98940.5, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 26831.9, "ram_available_mb": 98940.3, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.86, "peak": 40.97, "min": 18.52}, "VIN": {"avg": 68.09, "peak": 107.17, "min": 28.83}, "VIN_SYS_5V0": {"avg": 15.06, "peak": 16.15, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 16.28, "peak": 17.71, "min": 14.18}}, "power_watts_avg": 27.86, "energy_joules_est": 70.99, "sample_count": 25, "duration_seconds": 2.548}, "timestamp": "2026-01-19T14:34:12.019599"}
{"image_index": 126, "image_name": "000000013177.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013177.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2203.423, "latencies_ms": [2203.423], "images_per_second": 0.454, "prompt_tokens": 1117, "response_tokens_est": 43, "n_tiles": 1, "output_text": " The man is kneeling on the ground, which is in the foreground of the image, and the motorcycle is in the background. The motorcycle is to the left of the man, and the bicycle is behind him.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26831.9, "ram_available_mb": 98940.3, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26832.1, "ram_available_mb": 98940.0, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.12, "peak": 40.18, "min": 15.77}, "VIN": {"avg": 72.4, "peak": 122.72, "min": 28.77}, "VIN_SYS_5V0": {"avg": 14.88, "peak": 16.05, "min": 12.25}, "VDD_CPU_SOC_MSS": {"avg": 16.02, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 28.12, "energy_joules_est": 61.97, "sample_count": 22, "duration_seconds": 2.204}, "timestamp": "2026-01-19T14:34:14.296951"}
{"image_index": 126, "image_name": "000000013177.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013177.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1487.448, "latencies_ms": [1487.448], "images_per_second": 0.672, "prompt_tokens": 1111, "response_tokens_est": 15, "n_tiles": 1, "output_text": " A man is fixing a bicycle wheel on the ground next to a motorcycle.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26832.1, "ram_available_mb": 98940.0, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26832.1, "ram_available_mb": 98940.0, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.96, "peak": 40.18, "min": 16.16}, "VIN": {"avg": 78.31, "peak": 119.76, "min": 28.13}, "VIN_SYS_5V0": {"avg": 14.79, "peak": 16.05, "min": 12.15}, "VDD_CPU_SOC_MSS": {"avg": 15.85, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 30.96, "energy_joules_est": 46.06, "sample_count": 15, "duration_seconds": 1.488}, "timestamp": "2026-01-19T14:34:15.863991"}
{"image_index": 126, "image_name": "000000013177.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013177.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2083.878, "latencies_ms": [2083.878], "images_per_second": 0.48, "prompt_tokens": 1109, "response_tokens_est": 38, "n_tiles": 1, "output_text": " The image features a man in a green shirt and blue pants working on a bicycle wheel, with a motorcycle parked nearby. The lighting is natural, and the weather appears to be overcast.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26832.1, "ram_available_mb": 98940.0, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26832.1, "ram_available_mb": 98940.0, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.74, "peak": 40.57, "min": 19.71}, "VIN": {"avg": 75.98, "peak": 119.99, "min": 37.03}, "VIN_SYS_5V0": {"avg": 14.91, "peak": 16.05, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.98, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 29.74, "energy_joules_est": 61.99, "sample_count": 20, "duration_seconds": 2.084}, "timestamp": "2026-01-19T14:34:17.955321"}
{"image_index": 127, "image_name": "000000013201.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013201.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1439.31, "latencies_ms": [1439.31], "images_per_second": 0.695, "prompt_tokens": 1100, "response_tokens_est": 15, "n_tiles": 1, "output_text": " A man with long hair is skateboarding on a ramp in a park.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 26832.1, "ram_available_mb": 98940.0, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26832.1, "ram_available_mb": 98940.0, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.3, "peak": 40.18, "min": 18.13}, "VIN": {"avg": 84.17, "peak": 127.08, "min": 29.3}, "VIN_SYS_5V0": {"avg": 15.36, "peak": 16.26, "min": 13.29}, "VDD_CPU_SOC_MSS": {"avg": 16.48, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 32.3, "energy_joules_est": 46.51, "sample_count": 14, "duration_seconds": 1.44}, "timestamp": "2026-01-19T14:34:19.424463"}
{"image_index": 127, "image_name": "000000013201.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013201.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2534.84, "latencies_ms": [2534.84], "images_per_second": 0.395, "prompt_tokens": 1114, "response_tokens_est": 57, "n_tiles": 1, "output_text": " 1. skateboard: 1\n2. person: 1\n3. grass: 1\n4. fence: 1\n5. building: 1\n6. tree: 1\n7. sky: 1\n8. clouds: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26832.1, "ram_available_mb": 98940.0, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 26832.1, "ram_available_mb": 98940.0, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.07, "peak": 40.95, "min": 18.13}, "VIN": {"avg": 69.19, "peak": 110.91, "min": 30.23}, "VIN_SYS_5V0": {"avg": 15.08, "peak": 16.15, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 16.2, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 28.07, "energy_joules_est": 71.16, "sample_count": 25, "duration_seconds": 2.535}, "timestamp": "2026-01-19T14:34:22.034120"}
{"image_index": 127, "image_name": "000000013201.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013201.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2580.859, "latencies_ms": [2580.859], "images_per_second": 0.387, "prompt_tokens": 1118, "response_tokens_est": 58, "n_tiles": 1, "output_text": " The skateboarder is in the foreground, performing a trick on a ramp. The ramp is located in the middle ground, with the skateboarder's trick taking place in the foreground. The background features a fence and trees, providing a natural setting for the skateboarder's performance.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26832.1, "ram_available_mb": 98940.0, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 26832.1, "ram_available_mb": 98940.0, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.87, "peak": 40.16, "min": 15.77}, "VIN": {"avg": 68.6, "peak": 125.41, "min": 28.71}, "VIN_SYS_5V0": {"avg": 14.92, "peak": 16.05, "min": 12.15}, "VDD_CPU_SOC_MSS": {"avg": 16.01, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 26.87, "energy_joules_est": 69.36, "sample_count": 25, "duration_seconds": 2.581}, "timestamp": "2026-01-19T14:34:24.648472"}
{"image_index": 127, "image_name": "000000013201.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013201.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1457.286, "latencies_ms": [1457.286], "images_per_second": 0.686, "prompt_tokens": 1112, "response_tokens_est": 15, "n_tiles": 1, "output_text": " A skateboarder is performing a trick on a ramp in a park.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26832.1, "ram_available_mb": 98940.0, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26832.9, "ram_available_mb": 98939.3, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.43, "peak": 39.77, "min": 16.16}, "VIN": {"avg": 74.35, "peak": 102.42, "min": 32.97}, "VIN_SYS_5V0": {"avg": 15.19, "peak": 16.26, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 16.42, "peak": 17.71, "min": 14.18}}, "power_watts_avg": 31.43, "energy_joules_est": 45.82, "sample_count": 14, "duration_seconds": 1.458}, "timestamp": "2026-01-19T14:34:26.109871"}
{"image_index": 127, "image_name": "000000013201.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013201.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2145.483, "latencies_ms": [2145.483], "images_per_second": 0.466, "prompt_tokens": 1110, "response_tokens_est": 42, "n_tiles": 1, "output_text": " The image features a skateboarder performing a trick on a ramp, with the skateboarder wearing a black t-shirt and black pants. The lighting is natural, and the weather appears to be clear.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26832.9, "ram_available_mb": 98939.3, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26833.1, "ram_available_mb": 98939.1, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.81, "peak": 40.56, "min": 19.7}, "VIN": {"avg": 76.09, "peak": 122.57, "min": 30.4}, "VIN_SYS_5V0": {"avg": 15.27, "peak": 16.26, "min": 13.49}, "VDD_CPU_SOC_MSS": {"avg": 16.48, "peak": 17.71, "min": 14.57}}, "power_watts_avg": 29.81, "energy_joules_est": 63.97, "sample_count": 21, "duration_seconds": 2.146}, "timestamp": "2026-01-19T14:34:28.304001"}
{"image_index": 128, "image_name": "000000013291.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013291.jpg", "image_width": 500, "image_height": 335, "image_resolution": "500x335", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1799.205, "latencies_ms": [1799.205], "images_per_second": 0.556, "prompt_tokens": 1099, "response_tokens_est": 28, "n_tiles": 1, "output_text": " A group of five people are posing for a photo on a grassy field at sunset, each holding a frisbee with different colored designs.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 26833.1, "ram_available_mb": 98939.1, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26833.1, "ram_available_mb": 98939.1, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.85, "peak": 40.56, "min": 16.56}, "VIN": {"avg": 72.72, "peak": 108.58, "min": 29.7}, "VIN_SYS_5V0": {"avg": 15.0, "peak": 16.05, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 16.14, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 29.85, "energy_joules_est": 53.72, "sample_count": 18, "duration_seconds": 1.8}, "timestamp": "2026-01-19T14:34:30.185932"}
{"image_index": 128, "image_name": "000000013291.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013291.jpg", "image_width": 500, "image_height": 335, "image_resolution": "500x335", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1319.364, "latencies_ms": [1319.364], "images_per_second": 0.758, "prompt_tokens": 1113, "response_tokens_est": 9, "n_tiles": 1, "output_text": " 1. frisbee: 5", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26833.1, "ram_available_mb": 98939.1, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 26833.1, "ram_available_mb": 98939.1, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.21, "peak": 40.56, "min": 16.95}, "VIN": {"avg": 77.62, "peak": 120.87, "min": 29.45}, "VIN_SYS_5V0": {"avg": 14.82, "peak": 16.05, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 16.05, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 32.21, "energy_joules_est": 42.51, "sample_count": 13, "duration_seconds": 1.32}, "timestamp": "2026-01-19T14:34:31.551020"}
{"image_index": 128, "image_name": "000000013291.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013291.jpg", "image_width": 500, "image_height": 335, "image_resolution": "500x335", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3141.331, "latencies_ms": [3141.331], "images_per_second": 0.318, "prompt_tokens": 1117, "response_tokens_est": 79, "n_tiles": 1, "output_text": " The frisbee is held by the person on the left, the person in the middle is holding a frisbee, and the person on the right is holding a frisbee. The frisbee is in the foreground, and the person on the left is in the foreground. The person in the middle is in the middle, and the person on the right is in the background.", "error": null, "sys_before": {"cpu_percent": 40.0, "ram_used_mb": 26833.1, "ram_available_mb": 98939.1, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 26833.1, "ram_available_mb": 98939.1, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.75, "peak": 40.97, "min": 17.73}, "VIN": {"avg": 70.79, "peak": 120.62, "min": 28.57}, "VIN_SYS_5V0": {"avg": 14.98, "peak": 16.15, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 16.07, "peak": 17.32, "min": 13.78}}, "power_watts_avg": 26.75, "energy_joules_est": 84.04, "sample_count": 31, "duration_seconds": 3.142}, "timestamp": "2026-01-19T14:34:34.770956"}
{"image_index": 128, "image_name": "000000013291.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013291.jpg", "image_width": 500, "image_height": 335, "image_resolution": "500x335", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1434.099, "latencies_ms": [1434.099], "images_per_second": 0.697, "prompt_tokens": 1111, "response_tokens_est": 13, "n_tiles": 1, "output_text": " Five people are posing for a picture on a field at sunset.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26833.1, "ram_available_mb": 98939.1, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 26833.1, "ram_available_mb": 98939.1, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.07, "peak": 40.18, "min": 15.38}, "VIN": {"avg": 74.01, "peak": 123.13, "min": 28.84}, "VIN_SYS_5V0": {"avg": 14.77, "peak": 16.05, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.92, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 31.07, "energy_joules_est": 44.57, "sample_count": 14, "duration_seconds": 1.434}, "timestamp": "2026-01-19T14:34:36.238962"}
{"image_index": 128, "image_name": "000000013291.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013291.jpg", "image_width": 500, "image_height": 335, "image_resolution": "500x335", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2519.559, "latencies_ms": [2519.559], "images_per_second": 0.397, "prompt_tokens": 1109, "response_tokens_est": 56, "n_tiles": 1, "output_text": " The image features a group of five people posing for a photo on a grassy field at sunset. The sky is painted with hues of orange and blue, and the sun is setting behind the horizon. The people are holding frisbees, which are white with green and red designs.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26833.1, "ram_available_mb": 98939.1, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26833.1, "ram_available_mb": 98939.1, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.24, "peak": 40.95, "min": 18.13}, "VIN": {"avg": 70.03, "peak": 97.75, "min": 28.98}, "VIN_SYS_5V0": {"avg": 15.05, "peak": 16.15, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 16.06, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 28.24, "energy_joules_est": 71.16, "sample_count": 25, "duration_seconds": 2.52}, "timestamp": "2026-01-19T14:34:38.838790"}
{"image_index": 129, "image_name": "000000013348.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013348.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2051.724, "latencies_ms": [2051.724], "images_per_second": 0.487, "prompt_tokens": 1099, "response_tokens_est": 37, "n_tiles": 1, "output_text": " A large white airplane with red and black accents is parked at an airport gate, surrounded by a few vehicles and equipment, with a clear blue sky and a few clouds in the background.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 26833.1, "ram_available_mb": 98939.1, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26833.1, "ram_available_mb": 98939.1, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.58, "peak": 40.18, "min": 15.38}, "VIN": {"avg": 69.43, "peak": 104.34, "min": 28.83}, "VIN_SYS_5V0": {"avg": 14.83, "peak": 16.05, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.85, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 28.58, "energy_joules_est": 58.65, "sample_count": 20, "duration_seconds": 2.052}, "timestamp": "2026-01-19T14:34:40.933454"}
{"image_index": 129, "image_name": "000000013348.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013348.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2014.589, "latencies_ms": [2014.589], "images_per_second": 0.496, "prompt_tokens": 1113, "response_tokens_est": 36, "n_tiles": 1, "output_text": " airplane: 1, clouds: 1, runway: 1, terminal: 1, luggage cart: 1, palm tree: 1, sky: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26833.1, "ram_available_mb": 98939.1, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26834.3, "ram_available_mb": 98937.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.15, "peak": 40.56, "min": 17.34}, "VIN": {"avg": 70.89, "peak": 120.34, "min": 29.04}, "VIN_SYS_5V0": {"avg": 15.01, "peak": 16.15, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 16.12, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 29.15, "energy_joules_est": 58.74, "sample_count": 20, "duration_seconds": 2.015}, "timestamp": "2026-01-19T14:34:43.021145"}
{"image_index": 129, "image_name": "000000013348.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013348.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2370.183, "latencies_ms": [2370.183], "images_per_second": 0.422, "prompt_tokens": 1117, "response_tokens_est": 49, "n_tiles": 1, "output_text": " The airplane is positioned on the left side of the image, with the terminal building located in the background. The airplane is situated in the foreground, with the terminal building in the background. The airplane is closer to the viewer than the terminal building.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26834.3, "ram_available_mb": 98937.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 26834.3, "ram_available_mb": 98937.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.85, "peak": 40.16, "min": 16.56}, "VIN": {"avg": 70.52, "peak": 121.39, "min": 29.18}, "VIN_SYS_5V0": {"avg": 14.9, "peak": 16.05, "min": 12.15}, "VDD_CPU_SOC_MSS": {"avg": 16.04, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 27.85, "energy_joules_est": 66.02, "sample_count": 23, "duration_seconds": 2.37}, "timestamp": "2026-01-19T14:34:45.417303"}
{"image_index": 129, "image_name": "000000013348.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013348.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2762.67, "latencies_ms": [2762.67], "images_per_second": 0.362, "prompt_tokens": 1111, "response_tokens_est": 65, "n_tiles": 1, "output_text": " The image captures a moment at an airport where a large airplane, adorned with the red and white logo of Japan Airlines, is parked at a gate. The sky above is a clear blue, dotted with fluffy white clouds, and the ground below is marked with yellow arrows and lines, indicating the path for the airplane's movement.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26834.3, "ram_available_mb": 98937.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 26834.3, "ram_available_mb": 98937.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.67, "peak": 40.56, "min": 16.95}, "VIN": {"avg": 69.83, "peak": 110.31, "min": 28.54}, "VIN_SYS_5V0": {"avg": 15.1, "peak": 16.15, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 16.23, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 26.67, "energy_joules_est": 73.69, "sample_count": 27, "duration_seconds": 2.763}, "timestamp": "2026-01-19T14:34:48.228692"}
{"image_index": 129, "image_name": "000000013348.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013348.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1584.216, "latencies_ms": [1584.216], "images_per_second": 0.631, "prompt_tokens": 1109, "response_tokens_est": 19, "n_tiles": 1, "output_text": " The airplane is white with red and black accents, and the sky is blue with white clouds.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26834.3, "ram_available_mb": 98937.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 26834.6, "ram_available_mb": 98937.6, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.44, "peak": 40.18, "min": 15.77}, "VIN": {"avg": 66.36, "peak": 124.68, "min": 27.7}, "VIN_SYS_5V0": {"avg": 14.88, "peak": 16.05, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.99, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 30.44, "energy_joules_est": 48.24, "sample_count": 16, "duration_seconds": 1.585}, "timestamp": "2026-01-19T14:34:49.899063"}
{"image_index": 130, "image_name": "000000013546.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013546.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1567.195, "latencies_ms": [1567.195], "images_per_second": 0.638, "prompt_tokens": 1099, "response_tokens_est": 19, "n_tiles": 1, "output_text": " A man wearing a yellow shirt and black pants is skateboarding on a ramp in a park.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26834.6, "ram_available_mb": 98937.6, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 26834.6, "ram_available_mb": 98937.6, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.32, "peak": 40.97, "min": 18.53}, "VIN": {"avg": 74.89, "peak": 118.07, "min": 29.08}, "VIN_SYS_5V0": {"avg": 14.78, "peak": 16.05, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.89, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 31.32, "energy_joules_est": 49.1, "sample_count": 16, "duration_seconds": 1.568}, "timestamp": "2026-01-19T14:34:51.575654"}
{"image_index": 130, "image_name": "000000013546.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013546.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2602.796, "latencies_ms": [2602.796], "images_per_second": 0.384, "prompt_tokens": 1113, "response_tokens_est": 58, "n_tiles": 1, "output_text": " 1. person: 1\n2. skateboard: 1\n3. bench: 1\n4. trash can: 1\n5. fence: 1\n6. bench: 1\n7. person: 1\n8. bench: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26834.6, "ram_available_mb": 98937.6, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 26834.6, "ram_available_mb": 98937.6, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.5, "peak": 40.56, "min": 18.14}, "VIN": {"avg": 70.47, "peak": 106.06, "min": 33.02}, "VIN_SYS_5V0": {"avg": 14.88, "peak": 15.95, "min": 11.87}, "VDD_CPU_SOC_MSS": {"avg": 15.84, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 27.5, "energy_joules_est": 71.59, "sample_count": 25, "duration_seconds": 2.603}, "timestamp": "2026-01-19T14:34:54.184218"}
{"image_index": 130, "image_name": "000000013546.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013546.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2021.699, "latencies_ms": [2021.699], "images_per_second": 0.495, "prompt_tokens": 1117, "response_tokens_est": 37, "n_tiles": 1, "output_text": " The skateboarder is in the foreground, performing a trick on a ramp, while the bench is in the background. The skateboarder is closer to the camera than the bench.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26834.6, "ram_available_mb": 98937.6, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26834.6, "ram_available_mb": 98937.6, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.13, "peak": 40.57, "min": 16.95}, "VIN": {"avg": 73.68, "peak": 116.45, "min": 29.17}, "VIN_SYS_5V0": {"avg": 15.23, "peak": 16.15, "min": 13.29}, "VDD_CPU_SOC_MSS": {"avg": 16.18, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 29.13, "energy_joules_est": 58.91, "sample_count": 20, "duration_seconds": 2.022}, "timestamp": "2026-01-19T14:34:56.262961"}
{"image_index": 130, "image_name": "000000013546.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013546.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1477.987, "latencies_ms": [1477.987], "images_per_second": 0.677, "prompt_tokens": 1111, "response_tokens_est": 15, "n_tiles": 1, "output_text": " A skateboarder is performing a trick on a ramp in a park.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26834.6, "ram_available_mb": 98937.6, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 26834.5, "ram_available_mb": 98937.7, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.25, "peak": 40.56, "min": 16.95}, "VIN": {"avg": 76.97, "peak": 125.84, "min": 29.84}, "VIN_SYS_5V0": {"avg": 14.85, "peak": 16.05, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.93, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 31.25, "energy_joules_est": 46.21, "sample_count": 15, "duration_seconds": 1.479}, "timestamp": "2026-01-19T14:34:57.834309"}
{"image_index": 130, "image_name": "000000013546.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013546.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2735.979, "latencies_ms": [2735.979], "images_per_second": 0.365, "prompt_tokens": 1109, "response_tokens_est": 63, "n_tiles": 1, "output_text": " The image features a skateboarder performing a trick on a concrete ramp, with a vibrant green grass field in the background. The skateboarder is wearing a yellow shirt and black pants, and the ramp is adorned with graffiti. The lighting suggests it is a sunny day, and the weather appears to be clear.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26834.5, "ram_available_mb": 98937.7, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26834.5, "ram_available_mb": 98937.7, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.11, "peak": 40.97, "min": 17.73}, "VIN": {"avg": 67.45, "peak": 119.68, "min": 29.55}, "VIN_SYS_5V0": {"avg": 14.85, "peak": 16.05, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.75, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 27.11, "energy_joules_est": 74.18, "sample_count": 27, "duration_seconds": 2.736}, "timestamp": "2026-01-19T14:35:00.642141"}
{"image_index": 131, "image_name": "000000013597.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013597.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1553.856, "latencies_ms": [1553.856], "images_per_second": 0.644, "prompt_tokens": 1099, "response_tokens_est": 18, "n_tiles": 1, "output_text": " A slice of chocolate cake with caramel drizzle sits on a white plate with gold designs.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26834.5, "ram_available_mb": 98937.7, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 26834.5, "ram_available_mb": 98937.7, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.73, "peak": 40.16, "min": 15.38}, "VIN": {"avg": 76.92, "peak": 131.81, "min": 30.45}, "VIN_SYS_5V0": {"avg": 14.79, "peak": 16.05, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 15.88, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 30.73, "energy_joules_est": 47.78, "sample_count": 15, "duration_seconds": 1.555}, "timestamp": "2026-01-19T14:35:02.224065"}
{"image_index": 131, "image_name": "000000013597.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013597.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2698.43, "latencies_ms": [2698.43], "images_per_second": 0.371, "prompt_tokens": 1113, "response_tokens_est": 60, "n_tiles": 1, "output_text": " 1. plate: 1\n2. cake: 1\n3. syrup: 1\n4. chocolate: 1\n5. chocolate syrup: 1\n6. caramel: 1\n7. caramel sauce: 1\n8. chocolate drizzle: 1", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26834.5, "ram_available_mb": 98937.7, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26834.5, "ram_available_mb": 98937.7, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.23, "peak": 40.56, "min": 16.95}, "VIN": {"avg": 64.51, "peak": 103.84, "min": 28.54}, "VIN_SYS_5V0": {"avg": 14.98, "peak": 16.15, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 15.91, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 27.23, "energy_joules_est": 73.49, "sample_count": 27, "duration_seconds": 2.699}, "timestamp": "2026-01-19T14:35:05.025389"}
{"image_index": 131, "image_name": "000000013597.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013597.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2032.079, "latencies_ms": [2032.079], "images_per_second": 0.492, "prompt_tokens": 1117, "response_tokens_est": 36, "n_tiles": 1, "output_text": " The slice of chocolate cake is positioned in the foreground, with the plate and the background being blurred. The cake is placed on the plate, which is resting on a wooden surface.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26834.5, "ram_available_mb": 98937.7, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26834.5, "ram_available_mb": 98937.7, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.64, "peak": 40.18, "min": 14.59}, "VIN": {"avg": 76.52, "peak": 118.9, "min": 28.59}, "VIN_SYS_5V0": {"avg": 14.78, "peak": 16.05, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 15.73, "peak": 17.32, "min": 12.61}}, "power_watts_avg": 28.64, "energy_joules_est": 58.21, "sample_count": 20, "duration_seconds": 2.032}, "timestamp": "2026-01-19T14:35:07.095530"}
{"image_index": 131, "image_name": "000000013597.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013597.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1550.152, "latencies_ms": [1550.152], "images_per_second": 0.645, "prompt_tokens": 1111, "response_tokens_est": 18, "n_tiles": 1, "output_text": " A slice of chocolate cake with caramel drizzle is on a white plate with gold designs.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26834.5, "ram_available_mb": 98937.7, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 26834.5, "ram_available_mb": 98937.7, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.51, "peak": 40.56, "min": 17.34}, "VIN": {"avg": 70.26, "peak": 119.14, "min": 29.06}, "VIN_SYS_5V0": {"avg": 15.07, "peak": 16.15, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 16.22, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 31.51, "energy_joules_est": 48.85, "sample_count": 15, "duration_seconds": 1.55}, "timestamp": "2026-01-19T14:35:08.659579"}
{"image_index": 131, "image_name": "000000013597.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013597.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1926.59, "latencies_ms": [1926.59], "images_per_second": 0.519, "prompt_tokens": 1109, "response_tokens_est": 33, "n_tiles": 1, "output_text": " The slice of chocolate cake is on a white plate with gold designs, and the plate is on a wooden table. The lighting is warm and the cake is shiny.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26834.5, "ram_available_mb": 98937.7, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26834.5, "ram_available_mb": 98937.7, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.56, "peak": 40.95, "min": 20.49}, "VIN": {"avg": 75.18, "peak": 119.47, "min": 30.31}, "VIN_SYS_5V0": {"avg": 15.19, "peak": 16.15, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 16.35, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 30.56, "energy_joules_est": 58.89, "sample_count": 19, "duration_seconds": 1.927}, "timestamp": "2026-01-19T14:35:10.635324"}
{"image_index": 132, "image_name": "000000013659.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013659.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1527.968, "latencies_ms": [1527.968], "images_per_second": 0.654, "prompt_tokens": 1099, "response_tokens_est": 18, "n_tiles": 1, "output_text": " A man is sitting at a desk with a laptop and a computer in front of him.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 26834.5, "ram_available_mb": 98937.7, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26835.0, "ram_available_mb": 98937.2, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.54, "peak": 40.16, "min": 17.73}, "VIN": {"avg": 78.26, "peak": 113.08, "min": 30.33}, "VIN_SYS_5V0": {"avg": 15.06, "peak": 16.05, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 16.19, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 31.54, "energy_joules_est": 48.21, "sample_count": 15, "duration_seconds": 1.529}, "timestamp": "2026-01-19T14:35:12.210423"}
{"image_index": 132, "image_name": "000000013659.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013659.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2716.161, "latencies_ms": [2716.161], "images_per_second": 0.368, "prompt_tokens": 1113, "response_tokens_est": 61, "n_tiles": 1, "output_text": " 1. laptop: 2\n2. chair: 3\n3. person: 2\n4. box: 1\n5. bottle: 1\n6. bottle cap: 1\n7. person's hand: 1\n8. person's leg: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26835.0, "ram_available_mb": 98937.2, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26835.2, "ram_available_mb": 98937.0, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.18, "peak": 40.95, "min": 16.95}, "VIN": {"avg": 65.3, "peak": 103.76, "min": 27.31}, "VIN_SYS_5V0": {"avg": 14.93, "peak": 16.15, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 15.94, "peak": 17.32, "min": 13.78}}, "power_watts_avg": 27.18, "energy_joules_est": 73.84, "sample_count": 27, "duration_seconds": 2.717}, "timestamp": "2026-01-19T14:35:15.022883"}
{"image_index": 132, "image_name": "000000013659.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013659.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1838.498, "latencies_ms": [1838.498], "images_per_second": 0.544, "prompt_tokens": 1117, "response_tokens_est": 29, "n_tiles": 1, "output_text": " The man is sitting in the foreground, working on his laptop. The boxes are in the background, and the people are standing around the table.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26835.2, "ram_available_mb": 98937.0, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26835.2, "ram_available_mb": 98937.0, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.37, "peak": 40.16, "min": 14.59}, "VIN": {"avg": 69.04, "peak": 124.06, "min": 29.61}, "VIN_SYS_5V0": {"avg": 14.77, "peak": 16.05, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.79, "peak": 17.32, "min": 12.61}}, "power_watts_avg": 29.37, "energy_joules_est": 54.01, "sample_count": 18, "duration_seconds": 1.839}, "timestamp": "2026-01-19T14:35:16.902684"}
{"image_index": 132, "image_name": "000000013659.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013659.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1477.635, "latencies_ms": [1477.635], "images_per_second": 0.677, "prompt_tokens": 1111, "response_tokens_est": 16, "n_tiles": 1, "output_text": " A group of people are working in a cluttered office with computers and laptops.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26835.2, "ram_available_mb": 98937.0, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 26835.2, "ram_available_mb": 98937.0, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.77, "peak": 40.16, "min": 18.13}, "VIN": {"avg": 75.72, "peak": 122.14, "min": 28.16}, "VIN_SYS_5V0": {"avg": 15.07, "peak": 16.15, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 16.14, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 31.77, "energy_joules_est": 46.95, "sample_count": 15, "duration_seconds": 1.478}, "timestamp": "2026-01-19T14:35:18.477512"}
{"image_index": 132, "image_name": "000000013659.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013659.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1672.12, "latencies_ms": [1672.12], "images_per_second": 0.598, "prompt_tokens": 1109, "response_tokens_est": 23, "n_tiles": 1, "output_text": " The room is lit with fluorescent lighting, and the walls are painted white. The floor is covered with yellow paint.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26835.2, "ram_available_mb": 98937.0, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26835.2, "ram_available_mb": 98937.0, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.0, "peak": 40.56, "min": 19.32}, "VIN": {"avg": 69.93, "peak": 88.62, "min": 27.13}, "VIN_SYS_5V0": {"avg": 14.78, "peak": 16.05, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.86, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 31.0, "energy_joules_est": 51.84, "sample_count": 17, "duration_seconds": 1.672}, "timestamp": "2026-01-19T14:35:20.251516"}
{"image_index": 133, "image_name": "000000013729.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013729.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1881.814, "latencies_ms": [1881.814], "images_per_second": 0.531, "prompt_tokens": 1099, "response_tokens_est": 31, "n_tiles": 1, "output_text": " A group of people are playing a video game in a living room, with a woman holding a Wii remote and a man holding a Wii controller.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26835.2, "ram_available_mb": 98937.0, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26834.9, "ram_available_mb": 98937.2, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.51, "peak": 40.57, "min": 17.35}, "VIN": {"avg": 76.65, "peak": 125.36, "min": 29.35}, "VIN_SYS_5V0": {"avg": 14.73, "peak": 15.95, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.75, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 29.51, "energy_joules_est": 55.55, "sample_count": 19, "duration_seconds": 1.882}, "timestamp": "2026-01-19T14:35:22.240597"}
{"image_index": 133, "image_name": "000000013729.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013729.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2607.757, "latencies_ms": [2607.757], "images_per_second": 0.383, "prompt_tokens": 1113, "response_tokens_est": 59, "n_tiles": 1, "output_text": " 1. Wii remote: 2\n2. Person: 4\n3. Chair: 1\n4. Table: 1\n5. Bottle: 1\n6. Frame: 1\n7. Window: 2\n8. Bed: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26834.9, "ram_available_mb": 98937.2, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26834.9, "ram_available_mb": 98937.2, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.26, "peak": 40.57, "min": 16.16}, "VIN": {"avg": 72.91, "peak": 119.95, "min": 32.09}, "VIN_SYS_5V0": {"avg": 14.88, "peak": 16.05, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.84, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 27.26, "energy_joules_est": 71.09, "sample_count": 25, "duration_seconds": 2.608}, "timestamp": "2026-01-19T14:35:24.855190"}
{"image_index": 133, "image_name": "000000013729.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013729.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3042.461, "latencies_ms": [3042.461], "images_per_second": 0.329, "prompt_tokens": 1117, "response_tokens_est": 77, "n_tiles": 1, "output_text": " The woman is standing to the left of the man in the blue shirt, who is standing to the right of the man in the white shirt. The woman is in the foreground, while the man in the blue shirt is in the background. The man in the green shirt is standing in the middle of the room, with the woman and the man in the white shirt to his left.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26834.9, "ram_available_mb": 98937.2, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 26834.9, "ram_available_mb": 98937.2, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.17, "peak": 39.77, "min": 17.73}, "VIN": {"avg": 68.9, "peak": 107.19, "min": 29.71}, "VIN_SYS_5V0": {"avg": 15.13, "peak": 16.26, "min": 13.29}, "VDD_CPU_SOC_MSS": {"avg": 16.14, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 26.17, "energy_joules_est": 79.63, "sample_count": 30, "duration_seconds": 3.043}, "timestamp": "2026-01-19T14:35:27.974658"}
{"image_index": 133, "image_name": "000000013729.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013729.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1449.412, "latencies_ms": [1449.412], "images_per_second": 0.69, "prompt_tokens": 1111, "response_tokens_est": 14, "n_tiles": 1, "output_text": " A group of people are playing a video game in a living room.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26834.9, "ram_available_mb": 98937.2, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 26834.9, "ram_available_mb": 98937.2, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.32, "peak": 40.18, "min": 15.38}, "VIN": {"avg": 78.27, "peak": 119.71, "min": 29.7}, "VIN_SYS_5V0": {"avg": 14.86, "peak": 16.05, "min": 12.15}, "VDD_CPU_SOC_MSS": {"avg": 15.92, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 31.32, "energy_joules_est": 45.42, "sample_count": 14, "duration_seconds": 1.45}, "timestamp": "2026-01-19T14:35:29.440038"}
{"image_index": 133, "image_name": "000000013729.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013729.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1622.373, "latencies_ms": [1622.373], "images_per_second": 0.616, "prompt_tokens": 1109, "response_tokens_est": 22, "n_tiles": 1, "output_text": " The room is well lit with natural light coming from the windows, and the carpet is a light gray color.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26834.9, "ram_available_mb": 98937.2, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 26834.9, "ram_available_mb": 98937.2, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.7, "peak": 40.57, "min": 22.86}, "VIN": {"avg": 74.98, "peak": 122.72, "min": 28.43}, "VIN_SYS_5V0": {"avg": 15.25, "peak": 16.15, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 16.34, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 32.7, "energy_joules_est": 53.07, "sample_count": 16, "duration_seconds": 1.623}, "timestamp": "2026-01-19T14:35:31.115112"}
{"image_index": 134, "image_name": "000000013774.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013774.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1611.746, "latencies_ms": [1611.746], "images_per_second": 0.62, "prompt_tokens": 1099, "response_tokens_est": 21, "n_tiles": 1, "output_text": " A person stands on a frozen lake at sunset, with a frisbee in the air above them.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 26834.9, "ram_available_mb": 98937.2, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26834.9, "ram_available_mb": 98937.2, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.42, "peak": 40.57, "min": 18.92}, "VIN": {"avg": 76.45, "peak": 117.86, "min": 28.9}, "VIN_SYS_5V0": {"avg": 15.03, "peak": 16.15, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 16.14, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 31.42, "energy_joules_est": 50.67, "sample_count": 16, "duration_seconds": 1.613}, "timestamp": "2026-01-19T14:35:32.791384"}
{"image_index": 134, "image_name": "000000013774.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013774.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2580.152, "latencies_ms": [2580.152], "images_per_second": 0.388, "prompt_tokens": 1113, "response_tokens_est": 58, "n_tiles": 1, "output_text": " 1. person: 1\n2. sun: 1\n3. sky: 2\n4. water: 1\n5. ice: 1\n6. frisbee: 1\n7. shadow: 1\n8. reflection: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26834.9, "ram_available_mb": 98937.2, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26834.9, "ram_available_mb": 98937.2, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.77, "peak": 40.57, "min": 18.91}, "VIN": {"avg": 67.38, "peak": 123.59, "min": 28.85}, "VIN_SYS_5V0": {"avg": 15.01, "peak": 16.05, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.91, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 27.77, "energy_joules_est": 71.66, "sample_count": 25, "duration_seconds": 2.581}, "timestamp": "2026-01-19T14:35:35.394539"}
{"image_index": 134, "image_name": "000000013774.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013774.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2177.419, "latencies_ms": [2177.419], "images_per_second": 0.459, "prompt_tokens": 1117, "response_tokens_est": 43, "n_tiles": 1, "output_text": " The person is standing on the left side of the image, with the sun in the center and the horizon line in the background. The person is in the foreground, with the sun and horizon line in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26834.9, "ram_available_mb": 98937.2, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26834.9, "ram_available_mb": 98937.2, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.83, "peak": 40.57, "min": 16.95}, "VIN": {"avg": 69.2, "peak": 128.26, "min": 29.55}, "VIN_SYS_5V0": {"avg": 15.15, "peak": 16.15, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 16.18, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 28.83, "energy_joules_est": 62.8, "sample_count": 21, "duration_seconds": 2.178}, "timestamp": "2026-01-19T14:35:37.585025"}
{"image_index": 134, "image_name": "000000013774.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013774.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1628.009, "latencies_ms": [1628.009], "images_per_second": 0.614, "prompt_tokens": 1111, "response_tokens_est": 22, "n_tiles": 1, "output_text": " A person is walking on a frozen lake at sunset, with a frisbee in the air above them.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26834.9, "ram_available_mb": 98937.2, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 26834.9, "ram_available_mb": 98937.2, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.37, "peak": 40.18, "min": 18.13}, "VIN": {"avg": 70.19, "peak": 105.5, "min": 28.44}, "VIN_SYS_5V0": {"avg": 15.22, "peak": 16.15, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 16.36, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 31.37, "energy_joules_est": 51.08, "sample_count": 16, "duration_seconds": 1.628}, "timestamp": "2026-01-19T14:35:39.248764"}
{"image_index": 134, "image_name": "000000013774.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013774.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2957.091, "latencies_ms": [2957.091], "images_per_second": 0.338, "prompt_tokens": 1109, "response_tokens_est": 73, "n_tiles": 1, "output_text": " The image features a serene beach scene with a person standing on the shore, a bright sun in the sky, and a red frisbee flying in the air. The colors are vibrant, with the sun casting a warm glow on the water and the person's silhouette. The sky is a clear blue, and the water is a deep blue, reflecting the sunlight.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26834.9, "ram_available_mb": 98937.2, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 26835.9, "ram_available_mb": 98936.3, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.56, "peak": 40.57, "min": 18.14}, "VIN": {"avg": 65.22, "peak": 119.62, "min": 28.67}, "VIN_SYS_5V0": {"avg": 15.03, "peak": 16.15, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 16.1, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 26.56, "energy_joules_est": 78.55, "sample_count": 29, "duration_seconds": 2.958}, "timestamp": "2026-01-19T14:35:42.270461"}
{"image_index": 135, "image_name": "000000013923.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013923.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2037.304, "latencies_ms": [2037.304], "images_per_second": 0.491, "prompt_tokens": 1099, "response_tokens_est": 37, "n_tiles": 1, "output_text": " The image depicts a modern living room with a white sofa, a dining table with chairs, a television on a stand, and a variety of decorative items on the walls and shelves.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 26835.9, "ram_available_mb": 98936.3, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26835.9, "ram_available_mb": 98936.3, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.66, "peak": 40.16, "min": 15.38}, "VIN": {"avg": 70.2, "peak": 120.37, "min": 29.39}, "VIN_SYS_5V0": {"avg": 14.89, "peak": 16.05, "min": 12.15}, "VDD_CPU_SOC_MSS": {"avg": 15.93, "peak": 17.32, "min": 13.4}}, "power_watts_avg": 28.66, "energy_joules_est": 58.41, "sample_count": 20, "duration_seconds": 2.038}, "timestamp": "2026-01-19T14:35:44.359494"}
{"image_index": 135, "image_name": "000000013923.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013923.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2786.778, "latencies_ms": [2786.778], "images_per_second": 0.359, "prompt_tokens": 1113, "response_tokens_est": 66, "n_tiles": 1, "output_text": " 1. white sofa: 1\n2. red chair: 2\n3. white table: 1\n4. vase with flowers: 1\n5. television: 1\n6. black and white rug: 1\n7. wall art: 3\n8. window: 1", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26835.9, "ram_available_mb": 98936.3, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 26835.9, "ram_available_mb": 98936.3, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.78, "peak": 40.57, "min": 17.35}, "VIN": {"avg": 64.95, "peak": 127.47, "min": 28.96}, "VIN_SYS_5V0": {"avg": 14.97, "peak": 16.05, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.91, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 26.78, "energy_joules_est": 74.64, "sample_count": 27, "duration_seconds": 2.787}, "timestamp": "2026-01-19T14:35:47.184065"}
{"image_index": 135, "image_name": "000000013923.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013923.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2630.642, "latencies_ms": [2630.642], "images_per_second": 0.38, "prompt_tokens": 1117, "response_tokens_est": 60, "n_tiles": 1, "output_text": " The white sofa is positioned to the left of the television, which is situated in the middle of the room. The dining table is located in the foreground, with the red chairs placed around it. The living room extends into the background, with the large windows allowing natural light to flood the space.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26835.9, "ram_available_mb": 98936.3, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26835.9, "ram_available_mb": 98936.3, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.99, "peak": 40.57, "min": 16.16}, "VIN": {"avg": 69.67, "peak": 124.24, "min": 28.69}, "VIN_SYS_5V0": {"avg": 14.95, "peak": 16.15, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 16.01, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 26.99, "energy_joules_est": 71.01, "sample_count": 26, "duration_seconds": 2.631}, "timestamp": "2026-01-19T14:35:49.888130"}
{"image_index": 135, "image_name": "000000013923.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013923.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1696.78, "latencies_ms": [1696.78], "images_per_second": 0.589, "prompt_tokens": 1111, "response_tokens_est": 23, "n_tiles": 1, "output_text": " A living room with a white couch, red chairs, and a dining table with a vase of flowers on it.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26835.9, "ram_available_mb": 98936.3, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26835.9, "ram_available_mb": 98936.3, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.76, "peak": 40.18, "min": 15.77}, "VIN": {"avg": 71.96, "peak": 121.72, "min": 30.14}, "VIN_SYS_5V0": {"avg": 14.77, "peak": 16.05, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 15.84, "peak": 17.32, "min": 13.4}}, "power_watts_avg": 29.76, "energy_joules_est": 50.51, "sample_count": 17, "duration_seconds": 1.697}, "timestamp": "2026-01-19T14:35:51.681306"}
{"image_index": 135, "image_name": "000000013923.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013923.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1828.323, "latencies_ms": [1828.323], "images_per_second": 0.547, "prompt_tokens": 1109, "response_tokens_est": 28, "n_tiles": 1, "output_text": " The room is well lit with natural light coming in from the windows. The walls are painted white and the furniture is mostly black and white.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26835.9, "ram_available_mb": 98936.3, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26835.9, "ram_available_mb": 98936.3, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.03, "peak": 40.18, "min": 17.74}, "VIN": {"avg": 70.49, "peak": 110.95, "min": 28.71}, "VIN_SYS_5V0": {"avg": 14.76, "peak": 16.05, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.88, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 30.03, "energy_joules_est": 54.92, "sample_count": 18, "duration_seconds": 1.829}, "timestamp": "2026-01-19T14:35:53.562079"}
{"image_index": 136, "image_name": "000000014007.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014007.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1496.67, "latencies_ms": [1496.67], "images_per_second": 0.668, "prompt_tokens": 1099, "response_tokens_est": 16, "n_tiles": 1, "output_text": " A cat is standing on top of a blue refrigerator, looking out the window.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 26835.9, "ram_available_mb": 98936.3, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 26835.9, "ram_available_mb": 98936.3, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.59, "peak": 40.56, "min": 17.74}, "VIN": {"avg": 81.27, "peak": 131.27, "min": 30.04}, "VIN_SYS_5V0": {"avg": 14.9, "peak": 16.05, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 16.09, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 31.59, "energy_joules_est": 47.29, "sample_count": 15, "duration_seconds": 1.497}, "timestamp": "2026-01-19T14:35:55.131834"}
{"image_index": 136, "image_name": "000000014007.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014007.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2697.203, "latencies_ms": [2697.203], "images_per_second": 0.371, "prompt_tokens": 1113, "response_tokens_est": 56, "n_tiles": 1, "output_text": " 1. cat: 1\n2. refrigerator: 1\n3. cabinet: 1\n4. shelf: 1\n5. light: 1\n6. door: 1\n7. wall: 1\n8. floor: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26835.9, "ram_available_mb": 98936.3, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 26836.1, "ram_available_mb": 98936.1, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.03, "peak": 40.56, "min": 18.91}, "VIN": {"avg": 69.5, "peak": 125.26, "min": 30.57}, "VIN_SYS_5V0": {"avg": 14.69, "peak": 16.05, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.72, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 27.03, "energy_joules_est": 72.92, "sample_count": 26, "duration_seconds": 2.698}, "timestamp": "2026-01-19T14:35:57.840054"}
{"image_index": 136, "image_name": "000000014007.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014007.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2363.256, "latencies_ms": [2363.256], "images_per_second": 0.423, "prompt_tokens": 1117, "response_tokens_est": 48, "n_tiles": 1, "output_text": " The cat is positioned on the right side of the refrigerator, which is located in the foreground of the image. The refrigerator is situated in the middle of the image, with the cat's head and body occupying a significant portion of the frame.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26836.1, "ram_available_mb": 98936.1, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26836.1, "ram_available_mb": 98936.1, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.84, "peak": 40.18, "min": 16.16}, "VIN": {"avg": 67.27, "peak": 88.27, "min": 29.59}, "VIN_SYS_5V0": {"avg": 15.09, "peak": 16.15, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 16.07, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 27.84, "energy_joules_est": 65.8, "sample_count": 23, "duration_seconds": 2.364}, "timestamp": "2026-01-19T14:36:00.216527"}
{"image_index": 136, "image_name": "000000014007.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014007.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1436.63, "latencies_ms": [1436.63], "images_per_second": 0.696, "prompt_tokens": 1111, "response_tokens_est": 14, "n_tiles": 1, "output_text": " A cat is standing on top of a blue refrigerator in a kitchen.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26836.1, "ram_available_mb": 98936.1, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 26836.3, "ram_available_mb": 98935.9, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.83, "peak": 40.57, "min": 16.55}, "VIN": {"avg": 70.49, "peak": 104.72, "min": 30.95}, "VIN_SYS_5V0": {"avg": 15.18, "peak": 16.15, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 16.28, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 31.83, "energy_joules_est": 45.74, "sample_count": 14, "duration_seconds": 1.437}, "timestamp": "2026-01-19T14:36:01.681996"}
{"image_index": 136, "image_name": "000000014007.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014007.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2034.451, "latencies_ms": [2034.451], "images_per_second": 0.492, "prompt_tokens": 1109, "response_tokens_est": 37, "n_tiles": 1, "output_text": " The image features a cat standing on top of a blue refrigerator, with a white wall in the background. The cat is wearing a blue collar, and the refrigerator has a white door.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26836.3, "ram_available_mb": 98935.9, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26836.3, "ram_available_mb": 98935.9, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.24, "peak": 40.95, "min": 20.1}, "VIN": {"avg": 72.34, "peak": 109.8, "min": 29.0}, "VIN_SYS_5V0": {"avg": 15.07, "peak": 16.15, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 16.12, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 30.24, "energy_joules_est": 61.53, "sample_count": 20, "duration_seconds": 2.035}, "timestamp": "2026-01-19T14:36:03.768787"}
{"image_index": 137, "image_name": "000000014038.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014038.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2119.162, "latencies_ms": [2119.162], "images_per_second": 0.472, "prompt_tokens": 1099, "response_tokens_est": 40, "n_tiles": 1, "output_text": " The image depicts a room with a wooden floor, a white ceiling, and a yellow balloon hanging from the ceiling, with a white refrigerator, a wooden table, and a black chair in the room.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26836.3, "ram_available_mb": 98935.9, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26836.3, "ram_available_mb": 98935.9, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.61, "peak": 40.18, "min": 16.95}, "VIN": {"avg": 67.23, "peak": 127.65, "min": 29.54}, "VIN_SYS_5V0": {"avg": 14.9, "peak": 16.05, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.9, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 28.61, "energy_joules_est": 60.65, "sample_count": 21, "duration_seconds": 2.12}, "timestamp": "2026-01-19T14:36:05.957629"}
{"image_index": 137, "image_name": "000000014038.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014038.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2744.415, "latencies_ms": [2744.415], "images_per_second": 0.364, "prompt_tokens": 1113, "response_tokens_est": 64, "n_tiles": 1, "output_text": " 1. Balloons: 4\n2. Balloons: 2\n3. Balloons: 1\n4. Balloons: 1\n5. Balloons: 1\n6. Balloons: 1\n7. Balloons: 1\n8. Balloons: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26836.3, "ram_available_mb": 98935.9, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 26836.3, "ram_available_mb": 98935.9, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.59, "peak": 40.16, "min": 16.56}, "VIN": {"avg": 66.74, "peak": 118.62, "min": 29.83}, "VIN_SYS_5V0": {"avg": 14.92, "peak": 16.05, "min": 12.25}, "VDD_CPU_SOC_MSS": {"avg": 15.87, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 26.59, "energy_joules_est": 72.99, "sample_count": 27, "duration_seconds": 2.745}, "timestamp": "2026-01-19T14:36:08.765245"}
{"image_index": 137, "image_name": "000000014038.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014038.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3137.11, "latencies_ms": [3137.11], "images_per_second": 0.319, "prompt_tokens": 1117, "response_tokens_est": 79, "n_tiles": 1, "output_text": " The main objects are arranged in a way that the foreground is dominated by the wooden table and the balloons, while the background features the refrigerator, the wall, and the bookshelf. The balloons are positioned in the foreground, with the refrigerator and the wall in the background. The wooden table is located in the middle of the room, with the balloons on the left and the refrigerator on the right.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26836.3, "ram_available_mb": 98935.9, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 26836.3, "ram_available_mb": 98935.9, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 25.61, "peak": 40.18, "min": 15.77}, "VIN": {"avg": 60.01, "peak": 124.62, "min": 27.97}, "VIN_SYS_5V0": {"avg": 14.9, "peak": 16.05, "min": 12.25}, "VDD_CPU_SOC_MSS": {"avg": 15.96, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 25.61, "energy_joules_est": 80.35, "sample_count": 31, "duration_seconds": 3.137}, "timestamp": "2026-01-19T14:36:11.986548"}
{"image_index": 137, "image_name": "000000014038.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014038.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1513.852, "latencies_ms": [1513.852], "images_per_second": 0.661, "prompt_tokens": 1111, "response_tokens_est": 16, "n_tiles": 1, "output_text": " A room with balloons and a smiley face balloon is decorated for a party.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26836.3, "ram_available_mb": 98935.9, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 26836.3, "ram_available_mb": 98935.9, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.6, "peak": 40.18, "min": 14.98}, "VIN": {"avg": 72.94, "peak": 115.6, "min": 29.12}, "VIN_SYS_5V0": {"avg": 14.79, "peak": 16.05, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.85, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 30.6, "energy_joules_est": 46.34, "sample_count": 15, "duration_seconds": 1.514}, "timestamp": "2026-01-19T14:36:13.550375"}
{"image_index": 137, "image_name": "000000014038.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014038.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1447.704, "latencies_ms": [1447.704], "images_per_second": 0.691, "prompt_tokens": 1109, "response_tokens_est": 14, "n_tiles": 1, "output_text": " The room is lit by a chandelier and has wooden floors.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26836.3, "ram_available_mb": 98935.9, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 26836.3, "ram_available_mb": 98935.9, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.92, "peak": 40.57, "min": 20.5}, "VIN": {"avg": 77.34, "peak": 128.69, "min": 28.8}, "VIN_SYS_5V0": {"avg": 15.0, "peak": 16.05, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 16.08, "peak": 17.32, "min": 13.78}}, "power_watts_avg": 32.92, "energy_joules_est": 47.67, "sample_count": 14, "duration_seconds": 1.448}, "timestamp": "2026-01-19T14:36:15.012133"}
{"image_index": 138, "image_name": "000000014226.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014226.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1719.02, "latencies_ms": [1719.02], "images_per_second": 0.582, "prompt_tokens": 1099, "response_tokens_est": 26, "n_tiles": 1, "output_text": " A man wearing headphones is sitting at a table with a laptop in front of him, and there is a window in the background.", "error": null, "sys_before": {"cpu_percent": 14.3, "ram_used_mb": 26836.3, "ram_available_mb": 98935.9, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26836.3, "ram_available_mb": 98935.9, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.14, "peak": 40.56, "min": 21.67}, "VIN": {"avg": 76.25, "peak": 120.38, "min": 29.06}, "VIN_SYS_5V0": {"avg": 15.23, "peak": 16.15, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 16.33, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 32.14, "energy_joules_est": 55.27, "sample_count": 17, "duration_seconds": 1.72}, "timestamp": "2026-01-19T14:36:16.787639"}
{"image_index": 138, "image_name": "000000014226.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014226.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2565.786, "latencies_ms": [2565.786], "images_per_second": 0.39, "prompt_tokens": 1113, "response_tokens_est": 58, "n_tiles": 1, "output_text": " 1. man: 1\n2. laptop: 1\n3. chair: 1\n4. window: 1\n5. train: 1\n6. train tracks: 1\n7. headset: 1\n8. table: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26836.3, "ram_available_mb": 98935.9, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 26836.3, "ram_available_mb": 98935.9, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.67, "peak": 40.57, "min": 18.53}, "VIN": {"avg": 68.33, "peak": 120.07, "min": 28.75}, "VIN_SYS_5V0": {"avg": 15.0, "peak": 16.15, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 16.09, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 27.67, "energy_joules_est": 71.0, "sample_count": 25, "duration_seconds": 2.566}, "timestamp": "2026-01-19T14:36:19.392549"}
{"image_index": 138, "image_name": "000000014226.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014226.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2078.409, "latencies_ms": [2078.409], "images_per_second": 0.481, "prompt_tokens": 1117, "response_tokens_est": 39, "n_tiles": 1, "output_text": " The man is sitting on the left side of the image, with the laptop in front of him. The laptop is on the right side of the image, and the window is in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26836.3, "ram_available_mb": 98935.9, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26836.3, "ram_available_mb": 98935.9, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.02, "peak": 40.57, "min": 16.55}, "VIN": {"avg": 68.13, "peak": 121.1, "min": 28.99}, "VIN_SYS_5V0": {"avg": 15.12, "peak": 16.15, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 16.1, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 29.02, "energy_joules_est": 60.33, "sample_count": 20, "duration_seconds": 2.079}, "timestamp": "2026-01-19T14:36:21.482038"}
{"image_index": 138, "image_name": "000000014226.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014226.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1490.828, "latencies_ms": [1490.828], "images_per_second": 0.671, "prompt_tokens": 1111, "response_tokens_est": 17, "n_tiles": 1, "output_text": " A man wearing headphones is sitting at a table in a train, using a laptop.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26836.3, "ram_available_mb": 98935.9, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 26836.3, "ram_available_mb": 98935.9, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.91, "peak": 40.18, "min": 18.53}, "VIN": {"avg": 72.04, "peak": 98.47, "min": 28.01}, "VIN_SYS_5V0": {"avg": 15.25, "peak": 16.26, "min": 13.29}, "VDD_CPU_SOC_MSS": {"avg": 16.35, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 31.91, "energy_joules_est": 47.58, "sample_count": 15, "duration_seconds": 1.491}, "timestamp": "2026-01-19T14:36:23.046768"}
{"image_index": 138, "image_name": "000000014226.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014226.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1702.602, "latencies_ms": [1702.602], "images_per_second": 0.587, "prompt_tokens": 1109, "response_tokens_est": 24, "n_tiles": 1, "output_text": " The man is wearing a green shirt and has red hair. The laptop is silver and has an apple logo on it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26836.3, "ram_available_mb": 98935.9, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26836.3, "ram_available_mb": 98935.9, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.01, "peak": 40.57, "min": 19.31}, "VIN": {"avg": 72.85, "peak": 122.4, "min": 28.67}, "VIN_SYS_5V0": {"avg": 14.87, "peak": 16.05, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.96, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 31.01, "energy_joules_est": 52.82, "sample_count": 17, "duration_seconds": 1.703}, "timestamp": "2026-01-19T14:36:24.825334"}
{"image_index": 139, "image_name": "000000014380.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014380.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2860.416, "latencies_ms": [2860.416], "images_per_second": 0.35, "prompt_tokens": 1099, "response_tokens_est": 69, "n_tiles": 1, "output_text": " The image captures a striking white cable-stayed bridge with a unique, pointed design, spanning across a bustling train station with multiple tracks and platforms. The sky above is a clear blue, dotted with fluffy white clouds, and the cityscape in the background is a mix of buildings and trees, creating a harmonious blend of urban and natural elements.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26836.3, "ram_available_mb": 98935.9, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 26836.8, "ram_available_mb": 98935.4, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.51, "peak": 40.56, "min": 18.14}, "VIN": {"avg": 67.01, "peak": 120.77, "min": 30.03}, "VIN_SYS_5V0": {"avg": 14.88, "peak": 16.05, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 15.88, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 26.51, "energy_joules_est": 75.85, "sample_count": 28, "duration_seconds": 2.861}, "timestamp": "2026-01-19T14:36:27.753314"}
{"image_index": 139, "image_name": "000000014380.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014380.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2608.266, "latencies_ms": [2608.266], "images_per_second": 0.383, "prompt_tokens": 1113, "response_tokens_est": 59, "n_tiles": 1, "output_text": " 1. bridge: 1\n2. train tracks: 4\n3. train: 1\n4. train station: 1\n5. platform: 1\n6. platform sign: 1\n7. fence: 1\n8. sky: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26836.8, "ram_available_mb": 98935.4, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26836.8, "ram_available_mb": 98935.4, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.72, "peak": 40.18, "min": 15.77}, "VIN": {"avg": 70.79, "peak": 116.93, "min": 28.4}, "VIN_SYS_5V0": {"avg": 14.91, "peak": 16.05, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.95, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 26.72, "energy_joules_est": 69.7, "sample_count": 26, "duration_seconds": 2.609}, "timestamp": "2026-01-19T14:36:30.468352"}
{"image_index": 139, "image_name": "000000014380.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014380.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2475.069, "latencies_ms": [2475.069], "images_per_second": 0.404, "prompt_tokens": 1117, "response_tokens_est": 54, "n_tiles": 1, "output_text": " The train station is located in the foreground, with the bridge stretching across the image, and the city skyline is visible in the background. The train tracks are parallel to each other, and the bridge is positioned above them, creating a sense of depth in the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26836.8, "ram_available_mb": 98935.4, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26836.8, "ram_available_mb": 98935.4, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.33, "peak": 40.18, "min": 14.98}, "VIN": {"avg": 72.06, "peak": 123.2, "min": 29.45}, "VIN_SYS_5V0": {"avg": 14.85, "peak": 16.05, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.98, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 27.33, "energy_joules_est": 67.65, "sample_count": 24, "duration_seconds": 2.475}, "timestamp": "2026-01-19T14:36:32.964506"}
{"image_index": 139, "image_name": "000000014380.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014380.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2334.39, "latencies_ms": [2334.39], "images_per_second": 0.428, "prompt_tokens": 1111, "response_tokens_est": 49, "n_tiles": 1, "output_text": " The image captures a modern white bridge with a unique triangular design, spanning over a bustling train station. The sky above is a clear blue, dotted with fluffy white clouds, and the cityscape in the background is a mix of buildings and trees.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26836.8, "ram_available_mb": 98935.4, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26836.8, "ram_available_mb": 98935.4, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.96, "peak": 40.57, "min": 16.56}, "VIN": {"avg": 70.62, "peak": 104.97, "min": 29.4}, "VIN_SYS_5V0": {"avg": 15.11, "peak": 16.15, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 16.23, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 27.96, "energy_joules_est": 65.28, "sample_count": 23, "duration_seconds": 2.335}, "timestamp": "2026-01-19T14:36:35.353381"}
{"image_index": 139, "image_name": "000000014380.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014380.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1632.07, "latencies_ms": [1632.07], "images_per_second": 0.613, "prompt_tokens": 1109, "response_tokens_est": 21, "n_tiles": 1, "output_text": " The image features a white bridge with a unique design, and the sky is filled with fluffy white clouds.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26836.8, "ram_available_mb": 98935.4, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26836.8, "ram_available_mb": 98935.4, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.7, "peak": 40.18, "min": 16.16}, "VIN": {"avg": 69.77, "peak": 108.03, "min": 29.19}, "VIN_SYS_5V0": {"avg": 14.91, "peak": 16.05, "min": 12.36}, "VDD_CPU_SOC_MSS": {"avg": 16.12, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 30.7, "energy_joules_est": 50.12, "sample_count": 16, "duration_seconds": 1.633}, "timestamp": "2026-01-19T14:36:37.024744"}
{"image_index": 140, "image_name": "000000014439.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014439.jpg", "image_width": 640, "image_height": 404, "image_resolution": "640x404", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1666.155, "latencies_ms": [1666.155], "images_per_second": 0.6, "prompt_tokens": 1099, "response_tokens_est": 23, "n_tiles": 1, "output_text": " A group of people are flying a large kite with a purple and yellow design on a sunny day in a park.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26836.8, "ram_available_mb": 98935.4, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26836.8, "ram_available_mb": 98935.4, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.71, "peak": 40.95, "min": 19.32}, "VIN": {"avg": 75.33, "peak": 104.67, "min": 36.27}, "VIN_SYS_5V0": {"avg": 15.09, "peak": 16.05, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 16.24, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 31.71, "energy_joules_est": 52.86, "sample_count": 16, "duration_seconds": 1.667}, "timestamp": "2026-01-19T14:36:38.701713"}
{"image_index": 140, "image_name": "000000014439.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014439.jpg", "image_width": 640, "image_height": 404, "image_resolution": "640x404", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1936.119, "latencies_ms": [1936.119], "images_per_second": 0.516, "prompt_tokens": 1113, "response_tokens_est": 34, "n_tiles": 1, "output_text": " kite: 1, person: 2, ball: 1, chair: 1, backpack: 1, person: 1, tree: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26836.8, "ram_available_mb": 98935.4, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26836.8, "ram_available_mb": 98935.4, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.54, "peak": 40.97, "min": 20.49}, "VIN": {"avg": 74.71, "peak": 128.97, "min": 29.27}, "VIN_SYS_5V0": {"avg": 15.23, "peak": 16.26, "min": 13.29}, "VDD_CPU_SOC_MSS": {"avg": 16.39, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 30.54, "energy_joules_est": 59.14, "sample_count": 19, "duration_seconds": 1.937}, "timestamp": "2026-01-19T14:36:40.684083"}
{"image_index": 140, "image_name": "000000014439.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014439.jpg", "image_width": 640, "image_height": 404, "image_resolution": "640x404", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2294.506, "latencies_ms": [2294.506], "images_per_second": 0.436, "prompt_tokens": 1117, "response_tokens_est": 47, "n_tiles": 1, "output_text": " The kite is in the foreground, flying high in the sky, while the people are in the background, walking around the park. The kite is positioned to the left of the people, and the park is spread out in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26836.8, "ram_available_mb": 98935.4, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26836.8, "ram_available_mb": 98935.4, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.1, "peak": 40.57, "min": 17.35}, "VIN": {"avg": 72.4, "peak": 123.28, "min": 29.16}, "VIN_SYS_5V0": {"avg": 14.95, "peak": 16.15, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 16.09, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 28.1, "energy_joules_est": 64.48, "sample_count": 23, "duration_seconds": 2.295}, "timestamp": "2026-01-19T14:36:43.080663"}
{"image_index": 140, "image_name": "000000014439.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014439.jpg", "image_width": 640, "image_height": 404, "image_resolution": "640x404", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1439.638, "latencies_ms": [1439.638], "images_per_second": 0.695, "prompt_tokens": 1111, "response_tokens_est": 13, "n_tiles": 1, "output_text": " A group of people are flying a large kite in a park.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26836.8, "ram_available_mb": 98935.4, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 26836.8, "ram_available_mb": 98935.4, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.12, "peak": 40.18, "min": 15.38}, "VIN": {"avg": 72.4, "peak": 112.46, "min": 29.21}, "VIN_SYS_5V0": {"avg": 14.66, "peak": 15.95, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 15.89, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 31.12, "energy_joules_est": 44.82, "sample_count": 14, "duration_seconds": 1.44}, "timestamp": "2026-01-19T14:36:44.544391"}
{"image_index": 140, "image_name": "000000014439.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014439.jpg", "image_width": 640, "image_height": 404, "image_resolution": "640x404", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2234.332, "latencies_ms": [2234.332], "images_per_second": 0.448, "prompt_tokens": 1109, "response_tokens_est": 45, "n_tiles": 1, "output_text": " The kite is a vibrant mix of blue, purple, and yellow, with a long tail that trails behind it. The sky is clear and blue, and the sun is shining brightly, casting a warm glow over the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26836.8, "ram_available_mb": 98935.4, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26836.8, "ram_available_mb": 98935.4, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.53, "peak": 41.34, "min": 19.31}, "VIN": {"avg": 71.14, "peak": 117.35, "min": 29.5}, "VIN_SYS_5V0": {"avg": 15.09, "peak": 16.15, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 16.23, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 29.53, "energy_joules_est": 65.99, "sample_count": 22, "duration_seconds": 2.235}, "timestamp": "2026-01-19T14:36:46.835713"}
{"image_index": 141, "image_name": "000000014473.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014473.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2112.943, "latencies_ms": [2112.943], "images_per_second": 0.473, "prompt_tokens": 1099, "response_tokens_est": 40, "n_tiles": 1, "output_text": " The image captures a miniature model of a train set, featuring a red and black train with the Virgin logo, set against a backdrop of a lush green landscape with a fence and a clear blue sky.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 26836.8, "ram_available_mb": 98935.4, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 26836.8, "ram_available_mb": 98935.4, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.57, "peak": 40.56, "min": 16.16}, "VIN": {"avg": 73.55, "peak": 123.12, "min": 28.39}, "VIN_SYS_5V0": {"avg": 14.92, "peak": 16.05, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 16.03, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 28.57, "energy_joules_est": 60.38, "sample_count": 21, "duration_seconds": 2.113}, "timestamp": "2026-01-19T14:36:49.033313"}
{"image_index": 141, "image_name": "000000014473.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014473.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1976.945, "latencies_ms": [1976.945], "images_per_second": 0.506, "prompt_tokens": 1113, "response_tokens_est": 34, "n_tiles": 1, "output_text": " train: 1, workers: 4, tracks: 3, wires: 2, fence: 1, grass: 1, hills: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26836.8, "ram_available_mb": 98935.4, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 26836.8, "ram_available_mb": 98935.4, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.86, "peak": 40.18, "min": 16.16}, "VIN": {"avg": 74.24, "peak": 127.27, "min": 30.56}, "VIN_SYS_5V0": {"avg": 14.82, "peak": 16.05, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.94, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 28.86, "energy_joules_est": 57.06, "sample_count": 20, "duration_seconds": 1.977}, "timestamp": "2026-01-19T14:36:51.116671"}
{"image_index": 141, "image_name": "000000014473.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014473.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2303.313, "latencies_ms": [2303.313], "images_per_second": 0.434, "prompt_tokens": 1117, "response_tokens_est": 46, "n_tiles": 1, "output_text": " The train is positioned on the left side of the image, with the workers on the right side. The train is in the foreground, while the workers are in the background. The train is closer to the viewer than the workers.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26836.8, "ram_available_mb": 98935.4, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 26836.8, "ram_available_mb": 98935.4, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.59, "peak": 39.77, "min": 15.77}, "VIN": {"avg": 69.76, "peak": 97.0, "min": 27.43}, "VIN_SYS_5V0": {"avg": 14.7, "peak": 15.95, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 15.83, "peak": 17.32, "min": 12.61}}, "power_watts_avg": 27.59, "energy_joules_est": 63.56, "sample_count": 23, "duration_seconds": 2.304}, "timestamp": "2026-01-19T14:36:53.514544"}
{"image_index": 141, "image_name": "000000014473.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014473.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2055.82, "latencies_ms": [2055.82], "images_per_second": 0.486, "prompt_tokens": 1111, "response_tokens_est": 37, "n_tiles": 1, "output_text": " A model train is traveling on a track, passing by a group of people who are working on the track. The train is red and black, and the people are wearing orange uniforms.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26836.8, "ram_available_mb": 98935.4, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26836.8, "ram_available_mb": 98935.4, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.74, "peak": 40.18, "min": 15.38}, "VIN": {"avg": 71.44, "peak": 124.49, "min": 29.81}, "VIN_SYS_5V0": {"avg": 14.81, "peak": 15.95, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.93, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 28.74, "energy_joules_est": 59.1, "sample_count": 20, "duration_seconds": 2.056}, "timestamp": "2026-01-19T14:36:55.601103"}
{"image_index": 141, "image_name": "000000014473.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014473.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1979.186, "latencies_ms": [1979.186], "images_per_second": 0.505, "prompt_tokens": 1109, "response_tokens_est": 35, "n_tiles": 1, "output_text": " The image features a model train set with a red and black train, and the workers are wearing orange uniforms. The lighting is natural, and the weather appears to be clear.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26836.8, "ram_available_mb": 98935.4, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26836.8, "ram_available_mb": 98935.4, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.14, "peak": 40.56, "min": 17.35}, "VIN": {"avg": 70.86, "peak": 107.55, "min": 28.21}, "VIN_SYS_5V0": {"avg": 15.03, "peak": 16.15, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 16.16, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 29.14, "energy_joules_est": 57.69, "sample_count": 20, "duration_seconds": 1.98}, "timestamp": "2026-01-19T14:36:57.683452"}
{"image_index": 142, "image_name": "000000014831.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014831.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1983.914, "latencies_ms": [1983.914], "images_per_second": 0.504, "prompt_tokens": 1100, "response_tokens_est": 36, "n_tiles": 1, "output_text": " The image shows a close-up view of a cat's fur, which is a mix of brown and white colors, and the background is a textured fabric with a diamond pattern.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 26836.8, "ram_available_mb": 98935.4, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26836.8, "ram_available_mb": 98935.4, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.54, "peak": 39.39, "min": 15.77}, "VIN": {"avg": 68.77, "peak": 116.22, "min": 27.81}, "VIN_SYS_5V0": {"avg": 14.86, "peak": 16.05, "min": 11.77}, "VDD_CPU_SOC_MSS": {"avg": 15.92, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 28.54, "energy_joules_est": 56.64, "sample_count": 20, "duration_seconds": 1.984}, "timestamp": "2026-01-19T14:36:59.771955"}
{"image_index": 142, "image_name": "000000014831.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014831.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2605.164, "latencies_ms": [2605.164], "images_per_second": 0.384, "prompt_tokens": 1114, "response_tokens_est": 56, "n_tiles": 1, "output_text": " 1. cat: 1\n2. fur: 1\n3. blanket: 1\n4. fabric: 1\n5. pattern: 1\n6. texture: 1\n7. surface: 1\n8. background: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26836.8, "ram_available_mb": 98935.4, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26837.0, "ram_available_mb": 98935.2, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.49, "peak": 40.18, "min": 16.16}, "VIN": {"avg": 71.45, "peak": 120.1, "min": 28.94}, "VIN_SYS_5V0": {"avg": 14.73, "peak": 15.95, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 15.66, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 26.49, "energy_joules_est": 69.02, "sample_count": 26, "duration_seconds": 2.605}, "timestamp": "2026-01-19T14:37:02.473666"}
{"image_index": 142, "image_name": "000000014831.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014831.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2090.472, "latencies_ms": [2090.472], "images_per_second": 0.478, "prompt_tokens": 1118, "response_tokens_est": 39, "n_tiles": 1, "output_text": " The cat's fur is in the foreground, with the patterned blanket in the background. The cat's fur is on the left side of the image, while the blanket is on the right side.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26837.0, "ram_available_mb": 98935.2, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26837.0, "ram_available_mb": 98935.2, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.97, "peak": 39.78, "min": 14.98}, "VIN": {"avg": 74.5, "peak": 122.85, "min": 28.59}, "VIN_SYS_5V0": {"avg": 14.8, "peak": 16.05, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.69, "peak": 17.32, "min": 12.61}}, "power_watts_avg": 27.97, "energy_joules_est": 58.48, "sample_count": 21, "duration_seconds": 2.091}, "timestamp": "2026-01-19T14:37:04.664844"}
{"image_index": 142, "image_name": "000000014831.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014831.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1469.549, "latencies_ms": [1469.549], "images_per_second": 0.68, "prompt_tokens": 1112, "response_tokens_est": 15, "n_tiles": 1, "output_text": " A cat is laying on a bed with a brown and white fur coat.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26837.0, "ram_available_mb": 98935.2, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 26837.0, "ram_available_mb": 98935.2, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.55, "peak": 39.39, "min": 15.77}, "VIN": {"avg": 79.92, "peak": 121.73, "min": 27.55}, "VIN_SYS_5V0": {"avg": 14.7, "peak": 16.15, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.85, "peak": 17.32, "min": 12.61}}, "power_watts_avg": 30.55, "energy_joules_est": 44.91, "sample_count": 15, "duration_seconds": 1.47}, "timestamp": "2026-01-19T14:37:06.226547"}
{"image_index": 142, "image_name": "000000014831.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014831.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2363.23, "latencies_ms": [2363.23], "images_per_second": 0.423, "prompt_tokens": 1110, "response_tokens_est": 50, "n_tiles": 1, "output_text": " The image shows a close-up of a cat's fur, which is a mix of brown and white colors. The lighting appears to be natural, coming from the top left corner of the image, and the cat's fur is soft and fluffy.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26837.0, "ram_available_mb": 98935.2, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 26837.7, "ram_available_mb": 98934.4, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.44, "peak": 40.56, "min": 19.32}, "VIN": {"avg": 72.2, "peak": 122.38, "min": 30.63}, "VIN_SYS_5V0": {"avg": 14.9, "peak": 16.05, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 16.01, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 28.44, "energy_joules_est": 67.23, "sample_count": 23, "duration_seconds": 2.364}, "timestamp": "2026-01-19T14:37:08.618394"}
{"image_index": 143, "image_name": "000000014888.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014888.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2054.277, "latencies_ms": [2054.277], "images_per_second": 0.487, "prompt_tokens": 1099, "response_tokens_est": 39, "n_tiles": 1, "output_text": " The image captures a close-up view of a cow's udder, which is attached to a black and yellow object, possibly a milking machine, with red caps on the tubes.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 26837.7, "ram_available_mb": 98934.4, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 26838.7, "ram_available_mb": 98933.5, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.25, "peak": 40.18, "min": 17.35}, "VIN": {"avg": 72.73, "peak": 116.98, "min": 28.85}, "VIN_SYS_5V0": {"avg": 15.19, "peak": 16.15, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 16.26, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 29.25, "energy_joules_est": 60.11, "sample_count": 20, "duration_seconds": 2.055}, "timestamp": "2026-01-19T14:37:10.707065"}
{"image_index": 143, "image_name": "000000014888.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014888.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2176.531, "latencies_ms": [2176.531], "images_per_second": 0.459, "prompt_tokens": 1113, "response_tokens_est": 43, "n_tiles": 1, "output_text": " cow: 1, black: 1, white: 1, red: 1, black and white: 1, black object: 1, yellow sticker: 1, metal: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26838.7, "ram_available_mb": 98933.5, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26839.0, "ram_available_mb": 98933.2, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.0, "peak": 40.56, "min": 17.34}, "VIN": {"avg": 77.96, "peak": 122.14, "min": 49.82}, "VIN_SYS_5V0": {"avg": 15.15, "peak": 16.15, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 16.35, "peak": 17.71, "min": 14.18}}, "power_watts_avg": 29.0, "energy_joules_est": 63.13, "sample_count": 21, "duration_seconds": 2.177}, "timestamp": "2026-01-19T14:37:12.889209"}
{"image_index": 143, "image_name": "000000014888.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014888.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2443.75, "latencies_ms": [2443.75], "images_per_second": 0.409, "prompt_tokens": 1117, "response_tokens_est": 54, "n_tiles": 1, "output_text": " The cow's udder is positioned in the foreground, with the black and white cow's body extending into the background. The red and white tubes are attached to the udder, and the cow's head is visible in the top left corner of the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26839.0, "ram_available_mb": 98933.2, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26839.2, "ram_available_mb": 98933.0, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.86, "peak": 39.78, "min": 18.52}, "VIN": {"avg": 66.6, "peak": 106.21, "min": 29.16}, "VIN_SYS_5V0": {"avg": 15.19, "peak": 16.26, "min": 13.29}, "VDD_CPU_SOC_MSS": {"avg": 16.26, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 27.86, "energy_joules_est": 68.09, "sample_count": 24, "duration_seconds": 2.444}, "timestamp": "2026-01-19T14:37:15.395110"}
{"image_index": 143, "image_name": "000000014888.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014888.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1627.801, "latencies_ms": [1627.801], "images_per_second": 0.614, "prompt_tokens": 1111, "response_tokens_est": 21, "n_tiles": 1, "output_text": " A cow is milking itself in a barn, with a black and white cow standing next to it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26839.2, "ram_available_mb": 98933.0, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26839.2, "ram_available_mb": 98933.0, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.68, "peak": 40.18, "min": 15.77}, "VIN": {"avg": 68.53, "peak": 113.38, "min": 28.77}, "VIN_SYS_5V0": {"avg": 14.93, "peak": 16.05, "min": 12.36}, "VDD_CPU_SOC_MSS": {"avg": 15.99, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 30.68, "energy_joules_est": 49.96, "sample_count": 16, "duration_seconds": 1.628}, "timestamp": "2026-01-19T14:37:17.060636"}
{"image_index": 143, "image_name": "000000014888.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014888.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2336.316, "latencies_ms": [2336.316], "images_per_second": 0.428, "prompt_tokens": 1109, "response_tokens_est": 49, "n_tiles": 1, "output_text": " The image features a black and white cow with red and white tubes attached to its udder, standing on a dirty floor. The lighting is dim, and the cow is positioned in a stall with a black and yellow label on the wall.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26839.2, "ram_available_mb": 98933.0, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26839.5, "ram_available_mb": 98932.7, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.54, "peak": 40.57, "min": 18.91}, "VIN": {"avg": 69.83, "peak": 91.23, "min": 29.33}, "VIN_SYS_5V0": {"avg": 15.03, "peak": 16.15, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 16.17, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 28.54, "energy_joules_est": 66.69, "sample_count": 23, "duration_seconds": 2.337}, "timestamp": "2026-01-19T14:37:19.452557"}
{"image_index": 144, "image_name": "000000015079.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015079.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1596.577, "latencies_ms": [1596.577], "images_per_second": 0.626, "prompt_tokens": 1099, "response_tokens_est": 19, "n_tiles": 1, "output_text": " A sandwich with a bite taken out of it is on a plate with a knife beside it.", "error": null, "sys_before": {"cpu_percent": 14.3, "ram_used_mb": 26839.5, "ram_available_mb": 98932.7, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 26839.5, "ram_available_mb": 98932.7, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.48, "peak": 40.18, "min": 16.16}, "VIN": {"avg": 72.3, "peak": 122.65, "min": 27.31}, "VIN_SYS_5V0": {"avg": 14.85, "peak": 16.05, "min": 12.36}, "VDD_CPU_SOC_MSS": {"avg": 15.87, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 30.48, "energy_joules_est": 48.68, "sample_count": 16, "duration_seconds": 1.597}, "timestamp": "2026-01-19T14:37:21.131983"}
{"image_index": 144, "image_name": "000000015079.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015079.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1743.633, "latencies_ms": [1743.633], "images_per_second": 0.574, "prompt_tokens": 1113, "response_tokens_est": 24, "n_tiles": 1, "output_text": " plate: 1, knife: 1, sandwich: 1, bread: 2, butter: 1", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26839.5, "ram_available_mb": 98932.7, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26839.5, "ram_available_mb": 98932.7, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.55, "peak": 40.18, "min": 18.14}, "VIN": {"avg": 70.98, "peak": 116.87, "min": 29.67}, "VIN_SYS_5V0": {"avg": 14.7, "peak": 15.85, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.7, "peak": 16.92, "min": 13.0}}, "power_watts_avg": 30.55, "energy_joules_est": 53.28, "sample_count": 17, "duration_seconds": 1.744}, "timestamp": "2026-01-19T14:37:22.905495"}
{"image_index": 144, "image_name": "000000015079.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015079.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2255.916, "latencies_ms": [2255.916], "images_per_second": 0.443, "prompt_tokens": 1117, "response_tokens_est": 45, "n_tiles": 1, "output_text": " The sandwich is located in the foreground of the image, with the knife placed to the right of it. The plate is situated in the middle ground, with the sandwich and knife appearing to be in close proximity to each other.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26839.5, "ram_available_mb": 98932.7, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26839.5, "ram_available_mb": 98932.7, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.76, "peak": 40.57, "min": 18.92}, "VIN": {"avg": 61.26, "peak": 114.4, "min": 28.86}, "VIN_SYS_5V0": {"avg": 15.03, "peak": 16.15, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 16.02, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 28.76, "energy_joules_est": 64.89, "sample_count": 22, "duration_seconds": 2.256}, "timestamp": "2026-01-19T14:37:25.192355"}
{"image_index": 144, "image_name": "000000015079.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015079.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1444.543, "latencies_ms": [1444.543], "images_per_second": 0.692, "prompt_tokens": 1111, "response_tokens_est": 14, "n_tiles": 1, "output_text": " A plate with a sandwich and a knife on a green tablecloth.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26839.5, "ram_available_mb": 98932.7, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26839.4, "ram_available_mb": 98932.7, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.85, "peak": 40.57, "min": 16.95}, "VIN": {"avg": 75.96, "peak": 107.92, "min": 30.55}, "VIN_SYS_5V0": {"avg": 15.14, "peak": 16.15, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 16.23, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 31.85, "energy_joules_est": 46.02, "sample_count": 14, "duration_seconds": 1.445}, "timestamp": "2026-01-19T14:37:26.653072"}
{"image_index": 144, "image_name": "000000015079.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015079.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1878.929, "latencies_ms": [1878.929], "images_per_second": 0.532, "prompt_tokens": 1109, "response_tokens_est": 31, "n_tiles": 1, "output_text": " The sandwich is on a plate with a knife and the plate is on a green tablecloth. The lighting is dim and the sandwich is in the foreground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26839.4, "ram_available_mb": 98932.7, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26839.4, "ram_available_mb": 98932.7, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.75, "peak": 40.95, "min": 18.91}, "VIN": {"avg": 76.08, "peak": 118.57, "min": 27.78}, "VIN_SYS_5V0": {"avg": 15.14, "peak": 16.15, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 16.29, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 30.75, "energy_joules_est": 57.78, "sample_count": 19, "duration_seconds": 1.879}, "timestamp": "2026-01-19T14:37:28.637001"}
{"image_index": 145, "image_name": "000000015254.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015254.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1935.141, "latencies_ms": [1935.141], "images_per_second": 0.517, "prompt_tokens": 1099, "response_tokens_est": 32, "n_tiles": 1, "output_text": " A colorful and well-balanced meal is presented in a purple divided container, featuring a pasta dish topped with cheese, a salad, sliced carrots, and grapes.", "error": null, "sys_before": {"cpu_percent": 16.7, "ram_used_mb": 26839.4, "ram_available_mb": 98932.7, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26839.4, "ram_available_mb": 98932.7, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.28, "peak": 40.18, "min": 16.16}, "VIN": {"avg": 70.82, "peak": 127.35, "min": 29.99}, "VIN_SYS_5V0": {"avg": 14.66, "peak": 15.95, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 15.83, "peak": 16.93, "min": 13.0}}, "power_watts_avg": 29.28, "energy_joules_est": 56.68, "sample_count": 19, "duration_seconds": 1.936}, "timestamp": "2026-01-19T14:37:30.608545"}
{"image_index": 145, "image_name": "000000015254.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015254.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2029.615, "latencies_ms": [2029.615], "images_per_second": 0.493, "prompt_tokens": 1113, "response_tokens_est": 37, "n_tiles": 1, "output_text": " 1. purple tray\n2. orange container\n3. green container\n4. red container\n5. blue container\n6. carrots\n7. grapes\n8. salad", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26839.4, "ram_available_mb": 98932.7, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26839.4, "ram_available_mb": 98932.7, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.47, "peak": 40.57, "min": 18.14}, "VIN": {"avg": 69.95, "peak": 98.32, "min": 29.03}, "VIN_SYS_5V0": {"avg": 15.05, "peak": 16.15, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 16.16, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 29.47, "energy_joules_est": 59.83, "sample_count": 20, "duration_seconds": 2.03}, "timestamp": "2026-01-19T14:37:32.688704"}
{"image_index": 145, "image_name": "000000015254.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015254.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2994.122, "latencies_ms": [2994.122], "images_per_second": 0.334, "prompt_tokens": 1117, "response_tokens_est": 74, "n_tiles": 1, "output_text": " The main objects are arranged in a specific spatial relationship, with the salad in the top left, the pasta in the bottom left, the carrots in the top right, and the grapes in the bottom right. The carrots are positioned near the top right, while the grapes are near the bottom right. The salad is in the foreground, while the pasta is in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26839.4, "ram_available_mb": 98932.7, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 26839.4, "ram_available_mb": 98932.7, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.36, "peak": 40.57, "min": 16.95}, "VIN": {"avg": 67.07, "peak": 119.8, "min": 28.87}, "VIN_SYS_5V0": {"avg": 14.97, "peak": 16.15, "min": 12.46}, "VDD_CPU_SOC_MSS": {"avg": 15.98, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 26.36, "energy_joules_est": 78.94, "sample_count": 29, "duration_seconds": 2.995}, "timestamp": "2026-01-19T14:37:35.696710"}
{"image_index": 145, "image_name": "000000015254.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015254.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1717.812, "latencies_ms": [1717.812], "images_per_second": 0.582, "prompt_tokens": 1111, "response_tokens_est": 25, "n_tiles": 1, "output_text": " A colorful meal is served in a purple divided container, with a variety of food items including pasta, vegetables, and fruit.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26839.4, "ram_available_mb": 98932.7, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26839.4, "ram_available_mb": 98932.7, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.45, "peak": 40.56, "min": 16.55}, "VIN": {"avg": 77.12, "peak": 120.89, "min": 29.86}, "VIN_SYS_5V0": {"avg": 15.18, "peak": 16.15, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 16.23, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 30.45, "energy_joules_est": 52.32, "sample_count": 17, "duration_seconds": 1.718}, "timestamp": "2026-01-19T14:37:37.471848"}
{"image_index": 145, "image_name": "000000015254.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015254.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3008.39, "latencies_ms": [3008.39], "images_per_second": 0.332, "prompt_tokens": 1109, "response_tokens_est": 71, "n_tiles": 1, "output_text": " The image features a vibrant and colorful meal in a purple divided container, with a variety of food items including pasta, vegetables, and fruits. The lighting is bright and even, illuminating the food items and creating a visually appealing presentation. The container is made of plastic, and the food items are fresh and healthy, suggesting a well-balanced and nutritious meal.", "error": null, "sys_before": {"cpu_percent": 50.0, "ram_used_mb": 26839.4, "ram_available_mb": 98932.7, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 26839.4, "ram_available_mb": 98932.7, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.25, "peak": 40.56, "min": 18.14}, "VIN": {"avg": 66.61, "peak": 127.67, "min": 32.79}, "VIN_SYS_5V0": {"avg": 14.87, "peak": 16.05, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.87, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 26.25, "energy_joules_est": 78.98, "sample_count": 29, "duration_seconds": 3.009}, "timestamp": "2026-01-19T14:37:40.486307"}
{"image_index": 146, "image_name": "000000015272.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015272.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1423.786, "latencies_ms": [1423.786], "images_per_second": 0.702, "prompt_tokens": 1100, "response_tokens_est": 14, "n_tiles": 1, "output_text": " A traffic light hangs above a street with cherry blossoms in full bloom.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 26839.4, "ram_available_mb": 98932.7, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 26839.7, "ram_available_mb": 98932.5, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.54, "peak": 39.38, "min": 16.56}, "VIN": {"avg": 77.9, "peak": 124.51, "min": 28.45}, "VIN_SYS_5V0": {"avg": 15.23, "peak": 16.15, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 16.39, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 31.54, "energy_joules_est": 44.93, "sample_count": 14, "duration_seconds": 1.424}, "timestamp": "2026-01-19T14:37:41.953138"}
{"image_index": 146, "image_name": "000000015272.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015272.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1286.922, "latencies_ms": [1286.922], "images_per_second": 0.777, "prompt_tokens": 1114, "response_tokens_est": 8, "n_tiles": 1, "output_text": " 1. traffic light: 3", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26839.7, "ram_available_mb": 98932.5, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.4, "ram_used_mb": 26839.9, "ram_available_mb": 98932.3, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 33.49, "peak": 40.57, "min": 21.29}, "VIN": {"avg": 72.59, "peak": 104.94, "min": 27.08}, "VIN_SYS_5V0": {"avg": 15.06, "peak": 16.15, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 16.2, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 33.49, "energy_joules_est": 43.11, "sample_count": 13, "duration_seconds": 1.287}, "timestamp": "2026-01-19T14:37:43.312261"}
{"image_index": 146, "image_name": "000000015272.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015272.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2292.354, "latencies_ms": [2292.354], "images_per_second": 0.436, "prompt_tokens": 1118, "response_tokens_est": 45, "n_tiles": 1, "output_text": " The traffic light is positioned to the left of the cherry blossoms, and the traffic light is in the foreground of the image. The cherry blossoms are in the background of the image, and they are positioned behind the traffic light.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26839.9, "ram_available_mb": 98932.3, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26839.9, "ram_available_mb": 98932.3, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.32, "peak": 40.57, "min": 20.1}, "VIN": {"avg": 70.43, "peak": 130.79, "min": 33.21}, "VIN_SYS_5V0": {"avg": 14.92, "peak": 16.05, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.93, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 29.32, "energy_joules_est": 67.23, "sample_count": 22, "duration_seconds": 2.293}, "timestamp": "2026-01-19T14:37:45.609582"}
{"image_index": 146, "image_name": "000000015272.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015272.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2479.219, "latencies_ms": [2479.219], "images_per_second": 0.403, "prompt_tokens": 1112, "response_tokens_est": 56, "n_tiles": 1, "output_text": " The image captures a serene scene of a cherry blossom tree in full bloom, with its branches adorned with delicate pink and white flowers. The tree is situated in a park, and a traffic light is visible in the background, adding a touch of urbanity to the otherwise natural setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26839.9, "ram_available_mb": 98932.3, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26839.9, "ram_available_mb": 98932.3, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.63, "peak": 39.39, "min": 17.73}, "VIN": {"avg": 72.88, "peak": 126.47, "min": 30.5}, "VIN_SYS_5V0": {"avg": 15.24, "peak": 16.26, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 16.26, "peak": 17.71, "min": 14.18}}, "power_watts_avg": 27.63, "energy_joules_est": 68.51, "sample_count": 24, "duration_seconds": 2.48}, "timestamp": "2026-01-19T14:37:48.109434"}
{"image_index": 146, "image_name": "000000015272.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015272.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1759.667, "latencies_ms": [1759.667], "images_per_second": 0.568, "prompt_tokens": 1110, "response_tokens_est": 27, "n_tiles": 1, "output_text": " The image features a cherry blossom tree with pink flowers, a traffic light with red and green lights, and a building in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26839.9, "ram_available_mb": 98932.3, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26839.9, "ram_available_mb": 98932.3, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.36, "peak": 40.16, "min": 16.56}, "VIN": {"avg": 70.41, "peak": 113.21, "min": 31.73}, "VIN_SYS_5V0": {"avg": 15.22, "peak": 16.15, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 16.33, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 30.36, "energy_joules_est": 53.43, "sample_count": 17, "duration_seconds": 1.76}, "timestamp": "2026-01-19T14:37:49.875124"}
{"image_index": 147, "image_name": "000000015278.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015278.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1738.013, "latencies_ms": [1738.013], "images_per_second": 0.575, "prompt_tokens": 1099, "response_tokens_est": 27, "n_tiles": 1, "output_text": " The image features a plate with a serving of broccoli and a piece of salmon, with the broccoli being the main focus of the dish.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 26839.9, "ram_available_mb": 98932.3, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26839.9, "ram_available_mb": 98932.3, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.14, "peak": 40.57, "min": 20.1}, "VIN": {"avg": 71.22, "peak": 104.36, "min": 29.3}, "VIN_SYS_5V0": {"avg": 15.28, "peak": 16.26, "min": 13.29}, "VDD_CPU_SOC_MSS": {"avg": 16.4, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 31.14, "energy_joules_est": 54.14, "sample_count": 17, "duration_seconds": 1.739}, "timestamp": "2026-01-19T14:37:51.662351"}
{"image_index": 147, "image_name": "000000015278.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015278.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2978.056, "latencies_ms": [2978.056], "images_per_second": 0.336, "prompt_tokens": 1113, "response_tokens_est": 74, "n_tiles": 1, "output_text": " 1. Broccoli florets: 12\n2. Salmon: 1\n3. Plate: 1\n4. Sauce: 1\n5. Dried herbs: 1\n6. Chopped garlic: 1\n7. Chopped onion: 1\n8. Chopped red pepper: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26839.9, "ram_available_mb": 98932.3, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 26839.7, "ram_available_mb": 98932.5, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.56, "peak": 40.56, "min": 18.52}, "VIN": {"avg": 64.57, "peak": 107.07, "min": 30.05}, "VIN_SYS_5V0": {"avg": 14.99, "peak": 16.15, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 16.02, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 26.56, "energy_joules_est": 79.12, "sample_count": 29, "duration_seconds": 2.979}, "timestamp": "2026-01-19T14:37:54.678314"}
{"image_index": 147, "image_name": "000000015278.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015278.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1966.241, "latencies_ms": [1966.241], "images_per_second": 0.509, "prompt_tokens": 1117, "response_tokens_est": 35, "n_tiles": 1, "output_text": " The broccoli is in the foreground, while the salmon is in the background. The broccoli is on the left side of the plate, and the salmon is on the right side.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26839.7, "ram_available_mb": 98932.5, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26839.7, "ram_available_mb": 98932.5, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.47, "peak": 40.57, "min": 16.16}, "VIN": {"avg": 72.99, "peak": 119.09, "min": 28.88}, "VIN_SYS_5V0": {"avg": 15.1, "peak": 16.15, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 16.2, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 29.47, "energy_joules_est": 57.97, "sample_count": 19, "duration_seconds": 1.967}, "timestamp": "2026-01-19T14:37:56.660903"}
{"image_index": 147, "image_name": "000000015278.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015278.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2001.14, "latencies_ms": [2001.14], "images_per_second": 0.5, "prompt_tokens": 1111, "response_tokens_est": 37, "n_tiles": 1, "output_text": " In the image, there is a plate of food that includes broccoli and salmon. The broccoli is green and appears to be cooked, while the salmon is orange and appears to be grilled.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26839.7, "ram_available_mb": 98932.5, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26839.7, "ram_available_mb": 98932.5, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.6, "peak": 40.16, "min": 18.52}, "VIN": {"avg": 73.15, "peak": 118.84, "min": 29.11}, "VIN_SYS_5V0": {"avg": 15.22, "peak": 16.15, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 16.3, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 29.6, "energy_joules_est": 59.24, "sample_count": 20, "duration_seconds": 2.001}, "timestamp": "2026-01-19T14:37:58.743856"}
{"image_index": 147, "image_name": "000000015278.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015278.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2364.995, "latencies_ms": [2364.995], "images_per_second": 0.423, "prompt_tokens": 1109, "response_tokens_est": 48, "n_tiles": 1, "output_text": " The image features a plate of food with a vibrant green broccoli and a piece of salmon, with the salmon being cooked to a golden brown color. The lighting is bright and natural, illuminating the food and giving it a warm and inviting appearance.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26839.7, "ram_available_mb": 98932.5, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26839.7, "ram_available_mb": 98932.5, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.82, "peak": 40.56, "min": 16.56}, "VIN": {"avg": 64.83, "peak": 103.66, "min": 28.87}, "VIN_SYS_5V0": {"avg": 14.88, "peak": 16.05, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 15.83, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 27.82, "energy_joules_est": 65.8, "sample_count": 23, "duration_seconds": 2.365}, "timestamp": "2026-01-19T14:38:01.139914"}
{"image_index": 148, "image_name": "000000015335.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015335.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1598.277, "latencies_ms": [1598.277], "images_per_second": 0.626, "prompt_tokens": 1099, "response_tokens_est": 21, "n_tiles": 1, "output_text": " A man and two women are sitting at a table in a restaurant, with a woman eating a sandwich.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 26839.7, "ram_available_mb": 98932.5, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 26839.7, "ram_available_mb": 98932.5, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.88, "peak": 40.56, "min": 16.56}, "VIN": {"avg": 80.29, "peak": 126.32, "min": 29.17}, "VIN_SYS_5V0": {"avg": 15.08, "peak": 16.15, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 16.14, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 30.88, "energy_joules_est": 49.37, "sample_count": 16, "duration_seconds": 1.599}, "timestamp": "2026-01-19T14:38:02.812657"}
{"image_index": 148, "image_name": "000000015335.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015335.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2514.952, "latencies_ms": [2514.952], "images_per_second": 0.398, "prompt_tokens": 1113, "response_tokens_est": 56, "n_tiles": 1, "output_text": " 1. person: 3\n2. chair: 1\n3. table: 1\n4. person: 1\n5. person: 1\n6. person: 1\n7. person: 1\n8. person: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26839.7, "ram_available_mb": 98932.5, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26839.7, "ram_available_mb": 98932.5, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.72, "peak": 40.56, "min": 18.14}, "VIN": {"avg": 68.66, "peak": 103.32, "min": 27.66}, "VIN_SYS_5V0": {"avg": 14.96, "peak": 16.15, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.87, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 27.72, "energy_joules_est": 69.72, "sample_count": 25, "duration_seconds": 2.515}, "timestamp": "2026-01-19T14:38:05.410702"}
{"image_index": 148, "image_name": "000000015335.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015335.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2606.351, "latencies_ms": [2606.351], "images_per_second": 0.384, "prompt_tokens": 1117, "response_tokens_est": 59, "n_tiles": 1, "output_text": " The person on the left is sitting next to the person in the middle, who is sitting next to the person on the right. The person on the left is closer to the camera than the person in the middle. The person on the right is sitting in front of the person in the middle.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26839.7, "ram_available_mb": 98932.5, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26839.7, "ram_available_mb": 98932.5, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.78, "peak": 40.18, "min": 15.38}, "VIN": {"avg": 67.38, "peak": 124.63, "min": 28.93}, "VIN_SYS_5V0": {"avg": 14.83, "peak": 16.05, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.75, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 26.78, "energy_joules_est": 69.81, "sample_count": 26, "duration_seconds": 2.607}, "timestamp": "2026-01-19T14:38:08.111802"}
{"image_index": 148, "image_name": "000000015335.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015335.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1422.091, "latencies_ms": [1422.091], "images_per_second": 0.703, "prompt_tokens": 1111, "response_tokens_est": 13, "n_tiles": 1, "output_text": " A family of three is sitting at a table in a restaurant.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26839.7, "ram_available_mb": 98932.5, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 26839.7, "ram_available_mb": 98932.5, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.24, "peak": 40.18, "min": 15.38}, "VIN": {"avg": 74.12, "peak": 122.07, "min": 29.39}, "VIN_SYS_5V0": {"avg": 14.73, "peak": 16.05, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.83, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 31.24, "energy_joules_est": 44.44, "sample_count": 14, "duration_seconds": 1.423}, "timestamp": "2026-01-19T14:38:09.581602"}
{"image_index": 148, "image_name": "000000015335.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015335.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2030.105, "latencies_ms": [2030.105], "images_per_second": 0.493, "prompt_tokens": 1109, "response_tokens_est": 37, "n_tiles": 1, "output_text": " The image is taken in a dimly lit restaurant with warm lighting. The colors in the image are mostly warm tones, with the brown of the wood and the red of the wall.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26839.7, "ram_available_mb": 98932.5, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26839.7, "ram_available_mb": 98932.5, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.18, "peak": 40.97, "min": 19.71}, "VIN": {"avg": 71.63, "peak": 107.31, "min": 29.45}, "VIN_SYS_5V0": {"avg": 15.03, "peak": 16.15, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 16.08, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 30.18, "energy_joules_est": 61.28, "sample_count": 20, "duration_seconds": 2.03}, "timestamp": "2026-01-19T14:38:11.665983"}
{"image_index": 149, "image_name": "000000015338.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015338.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2149.414, "latencies_ms": [2149.414], "images_per_second": 0.465, "prompt_tokens": 1099, "response_tokens_est": 42, "n_tiles": 1, "output_text": " The image captures a bustling city street with a yellow bus parked on the side of the road, a white van and a white bus in the background, and a large building with a modern design in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26839.7, "ram_available_mb": 98932.5, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26839.7, "ram_available_mb": 98932.5, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.76, "peak": 40.57, "min": 16.95}, "VIN": {"avg": 69.18, "peak": 105.99, "min": 30.87}, "VIN_SYS_5V0": {"avg": 15.06, "peak": 16.05, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 16.07, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 28.76, "energy_joules_est": 61.84, "sample_count": 21, "duration_seconds": 2.15}, "timestamp": "2026-01-19T14:38:13.855382"}
{"image_index": 149, "image_name": "000000015338.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015338.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2671.75, "latencies_ms": [2671.75], "images_per_second": 0.374, "prompt_tokens": 1113, "response_tokens_est": 62, "n_tiles": 1, "output_text": " 1. Bus: 1\n2. Street: 1\n3. Building: 1\n4. Trees: 2\n5. Benches: 1\n6. Streetlamp: 1\n7. Sidewalk: 1\n8. Bike: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26839.7, "ram_available_mb": 98932.5, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26839.7, "ram_available_mb": 98932.5, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.05, "peak": 40.56, "min": 16.95}, "VIN": {"avg": 67.77, "peak": 104.63, "min": 29.56}, "VIN_SYS_5V0": {"avg": 15.05, "peak": 16.15, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 16.02, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 27.05, "energy_joules_est": 72.28, "sample_count": 26, "duration_seconds": 2.672}, "timestamp": "2026-01-19T14:38:16.565034"}
{"image_index": 149, "image_name": "000000015338.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015338.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2059.403, "latencies_ms": [2059.403], "images_per_second": 0.486, "prompt_tokens": 1117, "response_tokens_est": 38, "n_tiles": 1, "output_text": " The yellow bus is parked on the side of the road, while the white bus is driving down the road. The building is located in the background, and the trees are in the foreground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26839.7, "ram_available_mb": 98932.5, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 26839.7, "ram_available_mb": 98932.5, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.99, "peak": 40.16, "min": 16.16}, "VIN": {"avg": 69.17, "peak": 106.2, "min": 28.75}, "VIN_SYS_5V0": {"avg": 15.06, "peak": 16.15, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 16.12, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 28.99, "energy_joules_est": 59.71, "sample_count": 20, "duration_seconds": 2.06}, "timestamp": "2026-01-19T14:38:18.644143"}
{"image_index": 149, "image_name": "000000015338.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015338.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2556.384, "latencies_ms": [2556.384], "images_per_second": 0.391, "prompt_tokens": 1111, "response_tokens_est": 58, "n_tiles": 1, "output_text": " The image captures a bustling city street with a large building in the background. A yellow bus is driving down the road, while a white bus is parked on the side of the street. The scene is set in a modern city, with trees lining the sidewalk and a clear blue sky overhead.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26839.7, "ram_available_mb": 98932.5, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 26839.6, "ram_available_mb": 98932.5, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.53, "peak": 40.56, "min": 17.73}, "VIN": {"avg": 65.02, "peak": 114.84, "min": 29.34}, "VIN_SYS_5V0": {"avg": 15.1, "peak": 16.15, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 16.22, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 27.53, "energy_joules_est": 70.39, "sample_count": 25, "duration_seconds": 2.557}, "timestamp": "2026-01-19T14:38:21.238082"}
{"image_index": 149, "image_name": "000000015338.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015338.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1978.489, "latencies_ms": [1978.489], "images_per_second": 0.505, "prompt_tokens": 1109, "response_tokens_est": 35, "n_tiles": 1, "output_text": " The image features a vibrant yellow bus parked on the side of the road, with a white building in the background. The sky is clear and blue, indicating a sunny day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26839.6, "ram_available_mb": 98932.5, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26839.6, "ram_available_mb": 98932.5, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.01, "peak": 40.57, "min": 16.16}, "VIN": {"avg": 70.85, "peak": 99.13, "min": 28.24}, "VIN_SYS_5V0": {"avg": 14.98, "peak": 16.05, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 16.12, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 29.01, "energy_joules_est": 57.4, "sample_count": 20, "duration_seconds": 1.979}, "timestamp": "2026-01-19T14:38:23.319681"}
{"image_index": 150, "image_name": "000000015440.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015440.jpg", "image_width": 404, "image_height": 640, "image_resolution": "404x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2027.756, "latencies_ms": [2027.756], "images_per_second": 0.493, "prompt_tokens": 1100, "response_tokens_est": 37, "n_tiles": 1, "output_text": " The image captures a stop sign with a sun flare effect, set against a backdrop of a cityscape with buildings and parked cars, all bathed in the warm glow of the sun.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 26839.6, "ram_available_mb": 98932.5, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26839.6, "ram_available_mb": 98932.5, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.74, "peak": 40.56, "min": 15.77}, "VIN": {"avg": 75.51, "peak": 124.92, "min": 30.9}, "VIN_SYS_5V0": {"avg": 14.78, "peak": 16.05, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.91, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 28.74, "energy_joules_est": 58.3, "sample_count": 20, "duration_seconds": 2.028}, "timestamp": "2026-01-19T14:38:25.405678"}
{"image_index": 150, "image_name": "000000015440.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015440.jpg", "image_width": 404, "image_height": 640, "image_resolution": "404x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2562.301, "latencies_ms": [2562.301], "images_per_second": 0.39, "prompt_tokens": 1114, "response_tokens_est": 58, "n_tiles": 1, "output_text": " 1. stop sign: 1\n2. pole: 1\n3. railing: 1\n4. building: 1\n5. car: 1\n6. street: 1\n7. sidewalk: 1\n8. grass: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26839.6, "ram_available_mb": 98932.5, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 26839.6, "ram_available_mb": 98932.5, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.26, "peak": 40.18, "min": 16.95}, "VIN": {"avg": 65.91, "peak": 106.89, "min": 28.78}, "VIN_SYS_5V0": {"avg": 14.97, "peak": 16.15, "min": 12.46}, "VDD_CPU_SOC_MSS": {"avg": 16.14, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 27.26, "energy_joules_est": 69.86, "sample_count": 25, "duration_seconds": 2.563}, "timestamp": "2026-01-19T14:38:28.003455"}
{"image_index": 150, "image_name": "000000015440.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015440.jpg", "image_width": 404, "image_height": 640, "image_resolution": "404x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2381.265, "latencies_ms": [2381.265], "images_per_second": 0.42, "prompt_tokens": 1118, "response_tokens_est": 51, "n_tiles": 1, "output_text": " The stop sign is positioned to the left of the pole, with the pole being in the foreground and the background featuring a parking lot and buildings. The stop sign is also near the pole, while the parking lot and buildings are far away from the camera.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26839.6, "ram_available_mb": 98932.5, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26839.6, "ram_available_mb": 98932.5, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.82, "peak": 40.16, "min": 16.56}, "VIN": {"avg": 67.9, "peak": 101.28, "min": 28.85}, "VIN_SYS_5V0": {"avg": 15.03, "peak": 16.15, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 16.19, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 27.82, "energy_joules_est": 66.25, "sample_count": 23, "duration_seconds": 2.382}, "timestamp": "2026-01-19T14:38:30.407624"}
{"image_index": 150, "image_name": "000000015440.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015440.jpg", "image_width": 404, "image_height": 640, "image_resolution": "404x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2883.816, "latencies_ms": [2883.816], "images_per_second": 0.347, "prompt_tokens": 1112, "response_tokens_est": 71, "n_tiles": 1, "output_text": " The image captures a serene urban scene during sunset, where a stop sign stands prominently in the foreground, casting a warm glow on the surroundings. In the background, a parking lot is visible, with a few cars parked neatly, and a building with a sign that reads \"\u041c\u0410\u0420\u041a\u0415\u0422\" can be seen.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26839.6, "ram_available_mb": 98932.5, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 26839.9, "ram_available_mb": 98932.3, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.33, "peak": 39.39, "min": 16.95}, "VIN": {"avg": 68.68, "peak": 105.01, "min": 29.95}, "VIN_SYS_5V0": {"avg": 15.12, "peak": 16.26, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 16.13, "peak": 17.71, "min": 14.18}}, "power_watts_avg": 26.33, "energy_joules_est": 75.94, "sample_count": 28, "duration_seconds": 2.884}, "timestamp": "2026-01-19T14:38:33.319484"}
{"image_index": 150, "image_name": "000000015440.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015440.jpg", "image_width": 404, "image_height": 640, "image_resolution": "404x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2644.584, "latencies_ms": [2644.584], "images_per_second": 0.378, "prompt_tokens": 1110, "response_tokens_est": 56, "n_tiles": 1, "output_text": " The image features a red stop sign with a white \"STOP\" written on it, standing on a metal pole. The sun is shining brightly, casting a warm glow on the scene. The background includes a parking lot with cars and buildings, and the ground is covered with gravel.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26839.9, "ram_available_mb": 98932.3, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 26839.9, "ram_available_mb": 98932.3, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.32, "peak": 40.16, "min": 16.16}, "VIN": {"avg": 61.66, "peak": 98.72, "min": 29.03}, "VIN_SYS_5V0": {"avg": 14.8, "peak": 16.26, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 15.84, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 26.32, "energy_joules_est": 69.61, "sample_count": 26, "duration_seconds": 2.645}, "timestamp": "2026-01-19T14:38:36.019053"}
{"image_index": 151, "image_name": "000000015497.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015497.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1635.404, "latencies_ms": [1635.404], "images_per_second": 0.611, "prompt_tokens": 1099, "response_tokens_est": 21, "n_tiles": 1, "output_text": " A cat with a white and brown coat is lying on a black surface next to a white computer mouse.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 26839.9, "ram_available_mb": 98932.3, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 26839.9, "ram_available_mb": 98932.3, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.43, "peak": 40.16, "min": 15.38}, "VIN": {"avg": 71.03, "peak": 105.55, "min": 30.37}, "VIN_SYS_5V0": {"avg": 14.87, "peak": 15.95, "min": 12.15}, "VDD_CPU_SOC_MSS": {"avg": 15.85, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 30.43, "energy_joules_est": 49.78, "sample_count": 16, "duration_seconds": 1.636}, "timestamp": "2026-01-19T14:38:37.691193"}
{"image_index": 151, "image_name": "000000015497.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015497.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1991.638, "latencies_ms": [1991.638], "images_per_second": 0.502, "prompt_tokens": 1113, "response_tokens_est": 36, "n_tiles": 1, "output_text": " cat: 1, mouse: 1, cord: 1, black: 1, white: 1, brown: 1, black and white: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26839.9, "ram_available_mb": 98932.3, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26839.9, "ram_available_mb": 98932.3, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.72, "peak": 40.56, "min": 18.91}, "VIN": {"avg": 68.92, "peak": 120.68, "min": 27.08}, "VIN_SYS_5V0": {"avg": 15.06, "peak": 16.15, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 16.06, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 29.72, "energy_joules_est": 59.2, "sample_count": 20, "duration_seconds": 1.992}, "timestamp": "2026-01-19T14:38:39.772280"}
{"image_index": 151, "image_name": "000000015497.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015497.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2140.754, "latencies_ms": [2140.754], "images_per_second": 0.467, "prompt_tokens": 1117, "response_tokens_est": 41, "n_tiles": 1, "output_text": " The cat is in the foreground, lying on a black surface. The mouse is in the foreground, to the left of the cat. The cord is in the foreground, to the right of the cat.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26839.9, "ram_available_mb": 98932.3, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26840.4, "ram_available_mb": 98931.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.48, "peak": 40.18, "min": 16.16}, "VIN": {"avg": 76.11, "peak": 128.31, "min": 28.76}, "VIN_SYS_5V0": {"avg": 14.86, "peak": 16.05, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.93, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 28.48, "energy_joules_est": 60.98, "sample_count": 21, "duration_seconds": 2.141}, "timestamp": "2026-01-19T14:38:41.969790"}
{"image_index": 151, "image_name": "000000015497.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015497.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1465.843, "latencies_ms": [1465.843], "images_per_second": 0.682, "prompt_tokens": 1111, "response_tokens_est": 15, "n_tiles": 1, "output_text": " A cat is lying on a bed with a computer mouse next to it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26840.4, "ram_available_mb": 98931.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 26840.4, "ram_available_mb": 98931.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.28, "peak": 40.57, "min": 16.56}, "VIN": {"avg": 72.55, "peak": 98.9, "min": 28.21}, "VIN_SYS_5V0": {"avg": 14.93, "peak": 16.15, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.98, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 31.28, "energy_joules_est": 45.86, "sample_count": 15, "duration_seconds": 1.466}, "timestamp": "2026-01-19T14:38:43.533346"}
{"image_index": 151, "image_name": "000000015497.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015497.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1517.48, "latencies_ms": [1517.48], "images_per_second": 0.659, "prompt_tokens": 1109, "response_tokens_est": 16, "n_tiles": 1, "output_text": " The cat is white and brown with green eyes, and the mouse is white.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26840.4, "ram_available_mb": 98931.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 26840.4, "ram_available_mb": 98931.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.96, "peak": 40.57, "min": 19.32}, "VIN": {"avg": 71.73, "peak": 118.05, "min": 27.96}, "VIN_SYS_5V0": {"avg": 14.75, "peak": 15.95, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.8, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 31.96, "energy_joules_est": 48.51, "sample_count": 15, "duration_seconds": 1.518}, "timestamp": "2026-01-19T14:38:45.101851"}
{"image_index": 152, "image_name": "000000015517.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015517.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1878.821, "latencies_ms": [1878.821], "images_per_second": 0.532, "prompt_tokens": 1099, "response_tokens_est": 30, "n_tiles": 1, "output_text": " The image captures a bustling cityscape with a large parking lot filled with buses, a prominent white building, and a clear blue sky dotted with clouds.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 26840.4, "ram_available_mb": 98931.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26840.4, "ram_available_mb": 98931.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.02, "peak": 40.56, "min": 18.91}, "VIN": {"avg": 71.72, "peak": 120.99, "min": 30.39}, "VIN_SYS_5V0": {"avg": 14.86, "peak": 15.95, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.89, "peak": 16.92, "min": 13.79}}, "power_watts_avg": 30.02, "energy_joules_est": 56.41, "sample_count": 19, "duration_seconds": 1.879}, "timestamp": "2026-01-19T14:38:47.082591"}
{"image_index": 152, "image_name": "000000015517.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015517.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2522.703, "latencies_ms": [2522.703], "images_per_second": 0.396, "prompt_tokens": 1113, "response_tokens_est": 56, "n_tiles": 1, "output_text": " 1. Bus: 3\n2. Bus: 2\n3. Bus: 1\n4. Bus: 1\n5. Bus: 1\n6. Bus: 1\n7. Bus: 1\n8. Bus: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26840.4, "ram_available_mb": 98931.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26840.4, "ram_available_mb": 98931.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.19, "peak": 40.56, "min": 16.16}, "VIN": {"avg": 64.39, "peak": 111.74, "min": 29.92}, "VIN_SYS_5V0": {"avg": 14.89, "peak": 16.05, "min": 11.87}, "VDD_CPU_SOC_MSS": {"avg": 15.8, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 27.19, "energy_joules_est": 68.6, "sample_count": 25, "duration_seconds": 2.523}, "timestamp": "2026-01-19T14:38:49.693623"}
{"image_index": 152, "image_name": "000000015517.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015517.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1901.911, "latencies_ms": [1901.911], "images_per_second": 0.526, "prompt_tokens": 1117, "response_tokens_est": 32, "n_tiles": 1, "output_text": " The buses are positioned in the foreground, with the buildings in the background. The trees are located near the buses, while the overpass is situated further back.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26840.4, "ram_available_mb": 98931.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26840.4, "ram_available_mb": 98931.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.17, "peak": 40.56, "min": 15.38}, "VIN": {"avg": 71.54, "peak": 105.3, "min": 29.33}, "VIN_SYS_5V0": {"avg": 14.91, "peak": 16.05, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 15.85, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 29.17, "energy_joules_est": 55.49, "sample_count": 19, "duration_seconds": 1.902}, "timestamp": "2026-01-19T14:38:51.674453"}
{"image_index": 152, "image_name": "000000015517.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015517.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1796.038, "latencies_ms": [1796.038], "images_per_second": 0.557, "prompt_tokens": 1111, "response_tokens_est": 28, "n_tiles": 1, "output_text": " The image captures a bustling cityscape with a large parking lot filled with buses and cars, surrounded by tall buildings and a clear blue sky.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26840.4, "ram_available_mb": 98931.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26840.4, "ram_available_mb": 98931.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.92, "peak": 40.57, "min": 16.95}, "VIN": {"avg": 69.98, "peak": 114.03, "min": 27.25}, "VIN_SYS_5V0": {"avg": 14.87, "peak": 16.05, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 15.92, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 29.92, "energy_joules_est": 53.75, "sample_count": 18, "duration_seconds": 1.796}, "timestamp": "2026-01-19T14:38:53.557061"}
{"image_index": 152, "image_name": "000000015517.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015517.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2336.224, "latencies_ms": [2336.224], "images_per_second": 0.428, "prompt_tokens": 1109, "response_tokens_est": 46, "n_tiles": 1, "output_text": " The image features a vibrant cityscape with a mix of green and blue hues, indicative of a clear sky. The buildings are predominantly white and gray, with some having red accents, suggesting a blend of modern and traditional architectural styles.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26840.4, "ram_available_mb": 98931.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26840.4, "ram_available_mb": 98931.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.8, "peak": 40.18, "min": 16.95}, "VIN": {"avg": 68.5, "peak": 121.85, "min": 28.81}, "VIN_SYS_5V0": {"avg": 14.69, "peak": 15.85, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.65, "peak": 16.92, "min": 13.0}}, "power_watts_avg": 27.8, "energy_joules_est": 64.96, "sample_count": 23, "duration_seconds": 2.337}, "timestamp": "2026-01-19T14:38:55.956684"}
{"image_index": 153, "image_name": "000000015597.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015597.jpg", "image_width": 433, "image_height": 640, "image_resolution": "433x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1873.173, "latencies_ms": [1873.173], "images_per_second": 0.534, "prompt_tokens": 1100, "response_tokens_est": 31, "n_tiles": 1, "output_text": " A man wearing a cowboy hat and black shorts is riding a skateboard on a ramp in a skate park, with a large tent in the background.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 26840.4, "ram_available_mb": 98931.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26840.9, "ram_available_mb": 98931.3, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.92, "peak": 39.77, "min": 15.77}, "VIN": {"avg": 76.13, "peak": 121.17, "min": 27.6}, "VIN_SYS_5V0": {"avg": 14.82, "peak": 16.05, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 15.89, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 28.92, "energy_joules_est": 54.2, "sample_count": 19, "duration_seconds": 1.874}, "timestamp": "2026-01-19T14:38:57.938973"}
{"image_index": 153, "image_name": "000000015597.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015597.jpg", "image_width": 433, "image_height": 640, "image_resolution": "433x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2572.212, "latencies_ms": [2572.212], "images_per_second": 0.389, "prompt_tokens": 1114, "response_tokens_est": 57, "n_tiles": 1, "output_text": " 1. person: 1\n2. hat: 1\n3. shorts: 1\n4. skateboard: 1\n5. ramp: 1\n6. shadow: 1\n7. tent: 1\n8. ground: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26840.9, "ram_available_mb": 98931.3, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26840.8, "ram_available_mb": 98931.3, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.01, "peak": 40.16, "min": 16.16}, "VIN": {"avg": 65.99, "peak": 126.45, "min": 28.88}, "VIN_SYS_5V0": {"avg": 14.8, "peak": 16.05, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.87, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 27.01, "energy_joules_est": 69.49, "sample_count": 25, "duration_seconds": 2.573}, "timestamp": "2026-01-19T14:39:00.538330"}
{"image_index": 153, "image_name": "000000015597.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015597.jpg", "image_width": 433, "image_height": 640, "image_resolution": "433x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2535.674, "latencies_ms": [2535.674], "images_per_second": 0.394, "prompt_tokens": 1118, "response_tokens_est": 57, "n_tiles": 1, "output_text": " The skateboarder is positioned in the foreground, performing a trick on the ramp, while the tents are located in the background, providing a contrast between the foreground and background elements. The shadow of the skateboarder is cast on the ramp, indicating the direction of the light source.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26840.8, "ram_available_mb": 98931.3, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 26844.9, "ram_available_mb": 98927.3, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.17, "peak": 40.16, "min": 16.56}, "VIN": {"avg": 63.88, "peak": 94.83, "min": 29.45}, "VIN_SYS_5V0": {"avg": 15.07, "peak": 16.26, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 16.09, "peak": 17.71, "min": 13.79}}, "power_watts_avg": 27.17, "energy_joules_est": 68.91, "sample_count": 25, "duration_seconds": 2.536}, "timestamp": "2026-01-19T14:39:03.137763"}
{"image_index": 153, "image_name": "000000015597.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015597.jpg", "image_width": 433, "image_height": 640, "image_resolution": "433x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1596.403, "latencies_ms": [1596.403], "images_per_second": 0.626, "prompt_tokens": 1112, "response_tokens_est": 20, "n_tiles": 1, "output_text": " A man wearing a hat and shorts is riding a skateboard on a ramp in a skate park.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26844.9, "ram_available_mb": 98927.3, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26851.1, "ram_available_mb": 98921.1, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.21, "peak": 39.78, "min": 16.16}, "VIN": {"avg": 78.43, "peak": 122.86, "min": 29.68}, "VIN_SYS_5V0": {"avg": 14.89, "peak": 16.26, "min": 12.25}, "VDD_CPU_SOC_MSS": {"avg": 15.99, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 30.21, "energy_joules_est": 48.24, "sample_count": 16, "duration_seconds": 1.597}, "timestamp": "2026-01-19T14:39:04.805472"}
{"image_index": 153, "image_name": "000000015597.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015597.jpg", "image_width": 433, "image_height": 640, "image_resolution": "433x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2160.35, "latencies_ms": [2160.35], "images_per_second": 0.463, "prompt_tokens": 1110, "response_tokens_est": 42, "n_tiles": 1, "output_text": " The image features a skateboarder performing a trick on a ramp, with a clear blue sky in the background. The skateboarder is wearing a hat and shorts, and the ramp is made of wood.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26851.1, "ram_available_mb": 98921.1, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26851.1, "ram_available_mb": 98921.1, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.99, "peak": 40.57, "min": 18.92}, "VIN": {"avg": 73.18, "peak": 123.11, "min": 30.07}, "VIN_SYS_5V0": {"avg": 14.95, "peak": 16.15, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 16.01, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 28.99, "energy_joules_est": 62.64, "sample_count": 21, "duration_seconds": 2.161}, "timestamp": "2026-01-19T14:39:06.991396"}
{"image_index": 154, "image_name": "000000015660.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015660.jpg", "image_width": 640, "image_height": 348, "image_resolution": "640x348", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1810.501, "latencies_ms": [1810.501], "images_per_second": 0.552, "prompt_tokens": 1099, "response_tokens_est": 29, "n_tiles": 1, "output_text": " A person is standing on a surfboard with a windsurfing board attached to it, and there are several kites flying in the sky.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26851.1, "ram_available_mb": 98921.1, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26851.1, "ram_available_mb": 98921.1, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.07, "peak": 40.56, "min": 17.35}, "VIN": {"avg": 69.93, "peak": 125.11, "min": 28.91}, "VIN_SYS_5V0": {"avg": 15.08, "peak": 16.15, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 16.23, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 30.07, "energy_joules_est": 54.46, "sample_count": 18, "duration_seconds": 1.811}, "timestamp": "2026-01-19T14:39:08.872466"}
{"image_index": 154, "image_name": "000000015660.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015660.jpg", "image_width": 640, "image_height": 348, "image_resolution": "640x348", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2632.231, "latencies_ms": [2632.231], "images_per_second": 0.38, "prompt_tokens": 1113, "response_tokens_est": 60, "n_tiles": 1, "output_text": " 1. person: 1\n2. windsurf board: 1\n3. kite: 2\n4. ocean: 1\n5. sky: 2\n6. waves: 1\n7. water: 1\n8. kite string: 1", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26851.1, "ram_available_mb": 98921.1, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 26852.5, "ram_available_mb": 98919.6, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.05, "peak": 40.56, "min": 17.34}, "VIN": {"avg": 73.55, "peak": 129.4, "min": 28.99}, "VIN_SYS_5V0": {"avg": 14.94, "peak": 16.05, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.88, "peak": 17.32, "min": 13.4}}, "power_watts_avg": 27.05, "energy_joules_est": 71.21, "sample_count": 26, "duration_seconds": 2.632}, "timestamp": "2026-01-19T14:39:11.578449"}
{"image_index": 154, "image_name": "000000015660.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015660.jpg", "image_width": 640, "image_height": 348, "image_resolution": "640x348", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2177.638, "latencies_ms": [2177.638], "images_per_second": 0.459, "prompt_tokens": 1117, "response_tokens_est": 42, "n_tiles": 1, "output_text": " The kiteboarder is positioned in the foreground, with the ocean and sky in the background. The kites are flying in the sky, with the kiteboarder's kite being the closest to the camera.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26852.5, "ram_available_mb": 98919.6, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26852.8, "ram_available_mb": 98919.4, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.42, "peak": 40.18, "min": 15.38}, "VIN": {"avg": 74.15, "peak": 118.86, "min": 32.45}, "VIN_SYS_5V0": {"avg": 14.94, "peak": 16.05, "min": 12.25}, "VDD_CPU_SOC_MSS": {"avg": 15.95, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 28.42, "energy_joules_est": 61.91, "sample_count": 21, "duration_seconds": 2.178}, "timestamp": "2026-01-19T14:39:13.761303"}
{"image_index": 154, "image_name": "000000015660.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015660.jpg", "image_width": 640, "image_height": 348, "image_resolution": "640x348", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1727.902, "latencies_ms": [1727.902], "images_per_second": 0.579, "prompt_tokens": 1111, "response_tokens_est": 26, "n_tiles": 1, "output_text": " A man is windsurfing in the ocean with a kite. The sky is blue and there are other people in the water.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26852.8, "ram_available_mb": 98919.4, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26852.8, "ram_available_mb": 98919.4, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.77, "peak": 40.18, "min": 18.14}, "VIN": {"avg": 76.7, "peak": 121.79, "min": 29.4}, "VIN_SYS_5V0": {"avg": 15.25, "peak": 16.15, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 16.33, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 30.77, "energy_joules_est": 53.18, "sample_count": 17, "duration_seconds": 1.728}, "timestamp": "2026-01-19T14:39:15.537933"}
{"image_index": 154, "image_name": "000000015660.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015660.jpg", "image_width": 640, "image_height": 348, "image_resolution": "640x348", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2572.183, "latencies_ms": [2572.183], "images_per_second": 0.389, "prompt_tokens": 1109, "response_tokens_est": 58, "n_tiles": 1, "output_text": " The image features a vibrant scene with a clear blue sky and a vast ocean. The water is a deep blue, and the sky is a bright blue with a few white clouds scattered across it. The windsurfer is wearing a red shirt and is holding onto a blue and white sail.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26852.8, "ram_available_mb": 98919.4, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26852.8, "ram_available_mb": 98919.4, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.52, "peak": 40.56, "min": 18.14}, "VIN": {"avg": 68.22, "peak": 103.5, "min": 29.39}, "VIN_SYS_5V0": {"avg": 15.01, "peak": 16.05, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 16.06, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 27.52, "energy_joules_est": 70.79, "sample_count": 25, "duration_seconds": 2.572}, "timestamp": "2026-01-19T14:39:18.150136"}
{"image_index": 155, "image_name": "000000015746.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015746.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1562.769, "latencies_ms": [1562.769], "images_per_second": 0.64, "prompt_tokens": 1100, "response_tokens_est": 19, "n_tiles": 1, "output_text": " An old red fire hydrant is in the grass in front of a house with purple flowers.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 26852.8, "ram_available_mb": 98919.4, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 26852.8, "ram_available_mb": 98919.4, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.53, "peak": 40.16, "min": 16.16}, "VIN": {"avg": 72.75, "peak": 112.86, "min": 29.51}, "VIN_SYS_5V0": {"avg": 14.94, "peak": 16.15, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 16.07, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 30.53, "energy_joules_est": 47.73, "sample_count": 16, "duration_seconds": 1.563}, "timestamp": "2026-01-19T14:39:19.824233"}
{"image_index": 155, "image_name": "000000015746.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015746.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1761.853, "latencies_ms": [1761.853], "images_per_second": 0.568, "prompt_tokens": 1114, "response_tokens_est": 26, "n_tiles": 1, "output_text": " fire hydrant: 1\ngrass: 1\ntree: 1\nhouse: 1\nflowers: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26852.8, "ram_available_mb": 98919.4, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26852.8, "ram_available_mb": 98919.4, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.5, "peak": 40.18, "min": 18.14}, "VIN": {"avg": 69.6, "peak": 125.36, "min": 30.0}, "VIN_SYS_5V0": {"avg": 14.79, "peak": 16.05, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.86, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 30.5, "energy_joules_est": 53.75, "sample_count": 17, "duration_seconds": 1.762}, "timestamp": "2026-01-19T14:39:21.602797"}
{"image_index": 155, "image_name": "000000015746.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015746.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2144.146, "latencies_ms": [2144.146], "images_per_second": 0.466, "prompt_tokens": 1118, "response_tokens_est": 42, "n_tiles": 1, "output_text": " The fire hydrant is located in the foreground of the image, with the house and trees in the background. The hydrant is positioned to the left of the house, and the grass is in the foreground.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26852.8, "ram_available_mb": 98919.4, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26852.8, "ram_available_mb": 98919.4, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.1, "peak": 40.56, "min": 19.32}, "VIN": {"avg": 65.64, "peak": 112.96, "min": 28.94}, "VIN_SYS_5V0": {"avg": 15.17, "peak": 16.26, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 16.22, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 29.1, "energy_joules_est": 62.4, "sample_count": 21, "duration_seconds": 2.144}, "timestamp": "2026-01-19T14:39:23.790976"}
{"image_index": 155, "image_name": "000000015746.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015746.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1458.072, "latencies_ms": [1458.072], "images_per_second": 0.686, "prompt_tokens": 1112, "response_tokens_est": 15, "n_tiles": 1, "output_text": " A red fire hydrant is in the grass in front of a house.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26852.8, "ram_available_mb": 98919.4, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 26852.8, "ram_available_mb": 98919.4, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.1, "peak": 39.78, "min": 16.95}, "VIN": {"avg": 76.02, "peak": 122.46, "min": 27.69}, "VIN_SYS_5V0": {"avg": 14.99, "peak": 16.26, "min": 12.56}, "VDD_CPU_SOC_MSS": {"avg": 16.06, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 31.1, "energy_joules_est": 45.37, "sample_count": 15, "duration_seconds": 1.459}, "timestamp": "2026-01-19T14:39:25.356337"}
{"image_index": 155, "image_name": "000000015746.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015746.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1611.288, "latencies_ms": [1611.288], "images_per_second": 0.621, "prompt_tokens": 1110, "response_tokens_est": 20, "n_tiles": 1, "output_text": " The fire hydrant is red and black, and it is in a grassy area with dandelions.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26852.8, "ram_available_mb": 98919.4, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26853.5, "ram_available_mb": 98918.7, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.22, "peak": 40.57, "min": 18.91}, "VIN": {"avg": 68.51, "peak": 100.31, "min": 28.6}, "VIN_SYS_5V0": {"avg": 14.73, "peak": 16.05, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.9, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 31.22, "energy_joules_est": 50.31, "sample_count": 16, "duration_seconds": 1.612}, "timestamp": "2026-01-19T14:39:27.024811"}
{"image_index": 156, "image_name": "000000015751.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015751.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1631.285, "latencies_ms": [1631.285], "images_per_second": 0.613, "prompt_tokens": 1099, "response_tokens_est": 21, "n_tiles": 1, "output_text": " A bird is flying over a roof with a blue hue, while other birds are on the ground nearby.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26853.5, "ram_available_mb": 98918.7, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26853.8, "ram_available_mb": 98918.4, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.47, "peak": 40.57, "min": 19.32}, "VIN": {"avg": 78.31, "peak": 121.66, "min": 28.85}, "VIN_SYS_5V0": {"avg": 14.9, "peak": 16.05, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 16.04, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 31.47, "energy_joules_est": 51.35, "sample_count": 16, "duration_seconds": 1.632}, "timestamp": "2026-01-19T14:39:28.703319"}
{"image_index": 156, "image_name": "000000015751.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015751.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2086.419, "latencies_ms": [2086.419], "images_per_second": 0.479, "prompt_tokens": 1113, "response_tokens_est": 39, "n_tiles": 1, "output_text": " bird: 1, roof: 1, bird: 1, roof: 1, bird: 1, roof: 1, bird: 1, roof: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26853.8, "ram_available_mb": 98918.4, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26853.8, "ram_available_mb": 98918.4, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.24, "peak": 40.56, "min": 18.52}, "VIN": {"avg": 68.82, "peak": 127.31, "min": 29.15}, "VIN_SYS_5V0": {"avg": 14.98, "peak": 16.15, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.95, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 29.24, "energy_joules_est": 61.03, "sample_count": 21, "duration_seconds": 2.087}, "timestamp": "2026-01-19T14:39:30.887737"}
{"image_index": 156, "image_name": "000000015751.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015751.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2082.679, "latencies_ms": [2082.679], "images_per_second": 0.48, "prompt_tokens": 1117, "response_tokens_est": 38, "n_tiles": 1, "output_text": " The main object, a bird, is located in the foreground, flying towards the right side of the image. The bird is near the roof of the house, which is in the background.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26853.8, "ram_available_mb": 98918.4, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26853.8, "ram_available_mb": 98918.4, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.39, "peak": 39.78, "min": 15.77}, "VIN": {"avg": 66.82, "peak": 82.72, "min": 27.94}, "VIN_SYS_5V0": {"avg": 14.75, "peak": 15.95, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.75, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 28.39, "energy_joules_est": 59.14, "sample_count": 21, "duration_seconds": 2.083}, "timestamp": "2026-01-19T14:39:33.065379"}
{"image_index": 156, "image_name": "000000015751.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015751.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1495.228, "latencies_ms": [1495.228], "images_per_second": 0.669, "prompt_tokens": 1111, "response_tokens_est": 16, "n_tiles": 1, "output_text": " A bird is flying over a roof, while other birds are on the ground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26853.8, "ram_available_mb": 98918.4, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 26853.8, "ram_available_mb": 98918.4, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.09, "peak": 40.56, "min": 16.16}, "VIN": {"avg": 78.09, "peak": 118.75, "min": 29.53}, "VIN_SYS_5V0": {"avg": 14.79, "peak": 16.05, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.85, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 31.09, "energy_joules_est": 46.5, "sample_count": 15, "duration_seconds": 1.496}, "timestamp": "2026-01-19T14:39:34.633824"}
{"image_index": 156, "image_name": "000000015751.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015751.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2001.249, "latencies_ms": [2001.249], "images_per_second": 0.5, "prompt_tokens": 1109, "response_tokens_est": 35, "n_tiles": 1, "output_text": " The image features a bird with a blue and black body, flying over a wooden roof with a green hue. The lighting is natural, and the weather appears to be clear.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26853.8, "ram_available_mb": 98918.4, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26853.8, "ram_available_mb": 98918.4, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.6, "peak": 40.56, "min": 18.91}, "VIN": {"avg": 67.11, "peak": 118.22, "min": 27.64}, "VIN_SYS_5V0": {"avg": 14.84, "peak": 16.05, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.91, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 29.6, "energy_joules_est": 59.25, "sample_count": 20, "duration_seconds": 2.002}, "timestamp": "2026-01-19T14:39:36.722772"}
{"image_index": 157, "image_name": "000000015956.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015956.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1754.048, "latencies_ms": [1754.048], "images_per_second": 0.57, "prompt_tokens": 1099, "response_tokens_est": 26, "n_tiles": 1, "output_text": " In the image, a person is walking a horse in a barn, with a red door and a ladder visible in the background.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 26853.8, "ram_available_mb": 98918.4, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26853.8, "ram_available_mb": 98918.4, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.29, "peak": 40.18, "min": 16.16}, "VIN": {"avg": 71.56, "peak": 124.51, "min": 28.75}, "VIN_SYS_5V0": {"avg": 14.81, "peak": 16.05, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.84, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 30.29, "energy_joules_est": 53.16, "sample_count": 17, "duration_seconds": 1.755}, "timestamp": "2026-01-19T14:39:38.497676"}
{"image_index": 157, "image_name": "000000015956.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015956.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1973.342, "latencies_ms": [1973.342], "images_per_second": 0.507, "prompt_tokens": 1113, "response_tokens_est": 34, "n_tiles": 1, "output_text": " horse: 1, person: 1, door: 1, ladder: 1, bucket: 1, wall: 1, window: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26853.8, "ram_available_mb": 98918.4, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26853.8, "ram_available_mb": 98918.4, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.33, "peak": 40.57, "min": 19.7}, "VIN": {"avg": 69.48, "peak": 122.75, "min": 31.5}, "VIN_SYS_5V0": {"avg": 15.24, "peak": 16.26, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 16.22, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 30.33, "energy_joules_est": 59.87, "sample_count": 19, "duration_seconds": 1.974}, "timestamp": "2026-01-19T14:39:40.477479"}
{"image_index": 157, "image_name": "000000015956.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015956.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2210.603, "latencies_ms": [2210.603], "images_per_second": 0.452, "prompt_tokens": 1117, "response_tokens_est": 45, "n_tiles": 1, "output_text": " The horse is in the foreground, walking towards the camera. The person is in the background, walking away from the camera. The door is on the left side of the image, and the ladder is on the right side.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26853.8, "ram_available_mb": 98918.4, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26854.0, "ram_available_mb": 98918.2, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.69, "peak": 40.56, "min": 18.52}, "VIN": {"avg": 73.75, "peak": 121.96, "min": 29.54}, "VIN_SYS_5V0": {"avg": 15.2, "peak": 16.15, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 16.28, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 28.69, "energy_joules_est": 63.43, "sample_count": 22, "duration_seconds": 2.211}, "timestamp": "2026-01-19T14:39:42.763121"}
{"image_index": 157, "image_name": "000000015956.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015956.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1339.985, "latencies_ms": [1339.985], "images_per_second": 0.746, "prompt_tokens": 1111, "response_tokens_est": 10, "n_tiles": 1, "output_text": " A woman is walking a horse in a barn.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26854.0, "ram_available_mb": 98918.2, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 2.5, "ram_used_mb": 26854.0, "ram_available_mb": 98918.2, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.97, "peak": 40.16, "min": 16.16}, "VIN": {"avg": 76.46, "peak": 119.39, "min": 29.12}, "VIN_SYS_5V0": {"avg": 14.9, "peak": 16.05, "min": 12.15}, "VDD_CPU_SOC_MSS": {"avg": 15.99, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 31.97, "energy_joules_est": 42.85, "sample_count": 13, "duration_seconds": 1.34}, "timestamp": "2026-01-19T14:39:44.119682"}
{"image_index": 157, "image_name": "000000015956.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015956.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2163.594, "latencies_ms": [2163.594], "images_per_second": 0.462, "prompt_tokens": 1109, "response_tokens_est": 43, "n_tiles": 1, "output_text": " The image is taken in a stable with a brown horse and a woman walking in it. The horse is wearing a red harness and the woman is wearing blue jeans. The lighting is natural and the colors are warm.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26854.0, "ram_available_mb": 98918.2, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26858.0, "ram_available_mb": 98914.2, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.44, "peak": 41.34, "min": 20.48}, "VIN": {"avg": 75.73, "peak": 127.72, "min": 29.0}, "VIN_SYS_5V0": {"avg": 15.22, "peak": 16.26, "min": 13.29}, "VDD_CPU_SOC_MSS": {"avg": 16.33, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 30.44, "energy_joules_est": 65.87, "sample_count": 21, "duration_seconds": 2.164}, "timestamp": "2026-01-19T14:39:46.303000"}
{"image_index": 158, "image_name": "000000016010.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016010.jpg", "image_width": 640, "image_height": 471, "image_resolution": "640x471", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1945.26, "latencies_ms": [1945.26], "images_per_second": 0.514, "prompt_tokens": 1099, "response_tokens_est": 34, "n_tiles": 1, "output_text": " In the image, a group of animals, including zebras and sheep, are grazing in a grassy field, with a tree trunk and a fence visible in the foreground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26858.0, "ram_available_mb": 98914.2, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26858.0, "ram_available_mb": 98914.2, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.63, "peak": 40.57, "min": 17.34}, "VIN": {"avg": 68.09, "peak": 114.95, "min": 28.9}, "VIN_SYS_5V0": {"avg": 15.11, "peak": 16.15, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 16.18, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 29.63, "energy_joules_est": 57.66, "sample_count": 19, "duration_seconds": 1.946}, "timestamp": "2026-01-19T14:39:48.288971"}
{"image_index": 158, "image_name": "000000016010.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016010.jpg", "image_width": 640, "image_height": 471, "image_resolution": "640x471", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2524.148, "latencies_ms": [2524.148], "images_per_second": 0.396, "prompt_tokens": 1113, "response_tokens_est": 56, "n_tiles": 1, "output_text": " 1. tree: 1\n2. grass: 1\n3. zebra: 2\n4. sheep: 2\n5. rocks: 1\n6. fence: 1\n7. water: 1\n8. sky: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26858.0, "ram_available_mb": 98914.2, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26858.0, "ram_available_mb": 98914.2, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.45, "peak": 40.18, "min": 17.74}, "VIN": {"avg": 71.02, "peak": 123.56, "min": 29.13}, "VIN_SYS_5V0": {"avg": 14.99, "peak": 16.15, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 16.05, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 27.45, "energy_joules_est": 69.3, "sample_count": 25, "duration_seconds": 2.524}, "timestamp": "2026-01-19T14:39:50.890023"}
{"image_index": 158, "image_name": "000000016010.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016010.jpg", "image_width": 640, "image_height": 471, "image_resolution": "640x471", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2414.866, "latencies_ms": [2414.866], "images_per_second": 0.414, "prompt_tokens": 1117, "response_tokens_est": 51, "n_tiles": 1, "output_text": " The zebras are positioned in the foreground, with the sheep grazing in the background. The zebras are closer to the camera than the sheep, which are further away. The zebras are near the pond, while the sheep are grazing on the grass.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26858.0, "ram_available_mb": 98914.2, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26858.0, "ram_available_mb": 98914.2, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.25, "peak": 40.18, "min": 15.38}, "VIN": {"avg": 66.73, "peak": 125.51, "min": 28.33}, "VIN_SYS_5V0": {"avg": 14.84, "peak": 16.05, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 15.85, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 27.25, "energy_joules_est": 65.82, "sample_count": 24, "duration_seconds": 2.415}, "timestamp": "2026-01-19T14:39:53.393860"}
{"image_index": 158, "image_name": "000000016010.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016010.jpg", "image_width": 640, "image_height": 471, "image_resolution": "640x471", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2625.788, "latencies_ms": [2625.788], "images_per_second": 0.381, "prompt_tokens": 1111, "response_tokens_est": 59, "n_tiles": 1, "output_text": " In a lush green field, a herd of animals, including zebras and sheep, graze peacefully. The animals are scattered across the field, with some standing and others lying down, enjoying the fresh grass. In the background, tall trees rise up, providing a natural backdrop to this serene scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26858.0, "ram_available_mb": 98914.2, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26858.0, "ram_available_mb": 98914.2, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.62, "peak": 40.16, "min": 15.38}, "VIN": {"avg": 68.4, "peak": 120.39, "min": 28.48}, "VIN_SYS_5V0": {"avg": 14.8, "peak": 16.05, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.84, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 26.62, "energy_joules_est": 69.91, "sample_count": 26, "duration_seconds": 2.626}, "timestamp": "2026-01-19T14:39:56.098987"}
{"image_index": 158, "image_name": "000000016010.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016010.jpg", "image_width": 640, "image_height": 471, "image_resolution": "640x471", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2121.069, "latencies_ms": [2121.069], "images_per_second": 0.471, "prompt_tokens": 1109, "response_tokens_est": 40, "n_tiles": 1, "output_text": " The image features a grassy field with a variety of animals, including zebras, sheep, and deer. The sky is blue with some clouds, and the trees in the background are tall and green.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26858.0, "ram_available_mb": 98914.2, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26859.0, "ram_available_mb": 98913.2, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.27, "peak": 40.18, "min": 15.38}, "VIN": {"avg": 68.71, "peak": 119.61, "min": 31.03}, "VIN_SYS_5V0": {"avg": 14.86, "peak": 16.05, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 15.95, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 28.27, "energy_joules_est": 59.98, "sample_count": 21, "duration_seconds": 2.122}, "timestamp": "2026-01-19T14:39:58.281351"}
{"image_index": 159, "image_name": "000000016228.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016228.jpg", "image_width": 640, "image_height": 440, "image_resolution": "640x440", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1760.559, "latencies_ms": [1760.559], "images_per_second": 0.568, "prompt_tokens": 1099, "response_tokens_est": 26, "n_tiles": 1, "output_text": " A vintage trolley car with a green and gold body and a red roof is being pulled by two white horses in a park.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26859.0, "ram_available_mb": 98913.2, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26859.0, "ram_available_mb": 98913.2, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.36, "peak": 40.18, "min": 16.56}, "VIN": {"avg": 70.79, "peak": 125.22, "min": 30.7}, "VIN_SYS_5V0": {"avg": 14.92, "peak": 16.05, "min": 12.25}, "VDD_CPU_SOC_MSS": {"avg": 16.0, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 30.36, "energy_joules_est": 53.48, "sample_count": 17, "duration_seconds": 1.761}, "timestamp": "2026-01-19T14:40:00.054974"}
{"image_index": 159, "image_name": "000000016228.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016228.jpg", "image_width": 640, "image_height": 440, "image_resolution": "640x440", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1974.695, "latencies_ms": [1974.695], "images_per_second": 0.506, "prompt_tokens": 1113, "response_tokens_est": 35, "n_tiles": 1, "output_text": " trolley: 1, horse: 2, carriage: 1, people: 1, bench: 1, trees: 1, umbrella: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26859.0, "ram_available_mb": 98913.2, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26859.0, "ram_available_mb": 98913.2, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.68, "peak": 40.97, "min": 18.52}, "VIN": {"avg": 73.98, "peak": 115.56, "min": 28.07}, "VIN_SYS_5V0": {"avg": 15.21, "peak": 16.26, "min": 13.17}, "VDD_CPU_SOC_MSS": {"avg": 16.32, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 29.68, "energy_joules_est": 58.63, "sample_count": 20, "duration_seconds": 1.975}, "timestamp": "2026-01-19T14:40:02.136360"}
{"image_index": 159, "image_name": "000000016228.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016228.jpg", "image_width": 640, "image_height": 440, "image_resolution": "640x440", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2554.558, "latencies_ms": [2554.558], "images_per_second": 0.391, "prompt_tokens": 1117, "response_tokens_est": 56, "n_tiles": 1, "output_text": " The horse-drawn trolley is positioned in the foreground of the image, with the passengers seated inside. The trolley is moving towards the right side of the image, while the people are standing on the left side. The background features a park with trees and a clear sky.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26859.0, "ram_available_mb": 98913.2, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26859.0, "ram_available_mb": 98913.2, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.01, "peak": 40.18, "min": 15.77}, "VIN": {"avg": 67.37, "peak": 94.99, "min": 29.89}, "VIN_SYS_5V0": {"avg": 14.79, "peak": 16.05, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.84, "peak": 17.32, "min": 12.61}}, "power_watts_avg": 27.01, "energy_joules_est": 69.01, "sample_count": 25, "duration_seconds": 2.555}, "timestamp": "2026-01-19T14:40:04.744389"}
{"image_index": 159, "image_name": "000000016228.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016228.jpg", "image_width": 640, "image_height": 440, "image_resolution": "640x440", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1732.647, "latencies_ms": [1732.647], "images_per_second": 0.577, "prompt_tokens": 1111, "response_tokens_est": 25, "n_tiles": 1, "output_text": " A horse-drawn trolley car is driving down a street in a park, with people walking around and sitting on benches.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26859.0, "ram_available_mb": 98913.2, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26859.0, "ram_available_mb": 98913.2, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.22, "peak": 40.18, "min": 16.16}, "VIN": {"avg": 70.45, "peak": 120.17, "min": 30.39}, "VIN_SYS_5V0": {"avg": 14.93, "peak": 16.05, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 16.0, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 30.22, "energy_joules_est": 52.37, "sample_count": 17, "duration_seconds": 1.733}, "timestamp": "2026-01-19T14:40:06.516538"}
{"image_index": 159, "image_name": "000000016228.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016228.jpg", "image_width": 640, "image_height": 440, "image_resolution": "640x440", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2677.682, "latencies_ms": [2677.682], "images_per_second": 0.373, "prompt_tokens": 1109, "response_tokens_est": 59, "n_tiles": 1, "output_text": " The image features a vibrant scene with a green and red trolley car, white horses pulling it, and a yellow canopy overhead. The trolley car is adorned with gold accents and has a sign that reads \"DINING CAR\". The sky is clear and blue, suggesting a sunny day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26859.0, "ram_available_mb": 98913.2, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 26859.2, "ram_available_mb": 98913.0, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.12, "peak": 40.56, "min": 18.52}, "VIN": {"avg": 69.23, "peak": 120.53, "min": 29.37}, "VIN_SYS_5V0": {"avg": 14.93, "peak": 16.05, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 15.96, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 27.12, "energy_joules_est": 72.63, "sample_count": 26, "duration_seconds": 2.678}, "timestamp": "2026-01-19T14:40:09.226846"}
{"image_index": 160, "image_name": "000000016249.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016249.jpg", "image_width": 500, "image_height": 365, "image_resolution": "500x365", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2046.914, "latencies_ms": [2046.914], "images_per_second": 0.489, "prompt_tokens": 1099, "response_tokens_est": 38, "n_tiles": 1, "output_text": " An elderly man sits on a green bench reading a newspaper while a group of people sit on benches in front of a building with a sign that says \"S.C.C.C.\"", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26859.2, "ram_available_mb": 98913.0, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26859.2, "ram_available_mb": 98913.0, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.02, "peak": 40.18, "min": 16.16}, "VIN": {"avg": 69.02, "peak": 106.1, "min": 29.28}, "VIN_SYS_5V0": {"avg": 15.07, "peak": 16.05, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 16.2, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 29.02, "energy_joules_est": 59.42, "sample_count": 20, "duration_seconds": 2.047}, "timestamp": "2026-01-19T14:40:11.296088"}
{"image_index": 160, "image_name": "000000016249.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016249.jpg", "image_width": 500, "image_height": 365, "image_resolution": "500x365", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2502.978, "latencies_ms": [2502.978], "images_per_second": 0.4, "prompt_tokens": 1113, "response_tokens_est": 56, "n_tiles": 1, "output_text": " 1. man: 1\n2. bench: 4\n3. man: 1\n4. man: 1\n5. man: 1\n6. man: 1\n7. man: 1\n8. man: 1", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26859.2, "ram_available_mb": 98913.0, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26859.4, "ram_available_mb": 98912.7, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.48, "peak": 40.56, "min": 17.73}, "VIN": {"avg": 69.86, "peak": 126.48, "min": 27.56}, "VIN_SYS_5V0": {"avg": 15.09, "peak": 16.15, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 16.25, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 27.48, "energy_joules_est": 68.79, "sample_count": 25, "duration_seconds": 2.503}, "timestamp": "2026-01-19T14:40:13.895091"}
{"image_index": 160, "image_name": "000000016249.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016249.jpg", "image_width": 500, "image_height": 365, "image_resolution": "500x365", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2596.462, "latencies_ms": [2596.462], "images_per_second": 0.385, "prompt_tokens": 1117, "response_tokens_est": 58, "n_tiles": 1, "output_text": " The man on the left is sitting on a green bench, while the man on the right is sitting on a red bench. The man on the left is closer to the camera than the man on the right. The man on the left is sitting in front of the man on the right.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26859.4, "ram_available_mb": 98912.7, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 26859.4, "ram_available_mb": 98912.7, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.96, "peak": 40.18, "min": 15.38}, "VIN": {"avg": 64.65, "peak": 113.75, "min": 28.93}, "VIN_SYS_5V0": {"avg": 14.84, "peak": 16.05, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.91, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 26.96, "energy_joules_est": 70.01, "sample_count": 25, "duration_seconds": 2.597}, "timestamp": "2026-01-19T14:40:16.505436"}
{"image_index": 160, "image_name": "000000016249.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016249.jpg", "image_width": 500, "image_height": 365, "image_resolution": "500x365", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1603.751, "latencies_ms": [1603.751], "images_per_second": 0.624, "prompt_tokens": 1111, "response_tokens_est": 20, "n_tiles": 1, "output_text": " A group of people are sitting on benches in a public square, reading newspapers and enjoying the day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26859.4, "ram_available_mb": 98912.7, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 26859.4, "ram_available_mb": 98912.7, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.95, "peak": 40.56, "min": 16.95}, "VIN": {"avg": 73.27, "peak": 117.84, "min": 27.65}, "VIN_SYS_5V0": {"avg": 15.15, "peak": 16.15, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 16.26, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 30.95, "energy_joules_est": 49.65, "sample_count": 16, "duration_seconds": 1.604}, "timestamp": "2026-01-19T14:40:18.173193"}
{"image_index": 160, "image_name": "000000016249.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016249.jpg", "image_width": 500, "image_height": 365, "image_resolution": "500x365", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1998.472, "latencies_ms": [1998.472], "images_per_second": 0.5, "prompt_tokens": 1109, "response_tokens_est": 34, "n_tiles": 1, "output_text": " The image is taken during the day with natural light, and the ground is paved with concrete. The color of the benches is green, and the railing is red.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26859.4, "ram_available_mb": 98912.7, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26859.4, "ram_available_mb": 98912.7, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.47, "peak": 40.57, "min": 18.53}, "VIN": {"avg": 73.9, "peak": 125.94, "min": 28.76}, "VIN_SYS_5V0": {"avg": 14.84, "peak": 16.05, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.87, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 29.47, "energy_joules_est": 58.9, "sample_count": 20, "duration_seconds": 1.999}, "timestamp": "2026-01-19T14:40:20.243066"}
{"image_index": 161, "image_name": "000000016439.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016439.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1568.702, "latencies_ms": [1568.702], "images_per_second": 0.637, "prompt_tokens": 1099, "response_tokens_est": 19, "n_tiles": 1, "output_text": " A black desk with a laptop, a lamp, and a glass of orange juice on it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26859.4, "ram_available_mb": 98912.7, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 26859.4, "ram_available_mb": 98912.7, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.76, "peak": 40.57, "min": 16.56}, "VIN": {"avg": 71.45, "peak": 124.28, "min": 27.44}, "VIN_SYS_5V0": {"avg": 14.79, "peak": 16.05, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 15.8, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 30.76, "energy_joules_est": 48.28, "sample_count": 16, "duration_seconds": 1.57}, "timestamp": "2026-01-19T14:40:21.904296"}
{"image_index": 161, "image_name": "000000016439.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016439.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2127.808, "latencies_ms": [2127.808], "images_per_second": 0.47, "prompt_tokens": 1113, "response_tokens_est": 41, "n_tiles": 1, "output_text": " 1. black laptop\n2. white telephone\n3. black lamp\n4. glass of orange juice\n5. book\n6. white paper\n7. black pen\n8. black chair", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26859.4, "ram_available_mb": 98912.7, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26859.4, "ram_available_mb": 98912.7, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.02, "peak": 40.56, "min": 18.53}, "VIN": {"avg": 64.63, "peak": 117.27, "min": 29.62}, "VIN_SYS_5V0": {"avg": 14.94, "peak": 16.05, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.86, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 29.02, "energy_joules_est": 61.76, "sample_count": 21, "duration_seconds": 2.128}, "timestamp": "2026-01-19T14:40:24.094502"}
{"image_index": 161, "image_name": "000000016439.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016439.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2036.589, "latencies_ms": [2036.589], "images_per_second": 0.491, "prompt_tokens": 1117, "response_tokens_est": 37, "n_tiles": 1, "output_text": " The laptop is positioned to the left of the desk, with the lamp and telephone to its right. The desk is situated in the foreground, with the wall and painting in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26859.4, "ram_available_mb": 98912.7, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26859.4, "ram_available_mb": 98912.7, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.04, "peak": 40.57, "min": 16.56}, "VIN": {"avg": 74.46, "peak": 128.03, "min": 29.23}, "VIN_SYS_5V0": {"avg": 14.95, "peak": 16.05, "min": 12.36}, "VDD_CPU_SOC_MSS": {"avg": 15.9, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 29.04, "energy_joules_est": 59.17, "sample_count": 20, "duration_seconds": 2.037}, "timestamp": "2026-01-19T14:40:26.179923"}
{"image_index": 161, "image_name": "000000016439.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016439.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1761.063, "latencies_ms": [1761.063], "images_per_second": 0.568, "prompt_tokens": 1111, "response_tokens_est": 27, "n_tiles": 1, "output_text": " A desk with a laptop computer, a lamp, and a glass of orange juice is in a room with a painting on the wall.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26859.4, "ram_available_mb": 98912.7, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26859.4, "ram_available_mb": 98912.7, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.64, "peak": 40.56, "min": 17.35}, "VIN": {"avg": 71.89, "peak": 115.21, "min": 31.12}, "VIN_SYS_5V0": {"avg": 15.13, "peak": 16.15, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 16.21, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 30.64, "energy_joules_est": 53.97, "sample_count": 17, "duration_seconds": 1.761}, "timestamp": "2026-01-19T14:40:27.952808"}
{"image_index": 161, "image_name": "000000016439.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016439.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1570.917, "latencies_ms": [1570.917], "images_per_second": 0.637, "prompt_tokens": 1109, "response_tokens_est": 20, "n_tiles": 1, "output_text": " The room is well-lit with a warm yellow light, and the desk is made of wood.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26859.4, "ram_available_mb": 98912.7, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 26859.4, "ram_available_mb": 98912.7, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.86, "peak": 40.57, "min": 19.71}, "VIN": {"avg": 66.27, "peak": 111.82, "min": 27.86}, "VIN_SYS_5V0": {"avg": 15.25, "peak": 16.26, "min": 13.29}, "VDD_CPU_SOC_MSS": {"avg": 16.39, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 31.86, "energy_joules_est": 50.06, "sample_count": 16, "duration_seconds": 1.571}, "timestamp": "2026-01-19T14:40:29.609818"}
{"image_index": 162, "image_name": "000000016451.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016451.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2834.211, "latencies_ms": [2834.211], "images_per_second": 0.353, "prompt_tokens": 1432, "response_tokens_est": 51, "n_tiles": 1, "output_text": " The image captures a serene beach scene with a blue and white striped towel, a pink and white surfboard, and a blue surfboard resting on the sand, with a beach umbrella and chairs nearby, and a person enjoying the ocean waves in the distance.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 26859.4, "ram_available_mb": 98912.7, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26859.4, "ram_available_mb": 98912.7, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.2, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.9, "peak": 40.57, "min": 18.52}, "VIN": {"avg": 69.76, "peak": 115.7, "min": 29.27}, "VIN_SYS_5V0": {"avg": 15.16, "peak": 16.66, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 16.21, "peak": 17.71, "min": 13.0}}, "power_watts_avg": 28.9, "energy_joules_est": 81.93, "sample_count": 28, "duration_seconds": 2.835}, "timestamp": "2026-01-19T14:40:32.533140"}
{"image_index": 162, "image_name": "000000016451.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016451.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2367.59, "latencies_ms": [2367.59], "images_per_second": 0.422, "prompt_tokens": 1446, "response_tokens_est": 33, "n_tiles": 1, "output_text": " beach chair: 2\nsurfboard: 3\nbag: 1\numbrella: 1\nsand: 1\nwater: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26859.4, "ram_available_mb": 98912.7, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26859.4, "ram_available_mb": 98912.7, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.4, "peak": 40.16, "min": 15.77}, "VIN": {"avg": 74.14, "peak": 119.23, "min": 30.21}, "VIN_SYS_5V0": {"avg": 15.18, "peak": 16.56, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 16.26, "peak": 17.71, "min": 13.0}}, "power_watts_avg": 30.4, "energy_joules_est": 71.98, "sample_count": 23, "duration_seconds": 2.368}, "timestamp": "2026-01-19T14:40:34.912843"}
{"image_index": 162, "image_name": "000000016451.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016451.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3244.045, "latencies_ms": [3244.045], "images_per_second": 0.308, "prompt_tokens": 1450, "response_tokens_est": 67, "n_tiles": 1, "output_text": " The blue and white striped towel is positioned to the left of the blue surfboard, which is closer to the camera than the pink and white surfboard. The red cooler is located in the foreground, near the beach chairs and umbrellas, while the person in the water is situated in the background, far away from the camera.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26859.4, "ram_available_mb": 98912.7, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26859.4, "ram_available_mb": 98912.7, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.97, "peak": 40.16, "min": 18.13}, "VIN": {"avg": 70.11, "peak": 120.97, "min": 29.21}, "VIN_SYS_5V0": {"avg": 15.24, "peak": 16.76, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 16.21, "peak": 17.71, "min": 14.18}}, "power_watts_avg": 27.97, "energy_joules_est": 90.74, "sample_count": 32, "duration_seconds": 3.244}, "timestamp": "2026-01-19T14:40:38.230704"}
{"image_index": 162, "image_name": "000000016451.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016451.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2406.541, "latencies_ms": [2406.541], "images_per_second": 0.416, "prompt_tokens": 1444, "response_tokens_est": 36, "n_tiles": 1, "output_text": " The image captures a serene beach scene with a clear blue sky and calm ocean waves. People are enjoying the beach, with some relaxing on the sand and others swimming in the water.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26859.4, "ram_available_mb": 98912.7, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26859.4, "ram_available_mb": 98912.7, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.17, "peak": 40.18, "min": 15.77}, "VIN": {"avg": 77.77, "peak": 121.51, "min": 30.04}, "VIN_SYS_5V0": {"avg": 15.18, "peak": 16.56, "min": 12.15}, "VDD_CPU_SOC_MSS": {"avg": 16.27, "peak": 17.71, "min": 13.39}}, "power_watts_avg": 30.17, "energy_joules_est": 72.62, "sample_count": 24, "duration_seconds": 2.407}, "timestamp": "2026-01-19T14:40:40.712759"}
{"image_index": 162, "image_name": "000000016451.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016451.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1934.031, "latencies_ms": [1934.031], "images_per_second": 0.517, "prompt_tokens": 1442, "response_tokens_est": 18, "n_tiles": 1, "output_text": " The beach is covered in sand, the sky is blue, and the ocean is blue.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26859.4, "ram_available_mb": 98912.7, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 26859.7, "ram_available_mb": 98912.5, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.17, "peak": 40.16, "min": 16.95}, "VIN": {"avg": 77.99, "peak": 125.12, "min": 29.46}, "VIN_SYS_5V0": {"avg": 15.2, "peak": 16.56, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 16.35, "peak": 17.71, "min": 13.39}}, "power_watts_avg": 32.17, "energy_joules_est": 62.23, "sample_count": 19, "duration_seconds": 1.934}, "timestamp": "2026-01-19T14:40:42.698802"}
{"image_index": 163, "image_name": "000000016502.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016502.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1577.21, "latencies_ms": [1577.21], "images_per_second": 0.634, "prompt_tokens": 1100, "response_tokens_est": 20, "n_tiles": 1, "output_text": " A sheep with a black face is standing on a rocky hill under a blue sky with white clouds.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26859.7, "ram_available_mb": 98912.5, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 26859.7, "ram_available_mb": 98912.5, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.59, "peak": 40.57, "min": 20.1}, "VIN": {"avg": 71.63, "peak": 109.7, "min": 28.06}, "VIN_SYS_5V0": {"avg": 15.05, "peak": 16.26, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 16.14, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 31.59, "energy_joules_est": 49.83, "sample_count": 16, "duration_seconds": 1.577}, "timestamp": "2026-01-19T14:40:44.360080"}
{"image_index": 163, "image_name": "000000016502.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016502.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2621.27, "latencies_ms": [2621.27], "images_per_second": 0.381, "prompt_tokens": 1114, "response_tokens_est": 60, "n_tiles": 1, "output_text": " 1. sheep: 1\n2. rock: 1\n3. grass: 1\n4. sky: 1\n5. clouds: 1\n6. cloud: 1\n7. sheep's head: 1\n8. sheep's body: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26859.7, "ram_available_mb": 98912.5, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 26859.7, "ram_available_mb": 98912.5, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.23, "peak": 40.16, "min": 17.73}, "VIN": {"avg": 71.54, "peak": 125.46, "min": 30.07}, "VIN_SYS_5V0": {"avg": 14.9, "peak": 16.15, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.91, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 27.23, "energy_joules_est": 71.39, "sample_count": 26, "duration_seconds": 2.622}, "timestamp": "2026-01-19T14:40:47.065191"}
{"image_index": 163, "image_name": "000000016502.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016502.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2090.938, "latencies_ms": [2090.938], "images_per_second": 0.478, "prompt_tokens": 1118, "response_tokens_est": 39, "n_tiles": 1, "output_text": " The sheep is positioned on the left side of the image, with the sky occupying the majority of the background. The sheep is situated in the foreground, with the grass and rocks in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26859.7, "ram_available_mb": 98912.5, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26859.7, "ram_available_mb": 98912.5, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.2, "peak": 40.16, "min": 15.38}, "VIN": {"avg": 73.68, "peak": 120.67, "min": 27.83}, "VIN_SYS_5V0": {"avg": 14.81, "peak": 16.15, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.94, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 28.2, "energy_joules_est": 58.97, "sample_count": 21, "duration_seconds": 2.091}, "timestamp": "2026-01-19T14:40:49.260369"}
{"image_index": 163, "image_name": "000000016502.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016502.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1588.757, "latencies_ms": [1588.757], "images_per_second": 0.629, "prompt_tokens": 1112, "response_tokens_est": 20, "n_tiles": 1, "output_text": " A sheep with a black face is standing on a rocky hill under a blue sky with white clouds.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26859.7, "ram_available_mb": 98912.5, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26859.7, "ram_available_mb": 98912.5, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.19, "peak": 39.78, "min": 15.77}, "VIN": {"avg": 69.61, "peak": 119.04, "min": 29.01}, "VIN_SYS_5V0": {"avg": 14.74, "peak": 16.15, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 15.85, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 30.19, "energy_joules_est": 47.98, "sample_count": 16, "duration_seconds": 1.589}, "timestamp": "2026-01-19T14:40:50.935693"}
{"image_index": 163, "image_name": "000000016502.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016502.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1671.837, "latencies_ms": [1671.837], "images_per_second": 0.598, "prompt_tokens": 1110, "response_tokens_est": 23, "n_tiles": 1, "output_text": " The sheep is white with a black face and is standing on a rocky hill under a blue sky with white clouds.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26859.7, "ram_available_mb": 98912.5, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26859.7, "ram_available_mb": 98912.5, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.54, "peak": 40.18, "min": 18.53}, "VIN": {"avg": 74.45, "peak": 123.06, "min": 28.77}, "VIN_SYS_5V0": {"avg": 14.79, "peak": 16.15, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.86, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 30.54, "energy_joules_est": 51.07, "sample_count": 17, "duration_seconds": 1.672}, "timestamp": "2026-01-19T14:40:52.712797"}
{"image_index": 164, "image_name": "000000016598.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016598.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1450.728, "latencies_ms": [1450.728], "images_per_second": 0.689, "prompt_tokens": 1100, "response_tokens_est": 14, "n_tiles": 1, "output_text": " A woman with blue hair is taking a selfie in the mirror.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26859.7, "ram_available_mb": 98912.5, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26859.7, "ram_available_mb": 98912.5, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.18, "peak": 40.18, "min": 17.35}, "VIN": {"avg": 68.62, "peak": 97.69, "min": 27.9}, "VIN_SYS_5V0": {"avg": 14.62, "peak": 16.15, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.85, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 31.18, "energy_joules_est": 45.26, "sample_count": 15, "duration_seconds": 1.452}, "timestamp": "2026-01-19T14:40:54.273651"}
{"image_index": 164, "image_name": "000000016598.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016598.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2536.983, "latencies_ms": [2536.983], "images_per_second": 0.394, "prompt_tokens": 1114, "response_tokens_est": 56, "n_tiles": 1, "output_text": " 1. person: 1\n2. phone: 1\n3. shirt: 1\n4. tie: 1\n5. wall: 1\n6. ring: 1\n7. mirror: 1\n8. blue: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26859.7, "ram_available_mb": 98912.5, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 26859.7, "ram_available_mb": 98912.5, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.66, "peak": 40.18, "min": 18.52}, "VIN": {"avg": 68.37, "peak": 106.07, "min": 29.53}, "VIN_SYS_5V0": {"avg": 14.8, "peak": 16.05, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.94, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 27.66, "energy_joules_est": 70.19, "sample_count": 25, "duration_seconds": 2.537}, "timestamp": "2026-01-19T14:40:56.879618"}
{"image_index": 164, "image_name": "000000016598.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016598.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2428.343, "latencies_ms": [2428.343], "images_per_second": 0.412, "prompt_tokens": 1118, "response_tokens_est": 52, "n_tiles": 1, "output_text": " The person is in the foreground of the image, taking a selfie with a phone. The phone is held up to the camera, and the person's reflection is visible in the mirror. The mirror is located in the background, reflecting the person's image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26859.7, "ram_available_mb": 98912.5, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 26859.7, "ram_available_mb": 98912.5, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.25, "peak": 40.18, "min": 15.77}, "VIN": {"avg": 66.95, "peak": 123.1, "min": 29.57}, "VIN_SYS_5V0": {"avg": 14.87, "peak": 16.15, "min": 12.15}, "VDD_CPU_SOC_MSS": {"avg": 15.98, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 27.25, "energy_joules_est": 66.18, "sample_count": 24, "duration_seconds": 2.429}, "timestamp": "2026-01-19T14:40:59.375977"}
{"image_index": 164, "image_name": "000000016598.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016598.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1464.106, "latencies_ms": [1464.106], "images_per_second": 0.683, "prompt_tokens": 1112, "response_tokens_est": 14, "n_tiles": 1, "output_text": " A woman with blue hair is taking a selfie in the mirror.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26859.7, "ram_available_mb": 98912.5, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26859.7, "ram_available_mb": 98912.5, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.36, "peak": 39.38, "min": 15.77}, "VIN": {"avg": 73.79, "peak": 117.73, "min": 27.31}, "VIN_SYS_5V0": {"avg": 14.71, "peak": 16.05, "min": 12.15}, "VDD_CPU_SOC_MSS": {"avg": 15.93, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 30.36, "energy_joules_est": 44.46, "sample_count": 15, "duration_seconds": 1.465}, "timestamp": "2026-01-19T14:41:00.946345"}
{"image_index": 164, "image_name": "000000016598.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016598.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1666.801, "latencies_ms": [1666.801], "images_per_second": 0.6, "prompt_tokens": 1110, "response_tokens_est": 22, "n_tiles": 1, "output_text": " The woman has blue hair and is wearing a blue shirt. The lighting is bright and the colors are vibrant.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26859.7, "ram_available_mb": 98912.5, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26859.7, "ram_available_mb": 98912.5, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.24, "peak": 40.57, "min": 18.92}, "VIN": {"avg": 71.61, "peak": 94.63, "min": 32.1}, "VIN_SYS_5V0": {"avg": 14.73, "peak": 16.05, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.95, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 31.24, "energy_joules_est": 52.1, "sample_count": 16, "duration_seconds": 1.668}, "timestamp": "2026-01-19T14:41:02.619275"}
{"image_index": 165, "image_name": "000000016958.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016958.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2032.027, "latencies_ms": [2032.027], "images_per_second": 0.492, "prompt_tokens": 1099, "response_tokens_est": 38, "n_tiles": 1, "output_text": " The image depicts a room with a fireplace, a wooden cabinet, a table with a book on it, and a chair, all arranged in a way that suggests a cozy and inviting atmosphere.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 26859.7, "ram_available_mb": 98912.5, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26859.7, "ram_available_mb": 98912.5, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.0, "peak": 40.95, "min": 19.7}, "VIN": {"avg": 75.24, "peak": 127.99, "min": 29.65}, "VIN_SYS_5V0": {"avg": 15.21, "peak": 16.15, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 16.36, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 30.0, "energy_joules_est": 60.97, "sample_count": 20, "duration_seconds": 2.032}, "timestamp": "2026-01-19T14:41:04.707409"}
{"image_index": 165, "image_name": "000000016958.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016958.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1989.948, "latencies_ms": [1989.948], "images_per_second": 0.503, "prompt_tokens": 1113, "response_tokens_est": 35, "n_tiles": 1, "output_text": " fireplace: 1, table: 1, chair: 2, bookshelf: 1, painting: 2, vase: 2, floor: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26859.7, "ram_available_mb": 98912.5, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26859.7, "ram_available_mb": 98912.5, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.03, "peak": 40.18, "min": 16.95}, "VIN": {"avg": 69.37, "peak": 122.29, "min": 28.1}, "VIN_SYS_5V0": {"avg": 14.89, "peak": 16.15, "min": 12.46}, "VDD_CPU_SOC_MSS": {"avg": 15.94, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 29.03, "energy_joules_est": 57.79, "sample_count": 20, "duration_seconds": 1.991}, "timestamp": "2026-01-19T14:41:06.796069"}
{"image_index": 165, "image_name": "000000016958.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016958.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2574.546, "latencies_ms": [2574.546], "images_per_second": 0.388, "prompt_tokens": 1117, "response_tokens_est": 57, "n_tiles": 1, "output_text": " The fireplace is located on the left side of the room, with the wooden cabinet positioned against the wall to the right. The round wooden table is situated in the foreground, with the chairs placed in the background. The bookshelf is positioned in the far background, behind the wooden cabinet.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26859.7, "ram_available_mb": 98912.5, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 26859.7, "ram_available_mb": 98912.5, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.14, "peak": 40.18, "min": 16.16}, "VIN": {"avg": 70.41, "peak": 127.68, "min": 29.76}, "VIN_SYS_5V0": {"avg": 14.83, "peak": 15.95, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.89, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 27.14, "energy_joules_est": 69.89, "sample_count": 25, "duration_seconds": 2.575}, "timestamp": "2026-01-19T14:41:09.400487"}
{"image_index": 165, "image_name": "000000016958.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016958.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1571.754, "latencies_ms": [1571.754], "images_per_second": 0.636, "prompt_tokens": 1111, "response_tokens_est": 19, "n_tiles": 1, "output_text": " The image depicts a room with a fireplace, a wooden cabinet, and a table with chairs.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26859.7, "ram_available_mb": 98912.5, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 26859.7, "ram_available_mb": 98912.5, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.73, "peak": 40.56, "min": 16.56}, "VIN": {"avg": 72.31, "peak": 106.46, "min": 28.08}, "VIN_SYS_5V0": {"avg": 14.99, "peak": 16.15, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 16.17, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 30.73, "energy_joules_est": 48.31, "sample_count": 16, "duration_seconds": 1.572}, "timestamp": "2026-01-19T14:41:11.075558"}
{"image_index": 165, "image_name": "000000016958.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016958.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1977.488, "latencies_ms": [1977.488], "images_per_second": 0.506, "prompt_tokens": 1109, "response_tokens_est": 34, "n_tiles": 1, "output_text": " The room is well-lit with natural light, and the walls are painted white. The furniture is made of wood, and the floor is covered with a patterned carpet.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26859.7, "ram_available_mb": 98912.5, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26859.4, "ram_available_mb": 98912.8, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.25, "peak": 40.18, "min": 18.14}, "VIN": {"avg": 70.38, "peak": 116.02, "min": 30.32}, "VIN_SYS_5V0": {"avg": 14.76, "peak": 16.05, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.87, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 29.25, "energy_joules_est": 57.85, "sample_count": 20, "duration_seconds": 1.978}, "timestamp": "2026-01-19T14:41:13.159380"}
{"image_index": 166, "image_name": "000000017029.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017029.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1894.767, "latencies_ms": [1894.767], "images_per_second": 0.528, "prompt_tokens": 1432, "response_tokens_est": 16, "n_tiles": 1, "output_text": " A brown dog is jumping in the air to catch a red frisbee.", "error": null, "sys_before": {"cpu_percent": 7.1, "ram_used_mb": 26859.4, "ram_available_mb": 98912.8, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 2.4, "ram_used_mb": 26859.4, "ram_available_mb": 98912.8, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.2, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.66, "peak": 40.18, "min": 15.77}, "VIN": {"avg": 74.31, "peak": 121.74, "min": 28.73}, "VIN_SYS_5V0": {"avg": 15.15, "peak": 16.66, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 16.27, "peak": 17.71, "min": 12.61}}, "power_watts_avg": 31.66, "energy_joules_est": 60.01, "sample_count": 19, "duration_seconds": 1.896}, "timestamp": "2026-01-19T14:41:15.148817"}
{"image_index": 166, "image_name": "000000017029.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017029.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2285.641, "latencies_ms": [2285.641], "images_per_second": 0.438, "prompt_tokens": 1446, "response_tokens_est": 31, "n_tiles": 1, "output_text": " dog: 1, frisbee: 1, car: 1, tree: 1, grass: 1, mulch: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26859.4, "ram_available_mb": 98912.8, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26859.4, "ram_available_mb": 98912.8, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.5, "peak": 40.56, "min": 19.32}, "VIN": {"avg": 72.2, "peak": 107.82, "min": 32.55}, "VIN_SYS_5V0": {"avg": 15.19, "peak": 16.56, "min": 12.15}, "VDD_CPU_SOC_MSS": {"avg": 16.36, "peak": 17.71, "min": 13.39}}, "power_watts_avg": 31.5, "energy_joules_est": 72.01, "sample_count": 22, "duration_seconds": 2.286}, "timestamp": "2026-01-19T14:41:17.440383"}
{"image_index": 166, "image_name": "000000017029.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017029.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2546.663, "latencies_ms": [2546.663], "images_per_second": 0.393, "prompt_tokens": 1450, "response_tokens_est": 40, "n_tiles": 1, "output_text": " The dog is in the foreground, jumping up to catch the frisbee, which is in the middle ground. The frisbee is in the air, and the car is in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26859.4, "ram_available_mb": 98912.8, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26859.4, "ram_available_mb": 98912.8, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.28, "peak": 40.56, "min": 18.91}, "VIN": {"avg": 70.42, "peak": 118.07, "min": 29.49}, "VIN_SYS_5V0": {"avg": 15.38, "peak": 16.76, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 16.44, "peak": 18.11, "min": 14.57}}, "power_watts_avg": 30.28, "energy_joules_est": 77.12, "sample_count": 25, "duration_seconds": 2.547}, "timestamp": "2026-01-19T14:41:20.044768"}
{"image_index": 166, "image_name": "000000017029.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017029.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2006.029, "latencies_ms": [2006.029], "images_per_second": 0.498, "prompt_tokens": 1444, "response_tokens_est": 21, "n_tiles": 1, "output_text": " A brown dog is playing frisbee in a yard with a tree and a car in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26859.4, "ram_available_mb": 98912.8, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 26859.4, "ram_available_mb": 98912.8, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.87, "peak": 40.57, "min": 16.56}, "VIN": {"avg": 80.89, "peak": 118.92, "min": 29.6}, "VIN_SYS_5V0": {"avg": 15.22, "peak": 16.56, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 16.36, "peak": 17.71, "min": 13.39}}, "power_watts_avg": 31.87, "energy_joules_est": 63.95, "sample_count": 20, "duration_seconds": 2.007}, "timestamp": "2026-01-19T14:41:22.130083"}
{"image_index": 166, "image_name": "000000017029.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017029.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2465.326, "latencies_ms": [2465.326], "images_per_second": 0.406, "prompt_tokens": 1442, "response_tokens_est": 38, "n_tiles": 1, "output_text": " The image features a black dog with a red frisbee in its mouth, jumping in the air in a grassy yard. The lighting is natural, and the weather appears to be clear.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26859.4, "ram_available_mb": 98912.8, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26859.4, "ram_available_mb": 98912.8, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.61, "peak": 40.57, "min": 18.52}, "VIN": {"avg": 70.62, "peak": 110.2, "min": 30.58}, "VIN_SYS_5V0": {"avg": 15.18, "peak": 16.56, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 16.26, "peak": 17.71, "min": 13.39}}, "power_watts_avg": 30.61, "energy_joules_est": 75.47, "sample_count": 24, "duration_seconds": 2.466}, "timestamp": "2026-01-19T14:41:24.627668"}
{"image_index": 167, "image_name": "000000017031.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017031.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1748.371, "latencies_ms": [1748.371], "images_per_second": 0.572, "prompt_tokens": 1099, "response_tokens_est": 26, "n_tiles": 1, "output_text": " The image captures a giraffe in a natural setting, with its head and neck prominently displayed, and the background filled with lush greenery.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 26859.4, "ram_available_mb": 98912.8, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26859.4, "ram_available_mb": 98912.8, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.7, "peak": 40.56, "min": 17.74}, "VIN": {"avg": 72.57, "peak": 109.73, "min": 30.05}, "VIN_SYS_5V0": {"avg": 15.09, "peak": 16.15, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 16.14, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 30.7, "energy_joules_est": 53.7, "sample_count": 17, "duration_seconds": 1.749}, "timestamp": "2026-01-19T14:41:26.403905"}
{"image_index": 167, "image_name": "000000017031.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017031.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2072.89, "latencies_ms": [2072.89], "images_per_second": 0.482, "prompt_tokens": 1113, "response_tokens_est": 34, "n_tiles": 1, "output_text": " giraffe: 1, ear: 2, eye: 2, nose: 1, mouth: 1, horn: 1, tail: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26859.4, "ram_available_mb": 98912.8, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26860.4, "ram_available_mb": 98911.8, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.25, "peak": 40.56, "min": 18.92}, "VIN": {"avg": 71.35, "peak": 111.64, "min": 30.23}, "VIN_SYS_5V0": {"avg": 14.93, "peak": 16.15, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 16.08, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 29.25, "energy_joules_est": 60.65, "sample_count": 20, "duration_seconds": 2.073}, "timestamp": "2026-01-19T14:41:28.489700"}
{"image_index": 167, "image_name": "000000017031.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017031.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1896.908, "latencies_ms": [1896.908], "images_per_second": 0.527, "prompt_tokens": 1117, "response_tokens_est": 32, "n_tiles": 1, "output_text": " The giraffe is in the foreground, with its head and neck prominently displayed. The background is filled with lush green foliage, providing a natural habitat for the giraffe.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26860.4, "ram_available_mb": 98911.8, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26860.4, "ram_available_mb": 98911.8, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.52, "peak": 40.56, "min": 16.95}, "VIN": {"avg": 71.16, "peak": 118.42, "min": 27.69}, "VIN_SYS_5V0": {"avg": 15.06, "peak": 16.15, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 16.1, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 29.52, "energy_joules_est": 56.0, "sample_count": 19, "duration_seconds": 1.897}, "timestamp": "2026-01-19T14:41:30.474412"}
{"image_index": 167, "image_name": "000000017031.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017031.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2443.627, "latencies_ms": [2443.627], "images_per_second": 0.409, "prompt_tokens": 1111, "response_tokens_est": 50, "n_tiles": 1, "output_text": " In the heart of a verdant forest, a majestic giraffe stands tall and proud, its long neck reaching towards the sky. The giraffe's coat, a beautiful mosaic of brown and white spots, contrasts beautifully with the lush greenery of the trees.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26860.4, "ram_available_mb": 98911.8, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26860.4, "ram_available_mb": 98911.8, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.3, "peak": 40.18, "min": 16.56}, "VIN": {"avg": 67.34, "peak": 115.51, "min": 28.8}, "VIN_SYS_5V0": {"avg": 14.77, "peak": 16.05, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 15.75, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 27.3, "energy_joules_est": 66.73, "sample_count": 24, "duration_seconds": 2.444}, "timestamp": "2026-01-19T14:41:32.975988"}
{"image_index": 167, "image_name": "000000017031.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017031.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1891.614, "latencies_ms": [1891.614], "images_per_second": 0.529, "prompt_tokens": 1109, "response_tokens_est": 31, "n_tiles": 1, "output_text": " The giraffe's coat is a rich brown with white spots, and the sunlight filters through the green leaves of the trees, creating a warm and inviting atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26860.4, "ram_available_mb": 98911.8, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26860.4, "ram_available_mb": 98911.8, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.2, "peak": 40.18, "min": 15.77}, "VIN": {"avg": 74.83, "peak": 117.95, "min": 28.74}, "VIN_SYS_5V0": {"avg": 14.85, "peak": 16.05, "min": 12.15}, "VDD_CPU_SOC_MSS": {"avg": 15.85, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 29.2, "energy_joules_est": 55.25, "sample_count": 19, "duration_seconds": 1.892}, "timestamp": "2026-01-19T14:41:34.955037"}
{"image_index": 168, "image_name": "000000017115.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017115.jpg", "image_width": 443, "image_height": 640, "image_resolution": "443x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1700.537, "latencies_ms": [1700.537], "images_per_second": 0.588, "prompt_tokens": 1100, "response_tokens_est": 24, "n_tiles": 1, "output_text": " Two zebras with black and white stripes are standing in a fenced area with a chain link fence in the background.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 26860.4, "ram_available_mb": 98911.8, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26860.4, "ram_available_mb": 98911.8, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.15, "peak": 40.18, "min": 16.56}, "VIN": {"avg": 75.8, "peak": 130.74, "min": 29.65}, "VIN_SYS_5V0": {"avg": 14.8, "peak": 16.15, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.84, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 30.15, "energy_joules_est": 51.3, "sample_count": 17, "duration_seconds": 1.701}, "timestamp": "2026-01-19T14:41:36.735730"}
{"image_index": 168, "image_name": "000000017115.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017115.jpg", "image_width": 443, "image_height": 640, "image_resolution": "443x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2736.952, "latencies_ms": [2736.952], "images_per_second": 0.365, "prompt_tokens": 1114, "response_tokens_est": 64, "n_tiles": 1, "output_text": " 1. Zebra: 2\n2. Zebra: 2\n3. Zebra: 2\n4. Zebra: 2\n5. Zebra: 2\n6. Zebra: 2\n7. Zebra: 2\n8. Zebra: 2", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26860.4, "ram_available_mb": 98911.8, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 26860.4, "ram_available_mb": 98911.8, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.83, "peak": 40.16, "min": 18.13}, "VIN": {"avg": 68.92, "peak": 124.55, "min": 29.07}, "VIN_SYS_5V0": {"avg": 14.88, "peak": 16.15, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.94, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 26.83, "energy_joules_est": 73.45, "sample_count": 27, "duration_seconds": 2.737}, "timestamp": "2026-01-19T14:41:39.544079"}
{"image_index": 168, "image_name": "000000017115.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017115.jpg", "image_width": 443, "image_height": 640, "image_resolution": "443x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2652.34, "latencies_ms": [2652.34], "images_per_second": 0.377, "prompt_tokens": 1118, "response_tokens_est": 58, "n_tiles": 1, "output_text": " The zebras are positioned in the foreground of the image, with the fence serving as a boundary between them and the background. The zebras are facing away from the camera, with their tails prominently displayed, and the fence is located behind them, separating them from the rest of the environment.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26860.4, "ram_available_mb": 98911.8, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 26860.4, "ram_available_mb": 98911.8, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.52, "peak": 40.16, "min": 15.77}, "VIN": {"avg": 68.34, "peak": 125.96, "min": 28.51}, "VIN_SYS_5V0": {"avg": 14.82, "peak": 16.05, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 15.75, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 26.52, "energy_joules_est": 70.35, "sample_count": 26, "duration_seconds": 2.653}, "timestamp": "2026-01-19T14:41:42.252250"}
{"image_index": 168, "image_name": "000000017115.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017115.jpg", "image_width": 443, "image_height": 640, "image_resolution": "443x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1678.055, "latencies_ms": [1678.055], "images_per_second": 0.596, "prompt_tokens": 1112, "response_tokens_est": 23, "n_tiles": 1, "output_text": " Two zebras are standing in a fenced enclosure, their black and white stripes contrasting against the brown dirt ground.", "error": null, "sys_before": {"cpu_percent": 40.0, "ram_used_mb": 26860.4, "ram_available_mb": 98911.8, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26860.1, "ram_available_mb": 98912.1, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.69, "peak": 39.39, "min": 15.77}, "VIN": {"avg": 78.02, "peak": 122.41, "min": 29.04}, "VIN_SYS_5V0": {"avg": 14.84, "peak": 16.15, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.98, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 29.69, "energy_joules_est": 49.83, "sample_count": 17, "duration_seconds": 1.678}, "timestamp": "2026-01-19T14:41:44.028597"}
{"image_index": 168, "image_name": "000000017115.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017115.jpg", "image_width": 443, "image_height": 640, "image_resolution": "443x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2235.235, "latencies_ms": [2235.235], "images_per_second": 0.447, "prompt_tokens": 1110, "response_tokens_est": 45, "n_tiles": 1, "output_text": " The image features two zebras with their backs to the camera, standing in a grassy area with a chain link fence in the background. The zebras have black and white stripes, and the lighting appears to be natural daylight.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26860.1, "ram_available_mb": 98912.1, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26860.1, "ram_available_mb": 98912.1, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.35, "peak": 40.16, "min": 17.35}, "VIN": {"avg": 71.22, "peak": 117.34, "min": 30.01}, "VIN_SYS_5V0": {"avg": 14.85, "peak": 16.05, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.94, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 28.35, "energy_joules_est": 63.38, "sample_count": 22, "duration_seconds": 2.236}, "timestamp": "2026-01-19T14:41:46.318143"}
{"image_index": 169, "image_name": "000000017178.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017178.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1881.172, "latencies_ms": [1881.172], "images_per_second": 0.532, "prompt_tokens": 1099, "response_tokens_est": 31, "n_tiles": 1, "output_text": " In the image, a group of horses is walking down a road, with a car parked on the side, and a pile of poop on the ground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26860.1, "ram_available_mb": 98912.1, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26860.1, "ram_available_mb": 98912.1, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.28, "peak": 40.18, "min": 16.16}, "VIN": {"avg": 74.17, "peak": 128.01, "min": 28.91}, "VIN_SYS_5V0": {"avg": 14.87, "peak": 16.05, "min": 12.36}, "VDD_CPU_SOC_MSS": {"avg": 15.95, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 29.28, "energy_joules_est": 55.09, "sample_count": 19, "duration_seconds": 1.882}, "timestamp": "2026-01-19T14:41:48.298227"}
{"image_index": 169, "image_name": "000000017178.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017178.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2528.047, "latencies_ms": [2528.047], "images_per_second": 0.396, "prompt_tokens": 1113, "response_tokens_est": 56, "n_tiles": 1, "output_text": " 1. horse: 4\n2. car: 1\n3. horse: 2\n4. horse: 1\n5. horse: 1\n6. horse: 1\n7. horse: 1\n8. horse: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26860.1, "ram_available_mb": 98912.1, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26860.1, "ram_available_mb": 98912.1, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.26, "peak": 40.18, "min": 16.56}, "VIN": {"avg": 68.68, "peak": 106.06, "min": 28.81}, "VIN_SYS_5V0": {"avg": 14.82, "peak": 16.05, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.95, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 27.26, "energy_joules_est": 68.92, "sample_count": 25, "duration_seconds": 2.528}, "timestamp": "2026-01-19T14:41:50.881983"}
{"image_index": 169, "image_name": "000000017178.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017178.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2472.73, "latencies_ms": [2472.73], "images_per_second": 0.404, "prompt_tokens": 1117, "response_tokens_est": 54, "n_tiles": 1, "output_text": " The horses are positioned in the middle of the road, with the car parked on the right side of the street. The car is relatively close to the camera, while the horses are farther away. The horses are near the car, but not directly in front of it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26860.1, "ram_available_mb": 98912.1, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26860.1, "ram_available_mb": 98912.1, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.5, "peak": 40.18, "min": 16.16}, "VIN": {"avg": 69.29, "peak": 106.65, "min": 29.49}, "VIN_SYS_5V0": {"avg": 14.98, "peak": 16.05, "min": 12.36}, "VDD_CPU_SOC_MSS": {"avg": 16.01, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 27.5, "energy_joules_est": 68.01, "sample_count": 24, "duration_seconds": 2.473}, "timestamp": "2026-01-19T14:41:53.383190"}
{"image_index": 169, "image_name": "000000017178.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017178.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1540.92, "latencies_ms": [1540.92], "images_per_second": 0.649, "prompt_tokens": 1111, "response_tokens_est": 18, "n_tiles": 1, "output_text": " A group of horses are walking down a tree-lined street, passing a parked car.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26860.1, "ram_available_mb": 98912.1, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 26860.1, "ram_available_mb": 98912.1, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.41, "peak": 40.57, "min": 16.56}, "VIN": {"avg": 72.79, "peak": 99.26, "min": 29.59}, "VIN_SYS_5V0": {"avg": 15.11, "peak": 16.15, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 16.25, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 31.41, "energy_joules_est": 48.42, "sample_count": 15, "duration_seconds": 1.541}, "timestamp": "2026-01-19T14:41:54.944630"}
{"image_index": 169, "image_name": "000000017178.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017178.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2219.121, "latencies_ms": [2219.121], "images_per_second": 0.451, "prompt_tokens": 1109, "response_tokens_est": 45, "n_tiles": 1, "output_text": " The image features a group of horses walking down a tree-lined street, with a silver car parked on the side of the road. The horses are brown and black, and the trees are green, suggesting a sunny day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26860.1, "ram_available_mb": 98912.1, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26860.6, "ram_available_mb": 98911.6, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.23, "peak": 40.97, "min": 18.91}, "VIN": {"avg": 67.88, "peak": 120.34, "min": 29.19}, "VIN_SYS_5V0": {"avg": 15.14, "peak": 16.15, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 16.25, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 29.23, "energy_joules_est": 64.87, "sample_count": 22, "duration_seconds": 2.219}, "timestamp": "2026-01-19T14:41:57.230923"}
{"image_index": 170, "image_name": "000000017182.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017182.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1777.629, "latencies_ms": [1777.629], "images_per_second": 0.563, "prompt_tokens": 1099, "response_tokens_est": 27, "n_tiles": 1, "output_text": " In the image, there is a wooden desk with a blue book and a red apple on it, and a blackboard behind it.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 26860.6, "ram_available_mb": 98911.6, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26860.4, "ram_available_mb": 98911.8, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.68, "peak": 40.57, "min": 16.16}, "VIN": {"avg": 77.7, "peak": 127.45, "min": 28.64}, "VIN_SYS_5V0": {"avg": 14.81, "peak": 16.05, "min": 12.15}, "VDD_CPU_SOC_MSS": {"avg": 15.97, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 29.68, "energy_joules_est": 52.78, "sample_count": 18, "duration_seconds": 1.778}, "timestamp": "2026-01-19T14:41:59.115655"}
{"image_index": 170, "image_name": "000000017182.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017182.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1906.177, "latencies_ms": [1906.177], "images_per_second": 0.525, "prompt_tokens": 1113, "response_tokens_est": 30, "n_tiles": 1, "output_text": " desk: 1\nbooks: 1\napple: 1\nblackboard: 1\nchair: 1\ndoor: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26860.4, "ram_available_mb": 98911.8, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26860.4, "ram_available_mb": 98911.8, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.34, "peak": 40.18, "min": 16.95}, "VIN": {"avg": 68.41, "peak": 103.43, "min": 29.18}, "VIN_SYS_5V0": {"avg": 14.68, "peak": 15.95, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 15.75, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 29.34, "energy_joules_est": 55.93, "sample_count": 19, "duration_seconds": 1.906}, "timestamp": "2026-01-19T14:42:01.090295"}
{"image_index": 170, "image_name": "000000017182.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017182.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1972.368, "latencies_ms": [1972.368], "images_per_second": 0.507, "prompt_tokens": 1117, "response_tokens_est": 34, "n_tiles": 1, "output_text": " The books are on the desk, which is in front of the blackboard. The desk is to the left of the blackboard. The books are near the desk.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26860.4, "ram_available_mb": 98911.8, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26860.4, "ram_available_mb": 98911.8, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.69, "peak": 40.57, "min": 16.95}, "VIN": {"avg": 73.78, "peak": 124.06, "min": 32.03}, "VIN_SYS_5V0": {"avg": 14.88, "peak": 16.05, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.98, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 29.69, "energy_joules_est": 58.57, "sample_count": 19, "duration_seconds": 1.973}, "timestamp": "2026-01-19T14:42:03.071037"}
{"image_index": 170, "image_name": "000000017182.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017182.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1405.584, "latencies_ms": [1405.584], "images_per_second": 0.711, "prompt_tokens": 1111, "response_tokens_est": 13, "n_tiles": 1, "output_text": " A classroom with a desk, books, and a chalkboard.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26860.4, "ram_available_mb": 98911.8, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 26860.3, "ram_available_mb": 98911.8, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.38, "peak": 40.56, "min": 18.52}, "VIN": {"avg": 78.31, "peak": 119.13, "min": 28.76}, "VIN_SYS_5V0": {"avg": 15.21, "peak": 16.15, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 16.42, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 32.38, "energy_joules_est": 45.53, "sample_count": 14, "duration_seconds": 1.406}, "timestamp": "2026-01-19T14:42:04.526819"}
{"image_index": 170, "image_name": "000000017182.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017182.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1625.109, "latencies_ms": [1625.109], "images_per_second": 0.615, "prompt_tokens": 1109, "response_tokens_est": 21, "n_tiles": 1, "output_text": " The room is well-lit with natural light, and the walls are painted in a warm brown color.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26860.3, "ram_available_mb": 98911.8, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 26860.3, "ram_available_mb": 98911.8, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.25, "peak": 40.95, "min": 21.28}, "VIN": {"avg": 67.58, "peak": 105.41, "min": 27.24}, "VIN_SYS_5V0": {"avg": 15.04, "peak": 16.15, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 16.17, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 32.25, "energy_joules_est": 52.42, "sample_count": 16, "duration_seconds": 1.625}, "timestamp": "2026-01-19T14:42:06.191409"}
{"image_index": 171, "image_name": "000000017207.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017207.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1603.346, "latencies_ms": [1603.346], "images_per_second": 0.624, "prompt_tokens": 1099, "response_tokens_est": 21, "n_tiles": 1, "output_text": " A yellow and white bus with the number 475 on the back is parked on the street.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26860.3, "ram_available_mb": 98911.8, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26860.3, "ram_available_mb": 98911.8, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.66, "peak": 40.97, "min": 19.32}, "VIN": {"avg": 73.62, "peak": 117.99, "min": 29.5}, "VIN_SYS_5V0": {"avg": 15.14, "peak": 16.15, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 16.19, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 31.66, "energy_joules_est": 50.77, "sample_count": 16, "duration_seconds": 1.604}, "timestamp": "2026-01-19T14:42:07.856714"}
{"image_index": 171, "image_name": "000000017207.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017207.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1685.83, "latencies_ms": [1685.83], "images_per_second": 0.593, "prompt_tokens": 1113, "response_tokens_est": 24, "n_tiles": 1, "output_text": " bus: 1, car: 1, truck: 1, road: 1, sky: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26860.3, "ram_available_mb": 98911.8, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26860.3, "ram_available_mb": 98911.8, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.21, "peak": 40.95, "min": 19.31}, "VIN": {"avg": 75.94, "peak": 118.23, "min": 27.15}, "VIN_SYS_5V0": {"avg": 15.07, "peak": 16.15, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 16.14, "peak": 17.32, "min": 13.78}}, "power_watts_avg": 31.21, "energy_joules_est": 52.63, "sample_count": 17, "duration_seconds": 1.686}, "timestamp": "2026-01-19T14:42:09.629235"}
{"image_index": 171, "image_name": "000000017207.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017207.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2070.733, "latencies_ms": [2070.733], "images_per_second": 0.483, "prompt_tokens": 1117, "response_tokens_est": 38, "n_tiles": 1, "output_text": " The bus is parked on the left side of the road, while the white van is parked on the right side. The bus is in the foreground, while the van is in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26860.3, "ram_available_mb": 98911.8, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26860.3, "ram_available_mb": 98911.8, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.37, "peak": 40.56, "min": 17.74}, "VIN": {"avg": 71.35, "peak": 128.28, "min": 30.16}, "VIN_SYS_5V0": {"avg": 14.9, "peak": 16.05, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 16.02, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 29.37, "energy_joules_est": 60.83, "sample_count": 20, "duration_seconds": 2.071}, "timestamp": "2026-01-19T14:42:11.714397"}
{"image_index": 171, "image_name": "000000017207.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017207.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1395.379, "latencies_ms": [1395.379], "images_per_second": 0.717, "prompt_tokens": 1111, "response_tokens_est": 13, "n_tiles": 1, "output_text": " A yellow bus is driving down a busy street with other vehicles.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26860.3, "ram_available_mb": 98911.8, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 26860.1, "ram_available_mb": 98912.1, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.24, "peak": 40.16, "min": 18.13}, "VIN": {"avg": 74.53, "peak": 121.8, "min": 30.21}, "VIN_SYS_5V0": {"avg": 15.23, "peak": 16.15, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 16.39, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 32.24, "energy_joules_est": 44.99, "sample_count": 14, "duration_seconds": 1.396}, "timestamp": "2026-01-19T14:42:13.181608"}
{"image_index": 171, "image_name": "000000017207.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017207.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1829.318, "latencies_ms": [1829.318], "images_per_second": 0.547, "prompt_tokens": 1109, "response_tokens_est": 29, "n_tiles": 1, "output_text": " The image is taken during the day with a lot of traffic on the road. The colors in the image are vibrant and the lighting is natural.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26860.1, "ram_available_mb": 98912.1, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26860.1, "ram_available_mb": 98912.1, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.01, "peak": 40.97, "min": 20.89}, "VIN": {"avg": 75.18, "peak": 128.36, "min": 29.28}, "VIN_SYS_5V0": {"avg": 14.96, "peak": 16.05, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 16.03, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 31.01, "energy_joules_est": 56.74, "sample_count": 18, "duration_seconds": 1.83}, "timestamp": "2026-01-19T14:42:15.061783"}
{"image_index": 172, "image_name": "000000017379.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017379.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1977.35, "latencies_ms": [1977.35], "images_per_second": 0.506, "prompt_tokens": 1100, "response_tokens_est": 35, "n_tiles": 1, "output_text": " The image depicts a bathroom with a large mirror that reflects a television screen showing a football game, a sink with two faucets, and a trash can placed on the counter.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 26860.1, "ram_available_mb": 98912.1, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26859.7, "ram_available_mb": 98912.5, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.71, "peak": 40.57, "min": 18.14}, "VIN": {"avg": 68.44, "peak": 120.56, "min": 30.45}, "VIN_SYS_5V0": {"avg": 14.98, "peak": 16.05, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 16.25, "peak": 17.72, "min": 13.79}}, "power_watts_avg": 29.71, "energy_joules_est": 58.76, "sample_count": 19, "duration_seconds": 1.978}, "timestamp": "2026-01-19T14:42:17.057910"}
{"image_index": 172, "image_name": "000000017379.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017379.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1485.381, "latencies_ms": [1485.381], "images_per_second": 0.673, "prompt_tokens": 1114, "response_tokens_est": 16, "n_tiles": 1, "output_text": " television: 1\nmirror: 1\nsink: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26859.7, "ram_available_mb": 98912.5, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 26857.9, "ram_available_mb": 98914.2, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.59, "peak": 40.56, "min": 18.13}, "VIN": {"avg": 67.79, "peak": 101.49, "min": 27.71}, "VIN_SYS_5V0": {"avg": 15.17, "peak": 16.26, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 16.38, "peak": 17.71, "min": 14.57}}, "power_watts_avg": 31.59, "energy_joules_est": 46.93, "sample_count": 15, "duration_seconds": 1.486}, "timestamp": "2026-01-19T14:42:18.626878"}
{"image_index": 172, "image_name": "000000017379.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017379.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2160.143, "latencies_ms": [2160.143], "images_per_second": 0.463, "prompt_tokens": 1118, "response_tokens_est": 41, "n_tiles": 1, "output_text": " The television is positioned above the sink, with the mirror reflecting the television's image. The sink is located to the left of the television, and the trash can is situated to the right of the television.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26857.9, "ram_available_mb": 98914.2, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26857.9, "ram_available_mb": 98914.2, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.04, "peak": 40.57, "min": 19.31}, "VIN": {"avg": 72.02, "peak": 110.91, "min": 30.78}, "VIN_SYS_5V0": {"avg": 14.9, "peak": 16.05, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 16.08, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 29.04, "energy_joules_est": 62.74, "sample_count": 21, "duration_seconds": 2.161}, "timestamp": "2026-01-19T14:42:20.816156"}
{"image_index": 172, "image_name": "000000017379.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017379.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1636.555, "latencies_ms": [1636.555], "images_per_second": 0.611, "prompt_tokens": 1112, "response_tokens_est": 22, "n_tiles": 1, "output_text": " A bathroom with a large mirror and two sinks, a television mounted on the wall, and a trash can.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26857.9, "ram_available_mb": 98914.2, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 26857.9, "ram_available_mb": 98914.2, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.66, "peak": 39.78, "min": 16.95}, "VIN": {"avg": 60.49, "peak": 103.1, "min": 29.16}, "VIN_SYS_5V0": {"avg": 15.11, "peak": 16.26, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 16.26, "peak": 17.71, "min": 14.18}}, "power_watts_avg": 30.66, "energy_joules_est": 50.18, "sample_count": 16, "duration_seconds": 1.637}, "timestamp": "2026-01-19T14:42:22.490777"}
{"image_index": 172, "image_name": "000000017379.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017379.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1778.702, "latencies_ms": [1778.702], "images_per_second": 0.562, "prompt_tokens": 1110, "response_tokens_est": 27, "n_tiles": 1, "output_text": " The bathroom has a beige tiled wall and a brown countertop. The television is turned on and is showing a football game.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26857.9, "ram_available_mb": 98914.2, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26857.9, "ram_available_mb": 98914.2, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.29, "peak": 40.57, "min": 19.32}, "VIN": {"avg": 77.64, "peak": 125.43, "min": 27.36}, "VIN_SYS_5V0": {"avg": 14.96, "peak": 16.15, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 16.18, "peak": 17.71, "min": 13.79}}, "power_watts_avg": 30.29, "energy_joules_est": 53.89, "sample_count": 18, "duration_seconds": 1.779}, "timestamp": "2026-01-19T14:42:24.370221"}
{"image_index": 173, "image_name": "000000017436.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017436.jpg", "image_width": 481, "image_height": 640, "image_resolution": "481x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1896.481, "latencies_ms": [1896.481], "images_per_second": 0.527, "prompt_tokens": 1432, "response_tokens_est": 16, "n_tiles": 1, "output_text": " A man sits on a bench in a park with a church in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26857.9, "ram_available_mb": 98914.2, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 26857.9, "ram_available_mb": 98914.2, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.2, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.83, "peak": 40.57, "min": 16.56}, "VIN": {"avg": 75.08, "peak": 130.09, "min": 27.94}, "VIN_SYS_5V0": {"avg": 15.17, "peak": 16.66, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 16.31, "peak": 17.71, "min": 13.0}}, "power_watts_avg": 31.83, "energy_joules_est": 60.39, "sample_count": 19, "duration_seconds": 1.897}, "timestamp": "2026-01-19T14:42:26.356235"}
{"image_index": 173, "image_name": "000000017436.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017436.jpg", "image_width": 481, "image_height": 640, "image_resolution": "481x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2480.365, "latencies_ms": [2480.365], "images_per_second": 0.403, "prompt_tokens": 1446, "response_tokens_est": 38, "n_tiles": 1, "output_text": " 1. man sitting on bench\n2. bench\n3. street lamp\n4. tree\n5. building\n6. clock on tower\n7. bushes\n8. clouds", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26857.9, "ram_available_mb": 98914.2, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26854.5, "ram_available_mb": 98917.7, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.76, "peak": 40.57, "min": 19.32}, "VIN": {"avg": 74.16, "peak": 123.03, "min": 30.58}, "VIN_SYS_5V0": {"avg": 15.22, "peak": 16.56, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 16.35, "peak": 17.71, "min": 13.39}}, "power_watts_avg": 30.76, "energy_joules_est": 76.3, "sample_count": 24, "duration_seconds": 2.481}, "timestamp": "2026-01-19T14:42:28.848026"}
{"image_index": 173, "image_name": "000000017436.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017436.jpg", "image_width": 481, "image_height": 640, "image_resolution": "481x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2402.49, "latencies_ms": [2402.49], "images_per_second": 0.416, "prompt_tokens": 1450, "response_tokens_est": 36, "n_tiles": 1, "output_text": " The man is sitting on the left side of the bench, which is located in the foreground of the image. The church steeple is in the background, towering over the scene.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 26854.5, "ram_available_mb": 98917.7, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26854.7, "ram_available_mb": 98917.5, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.5, "peak": 40.57, "min": 18.14}, "VIN": {"avg": 74.57, "peak": 130.37, "min": 28.31}, "VIN_SYS_5V0": {"avg": 15.35, "peak": 16.76, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 16.44, "peak": 17.71, "min": 14.18}}, "power_watts_avg": 30.5, "energy_joules_est": 73.29, "sample_count": 24, "duration_seconds": 2.403}, "timestamp": "2026-01-19T14:42:31.352899"}
{"image_index": 173, "image_name": "000000017436.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017436.jpg", "image_width": 481, "image_height": 640, "image_resolution": "481x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1887.89, "latencies_ms": [1887.89], "images_per_second": 0.53, "prompt_tokens": 1444, "response_tokens_est": 16, "n_tiles": 1, "output_text": " A man sits on a bench in a park with a church in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26854.7, "ram_available_mb": 98917.5, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 2.4, "ram_used_mb": 26854.7, "ram_available_mb": 98917.5, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.87, "peak": 40.16, "min": 16.16}, "VIN": {"avg": 77.53, "peak": 127.07, "min": 30.47}, "VIN_SYS_5V0": {"avg": 15.07, "peak": 16.56, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 16.18, "peak": 17.71, "min": 12.61}}, "power_watts_avg": 31.87, "energy_joules_est": 60.18, "sample_count": 19, "duration_seconds": 1.888}, "timestamp": "2026-01-19T14:42:33.337271"}
{"image_index": 173, "image_name": "000000017436.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017436.jpg", "image_width": 481, "image_height": 640, "image_resolution": "481x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2370.179, "latencies_ms": [2370.179], "images_per_second": 0.422, "prompt_tokens": 1442, "response_tokens_est": 34, "n_tiles": 1, "output_text": " The image is in black and white, with a clear contrast between the man and the background. The sky is filled with clouds, and the trees are lush and green.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26854.7, "ram_available_mb": 98917.5, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26854.9, "ram_available_mb": 98917.2, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.11, "peak": 40.57, "min": 19.32}, "VIN": {"avg": 74.05, "peak": 122.12, "min": 29.6}, "VIN_SYS_5V0": {"avg": 15.13, "peak": 16.56, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 16.26, "peak": 17.71, "min": 13.0}}, "power_watts_avg": 31.11, "energy_joules_est": 73.75, "sample_count": 23, "duration_seconds": 2.37}, "timestamp": "2026-01-19T14:42:35.729982"}
{"image_index": 174, "image_name": "000000017627.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017627.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2262.143, "latencies_ms": [2262.143], "images_per_second": 0.442, "prompt_tokens": 1099, "response_tokens_est": 47, "n_tiles": 1, "output_text": " The image captures a bustling street scene in a city, with a variety of vehicles parked and moving along the road, including cars, a bus, and a truck, all under the watchful eye of a large stone wall in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26854.9, "ram_available_mb": 98917.2, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26854.9, "ram_available_mb": 98917.2, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.79, "peak": 40.16, "min": 18.53}, "VIN": {"avg": 73.57, "peak": 125.1, "min": 29.61}, "VIN_SYS_5V0": {"avg": 15.19, "peak": 16.15, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 16.21, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 28.79, "energy_joules_est": 65.15, "sample_count": 22, "duration_seconds": 2.263}, "timestamp": "2026-01-19T14:42:38.028027"}
{"image_index": 174, "image_name": "000000017627.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017627.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1954.458, "latencies_ms": [1954.458], "images_per_second": 0.512, "prompt_tokens": 1113, "response_tokens_est": 34, "n_tiles": 1, "output_text": " 1. street lamp\n2. car\n3. person\n4. building\n5. tree\n6. sign\n7. traffic light\n8. road", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26854.9, "ram_available_mb": 98917.2, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26852.4, "ram_available_mb": 98919.8, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.53, "peak": 40.56, "min": 16.95}, "VIN": {"avg": 76.43, "peak": 127.14, "min": 30.47}, "VIN_SYS_5V0": {"avg": 15.17, "peak": 16.15, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 16.12, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 29.53, "energy_joules_est": 57.72, "sample_count": 19, "duration_seconds": 1.955}, "timestamp": "2026-01-19T14:42:40.015018"}
{"image_index": 174, "image_name": "000000017627.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017627.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2022.505, "latencies_ms": [2022.505], "images_per_second": 0.494, "prompt_tokens": 1117, "response_tokens_est": 37, "n_tiles": 1, "output_text": " The cars are parked on the right side of the street, while the street is on the left side. The cars are parked near the sidewalk, and the street is near the sidewalk.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26852.4, "ram_available_mb": 98919.8, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26852.7, "ram_available_mb": 98919.5, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.39, "peak": 40.56, "min": 17.74}, "VIN": {"avg": 75.68, "peak": 122.12, "min": 28.82}, "VIN_SYS_5V0": {"avg": 15.11, "peak": 16.15, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 16.16, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 29.39, "energy_joules_est": 59.46, "sample_count": 20, "duration_seconds": 2.023}, "timestamp": "2026-01-19T14:42:42.099121"}
{"image_index": 174, "image_name": "000000017627.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017627.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2221.308, "latencies_ms": [2221.308], "images_per_second": 0.45, "prompt_tokens": 1111, "response_tokens_est": 44, "n_tiles": 1, "output_text": " The image captures a bustling street scene in a city, with a variety of vehicles parked and moving along the road. The buildings in the background suggest an urban setting, and the presence of a bus stop indicates public transportation.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26852.7, "ram_available_mb": 98919.5, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26852.7, "ram_available_mb": 98919.5, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.26, "peak": 40.56, "min": 16.95}, "VIN": {"avg": 72.18, "peak": 119.17, "min": 29.36}, "VIN_SYS_5V0": {"avg": 14.91, "peak": 16.05, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.96, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 28.26, "energy_joules_est": 62.79, "sample_count": 22, "duration_seconds": 2.222}, "timestamp": "2026-01-19T14:42:44.400022"}
{"image_index": 174, "image_name": "000000017627.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017627.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2226.865, "latencies_ms": [2226.865], "images_per_second": 0.449, "prompt_tokens": 1109, "response_tokens_est": 44, "n_tiles": 1, "output_text": " The image features a clear blue sky with a few clouds, and the sun is shining brightly, casting shadows on the ground. The cars are parked in a row, and the buildings in the background are made of stone.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26852.7, "ram_available_mb": 98919.5, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26852.7, "ram_available_mb": 98919.5, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.01, "peak": 40.18, "min": 15.77}, "VIN": {"avg": 64.53, "peak": 120.59, "min": 28.82}, "VIN_SYS_5V0": {"avg": 14.82, "peak": 16.05, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 15.84, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 28.01, "energy_joules_est": 62.38, "sample_count": 22, "duration_seconds": 2.227}, "timestamp": "2026-01-19T14:42:46.693317"}
{"image_index": 175, "image_name": "000000017714.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017714.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1827.009, "latencies_ms": [1827.009], "images_per_second": 0.547, "prompt_tokens": 1099, "response_tokens_est": 29, "n_tiles": 1, "output_text": " A wooden table with a white plate of food, a cup of tea, and a bowl of fruit is set on a white tiled floor.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 26852.7, "ram_available_mb": 98919.5, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26852.7, "ram_available_mb": 98919.5, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.79, "peak": 40.57, "min": 16.16}, "VIN": {"avg": 71.52, "peak": 105.92, "min": 29.64}, "VIN_SYS_5V0": {"avg": 14.9, "peak": 16.05, "min": 12.15}, "VDD_CPU_SOC_MSS": {"avg": 15.9, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 29.79, "energy_joules_est": 54.45, "sample_count": 18, "duration_seconds": 1.828}, "timestamp": "2026-01-19T14:42:48.582126"}
{"image_index": 175, "image_name": "000000017714.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017714.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2082.655, "latencies_ms": [2082.655], "images_per_second": 0.48, "prompt_tokens": 1113, "response_tokens_est": 39, "n_tiles": 1, "output_text": " plate: 1, cup: 1, fork: 1, knife: 1, banana: 1, watermelon: 1, cucumber: 1, fruit: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26852.7, "ram_available_mb": 98919.5, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26852.7, "ram_available_mb": 98919.5, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.89, "peak": 40.56, "min": 17.74}, "VIN": {"avg": 72.22, "peak": 107.16, "min": 27.98}, "VIN_SYS_5V0": {"avg": 14.93, "peak": 16.05, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 16.03, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 28.89, "energy_joules_est": 60.18, "sample_count": 21, "duration_seconds": 2.083}, "timestamp": "2026-01-19T14:42:50.769066"}
{"image_index": 175, "image_name": "000000017714.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017714.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2237.558, "latencies_ms": [2237.558], "images_per_second": 0.447, "prompt_tokens": 1117, "response_tokens_est": 44, "n_tiles": 1, "output_text": " The plate of food is located in the foreground, with the cup of tea and sugar bowl placed in the middle ground. The shadow of the table is cast on the floor, indicating the light source is coming from above.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26852.7, "ram_available_mb": 98919.5, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26852.7, "ram_available_mb": 98919.5, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.03, "peak": 40.18, "min": 15.77}, "VIN": {"avg": 73.33, "peak": 126.52, "min": 28.62}, "VIN_SYS_5V0": {"avg": 14.78, "peak": 16.05, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.77, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 28.03, "energy_joules_est": 62.73, "sample_count": 22, "duration_seconds": 2.238}, "timestamp": "2026-01-19T14:42:53.064188"}
{"image_index": 175, "image_name": "000000017714.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017714.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1804.228, "latencies_ms": [1804.228], "images_per_second": 0.554, "prompt_tokens": 1111, "response_tokens_est": 28, "n_tiles": 1, "output_text": " A wooden table with a plate of food, a cup of coffee, and a bowl of fruit is set on a white tiled floor.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26852.7, "ram_available_mb": 98919.5, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26852.7, "ram_available_mb": 98919.5, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.85, "peak": 40.57, "min": 16.56}, "VIN": {"avg": 73.87, "peak": 107.76, "min": 28.84}, "VIN_SYS_5V0": {"avg": 14.94, "peak": 16.05, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.94, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 29.85, "energy_joules_est": 53.86, "sample_count": 18, "duration_seconds": 1.804}, "timestamp": "2026-01-19T14:42:54.944324"}
{"image_index": 175, "image_name": "000000017714.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017714.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2214.19, "latencies_ms": [2214.19], "images_per_second": 0.452, "prompt_tokens": 1109, "response_tokens_est": 44, "n_tiles": 1, "output_text": " The image features a wooden table with a white plate of food, a cup of tea, and a bowl of fruit. The table is set in a room with white tiled floors and a tree shadow on the wall.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26852.7, "ram_available_mb": 98919.5, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26852.7, "ram_available_mb": 98919.5, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.46, "peak": 40.57, "min": 17.35}, "VIN": {"avg": 73.76, "peak": 128.11, "min": 29.28}, "VIN_SYS_5V0": {"avg": 14.91, "peak": 16.05, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.86, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 28.46, "energy_joules_est": 63.03, "sample_count": 22, "duration_seconds": 2.215}, "timestamp": "2026-01-19T14:42:57.240610"}
{"image_index": 176, "image_name": "000000017899.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017899.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1987.254, "latencies_ms": [1987.254], "images_per_second": 0.503, "prompt_tokens": 1100, "response_tokens_est": 36, "n_tiles": 1, "output_text": " An elderly woman wearing a green and white striped shirt and a pink apron is preparing food on a table with a variety of baked goods, including cookies, bread, and pastries.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 26852.7, "ram_available_mb": 98919.5, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26856.7, "ram_available_mb": 98915.5, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.62, "peak": 40.18, "min": 15.77}, "VIN": {"avg": 69.64, "peak": 107.96, "min": 27.65}, "VIN_SYS_5V0": {"avg": 14.89, "peak": 16.15, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 15.83, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 28.62, "energy_joules_est": 56.89, "sample_count": 20, "duration_seconds": 1.988}, "timestamp": "2026-01-19T14:42:59.334439"}
{"image_index": 176, "image_name": "000000017899.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017899.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2613.975, "latencies_ms": [2613.975], "images_per_second": 0.383, "prompt_tokens": 1114, "response_tokens_est": 57, "n_tiles": 1, "output_text": " 1. person: 1\n2. apron: 1\n3. table: 1\n4. chair: 1\n5. door: 1\n6. glass: 1\n7. cup: 1\n8. food: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26856.7, "ram_available_mb": 98915.5, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26856.7, "ram_available_mb": 98915.5, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.66, "peak": 40.18, "min": 16.16}, "VIN": {"avg": 65.17, "peak": 103.3, "min": 28.45}, "VIN_SYS_5V0": {"avg": 14.75, "peak": 16.05, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.63, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 26.66, "energy_joules_est": 69.7, "sample_count": 26, "duration_seconds": 2.615}, "timestamp": "2026-01-19T14:43:02.032709"}
{"image_index": 176, "image_name": "000000017899.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017899.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1950.569, "latencies_ms": [1950.569], "images_per_second": 0.513, "prompt_tokens": 1118, "response_tokens_est": 34, "n_tiles": 1, "output_text": " The person is standing to the left of the table, with the table being in the foreground. The person is near the table, with the door being in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26856.7, "ram_available_mb": 98915.5, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26856.6, "ram_available_mb": 98915.5, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.11, "peak": 40.16, "min": 14.98}, "VIN": {"avg": 71.13, "peak": 105.68, "min": 28.91}, "VIN_SYS_5V0": {"avg": 14.87, "peak": 16.15, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.89, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 29.11, "energy_joules_est": 56.79, "sample_count": 19, "duration_seconds": 1.951}, "timestamp": "2026-01-19T14:43:04.010521"}
{"image_index": 176, "image_name": "000000017899.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017899.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1616.748, "latencies_ms": [1616.748], "images_per_second": 0.619, "prompt_tokens": 1112, "response_tokens_est": 22, "n_tiles": 1, "output_text": " An elderly woman wearing a green and white striped shirt and a pink apron is preparing food on a table.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26856.6, "ram_available_mb": 98915.5, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26856.6, "ram_available_mb": 98915.5, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.0, "peak": 39.39, "min": 18.14}, "VIN": {"avg": 81.57, "peak": 127.88, "min": 29.46}, "VIN_SYS_5V0": {"avg": 15.2, "peak": 16.26, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 16.31, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 31.0, "energy_joules_est": 50.14, "sample_count": 16, "duration_seconds": 1.618}, "timestamp": "2026-01-19T14:43:05.684414"}
{"image_index": 176, "image_name": "000000017899.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017899.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1990.151, "latencies_ms": [1990.151], "images_per_second": 0.502, "prompt_tokens": 1110, "response_tokens_est": 36, "n_tiles": 1, "output_text": " The image is taken in a well-lit room with natural light coming through the window. The table is covered with a colorful tablecloth and there are various baked goods on it.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26856.6, "ram_available_mb": 98915.5, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26856.6, "ram_available_mb": 98915.5, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.45, "peak": 40.57, "min": 18.91}, "VIN": {"avg": 65.97, "peak": 104.73, "min": 29.23}, "VIN_SYS_5V0": {"avg": 15.0, "peak": 16.15, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.95, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 29.45, "energy_joules_est": 58.62, "sample_count": 20, "duration_seconds": 1.99}, "timestamp": "2026-01-19T14:43:07.770084"}
{"image_index": 177, "image_name": "000000017905.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017905.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2012.565, "latencies_ms": [2012.565], "images_per_second": 0.497, "prompt_tokens": 1100, "response_tokens_est": 37, "n_tiles": 1, "output_text": " A man stands in front of a traffic light with a sign that says \"AUSTRALIA MOST BAD TRAFFIC LIGHT\" and a red fire hydrant.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 26856.6, "ram_available_mb": 98915.5, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26856.6, "ram_available_mb": 98915.5, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.76, "peak": 40.16, "min": 16.16}, "VIN": {"avg": 63.65, "peak": 104.23, "min": 28.78}, "VIN_SYS_5V0": {"avg": 14.87, "peak": 16.05, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.87, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 28.76, "energy_joules_est": 57.9, "sample_count": 20, "duration_seconds": 2.013}, "timestamp": "2026-01-19T14:43:09.852321"}
{"image_index": 177, "image_name": "000000017905.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017905.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2543.871, "latencies_ms": [2543.871], "images_per_second": 0.393, "prompt_tokens": 1114, "response_tokens_est": 57, "n_tiles": 1, "output_text": " 1. man: 1\n2. traffic light: 1\n3. sign: 1\n4. pole: 1\n5. tree: 1\n6. flower: 1\n7. rock: 1\n8. ground: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26856.6, "ram_available_mb": 98915.5, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 26856.6, "ram_available_mb": 98915.5, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.19, "peak": 40.18, "min": 16.56}, "VIN": {"avg": 61.9, "peak": 114.58, "min": 30.39}, "VIN_SYS_5V0": {"avg": 15.03, "peak": 16.15, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 16.0, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 27.19, "energy_joules_est": 69.18, "sample_count": 25, "duration_seconds": 2.544}, "timestamp": "2026-01-19T14:43:12.456922"}
{"image_index": 177, "image_name": "000000017905.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017905.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2603.896, "latencies_ms": [2603.896], "images_per_second": 0.384, "prompt_tokens": 1118, "response_tokens_est": 59, "n_tiles": 1, "output_text": " The man is standing to the left of the traffic light, which is positioned in the middle of the image. The traffic light is in front of a sign that reads \"AUSTRALIA MOST BAD TRAFFIC LIGHT,\" which is located to the right of the man.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26856.6, "ram_available_mb": 98915.5, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26856.4, "ram_available_mb": 98915.8, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.7, "peak": 40.18, "min": 15.77}, "VIN": {"avg": 70.15, "peak": 117.1, "min": 28.66}, "VIN_SYS_5V0": {"avg": 14.94, "peak": 16.15, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.93, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 26.7, "energy_joules_est": 69.53, "sample_count": 26, "duration_seconds": 2.604}, "timestamp": "2026-01-19T14:43:15.165411"}
{"image_index": 177, "image_name": "000000017905.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017905.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1581.363, "latencies_ms": [1581.363], "images_per_second": 0.632, "prompt_tokens": 1112, "response_tokens_est": 20, "n_tiles": 1, "output_text": " A man stands in front of a traffic light and a sign that says \"Australia Most Dangerous Road\".", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26856.4, "ram_available_mb": 98915.8, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 26856.4, "ram_available_mb": 98915.8, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.97, "peak": 39.39, "min": 14.98}, "VIN": {"avg": 66.88, "peak": 88.63, "min": 27.64}, "VIN_SYS_5V0": {"avg": 14.75, "peak": 16.05, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 15.82, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 29.97, "energy_joules_est": 47.4, "sample_count": 16, "duration_seconds": 1.582}, "timestamp": "2026-01-19T14:43:16.839209"}
{"image_index": 177, "image_name": "000000017905.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017905.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1879.793, "latencies_ms": [1879.793], "images_per_second": 0.532, "prompt_tokens": 1110, "response_tokens_est": 31, "n_tiles": 1, "output_text": " The man is wearing a white t-shirt and khaki shorts. The traffic light is red, yellow, and green. The sky is overcast.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26856.4, "ram_available_mb": 98915.8, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26856.4, "ram_available_mb": 98915.8, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.65, "peak": 40.56, "min": 18.14}, "VIN": {"avg": 71.4, "peak": 122.36, "min": 28.42}, "VIN_SYS_5V0": {"avg": 14.86, "peak": 16.15, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.89, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 29.65, "energy_joules_est": 55.75, "sample_count": 19, "duration_seconds": 1.88}, "timestamp": "2026-01-19T14:43:18.812694"}
{"image_index": 178, "image_name": "000000017959.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017959.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1786.36, "latencies_ms": [1786.36], "images_per_second": 0.56, "prompt_tokens": 1099, "response_tokens_est": 27, "n_tiles": 1, "output_text": " A large group of people are gathered in a park to fly a variety of kites, including a large red and white fish kite.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 26856.4, "ram_available_mb": 98915.8, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26856.4, "ram_available_mb": 98915.8, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.77, "peak": 40.56, "min": 16.56}, "VIN": {"avg": 70.0, "peak": 121.75, "min": 27.1}, "VIN_SYS_5V0": {"avg": 14.75, "peak": 16.05, "min": 11.87}, "VDD_CPU_SOC_MSS": {"avg": 15.77, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 29.77, "energy_joules_est": 53.19, "sample_count": 18, "duration_seconds": 1.787}, "timestamp": "2026-01-19T14:43:20.693972"}
{"image_index": 178, "image_name": "000000017959.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017959.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3009.775, "latencies_ms": [3009.775], "images_per_second": 0.332, "prompt_tokens": 1113, "response_tokens_est": 74, "n_tiles": 1, "output_text": " 1. Kites: 12\n2. People: 10\n3. Trees: 10\n4. Buildings: 1\n5. Kite String: 1\n6. Kite String Ties: 1\n7. Kite String Ties: 1\n8. Kite String Ties: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26856.4, "ram_available_mb": 98915.8, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 26856.8, "ram_available_mb": 98915.3, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.23, "peak": 40.56, "min": 16.95}, "VIN": {"avg": 71.61, "peak": 126.9, "min": 28.85}, "VIN_SYS_5V0": {"avg": 14.91, "peak": 16.05, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.87, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 26.23, "energy_joules_est": 78.95, "sample_count": 29, "duration_seconds": 3.01}, "timestamp": "2026-01-19T14:43:23.730792"}
{"image_index": 178, "image_name": "000000017959.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017959.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2971.697, "latencies_ms": [2971.697], "images_per_second": 0.337, "prompt_tokens": 1117, "response_tokens_est": 73, "n_tiles": 1, "output_text": " The kites are positioned in the foreground, with the largest and most prominent one being in the center. The kites are arranged in a semi-circle formation, with the largest one at the front and the smaller ones behind it. The kites are flying at varying heights, with the largest one being at the highest point and the smaller ones at lower levels.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26856.8, "ram_available_mb": 98915.3, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 26857.1, "ram_available_mb": 98915.1, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.17, "peak": 40.57, "min": 16.16}, "VIN": {"avg": 67.97, "peak": 97.32, "min": 29.41}, "VIN_SYS_5V0": {"avg": 15.06, "peak": 16.15, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 16.14, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 26.17, "energy_joules_est": 77.78, "sample_count": 29, "duration_seconds": 2.972}, "timestamp": "2026-01-19T14:43:26.756870"}
{"image_index": 178, "image_name": "000000017959.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017959.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1856.223, "latencies_ms": [1856.223], "images_per_second": 0.539, "prompt_tokens": 1111, "response_tokens_est": 30, "n_tiles": 1, "output_text": " A large group of people are gathered in a park to fly kites. The kites are shaped like fish and are flying high in the sky.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26857.1, "ram_available_mb": 98915.1, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26856.8, "ram_available_mb": 98915.4, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.64, "peak": 40.18, "min": 15.77}, "VIN": {"avg": 71.44, "peak": 105.67, "min": 30.01}, "VIN_SYS_5V0": {"avg": 14.95, "peak": 16.05, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 16.12, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 29.64, "energy_joules_est": 55.03, "sample_count": 18, "duration_seconds": 1.856}, "timestamp": "2026-01-19T14:43:28.643845"}
{"image_index": 178, "image_name": "000000017959.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017959.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1625.296, "latencies_ms": [1625.296], "images_per_second": 0.615, "prompt_tokens": 1109, "response_tokens_est": 21, "n_tiles": 1, "output_text": " The sky is overcast, and the kites are vibrant with red, orange, and purple hues.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26856.8, "ram_available_mb": 98915.4, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 26856.8, "ram_available_mb": 98915.4, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.25, "peak": 40.56, "min": 18.14}, "VIN": {"avg": 70.13, "peak": 102.01, "min": 28.56}, "VIN_SYS_5V0": {"avg": 15.06, "peak": 16.15, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 16.22, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 31.25, "energy_joules_est": 50.8, "sample_count": 16, "duration_seconds": 1.626}, "timestamp": "2026-01-19T14:43:30.316523"}
{"image_index": 179, "image_name": "000000018150.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018150.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1578.538, "latencies_ms": [1578.538], "images_per_second": 0.633, "prompt_tokens": 1099, "response_tokens_est": 20, "n_tiles": 1, "output_text": " A man in a blue jacket is giving a slice of pizza to a young boy with curly hair.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 26856.8, "ram_available_mb": 98915.4, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 26856.8, "ram_available_mb": 98915.4, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.59, "peak": 40.95, "min": 19.31}, "VIN": {"avg": 72.66, "peak": 120.94, "min": 27.65}, "VIN_SYS_5V0": {"avg": 15.03, "peak": 16.15, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 16.12, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 31.59, "energy_joules_est": 49.88, "sample_count": 16, "duration_seconds": 1.579}, "timestamp": "2026-01-19T14:43:31.993934"}
{"image_index": 179, "image_name": "000000018150.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018150.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2534.504, "latencies_ms": [2534.504], "images_per_second": 0.395, "prompt_tokens": 1113, "response_tokens_est": 56, "n_tiles": 1, "output_text": " 1. man: 1\n2. boy: 1\n3. pizza: 1\n4. box: 1\n5. blanket: 1\n6. chair: 1\n7. window: 1\n8. door: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26856.8, "ram_available_mb": 98915.4, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26856.8, "ram_available_mb": 98915.4, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.51, "peak": 40.57, "min": 18.13}, "VIN": {"avg": 66.99, "peak": 122.27, "min": 30.11}, "VIN_SYS_5V0": {"avg": 14.85, "peak": 16.05, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.84, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 27.51, "energy_joules_est": 69.74, "sample_count": 25, "duration_seconds": 2.535}, "timestamp": "2026-01-19T14:43:34.601260"}
{"image_index": 179, "image_name": "000000018150.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018150.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2180.607, "latencies_ms": [2180.607], "images_per_second": 0.459, "prompt_tokens": 1117, "response_tokens_est": 42, "n_tiles": 1, "output_text": " The man is sitting on the left side of the image, while the child is on the right side. The pizza is in the middle of the image, and the man is holding it with his right hand.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26856.8, "ram_available_mb": 98915.4, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26856.8, "ram_available_mb": 98915.4, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.38, "peak": 40.16, "min": 15.37}, "VIN": {"avg": 67.89, "peak": 125.34, "min": 30.36}, "VIN_SYS_5V0": {"avg": 14.94, "peak": 16.05, "min": 12.25}, "VDD_CPU_SOC_MSS": {"avg": 15.97, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 28.38, "energy_joules_est": 61.9, "sample_count": 21, "duration_seconds": 2.181}, "timestamp": "2026-01-19T14:43:36.795566"}
{"image_index": 179, "image_name": "000000018150.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018150.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1621.836, "latencies_ms": [1621.836], "images_per_second": 0.617, "prompt_tokens": 1111, "response_tokens_est": 22, "n_tiles": 1, "output_text": " A man and a young boy are sitting on the floor in a living room, sharing a slice of pizza.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26856.8, "ram_available_mb": 98915.4, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 26856.8, "ram_available_mb": 98915.4, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.42, "peak": 40.18, "min": 18.13}, "VIN": {"avg": 74.63, "peak": 103.66, "min": 30.39}, "VIN_SYS_5V0": {"avg": 15.27, "peak": 16.15, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 16.31, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 31.42, "energy_joules_est": 50.97, "sample_count": 16, "duration_seconds": 1.622}, "timestamp": "2026-01-19T14:43:38.467797"}
{"image_index": 179, "image_name": "000000018150.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018150.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1848.744, "latencies_ms": [1848.744], "images_per_second": 0.541, "prompt_tokens": 1109, "response_tokens_est": 29, "n_tiles": 1, "output_text": " The image is taken in a room with orange walls and a tiled floor. The lighting is natural, coming from a window in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26856.8, "ram_available_mb": 98915.4, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26856.8, "ram_available_mb": 98915.4, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.55, "peak": 40.95, "min": 19.32}, "VIN": {"avg": 67.95, "peak": 105.41, "min": 27.91}, "VIN_SYS_5V0": {"avg": 15.05, "peak": 16.15, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 16.08, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 30.55, "energy_joules_est": 56.49, "sample_count": 18, "duration_seconds": 1.849}, "timestamp": "2026-01-19T14:43:40.345864"}
{"image_index": 180, "image_name": "000000018193.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018193.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1511.751, "latencies_ms": [1511.751], "images_per_second": 0.661, "prompt_tokens": 1099, "response_tokens_est": 17, "n_tiles": 1, "output_text": " A woman wearing a striped shirt is eating a hot dog while sitting in a chair.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 26856.8, "ram_available_mb": 98915.4, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 26856.8, "ram_available_mb": 98915.4, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.86, "peak": 40.16, "min": 18.14}, "VIN": {"avg": 76.33, "peak": 128.39, "min": 29.11}, "VIN_SYS_5V0": {"avg": 15.13, "peak": 16.15, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 16.19, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 31.86, "energy_joules_est": 48.18, "sample_count": 15, "duration_seconds": 1.512}, "timestamp": "2026-01-19T14:43:41.914201"}
{"image_index": 180, "image_name": "000000018193.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018193.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2532.467, "latencies_ms": [2532.467], "images_per_second": 0.395, "prompt_tokens": 1113, "response_tokens_est": 57, "n_tiles": 1, "output_text": " 1. person: 1\n2. chair: 1\n3. plate: 1\n4. food: 1\n5. bag: 1\n6. chair leg: 1\n7. ground: 1\n8. rock: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26856.8, "ram_available_mb": 98915.4, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26856.9, "ram_available_mb": 98915.2, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.97, "peak": 40.95, "min": 18.13}, "VIN": {"avg": 73.16, "peak": 123.0, "min": 29.1}, "VIN_SYS_5V0": {"avg": 15.02, "peak": 16.15, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 16.08, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 27.97, "energy_joules_est": 70.84, "sample_count": 25, "duration_seconds": 2.533}, "timestamp": "2026-01-19T14:43:44.517988"}
{"image_index": 180, "image_name": "000000018193.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018193.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2296.291, "latencies_ms": [2296.291], "images_per_second": 0.435, "prompt_tokens": 1117, "response_tokens_est": 45, "n_tiles": 1, "output_text": " The person is sitting on the left side of the image, with the plate of food placed in front of them on the right side. The plate of food is positioned in the foreground, while the person is in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26856.9, "ram_available_mb": 98915.2, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26856.9, "ram_available_mb": 98915.2, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.6, "peak": 40.18, "min": 15.77}, "VIN": {"avg": 64.74, "peak": 93.66, "min": 27.66}, "VIN_SYS_5V0": {"avg": 14.82, "peak": 16.05, "min": 12.15}, "VDD_CPU_SOC_MSS": {"avg": 15.77, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 27.6, "energy_joules_est": 63.38, "sample_count": 23, "duration_seconds": 2.297}, "timestamp": "2026-01-19T14:43:46.910239"}
{"image_index": 180, "image_name": "000000018193.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018193.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1446.957, "latencies_ms": [1446.957], "images_per_second": 0.691, "prompt_tokens": 1111, "response_tokens_est": 14, "n_tiles": 1, "output_text": " A woman is sitting in a chair and eating a sandwich while camping.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26856.9, "ram_available_mb": 98915.2, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 26856.9, "ram_available_mb": 98915.3, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.29, "peak": 40.18, "min": 14.98}, "VIN": {"avg": 73.19, "peak": 101.62, "min": 28.97}, "VIN_SYS_5V0": {"avg": 14.77, "peak": 16.05, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.86, "peak": 17.32, "min": 12.61}}, "power_watts_avg": 31.29, "energy_joules_est": 45.3, "sample_count": 14, "duration_seconds": 1.448}, "timestamp": "2026-01-19T14:43:48.374955"}
{"image_index": 180, "image_name": "000000018193.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018193.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1938.101, "latencies_ms": [1938.101], "images_per_second": 0.516, "prompt_tokens": 1109, "response_tokens_est": 34, "n_tiles": 1, "output_text": " The image is taken at night, with the subject illuminated by a flashlight. The subject is wearing a green and white striped shirt and is seated on a blue camping chair.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26856.9, "ram_available_mb": 98915.3, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26856.9, "ram_available_mb": 98915.3, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.02, "peak": 40.95, "min": 20.49}, "VIN": {"avg": 71.38, "peak": 120.51, "min": 29.39}, "VIN_SYS_5V0": {"avg": 15.18, "peak": 16.15, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 16.33, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 31.02, "energy_joules_est": 60.14, "sample_count": 19, "duration_seconds": 1.939}, "timestamp": "2026-01-19T14:43:50.356780"}
{"image_index": 181, "image_name": "000000018380.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018380.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1538.293, "latencies_ms": [1538.293], "images_per_second": 0.65, "prompt_tokens": 1099, "response_tokens_est": 18, "n_tiles": 1, "output_text": " A family of 12 is gathered around a long table, enjoying a meal together.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26856.9, "ram_available_mb": 98915.3, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 26856.9, "ram_available_mb": 98915.3, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.62, "peak": 40.57, "min": 17.35}, "VIN": {"avg": 73.2, "peak": 111.16, "min": 29.11}, "VIN_SYS_5V0": {"avg": 15.01, "peak": 16.05, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 16.22, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 31.62, "energy_joules_est": 48.65, "sample_count": 15, "duration_seconds": 1.539}, "timestamp": "2026-01-19T14:43:51.931217"}
{"image_index": 181, "image_name": "000000018380.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018380.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2113.783, "latencies_ms": [2113.783], "images_per_second": 0.473, "prompt_tokens": 1113, "response_tokens_est": 40, "n_tiles": 1, "output_text": " tablecloth: 1, plates: 10, cups: 10, glasses: 10, food: 10, chairs: 4, people: 14", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26856.9, "ram_available_mb": 98915.3, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26856.9, "ram_available_mb": 98915.3, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.49, "peak": 40.97, "min": 18.91}, "VIN": {"avg": 73.17, "peak": 105.59, "min": 29.72}, "VIN_SYS_5V0": {"avg": 15.05, "peak": 16.15, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 16.14, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 29.49, "energy_joules_est": 62.36, "sample_count": 21, "duration_seconds": 2.115}, "timestamp": "2026-01-19T14:43:54.127177"}
{"image_index": 181, "image_name": "000000018380.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018380.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2788.6, "latencies_ms": [2788.6], "images_per_second": 0.359, "prompt_tokens": 1117, "response_tokens_est": 66, "n_tiles": 1, "output_text": " The main objects are arranged in a way that the family is seated around the table, with the adults and children positioned at varying distances from the camera. The table is centrally located, with the family members positioned in a semi-circle around it. The background includes a doorway leading to another room, and a painting on the wall.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26856.9, "ram_available_mb": 98915.3, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 26856.9, "ram_available_mb": 98915.3, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.66, "peak": 40.18, "min": 16.16}, "VIN": {"avg": 71.15, "peak": 124.35, "min": 29.48}, "VIN_SYS_5V0": {"avg": 14.88, "peak": 16.05, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 15.98, "peak": 17.32, "min": 13.4}}, "power_watts_avg": 26.66, "energy_joules_est": 74.35, "sample_count": 27, "duration_seconds": 2.789}, "timestamp": "2026-01-19T14:43:56.939600"}
{"image_index": 181, "image_name": "000000018380.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018380.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1591.818, "latencies_ms": [1591.818], "images_per_second": 0.628, "prompt_tokens": 1111, "response_tokens_est": 20, "n_tiles": 1, "output_text": " A group of people are gathered around a long table in a dining room, enjoying a meal together.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26856.9, "ram_available_mb": 98915.3, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26847.6, "ram_available_mb": 98924.6, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.83, "peak": 40.56, "min": 16.56}, "VIN": {"avg": 74.71, "peak": 108.55, "min": 29.54}, "VIN_SYS_5V0": {"avg": 15.06, "peak": 16.15, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 16.24, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 30.83, "energy_joules_est": 49.09, "sample_count": 16, "duration_seconds": 1.592}, "timestamp": "2026-01-19T14:43:58.608555"}
{"image_index": 181, "image_name": "000000018380.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018380.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1990.954, "latencies_ms": [1990.954], "images_per_second": 0.502, "prompt_tokens": 1109, "response_tokens_est": 34, "n_tiles": 1, "output_text": " The image is taken in a well-lit room with natural light coming from the windows. The colors in the image are vibrant and the materials are mostly wooden and fabric.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26847.6, "ram_available_mb": 98924.6, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26847.6, "ram_available_mb": 98924.6, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.39, "peak": 40.57, "min": 18.52}, "VIN": {"avg": 74.1, "peak": 122.41, "min": 28.75}, "VIN_SYS_5V0": {"avg": 14.79, "peak": 16.05, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.87, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 29.39, "energy_joules_est": 58.53, "sample_count": 20, "duration_seconds": 1.991}, "timestamp": "2026-01-19T14:44:00.698790"}
{"image_index": 182, "image_name": "000000018491.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018491.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1731.122, "latencies_ms": [1731.122], "images_per_second": 0.578, "prompt_tokens": 1099, "response_tokens_est": 25, "n_tiles": 1, "output_text": " A baseball game is taking place with players sliding into a base, and a catcher and umpire are standing nearby.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26847.6, "ram_available_mb": 98924.6, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26847.6, "ram_available_mb": 98924.6, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.01, "peak": 40.57, "min": 15.77}, "VIN": {"avg": 72.99, "peak": 124.92, "min": 28.99}, "VIN_SYS_5V0": {"avg": 14.74, "peak": 15.95, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 15.82, "peak": 17.32, "min": 12.61}}, "power_watts_avg": 30.01, "energy_joules_est": 51.97, "sample_count": 17, "duration_seconds": 1.732}, "timestamp": "2026-01-19T14:44:02.478166"}
{"image_index": 182, "image_name": "000000018491.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018491.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2620.56, "latencies_ms": [2620.56], "images_per_second": 0.382, "prompt_tokens": 1113, "response_tokens_est": 60, "n_tiles": 1, "output_text": " 1. baseball player: 2\n2. catcher: 1\n3. umpire: 1\n4. batter: 1\n5. field: 1\n6. fence: 1\n7. bench: 1\n8. spectators: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26847.6, "ram_available_mb": 98924.6, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 26847.6, "ram_available_mb": 98924.6, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.39, "peak": 40.56, "min": 18.13}, "VIN": {"avg": 69.53, "peak": 107.15, "min": 28.77}, "VIN_SYS_5V0": {"avg": 14.98, "peak": 16.15, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 16.13, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 27.39, "energy_joules_est": 71.79, "sample_count": 26, "duration_seconds": 2.621}, "timestamp": "2026-01-19T14:44:05.178222"}
{"image_index": 182, "image_name": "000000018491.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018491.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2284.983, "latencies_ms": [2284.983], "images_per_second": 0.438, "prompt_tokens": 1117, "response_tokens_est": 46, "n_tiles": 1, "output_text": " The baseball player is in the foreground, sliding into the base, while the catcher and umpire are in the background, observing the play. The player is closer to the camera than the catcher and umpire.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26847.6, "ram_available_mb": 98924.6, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26847.9, "ram_available_mb": 98924.3, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.03, "peak": 40.18, "min": 15.38}, "VIN": {"avg": 63.87, "peak": 126.58, "min": 30.43}, "VIN_SYS_5V0": {"avg": 14.88, "peak": 16.05, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 15.93, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 28.03, "energy_joules_est": 64.06, "sample_count": 22, "duration_seconds": 2.285}, "timestamp": "2026-01-19T14:44:07.471589"}
{"image_index": 182, "image_name": "000000018491.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018491.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1743.359, "latencies_ms": [1743.359], "images_per_second": 0.574, "prompt_tokens": 1111, "response_tokens_est": 26, "n_tiles": 1, "output_text": " A baseball game is taking place on a field with a fence in the background, and a group of people are watching the game.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26847.9, "ram_available_mb": 98924.3, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26847.9, "ram_available_mb": 98924.3, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.68, "peak": 40.56, "min": 17.34}, "VIN": {"avg": 80.19, "peak": 119.48, "min": 28.2}, "VIN_SYS_5V0": {"avg": 15.28, "peak": 16.15, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 16.4, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 30.68, "energy_joules_est": 53.5, "sample_count": 17, "duration_seconds": 1.744}, "timestamp": "2026-01-19T14:44:09.241905"}
{"image_index": 182, "image_name": "000000018491.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018491.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1849.344, "latencies_ms": [1849.344], "images_per_second": 0.541, "prompt_tokens": 1109, "response_tokens_est": 30, "n_tiles": 1, "output_text": " The image captures a dynamic moment in a baseball game, with players in action on a field bathed in sunlight, and spectators watching from the stands.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26847.9, "ram_available_mb": 98924.3, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26847.9, "ram_available_mb": 98924.3, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.55, "peak": 40.56, "min": 18.52}, "VIN": {"avg": 69.43, "peak": 106.06, "min": 28.7}, "VIN_SYS_5V0": {"avg": 15.18, "peak": 16.15, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 16.25, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 30.55, "energy_joules_est": 56.51, "sample_count": 18, "duration_seconds": 1.85}, "timestamp": "2026-01-19T14:44:11.106995"}
{"image_index": 183, "image_name": "000000018519.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018519.jpg", "image_width": 515, "image_height": 640, "image_resolution": "515x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2090.977, "latencies_ms": [2090.977], "images_per_second": 0.478, "prompt_tokens": 1432, "response_tokens_est": 24, "n_tiles": 1, "output_text": " A person wearing a black helmet and black clothing is skateboarding on a ramp, with their shadow visible on the ramp.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 26847.9, "ram_available_mb": 98924.3, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 26848.1, "ram_available_mb": 98924.1, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.2, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.61, "peak": 40.57, "min": 18.52}, "VIN": {"avg": 74.05, "peak": 113.1, "min": 28.5}, "VIN_SYS_5V0": {"avg": 15.53, "peak": 16.76, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 16.7, "peak": 18.11, "min": 14.18}}, "power_watts_avg": 31.61, "energy_joules_est": 66.12, "sample_count": 21, "duration_seconds": 2.092}, "timestamp": "2026-01-19T14:44:13.301586"}
{"image_index": 183, "image_name": "000000018519.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018519.jpg", "image_width": 515, "image_height": 640, "image_resolution": "515x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3040.794, "latencies_ms": [3040.794], "images_per_second": 0.329, "prompt_tokens": 1446, "response_tokens_est": 59, "n_tiles": 1, "output_text": " 1. person: 1\n2. helmet: 1\n3. knee pads: 2\n4. elbow pads: 1\n5. skateboard: 1\n6. rail: 1\n7. shadow: 1\n8. grass: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26848.1, "ram_available_mb": 98924.1, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26848.5, "ram_available_mb": 98923.6, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.32, "peak": 39.78, "min": 17.35}, "VIN": {"avg": 69.21, "peak": 122.38, "min": 28.65}, "VIN_SYS_5V0": {"avg": 15.02, "peak": 16.56, "min": 11.87}, "VDD_CPU_SOC_MSS": {"avg": 16.13, "peak": 17.71, "min": 13.0}}, "power_watts_avg": 28.32, "energy_joules_est": 86.12, "sample_count": 30, "duration_seconds": 3.041}, "timestamp": "2026-01-19T14:44:16.416426"}
{"image_index": 183, "image_name": "000000018519.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018519.jpg", "image_width": 515, "image_height": 640, "image_resolution": "515x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2992.44, "latencies_ms": [2992.44], "images_per_second": 0.334, "prompt_tokens": 1450, "response_tokens_est": 51, "n_tiles": 1, "output_text": " The skateboarder is positioned in the foreground, performing a trick on the ramp, while the background features a grassy area with trees and a fence. The shadow of the skateboarder is cast on the ramp, indicating the direction of the light source.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26848.5, "ram_available_mb": 98923.6, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26848.5, "ram_available_mb": 98923.6, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.84, "peak": 40.18, "min": 15.77}, "VIN": {"avg": 71.78, "peak": 126.98, "min": 27.27}, "VIN_SYS_5V0": {"avg": 14.85, "peak": 16.56, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 15.88, "peak": 17.71, "min": 13.0}}, "power_watts_avg": 27.84, "energy_joules_est": 83.32, "sample_count": 29, "duration_seconds": 2.993}, "timestamp": "2026-01-19T14:44:19.439570"}
{"image_index": 183, "image_name": "000000018519.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018519.jpg", "image_width": 515, "image_height": 640, "image_resolution": "515x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1887.706, "latencies_ms": [1887.706], "images_per_second": 0.53, "prompt_tokens": 1444, "response_tokens_est": 16, "n_tiles": 1, "output_text": " A skateboarder is performing a trick on a ramp in a skate park.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26848.5, "ram_available_mb": 98923.6, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 26848.8, "ram_available_mb": 98923.4, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.91, "peak": 40.16, "min": 16.16}, "VIN": {"avg": 73.46, "peak": 120.84, "min": 27.24}, "VIN_SYS_5V0": {"avg": 15.27, "peak": 16.66, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 16.39, "peak": 17.71, "min": 13.39}}, "power_watts_avg": 31.91, "energy_joules_est": 60.25, "sample_count": 19, "duration_seconds": 1.888}, "timestamp": "2026-01-19T14:44:21.424621"}
{"image_index": 183, "image_name": "000000018519.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018519.jpg", "image_width": 515, "image_height": 640, "image_resolution": "515x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3256.41, "latencies_ms": [3256.41], "images_per_second": 0.307, "prompt_tokens": 1442, "response_tokens_est": 64, "n_tiles": 1, "output_text": " The image features a skateboarder in action, wearing a black helmet and protective gear, with a concrete ramp and a grassy area in the background. The lighting suggests it is a sunny day, and the colors are vibrant, with the skateboarder's black attire contrasting against the green grass and the gray concrete ramp.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26848.8, "ram_available_mb": 98923.4, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 26848.8, "ram_available_mb": 98923.4, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.81, "peak": 40.18, "min": 18.13}, "VIN": {"avg": 67.83, "peak": 110.68, "min": 29.83}, "VIN_SYS_5V0": {"avg": 14.97, "peak": 16.46, "min": 11.66}, "VDD_CPU_SOC_MSS": {"avg": 15.98, "peak": 17.71, "min": 13.0}}, "power_watts_avg": 27.81, "energy_joules_est": 90.57, "sample_count": 32, "duration_seconds": 3.257}, "timestamp": "2026-01-19T14:44:24.752874"}
{"image_index": 184, "image_name": "000000018575.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018575.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2076.524, "latencies_ms": [2076.524], "images_per_second": 0.482, "prompt_tokens": 1099, "response_tokens_est": 39, "n_tiles": 1, "output_text": " The image shows a dining table with a plate of food, including a sandwich, a side of fries, and a salad, along with a glass of water and a bottle of condiments.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 26848.8, "ram_available_mb": 98923.4, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26848.8, "ram_available_mb": 98923.4, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.9, "peak": 40.18, "min": 15.77}, "VIN": {"avg": 69.91, "peak": 104.01, "min": 34.93}, "VIN_SYS_5V0": {"avg": 14.92, "peak": 16.05, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.98, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 28.9, "energy_joules_est": 60.03, "sample_count": 20, "duration_seconds": 2.077}, "timestamp": "2026-01-19T14:44:26.843984"}
{"image_index": 184, "image_name": "000000018575.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018575.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2541.302, "latencies_ms": [2541.302], "images_per_second": 0.393, "prompt_tokens": 1113, "response_tokens_est": 58, "n_tiles": 1, "output_text": " 1. plate: 2\n2. fries: 1\n3. burger: 1\n4. tomato: 1\n5. lettuce: 1\n6. pickles: 2\n7. condiment: 1\n8. glass: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26848.8, "ram_available_mb": 98923.4, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 26848.9, "ram_available_mb": 98923.2, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.72, "peak": 40.16, "min": 18.53}, "VIN": {"avg": 67.19, "peak": 103.7, "min": 28.75}, "VIN_SYS_5V0": {"avg": 15.19, "peak": 16.26, "min": 13.39}, "VDD_CPU_SOC_MSS": {"avg": 16.28, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 27.72, "energy_joules_est": 70.46, "sample_count": 25, "duration_seconds": 2.542}, "timestamp": "2026-01-19T14:44:29.447378"}
{"image_index": 184, "image_name": "000000018575.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018575.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2165.705, "latencies_ms": [2165.705], "images_per_second": 0.462, "prompt_tokens": 1117, "response_tokens_est": 42, "n_tiles": 1, "output_text": " The plate of fries is located to the left of the plate with the sandwich, and the plate with the salad is in the background. The sandwich is in the foreground, and the salad is in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26848.9, "ram_available_mb": 98923.2, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26857.2, "ram_available_mb": 98915.0, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.52, "peak": 40.18, "min": 15.77}, "VIN": {"avg": 71.08, "peak": 115.98, "min": 30.33}, "VIN_SYS_5V0": {"avg": 14.94, "peak": 16.05, "min": 12.25}, "VDD_CPU_SOC_MSS": {"avg": 16.03, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 28.52, "energy_joules_est": 61.79, "sample_count": 21, "duration_seconds": 2.166}, "timestamp": "2026-01-19T14:44:31.633666"}
{"image_index": 184, "image_name": "000000018575.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018575.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1449.719, "latencies_ms": [1449.719], "images_per_second": 0.69, "prompt_tokens": 1111, "response_tokens_est": 15, "n_tiles": 1, "output_text": " A meal is set on a table with a drink and condiments.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26857.2, "ram_available_mb": 98915.0, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 26857.4, "ram_available_mb": 98914.8, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.21, "peak": 40.16, "min": 17.73}, "VIN": {"avg": 78.91, "peak": 126.71, "min": 30.78}, "VIN_SYS_5V0": {"avg": 15.25, "peak": 16.15, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 16.45, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 32.21, "energy_joules_est": 46.7, "sample_count": 14, "duration_seconds": 1.45}, "timestamp": "2026-01-19T14:44:33.100349"}
{"image_index": 184, "image_name": "000000018575.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018575.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2996.552, "latencies_ms": [2996.552], "images_per_second": 0.334, "prompt_tokens": 1109, "response_tokens_est": 72, "n_tiles": 1, "output_text": " The image features a white table with a variety of food items, including a plate of french fries, a plate of salad, and a plate of chicken. The food is arranged in a visually appealing manner, with the salad and chicken on separate plates. The lighting in the image is bright and even, and the colors of the food are vibrant and appetizing.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26857.4, "ram_available_mb": 98914.8, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 26857.4, "ram_available_mb": 98914.8, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.99, "peak": 40.57, "min": 18.52}, "VIN": {"avg": 67.34, "peak": 129.39, "min": 28.86}, "VIN_SYS_5V0": {"avg": 15.05, "peak": 16.15, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 16.1, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 26.99, "energy_joules_est": 80.88, "sample_count": 29, "duration_seconds": 2.997}, "timestamp": "2026-01-19T14:44:36.123351"}
{"image_index": 185, "image_name": "000000018737.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018737.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1493.206, "latencies_ms": [1493.206], "images_per_second": 0.67, "prompt_tokens": 1099, "response_tokens_est": 16, "n_tiles": 1, "output_text": " A red motorcycle is parked on a sandy beach with palm trees in the background.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 26857.4, "ram_available_mb": 98914.8, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 26857.4, "ram_available_mb": 98914.8, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.99, "peak": 40.56, "min": 15.77}, "VIN": {"avg": 73.72, "peak": 106.46, "min": 28.69}, "VIN_SYS_5V0": {"avg": 14.95, "peak": 16.05, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 16.09, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 30.99, "energy_joules_est": 46.3, "sample_count": 15, "duration_seconds": 1.494}, "timestamp": "2026-01-19T14:44:37.695908"}
{"image_index": 185, "image_name": "000000018737.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018737.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2790.725, "latencies_ms": [2790.725], "images_per_second": 0.358, "prompt_tokens": 1113, "response_tokens_est": 66, "n_tiles": 1, "output_text": " 1. Motorcycle: 1\n2. Tires: 2\n3. Seat: 1\n4. Handlebars: 1\n5. Windshield: 1\n6. Rear fender: 1\n7. Front fender: 1\n8. Chain: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26857.4, "ram_available_mb": 98914.8, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 26857.4, "ram_available_mb": 98914.8, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.24, "peak": 40.56, "min": 19.31}, "VIN": {"avg": 68.25, "peak": 124.88, "min": 28.92}, "VIN_SYS_5V0": {"avg": 14.91, "peak": 16.05, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.88, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 27.24, "energy_joules_est": 76.03, "sample_count": 27, "duration_seconds": 2.791}, "timestamp": "2026-01-19T14:44:40.511253"}
{"image_index": 185, "image_name": "000000018737.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018737.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2118.369, "latencies_ms": [2118.369], "images_per_second": 0.472, "prompt_tokens": 1117, "response_tokens_est": 39, "n_tiles": 1, "output_text": " The motorcycle is positioned on the left side of the image, with the beach and palm trees in the background. The motorcycle is in the foreground, with the beach and palm trees in the background.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26857.4, "ram_available_mb": 98914.8, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26857.8, "ram_available_mb": 98914.4, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.51, "peak": 40.16, "min": 16.55}, "VIN": {"avg": 68.5, "peak": 106.24, "min": 29.95}, "VIN_SYS_5V0": {"avg": 15.05, "peak": 16.15, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 16.09, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 28.51, "energy_joules_est": 60.41, "sample_count": 21, "duration_seconds": 2.119}, "timestamp": "2026-01-19T14:44:42.704074"}
{"image_index": 185, "image_name": "000000018737.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018737.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1511.479, "latencies_ms": [1511.479], "images_per_second": 0.662, "prompt_tokens": 1111, "response_tokens_est": 16, "n_tiles": 1, "output_text": " A red motorcycle is parked on a sandy beach with palm trees in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26857.8, "ram_available_mb": 98914.4, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 26857.8, "ram_available_mb": 98914.4, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.96, "peak": 40.16, "min": 16.16}, "VIN": {"avg": 78.13, "peak": 122.4, "min": 28.71}, "VIN_SYS_5V0": {"avg": 14.79, "peak": 16.05, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 15.83, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 30.96, "energy_joules_est": 46.8, "sample_count": 15, "duration_seconds": 1.512}, "timestamp": "2026-01-19T14:44:44.271553"}
{"image_index": 185, "image_name": "000000018737.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018737.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1426.319, "latencies_ms": [1426.319], "images_per_second": 0.701, "prompt_tokens": 1109, "response_tokens_est": 13, "n_tiles": 1, "output_text": " The motorcycle is red and black, and the sky is blue.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26857.8, "ram_available_mb": 98914.4, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 26857.8, "ram_available_mb": 98914.4, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.67, "peak": 40.56, "min": 20.1}, "VIN": {"avg": 77.16, "peak": 115.55, "min": 29.07}, "VIN_SYS_5V0": {"avg": 14.92, "peak": 16.05, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.97, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 32.67, "energy_joules_est": 46.61, "sample_count": 14, "duration_seconds": 1.427}, "timestamp": "2026-01-19T14:44:45.749246"}
{"image_index": 186, "image_name": "000000018770.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018770.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1600.484, "latencies_ms": [1600.484], "images_per_second": 0.625, "prompt_tokens": 1099, "response_tokens_est": 20, "n_tiles": 1, "output_text": " A man in a suit and tie is standing in front of a dark wall with a white switch.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26857.8, "ram_available_mb": 98914.4, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 26857.8, "ram_available_mb": 98914.4, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.2, "peak": 40.95, "min": 21.67}, "VIN": {"avg": 80.18, "peak": 129.03, "min": 28.75}, "VIN_SYS_5V0": {"avg": 14.94, "peak": 16.05, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 16.02, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 32.2, "energy_joules_est": 51.56, "sample_count": 16, "duration_seconds": 1.601}, "timestamp": "2026-01-19T14:44:47.423152"}
{"image_index": 186, "image_name": "000000018770.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018770.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2577.135, "latencies_ms": [2577.135], "images_per_second": 0.388, "prompt_tokens": 1113, "response_tokens_est": 57, "n_tiles": 1, "output_text": " 1. man: 1\n2. suit: 1\n3. tie: 1\n4. shirt: 1\n5. wall: 1\n6. light switch: 1\n7. door: 1\n8. floor: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26857.8, "ram_available_mb": 98914.4, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26857.8, "ram_available_mb": 98914.4, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.74, "peak": 40.57, "min": 19.31}, "VIN": {"avg": 73.69, "peak": 122.35, "min": 28.84}, "VIN_SYS_5V0": {"avg": 14.92, "peak": 16.05, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.91, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 27.74, "energy_joules_est": 71.5, "sample_count": 25, "duration_seconds": 2.578}, "timestamp": "2026-01-19T14:44:50.028447"}
{"image_index": 186, "image_name": "000000018770.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018770.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1898.088, "latencies_ms": [1898.088], "images_per_second": 0.527, "prompt_tokens": 1117, "response_tokens_est": 32, "n_tiles": 1, "output_text": " The man is positioned to the left of the wall, with the wall being the background. The man is in the foreground, with the wall being the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26857.8, "ram_available_mb": 98914.4, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26849.5, "ram_available_mb": 98922.6, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.44, "peak": 40.56, "min": 16.55}, "VIN": {"avg": 73.4, "peak": 106.46, "min": 29.02}, "VIN_SYS_5V0": {"avg": 15.06, "peak": 16.15, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 16.16, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 29.44, "energy_joules_est": 55.89, "sample_count": 19, "duration_seconds": 1.898}, "timestamp": "2026-01-19T14:44:52.004605"}
{"image_index": 186, "image_name": "000000018770.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018770.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1486.238, "latencies_ms": [1486.238], "images_per_second": 0.673, "prompt_tokens": 1111, "response_tokens_est": 15, "n_tiles": 1, "output_text": " A man in a suit and tie is standing in front of a wall.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26849.5, "ram_available_mb": 98922.6, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 26849.6, "ram_available_mb": 98922.6, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.09, "peak": 40.16, "min": 16.95}, "VIN": {"avg": 74.15, "peak": 125.13, "min": 28.3}, "VIN_SYS_5V0": {"avg": 14.76, "peak": 16.05, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 15.85, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 31.09, "energy_joules_est": 46.22, "sample_count": 15, "duration_seconds": 1.487}, "timestamp": "2026-01-19T14:44:53.572047"}
{"image_index": 186, "image_name": "000000018770.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018770.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1843.025, "latencies_ms": [1843.025], "images_per_second": 0.543, "prompt_tokens": 1109, "response_tokens_est": 29, "n_tiles": 1, "output_text": " The man is wearing a black suit with a white shirt and a black tie. The lighting is dim and the man's face is not visible.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26849.6, "ram_available_mb": 98922.6, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26849.6, "ram_available_mb": 98922.6, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.58, "peak": 40.57, "min": 19.71}, "VIN": {"avg": 68.77, "peak": 123.75, "min": 30.04}, "VIN_SYS_5V0": {"avg": 14.83, "peak": 16.05, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.92, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 30.58, "energy_joules_est": 56.37, "sample_count": 18, "duration_seconds": 1.843}, "timestamp": "2026-01-19T14:44:55.450500"}
{"image_index": 187, "image_name": "000000018833.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018833.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1689.546, "latencies_ms": [1689.546], "images_per_second": 0.592, "prompt_tokens": 1099, "response_tokens_est": 23, "n_tiles": 1, "output_text": " A cat is sleeping on a pair of shoes, with its head resting on the toe of one of the shoes.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26849.6, "ram_available_mb": 98922.6, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26849.6, "ram_available_mb": 98922.6, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.64, "peak": 40.57, "min": 18.14}, "VIN": {"avg": 69.16, "peak": 104.05, "min": 27.24}, "VIN_SYS_5V0": {"avg": 14.98, "peak": 16.05, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 16.14, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 30.64, "energy_joules_est": 51.79, "sample_count": 17, "duration_seconds": 1.69}, "timestamp": "2026-01-19T14:44:57.228368"}
{"image_index": 187, "image_name": "000000018833.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018833.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2753.777, "latencies_ms": [2753.777], "images_per_second": 0.363, "prompt_tokens": 1113, "response_tokens_est": 61, "n_tiles": 1, "output_text": " 1. Cat: 1\n2. Sneaker: 2\n3. Wall: 1\n4. Floor: 1\n5. Shoe: 1\n6. Laces: 1\n7. Nose: 1\n8. Paw: 1", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26849.6, "ram_available_mb": 98922.6, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 26849.8, "ram_available_mb": 98922.3, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.57, "peak": 40.16, "min": 17.35}, "VIN": {"avg": 65.52, "peak": 114.61, "min": 29.35}, "VIN_SYS_5V0": {"avg": 14.76, "peak": 16.05, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.75, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 26.57, "energy_joules_est": 73.18, "sample_count": 27, "duration_seconds": 2.754}, "timestamp": "2026-01-19T14:45:00.038930"}
{"image_index": 187, "image_name": "000000018833.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018833.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2043.765, "latencies_ms": [2043.765], "images_per_second": 0.489, "prompt_tokens": 1117, "response_tokens_est": 37, "n_tiles": 1, "output_text": " The cat is sleeping on the left side of the shoe, which is on the right side of the image. The cat is in the foreground, while the shoe is in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26849.8, "ram_available_mb": 98922.3, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 26849.1, "ram_available_mb": 98923.0, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.88, "peak": 40.18, "min": 15.38}, "VIN": {"avg": 72.93, "peak": 122.0, "min": 29.22}, "VIN_SYS_5V0": {"avg": 14.97, "peak": 16.05, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 16.04, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 28.88, "energy_joules_est": 59.04, "sample_count": 20, "duration_seconds": 2.044}, "timestamp": "2026-01-19T14:45:02.114318"}
{"image_index": 187, "image_name": "000000018833.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018833.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1338.731, "latencies_ms": [1338.731], "images_per_second": 0.747, "prompt_tokens": 1111, "response_tokens_est": 10, "n_tiles": 1, "output_text": " A cat is sleeping on a pair of shoes.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26849.1, "ram_available_mb": 98923.0, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 26849.1, "ram_available_mb": 98923.0, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.54, "peak": 40.56, "min": 17.73}, "VIN": {"avg": 72.33, "peak": 105.66, "min": 29.2}, "VIN_SYS_5V0": {"avg": 15.12, "peak": 16.15, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 16.29, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 32.54, "energy_joules_est": 43.58, "sample_count": 13, "duration_seconds": 1.339}, "timestamp": "2026-01-19T14:45:03.469485"}
{"image_index": 187, "image_name": "000000018833.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018833.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1586.473, "latencies_ms": [1586.473], "images_per_second": 0.63, "prompt_tokens": 1109, "response_tokens_est": 20, "n_tiles": 1, "output_text": " The cat is brown and white, the shoes are blue and white, and the wall is white.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26849.1, "ram_available_mb": 98923.0, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26849.1, "ram_available_mb": 98923.0, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 33.06, "peak": 41.34, "min": 22.46}, "VIN": {"avg": 74.93, "peak": 105.49, "min": 27.66}, "VIN_SYS_5V0": {"avg": 15.19, "peak": 16.15, "min": 13.29}, "VDD_CPU_SOC_MSS": {"avg": 16.41, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 33.06, "energy_joules_est": 52.46, "sample_count": 16, "duration_seconds": 1.587}, "timestamp": "2026-01-19T14:45:05.132175"}
{"image_index": 188, "image_name": "000000018837.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018837.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2000.924, "latencies_ms": [2000.924], "images_per_second": 0.5, "prompt_tokens": 1099, "response_tokens_est": 36, "n_tiles": 1, "output_text": " A green Isuzu truck with a red and white striped front is parked on the side of the road, with two workers in green uniforms standing on the back of the truck.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 26849.1, "ram_available_mb": 98923.0, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26849.1, "ram_available_mb": 98923.0, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.57, "peak": 40.57, "min": 18.53}, "VIN": {"avg": 71.42, "peak": 125.65, "min": 29.69}, "VIN_SYS_5V0": {"avg": 14.88, "peak": 16.05, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.92, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 29.57, "energy_joules_est": 59.19, "sample_count": 20, "duration_seconds": 2.002}, "timestamp": "2026-01-19T14:45:07.217935"}
{"image_index": 188, "image_name": "000000018837.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018837.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2756.402, "latencies_ms": [2756.402], "images_per_second": 0.363, "prompt_tokens": 1113, "response_tokens_est": 65, "n_tiles": 1, "output_text": " 1. truck: 1\n2. people: 2\n3. building: 1\n4. license plate: 1\n5. truck's front: 1\n6. truck's side: 1\n7. truck's back: 1\n8. truck's rear: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26849.1, "ram_available_mb": 98923.0, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 26849.1, "ram_available_mb": 98923.0, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.85, "peak": 40.56, "min": 16.56}, "VIN": {"avg": 67.49, "peak": 125.91, "min": 28.77}, "VIN_SYS_5V0": {"avg": 14.92, "peak": 16.05, "min": 12.15}, "VDD_CPU_SOC_MSS": {"avg": 15.85, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 26.85, "energy_joules_est": 74.02, "sample_count": 27, "duration_seconds": 2.757}, "timestamp": "2026-01-19T14:45:10.019591"}
{"image_index": 188, "image_name": "000000018837.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018837.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2312.279, "latencies_ms": [2312.279], "images_per_second": 0.432, "prompt_tokens": 1117, "response_tokens_est": 48, "n_tiles": 1, "output_text": " The green truck is in the foreground, parked on the street. The two workers are on top of the truck, one is closer to the front and the other is further back. The building in the background is farther away from the truck.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26849.1, "ram_available_mb": 98923.0, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 26850.8, "ram_available_mb": 98921.4, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.88, "peak": 40.16, "min": 16.16}, "VIN": {"avg": 70.81, "peak": 106.16, "min": 28.22}, "VIN_SYS_5V0": {"avg": 14.99, "peak": 16.15, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 16.11, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 27.88, "energy_joules_est": 64.48, "sample_count": 23, "duration_seconds": 2.313}, "timestamp": "2026-01-19T14:45:12.412070"}
{"image_index": 188, "image_name": "000000018837.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018837.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1422.855, "latencies_ms": [1422.855], "images_per_second": 0.703, "prompt_tokens": 1111, "response_tokens_est": 13, "n_tiles": 1, "output_text": " A green truck with a white logo is driving down the street.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26850.8, "ram_available_mb": 98921.4, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 26850.8, "ram_available_mb": 98921.4, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.49, "peak": 40.18, "min": 15.77}, "VIN": {"avg": 74.03, "peak": 122.77, "min": 27.35}, "VIN_SYS_5V0": {"avg": 14.81, "peak": 16.05, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 15.89, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 31.49, "energy_joules_est": 44.82, "sample_count": 14, "duration_seconds": 1.423}, "timestamp": "2026-01-19T14:45:13.871151"}
{"image_index": 188, "image_name": "000000018837.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018837.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1403.274, "latencies_ms": [1403.274], "images_per_second": 0.713, "prompt_tokens": 1109, "response_tokens_est": 13, "n_tiles": 1, "output_text": " The truck is green and white, and the sky is clear.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26850.8, "ram_available_mb": 98921.4, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 26850.8, "ram_available_mb": 98921.4, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 33.48, "peak": 41.36, "min": 22.07}, "VIN": {"avg": 72.39, "peak": 116.53, "min": 28.68}, "VIN_SYS_5V0": {"avg": 15.18, "peak": 16.15, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 16.23, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 33.48, "energy_joules_est": 47.0, "sample_count": 14, "duration_seconds": 1.404}, "timestamp": "2026-01-19T14:45:15.332989"}
{"image_index": 189, "image_name": "000000019042.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019042.jpg", "image_width": 640, "image_height": 371, "image_resolution": "640x371", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1581.487, "latencies_ms": [1581.487], "images_per_second": 0.632, "prompt_tokens": 1099, "response_tokens_est": 19, "n_tiles": 1, "output_text": " A person is standing on a rocky riverbank, with a bridge and greenery in the background.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 26850.8, "ram_available_mb": 98921.4, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 26850.8, "ram_available_mb": 98921.4, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.03, "peak": 40.97, "min": 21.28}, "VIN": {"avg": 69.91, "peak": 103.52, "min": 29.16}, "VIN_SYS_5V0": {"avg": 14.97, "peak": 16.15, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 16.02, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 32.03, "energy_joules_est": 50.67, "sample_count": 16, "duration_seconds": 1.582}, "timestamp": "2026-01-19T14:45:17.006633"}
{"image_index": 189, "image_name": "000000019042.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019042.jpg", "image_width": 640, "image_height": 371, "image_resolution": "640x371", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2567.033, "latencies_ms": [2567.033], "images_per_second": 0.39, "prompt_tokens": 1113, "response_tokens_est": 57, "n_tiles": 1, "output_text": " 1. person: 1\n2. rocks: 20\n3. water: 1\n4. bridge: 1\n5. trees: 1\n6. bushes: 1\n7. grass: 1\n8. sky: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26850.8, "ram_available_mb": 98921.4, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26850.8, "ram_available_mb": 98921.4, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.56, "peak": 40.16, "min": 18.53}, "VIN": {"avg": 67.71, "peak": 109.76, "min": 29.67}, "VIN_SYS_5V0": {"avg": 14.91, "peak": 16.05, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.86, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 27.56, "energy_joules_est": 70.76, "sample_count": 25, "duration_seconds": 2.568}, "timestamp": "2026-01-19T14:45:19.610425"}
{"image_index": 189, "image_name": "000000019042.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019042.jpg", "image_width": 640, "image_height": 371, "image_resolution": "640x371", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2110.558, "latencies_ms": [2110.558], "images_per_second": 0.474, "prompt_tokens": 1117, "response_tokens_est": 40, "n_tiles": 1, "output_text": " The person is standing on the left side of the river, with the bridge and greenery in the background. The rocks are scattered throughout the river, with the person standing near the middle of the river.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26850.8, "ram_available_mb": 98921.4, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26850.8, "ram_available_mb": 98921.4, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.57, "peak": 40.16, "min": 16.55}, "VIN": {"avg": 68.53, "peak": 95.5, "min": 28.58}, "VIN_SYS_5V0": {"avg": 15.07, "peak": 16.15, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 16.05, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 28.57, "energy_joules_est": 60.31, "sample_count": 21, "duration_seconds": 2.111}, "timestamp": "2026-01-19T14:45:21.795686"}
{"image_index": 189, "image_name": "000000019042.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019042.jpg", "image_width": 640, "image_height": 371, "image_resolution": "640x371", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1916.54, "latencies_ms": [1916.54], "images_per_second": 0.522, "prompt_tokens": 1111, "response_tokens_est": 32, "n_tiles": 1, "output_text": " A person is standing on a rocky riverbank, looking at the water. The river is flowing over the rocks, and there is a bridge in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26850.8, "ram_available_mb": 98921.4, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26850.8, "ram_available_mb": 98921.4, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.24, "peak": 40.18, "min": 16.16}, "VIN": {"avg": 71.01, "peak": 122.53, "min": 29.04}, "VIN_SYS_5V0": {"avg": 14.86, "peak": 16.05, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.91, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 29.24, "energy_joules_est": 56.05, "sample_count": 19, "duration_seconds": 1.917}, "timestamp": "2026-01-19T14:45:23.778529"}
{"image_index": 189, "image_name": "000000019042.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019042.jpg", "image_width": 640, "image_height": 371, "image_resolution": "640x371", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1841.274, "latencies_ms": [1841.274], "images_per_second": 0.543, "prompt_tokens": 1109, "response_tokens_est": 30, "n_tiles": 1, "output_text": " The image features a river with a person standing on the rocks, the water is clear and blue, and the sky is clear with a few clouds.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26850.8, "ram_available_mb": 98921.4, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26850.8, "ram_available_mb": 98921.4, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.16, "peak": 40.57, "min": 17.35}, "VIN": {"avg": 69.4, "peak": 99.07, "min": 28.95}, "VIN_SYS_5V0": {"avg": 14.99, "peak": 16.05, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 16.01, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 30.16, "energy_joules_est": 55.55, "sample_count": 18, "duration_seconds": 1.842}, "timestamp": "2026-01-19T14:45:25.652211"}
{"image_index": 190, "image_name": "000000019109.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019109.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1921.351, "latencies_ms": [1921.351], "images_per_second": 0.52, "prompt_tokens": 1099, "response_tokens_est": 33, "n_tiles": 1, "output_text": " A row of parked motorcycles in front of a building with a red awning and a green awning, with a green and gold building in the background.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 26850.8, "ram_available_mb": 98921.4, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26852.0, "ram_available_mb": 98920.2, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.89, "peak": 40.56, "min": 18.13}, "VIN": {"avg": 72.16, "peak": 108.29, "min": 29.15}, "VIN_SYS_5V0": {"avg": 15.09, "peak": 16.15, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 16.21, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 29.89, "energy_joules_est": 57.44, "sample_count": 19, "duration_seconds": 1.922}, "timestamp": "2026-01-19T14:45:27.634416"}
{"image_index": 190, "image_name": "000000019109.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019109.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2674.311, "latencies_ms": [2674.311], "images_per_second": 0.374, "prompt_tokens": 1113, "response_tokens_est": 62, "n_tiles": 1, "output_text": " 1. Motorcycles: 12\n2. People: 4\n3. Street: 1\n4. Building: 1\n5. Sign: 1\n6. Awning: 1\n7. Streetlight: 1\n8. Poster: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26852.0, "ram_available_mb": 98920.2, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 26851.8, "ram_available_mb": 98920.4, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.21, "peak": 40.56, "min": 17.35}, "VIN": {"avg": 69.64, "peak": 118.32, "min": 29.43}, "VIN_SYS_5V0": {"avg": 15.03, "peak": 16.15, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 16.16, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 27.21, "energy_joules_est": 72.78, "sample_count": 26, "duration_seconds": 2.675}, "timestamp": "2026-01-19T14:45:30.334430"}
{"image_index": 190, "image_name": "000000019109.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019109.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1903.175, "latencies_ms": [1903.175], "images_per_second": 0.525, "prompt_tokens": 1117, "response_tokens_est": 32, "n_tiles": 1, "output_text": " The motorcycles are parked on the right side of the street, while the buildings are on the left side. The motorcycles are closer to the viewer than the buildings.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26851.8, "ram_available_mb": 98920.4, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26851.8, "ram_available_mb": 98920.4, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.46, "peak": 40.16, "min": 16.56}, "VIN": {"avg": 69.53, "peak": 101.45, "min": 30.14}, "VIN_SYS_5V0": {"avg": 15.1, "peak": 16.15, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 16.12, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 29.46, "energy_joules_est": 56.08, "sample_count": 19, "duration_seconds": 1.904}, "timestamp": "2026-01-19T14:45:32.311081"}
{"image_index": 190, "image_name": "000000019109.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019109.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3293.307, "latencies_ms": [3293.307], "images_per_second": 0.304, "prompt_tokens": 1111, "response_tokens_est": 85, "n_tiles": 1, "output_text": " The image captures a bustling street scene in Paris, France, where a row of parked scooters lines the sidewalk in front of a row of shops. The scooters, predominantly black and silver, are parked neatly in front of a variety of shops, including a barber shop and a bakery. The buildings are adorned with ornate details, and the street is lined with trees, adding a touch of greenery to the urban setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26851.8, "ram_available_mb": 98920.4, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 26851.8, "ram_available_mb": 98920.4, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 25.6, "peak": 40.57, "min": 16.95}, "VIN": {"avg": 69.7, "peak": 118.36, "min": 30.12}, "VIN_SYS_5V0": {"avg": 14.93, "peak": 16.15, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.87, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 25.6, "energy_joules_est": 84.32, "sample_count": 32, "duration_seconds": 3.294}, "timestamp": "2026-01-19T14:45:35.640655"}
{"image_index": 190, "image_name": "000000019109.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019109.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2706.16, "latencies_ms": [2706.16], "images_per_second": 0.37, "prompt_tokens": 1109, "response_tokens_est": 63, "n_tiles": 1, "output_text": " The image captures a vibrant scene on a sunny day, with the motorbikes parked in front of the building bathed in the warm glow of the sunlight. The motorbikes, with their sleek designs and metallic bodies, stand out against the backdrop of the building's red awnings and green shutters.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26851.8, "ram_available_mb": 98920.4, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 26851.8, "ram_available_mb": 98920.4, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.63, "peak": 40.16, "min": 16.16}, "VIN": {"avg": 70.91, "peak": 115.22, "min": 29.01}, "VIN_SYS_5V0": {"avg": 14.97, "peak": 16.15, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 16.07, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 26.63, "energy_joules_est": 72.07, "sample_count": 27, "duration_seconds": 2.706}, "timestamp": "2026-01-19T14:45:38.448906"}
{"image_index": 191, "image_name": "000000019221.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019221.jpg", "image_width": 640, "image_height": 478, "image_resolution": "640x478", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1525.133, "latencies_ms": [1525.133], "images_per_second": 0.656, "prompt_tokens": 1099, "response_tokens_est": 16, "n_tiles": 1, "output_text": " A person is holding a piece of broccoli with a small brown seed on it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26851.8, "ram_available_mb": 98920.4, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 26851.8, "ram_available_mb": 98920.4, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.46, "peak": 40.16, "min": 14.98}, "VIN": {"avg": 73.2, "peak": 120.38, "min": 28.06}, "VIN_SYS_5V0": {"avg": 14.65, "peak": 15.95, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.75, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 30.46, "energy_joules_est": 46.48, "sample_count": 15, "duration_seconds": 1.526}, "timestamp": "2026-01-19T14:45:40.017657"}
{"image_index": 191, "image_name": "000000019221.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019221.jpg", "image_width": 640, "image_height": 478, "image_resolution": "640x478", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2171.827, "latencies_ms": [2171.827], "images_per_second": 0.46, "prompt_tokens": 1113, "response_tokens_est": 40, "n_tiles": 1, "output_text": " broccoli: 1, hand: 1, broccoli bud: 1, broccoli stem: 1, broccoli leaves: 1, broccoli flower: 1, broccoli flower bud: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26851.8, "ram_available_mb": 98920.4, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26851.8, "ram_available_mb": 98920.4, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.36, "peak": 40.57, "min": 20.1}, "VIN": {"avg": 72.09, "peak": 125.93, "min": 29.59}, "VIN_SYS_5V0": {"avg": 14.98, "peak": 16.05, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 15.95, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 29.36, "energy_joules_est": 63.77, "sample_count": 21, "duration_seconds": 2.172}, "timestamp": "2026-01-19T14:45:42.201473"}
{"image_index": 191, "image_name": "000000019221.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019221.jpg", "image_width": 640, "image_height": 478, "image_resolution": "640x478", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1621.867, "latencies_ms": [1621.867], "images_per_second": 0.617, "prompt_tokens": 1117, "response_tokens_est": 21, "n_tiles": 1, "output_text": " The broccoli is in the foreground, held by a hand, and the background is a tiled wall.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26851.8, "ram_available_mb": 98920.4, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 26852.3, "ram_available_mb": 98919.9, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.1, "peak": 40.56, "min": 17.35}, "VIN": {"avg": 70.72, "peak": 98.73, "min": 29.94}, "VIN_SYS_5V0": {"avg": 15.12, "peak": 16.15, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 16.21, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 31.1, "energy_joules_est": 50.45, "sample_count": 16, "duration_seconds": 1.622}, "timestamp": "2026-01-19T14:45:43.869835"}
{"image_index": 191, "image_name": "000000019221.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019221.jpg", "image_width": 640, "image_height": 478, "image_resolution": "640x478", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1500.538, "latencies_ms": [1500.538], "images_per_second": 0.666, "prompt_tokens": 1111, "response_tokens_est": 16, "n_tiles": 1, "output_text": " A person is holding a piece of broccoli with a small brown seed on it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26852.3, "ram_available_mb": 98919.9, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 26852.3, "ram_available_mb": 98919.9, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.17, "peak": 40.56, "min": 19.31}, "VIN": {"avg": 71.6, "peak": 124.22, "min": 29.3}, "VIN_SYS_5V0": {"avg": 15.02, "peak": 16.15, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 16.11, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 32.17, "energy_joules_est": 48.29, "sample_count": 15, "duration_seconds": 1.501}, "timestamp": "2026-01-19T14:45:45.430979"}
{"image_index": 191, "image_name": "000000019221.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019221.jpg", "image_width": 640, "image_height": 478, "image_resolution": "640x478", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1407.514, "latencies_ms": [1407.514], "images_per_second": 0.71, "prompt_tokens": 1109, "response_tokens_est": 12, "n_tiles": 1, "output_text": " The broccoli is green and the person's hand is pink.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26852.3, "ram_available_mb": 98919.9, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 26852.3, "ram_available_mb": 98919.9, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.7, "peak": 40.57, "min": 20.49}, "VIN": {"avg": 80.69, "peak": 123.09, "min": 28.97}, "VIN_SYS_5V0": {"avg": 14.91, "peak": 16.15, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.97, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 32.7, "energy_joules_est": 46.04, "sample_count": 14, "duration_seconds": 1.408}, "timestamp": "2026-01-19T14:45:46.894306"}
{"image_index": 192, "image_name": "000000019402.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019402.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1911.583, "latencies_ms": [1911.583], "images_per_second": 0.523, "prompt_tokens": 1099, "response_tokens_est": 33, "n_tiles": 1, "output_text": " Two people are standing close to each other, one of them is wearing a black jacket with a fur hood, and they are both looking up with their mouths open.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 26852.3, "ram_available_mb": 98919.9, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26852.3, "ram_available_mb": 98919.9, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.75, "peak": 40.95, "min": 20.1}, "VIN": {"avg": 68.75, "peak": 115.78, "min": 29.3}, "VIN_SYS_5V0": {"avg": 15.03, "peak": 16.15, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 16.12, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 30.75, "energy_joules_est": 58.8, "sample_count": 19, "duration_seconds": 1.912}, "timestamp": "2026-01-19T14:45:48.879940"}
{"image_index": 192, "image_name": "000000019402.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019402.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2518.137, "latencies_ms": [2518.137], "images_per_second": 0.397, "prompt_tokens": 1113, "response_tokens_est": 56, "n_tiles": 1, "output_text": " 1. person: 2\n2. jacket: 2\n3. hood: 1\n4. hat: 1\n5. person: 1\n6. person: 1\n7. person: 1\n8. person: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26852.3, "ram_available_mb": 98919.9, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 26852.7, "ram_available_mb": 98919.4, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.42, "peak": 40.57, "min": 16.95}, "VIN": {"avg": 68.72, "peak": 95.21, "min": 28.61}, "VIN_SYS_5V0": {"avg": 14.92, "peak": 16.05, "min": 12.25}, "VDD_CPU_SOC_MSS": {"avg": 15.97, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 27.42, "energy_joules_est": 69.06, "sample_count": 25, "duration_seconds": 2.519}, "timestamp": "2026-01-19T14:45:51.484682"}
{"image_index": 192, "image_name": "000000019402.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019402.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2499.259, "latencies_ms": [2499.259], "images_per_second": 0.4, "prompt_tokens": 1117, "response_tokens_est": 55, "n_tiles": 1, "output_text": " The person on the left is closer to the camera than the person on the right. The person on the right is in the background, while the person on the left is in the foreground. The person on the left is closer to the camera than the person on the right.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26852.7, "ram_available_mb": 98919.4, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26852.4, "ram_available_mb": 98919.8, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.11, "peak": 40.18, "min": 15.38}, "VIN": {"avg": 70.45, "peak": 120.66, "min": 28.22}, "VIN_SYS_5V0": {"avg": 14.79, "peak": 16.05, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.81, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 27.11, "energy_joules_est": 67.77, "sample_count": 25, "duration_seconds": 2.5}, "timestamp": "2026-01-19T14:45:54.084548"}
{"image_index": 192, "image_name": "000000019402.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019402.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1732.305, "latencies_ms": [1732.305], "images_per_second": 0.577, "prompt_tokens": 1111, "response_tokens_est": 25, "n_tiles": 1, "output_text": " Two people are standing close together in a dimly lit room, one of them wearing a black jacket with a fur hood.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26852.4, "ram_available_mb": 98919.8, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26852.3, "ram_available_mb": 98919.9, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.99, "peak": 40.18, "min": 14.98}, "VIN": {"avg": 72.07, "peak": 122.31, "min": 29.8}, "VIN_SYS_5V0": {"avg": 14.77, "peak": 16.05, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 15.89, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 29.99, "energy_joules_est": 51.96, "sample_count": 17, "duration_seconds": 1.733}, "timestamp": "2026-01-19T14:45:55.854152"}
{"image_index": 192, "image_name": "000000019402.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019402.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1664.74, "latencies_ms": [1664.74], "images_per_second": 0.601, "prompt_tokens": 1109, "response_tokens_est": 23, "n_tiles": 1, "output_text": " The image is taken in a dimly lit environment with a yellowish hue, and the subjects are wearing winter clothing.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26852.3, "ram_available_mb": 98919.9, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26852.3, "ram_available_mb": 98919.9, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.05, "peak": 40.56, "min": 19.32}, "VIN": {"avg": 79.25, "peak": 119.93, "min": 27.41}, "VIN_SYS_5V0": {"avg": 15.1, "peak": 16.15, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 16.17, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 31.05, "energy_joules_est": 51.7, "sample_count": 17, "duration_seconds": 1.665}, "timestamp": "2026-01-19T14:45:57.615798"}
{"image_index": 193, "image_name": "000000019432.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019432.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1498.144, "latencies_ms": [1498.144], "images_per_second": 0.667, "prompt_tokens": 1099, "response_tokens_est": 16, "n_tiles": 1, "output_text": " A man with long hair is playing tennis on a blue court with white lines.", "error": null, "sys_before": {"cpu_percent": 6.7, "ram_used_mb": 26852.3, "ram_available_mb": 98919.9, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 26852.3, "ram_available_mb": 98919.9, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.52, "peak": 40.56, "min": 17.35}, "VIN": {"avg": 76.04, "peak": 123.42, "min": 28.51}, "VIN_SYS_5V0": {"avg": 14.75, "peak": 15.95, "min": 11.87}, "VDD_CPU_SOC_MSS": {"avg": 15.8, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 31.52, "energy_joules_est": 47.24, "sample_count": 15, "duration_seconds": 1.499}, "timestamp": "2026-01-19T14:45:59.188572"}
{"image_index": 193, "image_name": "000000019432.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019432.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2775.662, "latencies_ms": [2775.662], "images_per_second": 0.36, "prompt_tokens": 1113, "response_tokens_est": 63, "n_tiles": 1, "output_text": " 1. man: 1\n2. tennis racket: 1\n3. tennis ball: 1\n4. chair: 1\n5. tennis court: 1\n6. white chair: 1\n7. white wall: 1\n8. green fence: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26852.3, "ram_available_mb": 98919.9, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 26852.3, "ram_available_mb": 98919.9, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.24, "peak": 40.56, "min": 18.91}, "VIN": {"avg": 64.97, "peak": 105.49, "min": 30.59}, "VIN_SYS_5V0": {"avg": 14.95, "peak": 16.05, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.85, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 27.24, "energy_joules_est": 75.62, "sample_count": 27, "duration_seconds": 2.776}, "timestamp": "2026-01-19T14:46:01.981252"}
{"image_index": 193, "image_name": "000000019432.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019432.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2203.041, "latencies_ms": [2203.041], "images_per_second": 0.454, "prompt_tokens": 1117, "response_tokens_est": 44, "n_tiles": 1, "output_text": " The tennis player is positioned on the left side of the image, with the tennis ball in the center and the chairs on the right side. The player is closer to the camera than the chairs, which are further away.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26852.3, "ram_available_mb": 98919.9, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26852.3, "ram_available_mb": 98919.9, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.33, "peak": 40.18, "min": 16.95}, "VIN": {"avg": 67.27, "peak": 123.25, "min": 28.02}, "VIN_SYS_5V0": {"avg": 15.1, "peak": 16.15, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 16.2, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 28.33, "energy_joules_est": 62.42, "sample_count": 22, "duration_seconds": 2.203}, "timestamp": "2026-01-19T14:46:04.268332"}
{"image_index": 193, "image_name": "000000019432.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019432.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1418.487, "latencies_ms": [1418.487], "images_per_second": 0.705, "prompt_tokens": 1111, "response_tokens_est": 13, "n_tiles": 1, "output_text": " A man with long hair is playing tennis on a blue court.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26852.3, "ram_available_mb": 98919.9, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 26852.3, "ram_available_mb": 98919.9, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.4, "peak": 40.16, "min": 15.77}, "VIN": {"avg": 66.86, "peak": 83.56, "min": 28.66}, "VIN_SYS_5V0": {"avg": 14.82, "peak": 16.05, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.94, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 31.4, "energy_joules_est": 44.55, "sample_count": 14, "duration_seconds": 1.419}, "timestamp": "2026-01-19T14:46:05.725400"}
{"image_index": 193, "image_name": "000000019432.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019432.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1539.55, "latencies_ms": [1539.55], "images_per_second": 0.65, "prompt_tokens": 1109, "response_tokens_est": 18, "n_tiles": 1, "output_text": " The tennis court is blue, the man is wearing white, and the sun is shining.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26852.3, "ram_available_mb": 98919.9, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 26852.3, "ram_available_mb": 98919.9, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.96, "peak": 40.95, "min": 22.07}, "VIN": {"avg": 74.59, "peak": 116.89, "min": 29.49}, "VIN_SYS_5V0": {"avg": 15.18, "peak": 16.15, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 16.22, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 32.96, "energy_joules_est": 50.75, "sample_count": 15, "duration_seconds": 1.54}, "timestamp": "2026-01-19T14:46:07.286042"}
{"image_index": 194, "image_name": "000000019742.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019742.jpg", "image_width": 500, "image_height": 374, "image_resolution": "500x374", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1491.781, "latencies_ms": [1491.781], "images_per_second": 0.67, "prompt_tokens": 1099, "response_tokens_est": 16, "n_tiles": 1, "output_text": " A red glass vase with a white candle and string lights on a wooden shelf.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26852.3, "ram_available_mb": 98919.9, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 26852.3, "ram_available_mb": 98919.9, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.49, "peak": 40.95, "min": 20.89}, "VIN": {"avg": 77.9, "peak": 118.98, "min": 28.64}, "VIN_SYS_5V0": {"avg": 15.13, "peak": 16.15, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 16.25, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 32.49, "energy_joules_est": 48.48, "sample_count": 15, "duration_seconds": 1.492}, "timestamp": "2026-01-19T14:46:08.863047"}
{"image_index": 194, "image_name": "000000019742.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019742.jpg", "image_width": 500, "image_height": 374, "image_resolution": "500x374", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2600.525, "latencies_ms": [2600.525], "images_per_second": 0.385, "prompt_tokens": 1113, "response_tokens_est": 58, "n_tiles": 1, "output_text": " 1. candle: 1\n2. glass: 1\n3. vase: 1\n4. wooden frame: 1\n5. string lights: 1\n6. mirror: 1\n7. table: 1\n8. wall: 1", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26852.3, "ram_available_mb": 98919.9, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 26852.2, "ram_available_mb": 98919.9, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.44, "peak": 40.57, "min": 17.34}, "VIN": {"avg": 70.4, "peak": 124.94, "min": 30.37}, "VIN_SYS_5V0": {"avg": 14.89, "peak": 16.05, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.93, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 27.44, "energy_joules_est": 71.38, "sample_count": 26, "duration_seconds": 2.601}, "timestamp": "2026-01-19T14:46:11.569583"}
{"image_index": 194, "image_name": "000000019742.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019742.jpg", "image_width": 500, "image_height": 374, "image_resolution": "500x374", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2093.476, "latencies_ms": [2093.476], "images_per_second": 0.478, "prompt_tokens": 1117, "response_tokens_est": 38, "n_tiles": 1, "output_text": " The candle is positioned to the left of the vase, which is placed in the center of the image. The vase is situated in the foreground, while the candle is located in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26852.2, "ram_available_mb": 98919.9, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26844.5, "ram_available_mb": 98927.7, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.12, "peak": 40.18, "min": 14.98}, "VIN": {"avg": 72.57, "peak": 125.51, "min": 28.98}, "VIN_SYS_5V0": {"avg": 14.71, "peak": 15.95, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.84, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 28.12, "energy_joules_est": 58.88, "sample_count": 21, "duration_seconds": 2.094}, "timestamp": "2026-01-19T14:46:13.758128"}
{"image_index": 194, "image_name": "000000019742.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019742.jpg", "image_width": 500, "image_height": 374, "image_resolution": "500x374", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1521.62, "latencies_ms": [1521.62], "images_per_second": 0.657, "prompt_tokens": 1111, "response_tokens_est": 16, "n_tiles": 1, "output_text": " A red glass vase with a white candle and string lights on a wooden shelf.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26844.5, "ram_available_mb": 98927.7, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 26844.2, "ram_available_mb": 98928.0, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.76, "peak": 40.18, "min": 15.77}, "VIN": {"avg": 69.67, "peak": 97.16, "min": 28.8}, "VIN_SYS_5V0": {"avg": 14.71, "peak": 16.05, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.85, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 30.76, "energy_joules_est": 46.82, "sample_count": 15, "duration_seconds": 1.522}, "timestamp": "2026-01-19T14:46:15.328301"}
{"image_index": 194, "image_name": "000000019742.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019742.jpg", "image_width": 500, "image_height": 374, "image_resolution": "500x374", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1348.65, "latencies_ms": [1348.65], "images_per_second": 0.741, "prompt_tokens": 1109, "response_tokens_est": 10, "n_tiles": 1, "output_text": " The vase is orange and the candle is white.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26844.2, "ram_available_mb": 98928.0, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.5, "ram_used_mb": 26844.2, "ram_available_mb": 98928.0, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 33.43, "peak": 40.97, "min": 20.5}, "VIN": {"avg": 78.34, "peak": 127.84, "min": 29.72}, "VIN_SYS_5V0": {"avg": 15.02, "peak": 16.05, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 16.17, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 33.43, "energy_joules_est": 45.1, "sample_count": 13, "duration_seconds": 1.349}, "timestamp": "2026-01-19T14:46:16.689593"}
{"image_index": 195, "image_name": "000000019786.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019786.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1777.319, "latencies_ms": [1777.319], "images_per_second": 0.563, "prompt_tokens": 1099, "response_tokens_est": 28, "n_tiles": 1, "output_text": " A man in a gray shirt and jeans is bending over in front of a camera, while another man in a green shirt stands behind him.", "error": null, "sys_before": {"cpu_percent": 16.7, "ram_used_mb": 26844.2, "ram_available_mb": 98928.0, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26844.2, "ram_available_mb": 98928.0, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.89, "peak": 40.95, "min": 19.71}, "VIN": {"avg": 74.56, "peak": 123.95, "min": 29.09}, "VIN_SYS_5V0": {"avg": 15.21, "peak": 16.26, "min": 13.29}, "VDD_CPU_SOC_MSS": {"avg": 16.27, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 31.89, "energy_joules_est": 56.69, "sample_count": 18, "duration_seconds": 1.778}, "timestamp": "2026-01-19T14:46:18.571914"}
{"image_index": 195, "image_name": "000000019786.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019786.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2621.736, "latencies_ms": [2621.736], "images_per_second": 0.381, "prompt_tokens": 1113, "response_tokens_est": 57, "n_tiles": 1, "output_text": " 1. man: 2\n2. suitcase: 1\n3. laptop: 1\n4. couch: 1\n5. jacket: 1\n6. man: 1\n7. man: 1\n8. man: 1", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26844.2, "ram_available_mb": 98928.0, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26844.2, "ram_available_mb": 98928.0, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.87, "peak": 40.57, "min": 16.56}, "VIN": {"avg": 67.34, "peak": 125.41, "min": 27.6}, "VIN_SYS_5V0": {"avg": 14.77, "peak": 16.05, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.7, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 26.87, "energy_joules_est": 70.45, "sample_count": 26, "duration_seconds": 2.622}, "timestamp": "2026-01-19T14:46:21.278880"}
{"image_index": 195, "image_name": "000000019786.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019786.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2806.563, "latencies_ms": [2806.563], "images_per_second": 0.356, "prompt_tokens": 1117, "response_tokens_est": 67, "n_tiles": 1, "output_text": " The man in the gray shirt is standing in the foreground, bending over to interact with the man in the green shirt, who is standing in the background. The man in the gray shirt is positioned to the left of the man in the green shirt, and the couch is located in the background, behind the man in the gray shirt.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26844.2, "ram_available_mb": 98928.0, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 26842.8, "ram_available_mb": 98929.4, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.51, "peak": 40.18, "min": 14.98}, "VIN": {"avg": 71.9, "peak": 126.19, "min": 33.41}, "VIN_SYS_5V0": {"avg": 14.94, "peak": 16.05, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.82, "peak": 17.32, "min": 12.61}}, "power_watts_avg": 26.51, "energy_joules_est": 74.42, "sample_count": 27, "duration_seconds": 2.807}, "timestamp": "2026-01-19T14:46:24.090369"}
{"image_index": 195, "image_name": "000000019786.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019786.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1739.586, "latencies_ms": [1739.586], "images_per_second": 0.575, "prompt_tokens": 1111, "response_tokens_est": 27, "n_tiles": 1, "output_text": " Two men are in a room with a couch and a laptop. One man is holding a camera and the other is holding a microphone.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26842.8, "ram_available_mb": 98929.4, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26842.4, "ram_available_mb": 98929.7, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.77, "peak": 39.77, "min": 18.13}, "VIN": {"avg": 72.45, "peak": 102.38, "min": 29.9}, "VIN_SYS_5V0": {"avg": 15.37, "peak": 16.26, "min": 13.49}, "VDD_CPU_SOC_MSS": {"avg": 16.44, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 30.77, "energy_joules_est": 53.53, "sample_count": 17, "duration_seconds": 1.74}, "timestamp": "2026-01-19T14:46:25.864244"}
{"image_index": 195, "image_name": "000000019786.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019786.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2120.939, "latencies_ms": [2120.939], "images_per_second": 0.471, "prompt_tokens": 1109, "response_tokens_est": 41, "n_tiles": 1, "output_text": " The image features a man wearing a gray shirt and jeans, standing in a room with a brown couch and a silver suitcase nearby. The lighting in the room is bright, and the colors are warm.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26842.4, "ram_available_mb": 98929.7, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26842.4, "ram_available_mb": 98929.7, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.19, "peak": 40.56, "min": 18.91}, "VIN": {"avg": 71.12, "peak": 122.89, "min": 29.49}, "VIN_SYS_5V0": {"avg": 15.08, "peak": 16.15, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 16.24, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 29.19, "energy_joules_est": 61.92, "sample_count": 21, "duration_seconds": 2.121}, "timestamp": "2026-01-19T14:46:28.049873"}
{"image_index": 196, "image_name": "000000019924.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019924.jpg", "image_width": 458, "image_height": 500, "image_resolution": "458x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1919.378, "latencies_ms": [1919.378], "images_per_second": 0.521, "prompt_tokens": 1432, "response_tokens_est": 17, "n_tiles": 1, "output_text": " A woman wearing a hat and a striped shirt is holding a cigarette in her hand.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26842.4, "ram_available_mb": 98929.7, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.5, "ram_used_mb": 26842.2, "ram_available_mb": 98930.0, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.2, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.87, "peak": 40.18, "min": 16.56}, "VIN": {"avg": 77.31, "peak": 129.13, "min": 29.51}, "VIN_SYS_5V0": {"avg": 15.31, "peak": 16.76, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 16.47, "peak": 17.71, "min": 13.39}}, "power_watts_avg": 31.87, "energy_joules_est": 61.19, "sample_count": 19, "duration_seconds": 1.92}, "timestamp": "2026-01-19T14:46:30.037570"}
{"image_index": 196, "image_name": "000000019924.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019924.jpg", "image_width": 458, "image_height": 500, "image_resolution": "458x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2966.496, "latencies_ms": [2966.496], "images_per_second": 0.337, "prompt_tokens": 1446, "response_tokens_est": 57, "n_tiles": 1, "output_text": " 1. hat: 1\n2. woman: 1\n3. cigarette: 1\n4. necklace: 1\n5. bracelet: 1\n6. necklace: 1\n7. necklace: 1\n8. necklace: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26842.2, "ram_available_mb": 98930.0, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26842.2, "ram_available_mb": 98930.0, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.05, "peak": 40.56, "min": 19.31}, "VIN": {"avg": 72.31, "peak": 123.98, "min": 28.97}, "VIN_SYS_5V0": {"avg": 15.18, "peak": 16.66, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 16.26, "peak": 17.71, "min": 13.79}}, "power_watts_avg": 29.05, "energy_joules_est": 86.18, "sample_count": 29, "duration_seconds": 2.967}, "timestamp": "2026-01-19T14:46:33.057558"}
{"image_index": 196, "image_name": "000000019924.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019924.jpg", "image_width": 458, "image_height": 500, "image_resolution": "458x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2360.502, "latencies_ms": [2360.502], "images_per_second": 0.424, "prompt_tokens": 1450, "response_tokens_est": 34, "n_tiles": 1, "output_text": " The woman is in the foreground, wearing a hat and a striped shirt. The cigarette is in her hand, and she is smiling. The background is plain and white.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26842.2, "ram_available_mb": 98930.0, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26842.2, "ram_available_mb": 98930.0, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.49, "peak": 40.16, "min": 16.56}, "VIN": {"avg": 74.94, "peak": 121.8, "min": 29.92}, "VIN_SYS_5V0": {"avg": 15.22, "peak": 16.66, "min": 12.36}, "VDD_CPU_SOC_MSS": {"avg": 16.31, "peak": 17.71, "min": 13.39}}, "power_watts_avg": 30.49, "energy_joules_est": 71.99, "sample_count": 23, "duration_seconds": 2.361}, "timestamp": "2026-01-19T14:46:35.449513"}
{"image_index": 196, "image_name": "000000019924.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019924.jpg", "image_width": 458, "image_height": 500, "image_resolution": "458x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1883.12, "latencies_ms": [1883.12], "images_per_second": 0.531, "prompt_tokens": 1444, "response_tokens_est": 16, "n_tiles": 1, "output_text": " A woman wearing a hat and a striped shirt is holding a cigarette and smiling.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26842.2, "ram_available_mb": 98930.0, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.4, "ram_used_mb": 26842.2, "ram_available_mb": 98930.0, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.55, "peak": 40.56, "min": 18.14}, "VIN": {"avg": 76.77, "peak": 119.79, "min": 27.41}, "VIN_SYS_5V0": {"avg": 15.32, "peak": 16.66, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 16.47, "peak": 17.71, "min": 13.79}}, "power_watts_avg": 32.55, "energy_joules_est": 61.31, "sample_count": 19, "duration_seconds": 1.883}, "timestamp": "2026-01-19T14:46:37.429012"}
{"image_index": 196, "image_name": "000000019924.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019924.jpg", "image_width": 458, "image_height": 500, "image_resolution": "458x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2575.764, "latencies_ms": [2575.764], "images_per_second": 0.388, "prompt_tokens": 1442, "response_tokens_est": 42, "n_tiles": 1, "output_text": " The image is a black and white photograph with a high contrast, and the lighting is soft and diffused. The subject is wearing a black hat and a striped shirt, and the background is plain and white.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26842.2, "ram_available_mb": 98930.0, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26842.2, "ram_available_mb": 98930.0, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.17, "peak": 40.18, "min": 18.92}, "VIN": {"avg": 72.84, "peak": 117.67, "min": 28.62}, "VIN_SYS_5V0": {"avg": 15.14, "peak": 16.56, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 16.22, "peak": 17.71, "min": 13.0}}, "power_watts_avg": 30.17, "energy_joules_est": 77.72, "sample_count": 25, "duration_seconds": 2.576}, "timestamp": "2026-01-19T14:46:40.029488"}
{"image_index": 197, "image_name": "000000020059.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020059.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1509.355, "latencies_ms": [1509.355], "images_per_second": 0.663, "prompt_tokens": 1099, "response_tokens_est": 17, "n_tiles": 1, "output_text": " Two zebras are grazing in a grassy enclosure with trees and rocks in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26842.2, "ram_available_mb": 98930.0, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26842.2, "ram_available_mb": 98930.0, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.49, "peak": 40.57, "min": 17.35}, "VIN": {"avg": 70.05, "peak": 104.48, "min": 29.22}, "VIN_SYS_5V0": {"avg": 15.08, "peak": 16.15, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 16.22, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 31.49, "energy_joules_est": 47.55, "sample_count": 15, "duration_seconds": 1.51}, "timestamp": "2026-01-19T14:46:41.604315"}
{"image_index": 197, "image_name": "000000020059.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020059.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1188.352, "latencies_ms": [1188.352], "images_per_second": 0.842, "prompt_tokens": 1113, "response_tokens_est": 4, "n_tiles": 1, "output_text": " zebra: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26842.2, "ram_available_mb": 98930.0, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.1, "ram_used_mb": 26842.4, "ram_available_mb": 98929.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 33.48, "peak": 40.56, "min": 20.1}, "VIN": {"avg": 80.13, "peak": 122.02, "min": 28.68}, "VIN_SYS_5V0": {"avg": 14.93, "peak": 16.15, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 16.04, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 33.48, "energy_joules_est": 39.8, "sample_count": 12, "duration_seconds": 1.189}, "timestamp": "2026-01-19T14:46:42.856990"}
{"image_index": 197, "image_name": "000000020059.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020059.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3118.79, "latencies_ms": [3118.79], "images_per_second": 0.321, "prompt_tokens": 1117, "response_tokens_est": 75, "n_tiles": 1, "output_text": " The zebra on the left is positioned slightly behind the one on the right, creating a sense of depth in the image. The foreground zebra is closer to the camera, while the background zebra is farther away, giving a sense of distance. The zebra on the right is positioned near the center of the image, while the one on the left is positioned more towards the left side.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26842.4, "ram_available_mb": 98929.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 26842.4, "ram_available_mb": 98929.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.92, "peak": 41.34, "min": 16.95}, "VIN": {"avg": 66.66, "peak": 118.49, "min": 29.92}, "VIN_SYS_5V0": {"avg": 14.86, "peak": 16.15, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.79, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 26.92, "energy_joules_est": 83.97, "sample_count": 31, "duration_seconds": 3.119}, "timestamp": "2026-01-19T14:46:46.076567"}
{"image_index": 197, "image_name": "000000020059.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020059.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1538.845, "latencies_ms": [1538.845], "images_per_second": 0.65, "prompt_tokens": 1111, "response_tokens_est": 17, "n_tiles": 1, "output_text": " Two zebras are grazing in a grassy enclosure with trees and rocks in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26842.4, "ram_available_mb": 98929.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 26842.4, "ram_available_mb": 98929.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.44, "peak": 40.18, "min": 14.59}, "VIN": {"avg": 67.32, "peak": 116.83, "min": 29.57}, "VIN_SYS_5V0": {"avg": 14.65, "peak": 16.05, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 15.7, "peak": 17.32, "min": 12.61}}, "power_watts_avg": 30.44, "energy_joules_est": 46.85, "sample_count": 15, "duration_seconds": 1.539}, "timestamp": "2026-01-19T14:46:47.653932"}
{"image_index": 197, "image_name": "000000020059.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020059.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2368.172, "latencies_ms": [2368.172], "images_per_second": 0.422, "prompt_tokens": 1109, "response_tokens_est": 50, "n_tiles": 1, "output_text": " The image features two zebras grazing in a grassy enclosure with a rocky wall in the background. The zebras are black and white, and the grass is green. The lighting is bright and natural, suggesting that the photo was taken during the day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26842.4, "ram_available_mb": 98929.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26842.4, "ram_available_mb": 98929.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.74, "peak": 40.95, "min": 19.7}, "VIN": {"avg": 69.13, "peak": 103.19, "min": 29.84}, "VIN_SYS_5V0": {"avg": 15.03, "peak": 16.15, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 16.13, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 28.74, "energy_joules_est": 68.07, "sample_count": 23, "duration_seconds": 2.368}, "timestamp": "2026-01-19T14:46:50.049971"}
{"image_index": 198, "image_name": "000000020107.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020107.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1544.443, "latencies_ms": [1544.443], "images_per_second": 0.647, "prompt_tokens": 1100, "response_tokens_est": 19, "n_tiles": 1, "output_text": " An old, rusted fire hydrant with a chain on top is sitting on a sidewalk.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26842.4, "ram_available_mb": 98929.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 26842.4, "ram_available_mb": 98929.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.25, "peak": 39.77, "min": 16.56}, "VIN": {"avg": 73.31, "peak": 116.66, "min": 28.82}, "VIN_SYS_5V0": {"avg": 15.13, "peak": 16.05, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 16.27, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 31.25, "energy_joules_est": 48.28, "sample_count": 15, "duration_seconds": 1.545}, "timestamp": "2026-01-19T14:46:51.620474"}
{"image_index": 198, "image_name": "000000020107.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020107.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1503.738, "latencies_ms": [1503.738], "images_per_second": 0.665, "prompt_tokens": 1114, "response_tokens_est": 17, "n_tiles": 1, "output_text": " hydrant: 1\nchain: 2\npink flowers: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26842.4, "ram_available_mb": 98929.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 26842.4, "ram_available_mb": 98929.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.51, "peak": 40.57, "min": 20.89}, "VIN": {"avg": 73.41, "peak": 120.66, "min": 30.63}, "VIN_SYS_5V0": {"avg": 15.19, "peak": 16.15, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 16.35, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 32.51, "energy_joules_est": 48.9, "sample_count": 15, "duration_seconds": 1.504}, "timestamp": "2026-01-19T14:46:53.185224"}
{"image_index": 198, "image_name": "000000020107.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020107.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2309.423, "latencies_ms": [2309.423], "images_per_second": 0.433, "prompt_tokens": 1118, "response_tokens_est": 48, "n_tiles": 1, "output_text": " The fire hydrant is located in the foreground of the image, with a stone wall and a garden in the background. The fire hydrant is positioned to the left of the stone wall, and the garden is located behind the stone wall.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26842.4, "ram_available_mb": 98929.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26842.4, "ram_available_mb": 98929.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.59, "peak": 40.57, "min": 18.53}, "VIN": {"avg": 69.29, "peak": 103.27, "min": 29.19}, "VIN_SYS_5V0": {"avg": 14.98, "peak": 16.15, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 16.11, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 28.59, "energy_joules_est": 66.04, "sample_count": 23, "duration_seconds": 2.31}, "timestamp": "2026-01-19T14:46:55.566119"}
{"image_index": 198, "image_name": "000000020107.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020107.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1509.015, "latencies_ms": [1509.015], "images_per_second": 0.663, "prompt_tokens": 1112, "response_tokens_est": 16, "n_tiles": 1, "output_text": " A rusted fire hydrant sits on the ground next to a stone wall.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26842.4, "ram_available_mb": 98929.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26842.4, "ram_available_mb": 98929.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.76, "peak": 39.78, "min": 15.77}, "VIN": {"avg": 75.5, "peak": 129.38, "min": 27.79}, "VIN_SYS_5V0": {"avg": 14.84, "peak": 16.15, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 15.98, "peak": 17.71, "min": 13.39}}, "power_watts_avg": 30.76, "energy_joules_est": 46.43, "sample_count": 15, "duration_seconds": 1.509}, "timestamp": "2026-01-19T14:46:57.131189"}
{"image_index": 198, "image_name": "000000020107.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020107.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1526.166, "latencies_ms": [1526.166], "images_per_second": 0.655, "prompt_tokens": 1110, "response_tokens_est": 17, "n_tiles": 1, "output_text": " The fire hydrant is rusted and dirty, with a chain attached to it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26842.4, "ram_available_mb": 98929.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 26842.1, "ram_available_mb": 98930.1, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.02, "peak": 40.56, "min": 19.71}, "VIN": {"avg": 75.17, "peak": 126.24, "min": 29.24}, "VIN_SYS_5V0": {"avg": 14.96, "peak": 16.15, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 16.04, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 32.02, "energy_joules_est": 48.88, "sample_count": 15, "duration_seconds": 1.527}, "timestamp": "2026-01-19T14:46:58.688283"}
{"image_index": 199, "image_name": "000000020247.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020247.jpg", "image_width": 640, "image_height": 457, "image_resolution": "640x457", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1928.256, "latencies_ms": [1928.256], "images_per_second": 0.519, "prompt_tokens": 1099, "response_tokens_est": 32, "n_tiles": 1, "output_text": " In the image, there are two brown bears walking on a road, one of them is a cub, and they are surrounded by a dry, grassy landscape.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 26842.1, "ram_available_mb": 98930.1, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26842.1, "ram_available_mb": 98930.1, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.44, "peak": 40.95, "min": 20.1}, "VIN": {"avg": 71.65, "peak": 105.97, "min": 28.77}, "VIN_SYS_5V0": {"avg": 15.04, "peak": 16.15, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 16.1, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 30.44, "energy_joules_est": 58.71, "sample_count": 19, "duration_seconds": 1.929}, "timestamp": "2026-01-19T14:47:00.672702"}
{"image_index": 199, "image_name": "000000020247.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020247.jpg", "image_width": 640, "image_height": 457, "image_resolution": "640x457", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2571.662, "latencies_ms": [2571.662], "images_per_second": 0.389, "prompt_tokens": 1113, "response_tokens_est": 57, "n_tiles": 1, "output_text": " 1. bear: 2\n2. road: 1\n3. grass: 1\n4. dirt: 1\n5. road surface: 1\n6. bear: 1\n7. bear: 1\n8. bear: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26842.1, "ram_available_mb": 98930.1, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26842.1, "ram_available_mb": 98930.1, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.43, "peak": 40.57, "min": 17.35}, "VIN": {"avg": 72.98, "peak": 120.65, "min": 29.73}, "VIN_SYS_5V0": {"avg": 14.95, "peak": 16.05, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.95, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 27.43, "energy_joules_est": 70.55, "sample_count": 25, "duration_seconds": 2.572}, "timestamp": "2026-01-19T14:47:03.271263"}
{"image_index": 199, "image_name": "000000020247.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020247.jpg", "image_width": 640, "image_height": 457, "image_resolution": "640x457", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2241.517, "latencies_ms": [2241.517], "images_per_second": 0.446, "prompt_tokens": 1117, "response_tokens_est": 45, "n_tiles": 1, "output_text": " The brown bear in the foreground is walking towards the camera, while the brown bear in the background is walking away from the camera. The brown bear in the foreground is closer to the camera than the brown bear in the background.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26842.1, "ram_available_mb": 98930.1, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26842.1, "ram_available_mb": 98930.1, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.36, "peak": 40.57, "min": 16.55}, "VIN": {"avg": 58.58, "peak": 104.33, "min": 29.34}, "VIN_SYS_5V0": {"avg": 15.06, "peak": 16.15, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 16.09, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 28.36, "energy_joules_est": 63.58, "sample_count": 22, "duration_seconds": 2.242}, "timestamp": "2026-01-19T14:47:05.546453"}
{"image_index": 199, "image_name": "000000020247.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020247.jpg", "image_width": 640, "image_height": 457, "image_resolution": "640x457", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1521.841, "latencies_ms": [1521.841], "images_per_second": 0.657, "prompt_tokens": 1111, "response_tokens_est": 17, "n_tiles": 1, "output_text": " Two brown bears are walking on a dirt road, one of them is a cub.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26842.1, "ram_available_mb": 98930.1, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 26842.1, "ram_available_mb": 98930.1, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.38, "peak": 40.57, "min": 16.95}, "VIN": {"avg": 69.38, "peak": 100.33, "min": 30.22}, "VIN_SYS_5V0": {"avg": 15.07, "peak": 16.15, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 16.19, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 31.38, "energy_joules_est": 47.77, "sample_count": 15, "duration_seconds": 1.522}, "timestamp": "2026-01-19T14:47:07.107970"}
{"image_index": 199, "image_name": "000000020247.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020247.jpg", "image_width": 640, "image_height": 457, "image_resolution": "640x457", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2224.74, "latencies_ms": [2224.74], "images_per_second": 0.449, "prompt_tokens": 1109, "response_tokens_est": 44, "n_tiles": 1, "output_text": " The image features two brown bears walking on a dirt road, with one bear having a lighter coat than the other. The lighting is natural and bright, suggesting daytime, and the bears appear to be in a natural habitat.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26842.1, "ram_available_mb": 98930.1, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26842.1, "ram_available_mb": 98930.1, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.08, "peak": 40.56, "min": 18.91}, "VIN": {"avg": 64.82, "peak": 126.13, "min": 28.95}, "VIN_SYS_5V0": {"avg": 14.96, "peak": 16.15, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 16.05, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 29.08, "energy_joules_est": 64.7, "sample_count": 22, "duration_seconds": 2.225}, "timestamp": "2026-01-19T14:47:09.393433"}
{"image_index": 200, "image_name": "000000020333.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020333.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1652.309, "latencies_ms": [1652.309], "images_per_second": 0.605, "prompt_tokens": 1100, "response_tokens_est": 22, "n_tiles": 1, "output_text": " A young child with blonde hair is kneeling on the ground next to a metal bucket filled with dirt.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 26842.1, "ram_available_mb": 98930.1, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26842.1, "ram_available_mb": 98930.1, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.56, "peak": 40.16, "min": 16.16}, "VIN": {"avg": 73.36, "peak": 130.11, "min": 30.28}, "VIN_SYS_5V0": {"avg": 14.88, "peak": 16.05, "min": 12.25}, "VDD_CPU_SOC_MSS": {"avg": 16.02, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 30.56, "energy_joules_est": 50.51, "sample_count": 16, "duration_seconds": 1.653}, "timestamp": "2026-01-19T14:47:11.066444"}
{"image_index": 200, "image_name": "000000020333.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020333.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2676.075, "latencies_ms": [2676.075], "images_per_second": 0.374, "prompt_tokens": 1114, "response_tokens_est": 56, "n_tiles": 1, "output_text": " 1. child: 1\n2. shirt: 1\n3. tie: 1\n4. pants: 1\n5. shoes: 1\n6. bucket: 1\n7. dirt: 1\n8. leaves: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26842.1, "ram_available_mb": 98930.1, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 26843.3, "ram_available_mb": 98928.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.91, "peak": 40.57, "min": 18.91}, "VIN": {"avg": 62.0, "peak": 85.48, "min": 30.19}, "VIN_SYS_5V0": {"avg": 14.83, "peak": 16.26, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 15.93, "peak": 17.71, "min": 14.57}}, "power_watts_avg": 26.91, "energy_joules_est": 72.03, "sample_count": 26, "duration_seconds": 2.677}, "timestamp": "2026-01-19T14:47:13.763723"}
{"image_index": 200, "image_name": "000000020333.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020333.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2085.477, "latencies_ms": [2085.477], "images_per_second": 0.48, "prompt_tokens": 1118, "response_tokens_est": 38, "n_tiles": 1, "output_text": " The child is in the foreground, kneeling on the ground, and the bucket is in the middle ground. The child is looking at the bucket, and the bucket is on the ground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26843.3, "ram_available_mb": 98928.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26843.3, "ram_available_mb": 98928.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.33, "peak": 40.18, "min": 16.16}, "VIN": {"avg": 73.86, "peak": 121.53, "min": 28.78}, "VIN_SYS_5V0": {"avg": 15.0, "peak": 16.15, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 16.03, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 28.33, "energy_joules_est": 59.09, "sample_count": 21, "duration_seconds": 2.086}, "timestamp": "2026-01-19T14:47:15.949434"}
{"image_index": 200, "image_name": "000000020333.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020333.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1638.656, "latencies_ms": [1638.656], "images_per_second": 0.61, "prompt_tokens": 1112, "response_tokens_est": 22, "n_tiles": 1, "output_text": " A young child is playing in a dirt area with a bucket, wearing a white shirt and a colorful tie.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26843.3, "ram_available_mb": 98928.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 26843.3, "ram_available_mb": 98928.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.36, "peak": 39.77, "min": 15.77}, "VIN": {"avg": 74.39, "peak": 123.72, "min": 29.47}, "VIN_SYS_5V0": {"avg": 14.81, "peak": 16.15, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.92, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 30.36, "energy_joules_est": 49.76, "sample_count": 16, "duration_seconds": 1.639}, "timestamp": "2026-01-19T14:47:17.617313"}
{"image_index": 200, "image_name": "000000020333.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020333.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1839.993, "latencies_ms": [1839.993], "images_per_second": 0.543, "prompt_tokens": 1110, "response_tokens_est": 30, "n_tiles": 1, "output_text": " The image is a black and white photo with a child wearing a white shirt and a colorful tie, and the child is kneeling on the ground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26843.3, "ram_available_mb": 98928.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26843.3, "ram_available_mb": 98928.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.47, "peak": 40.56, "min": 19.7}, "VIN": {"avg": 70.45, "peak": 101.12, "min": 30.01}, "VIN_SYS_5V0": {"avg": 15.07, "peak": 16.15, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 16.27, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 30.47, "energy_joules_est": 56.07, "sample_count": 18, "duration_seconds": 1.84}, "timestamp": "2026-01-19T14:47:19.504981"}
{"image_index": 201, "image_name": "000000020553.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020553.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1901.895, "latencies_ms": [1901.895], "images_per_second": 0.526, "prompt_tokens": 1099, "response_tokens_est": 31, "n_tiles": 1, "output_text": " A collection of stuffed animals, including a teddy bear, are placed on a makeshift bed, surrounded by bottles and flowers, in a barren, sandy landscape.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26843.3, "ram_available_mb": 98928.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26843.3, "ram_available_mb": 98928.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.59, "peak": 40.57, "min": 17.73}, "VIN": {"avg": 69.04, "peak": 113.2, "min": 27.84}, "VIN_SYS_5V0": {"avg": 14.98, "peak": 16.15, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 16.06, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 29.59, "energy_joules_est": 56.3, "sample_count": 19, "duration_seconds": 1.903}, "timestamp": "2026-01-19T14:47:21.491135"}
{"image_index": 201, "image_name": "000000020553.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020553.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1744.244, "latencies_ms": [1744.244], "images_per_second": 0.573, "prompt_tokens": 1113, "response_tokens_est": 26, "n_tiles": 1, "output_text": " teddy bear: 2\nbottle: 3\nflowers: 2\ntable: 1\nbed: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26843.3, "ram_available_mb": 98928.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26843.3, "ram_available_mb": 98928.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.45, "peak": 40.57, "min": 16.16}, "VIN": {"avg": 78.16, "peak": 126.44, "min": 28.78}, "VIN_SYS_5V0": {"avg": 14.96, "peak": 16.05, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 16.07, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 30.45, "energy_joules_est": 53.12, "sample_count": 17, "duration_seconds": 1.745}, "timestamp": "2026-01-19T14:47:23.264169"}
{"image_index": 201, "image_name": "000000020553.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020553.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2207.833, "latencies_ms": [2207.833], "images_per_second": 0.453, "prompt_tokens": 1117, "response_tokens_est": 44, "n_tiles": 1, "output_text": " The teddy bear is positioned to the left of the table, which is situated in the foreground of the image. The table is located near the center of the image, with the teddy bear and other objects arranged around it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26843.3, "ram_available_mb": 98928.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26843.3, "ram_available_mb": 98928.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.79, "peak": 40.95, "min": 18.53}, "VIN": {"avg": 72.46, "peak": 127.0, "min": 28.6}, "VIN_SYS_5V0": {"avg": 15.13, "peak": 16.26, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 16.21, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 28.79, "energy_joules_est": 63.58, "sample_count": 22, "duration_seconds": 2.208}, "timestamp": "2026-01-19T14:47:25.556919"}
{"image_index": 201, "image_name": "000000020553.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020553.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1803.446, "latencies_ms": [1803.446], "images_per_second": 0.554, "prompt_tokens": 1111, "response_tokens_est": 28, "n_tiles": 1, "output_text": " In a barren, dry landscape, a makeshift bed is set up with a teddy bear and other stuffed animals, surrounded by bottles and flowers.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26843.3, "ram_available_mb": 98928.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26843.6, "ram_available_mb": 98928.6, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.72, "peak": 40.57, "min": 15.77}, "VIN": {"avg": 75.41, "peak": 119.09, "min": 27.55}, "VIN_SYS_5V0": {"avg": 14.84, "peak": 16.05, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 15.99, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 29.72, "energy_joules_est": 53.61, "sample_count": 18, "duration_seconds": 1.804}, "timestamp": "2026-01-19T14:47:27.442668"}
{"image_index": 201, "image_name": "000000020553.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020553.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1613.073, "latencies_ms": [1613.073], "images_per_second": 0.62, "prompt_tokens": 1109, "response_tokens_est": 21, "n_tiles": 1, "output_text": " The image features a dirt ground with a table and a bed, and the lighting is bright and sunny.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26843.6, "ram_available_mb": 98928.6, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26843.8, "ram_available_mb": 98928.4, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.02, "peak": 40.56, "min": 16.95}, "VIN": {"avg": 70.39, "peak": 119.06, "min": 27.22}, "VIN_SYS_5V0": {"avg": 14.93, "peak": 16.05, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 16.04, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 31.02, "energy_joules_est": 50.05, "sample_count": 16, "duration_seconds": 1.613}, "timestamp": "2026-01-19T14:47:29.114193"}
{"image_index": 202, "image_name": "000000020571.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020571.jpg", "image_width": 539, "image_height": 640, "image_resolution": "539x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 4191.252, "latencies_ms": [4191.252], "images_per_second": 0.239, "prompt_tokens": 1432, "response_tokens_est": 101, "n_tiles": 1, "output_text": " The image captures a serene harbor scene, where a green fishing boat named \"JAMIE\" is docked, surrounded by other boats and a bustling harbor with people and equipment. The boats are moored to a sturdy wooden pier, which is adorned with a variety of colorful buoys and life jackets, adding a vibrant touch to the otherwise tranquil setting. In the background, a picturesque landscape of rolling hills and a misty sky creates a serene backdrop, enhancing the overall peaceful ambiance of the scene.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 26843.8, "ram_available_mb": 98928.4, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 26843.2, "ram_available_mb": 98928.9, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.2, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.08, "peak": 40.56, "min": 18.13}, "VIN": {"avg": 68.33, "peak": 126.26, "min": 29.7}, "VIN_SYS_5V0": {"avg": 15.11, "peak": 16.76, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 16.16, "peak": 18.11, "min": 13.39}}, "power_watts_avg": 26.08, "energy_joules_est": 109.32, "sample_count": 41, "duration_seconds": 4.192}, "timestamp": "2026-01-19T14:47:33.386438"}
{"image_index": 202, "image_name": "000000020571.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020571.jpg", "image_width": 539, "image_height": 640, "image_resolution": "539x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2948.253, "latencies_ms": [2948.253], "images_per_second": 0.339, "prompt_tokens": 1446, "response_tokens_est": 56, "n_tiles": 1, "output_text": " 1. Boat: 2\n2. Boat: 1\n3. Boat: 1\n4. Boat: 1\n5. Boat: 1\n6. Boat: 1\n7. Boat: 1\n8. Boat: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26843.2, "ram_available_mb": 98928.9, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26843.2, "ram_available_mb": 98928.9, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.37, "peak": 40.18, "min": 15.38}, "VIN": {"avg": 71.68, "peak": 131.21, "min": 30.04}, "VIN_SYS_5V0": {"avg": 15.08, "peak": 16.56, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 16.14, "peak": 17.71, "min": 13.0}}, "power_watts_avg": 28.37, "energy_joules_est": 83.65, "sample_count": 29, "duration_seconds": 2.949}, "timestamp": "2026-01-19T14:47:36.409392"}
{"image_index": 202, "image_name": "000000020571.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020571.jpg", "image_width": 539, "image_height": 640, "image_resolution": "539x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3110.365, "latencies_ms": [3110.365], "images_per_second": 0.322, "prompt_tokens": 1450, "response_tokens_est": 62, "n_tiles": 1, "output_text": " The main objects are positioned in the foreground, with the fishing boats and the dock being the closest to the viewer. The boats are on the left side of the image, while the dock is on the right side. The background features a calm body of water, with hills and other boats visible in the distance.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26843.2, "ram_available_mb": 98928.9, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26843.2, "ram_available_mb": 98928.9, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.17, "peak": 40.16, "min": 15.77}, "VIN": {"avg": 72.88, "peak": 119.55, "min": 30.62}, "VIN_SYS_5V0": {"avg": 15.15, "peak": 16.56, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 16.15, "peak": 17.71, "min": 13.39}}, "power_watts_avg": 28.17, "energy_joules_est": 87.63, "sample_count": 30, "duration_seconds": 3.111}, "timestamp": "2026-01-19T14:47:39.536039"}
{"image_index": 202, "image_name": "000000020571.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020571.jpg", "image_width": 539, "image_height": 640, "image_resolution": "539x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3126.239, "latencies_ms": [3126.239], "images_per_second": 0.32, "prompt_tokens": 1444, "response_tokens_est": 63, "n_tiles": 1, "output_text": " The image captures a serene harbor scene where several fishing boats are docked at a pier. The boats, painted in hues of green and white, are adorned with various equipment and equipment, suggesting a bustling fishing operation. The harbor is nestled amidst lush green hills, adding a touch of tranquility to the otherwise busy setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26843.2, "ram_available_mb": 98928.9, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26843.2, "ram_available_mb": 98928.9, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.08, "peak": 40.18, "min": 16.95}, "VIN": {"avg": 68.65, "peak": 120.58, "min": 27.6}, "VIN_SYS_5V0": {"avg": 15.25, "peak": 16.66, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 16.32, "peak": 17.71, "min": 14.18}}, "power_watts_avg": 28.08, "energy_joules_est": 87.79, "sample_count": 31, "duration_seconds": 3.127}, "timestamp": "2026-01-19T14:47:42.756734"}
{"image_index": 202, "image_name": "000000020571.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020571.jpg", "image_width": 539, "image_height": 640, "image_resolution": "539x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2680.757, "latencies_ms": [2680.757], "images_per_second": 0.373, "prompt_tokens": 1442, "response_tokens_est": 46, "n_tiles": 1, "output_text": " The image features a harbor with boats docked at the pier, the water is calm and the sky is cloudy. The boats are painted in various colors, including green, blue, and white, and are equipped with fishing equipment.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26843.2, "ram_available_mb": 98928.9, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26843.2, "ram_available_mb": 98928.9, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.26, "peak": 40.18, "min": 15.38}, "VIN": {"avg": 70.57, "peak": 118.03, "min": 29.45}, "VIN_SYS_5V0": {"avg": 15.13, "peak": 16.56, "min": 11.87}, "VDD_CPU_SOC_MSS": {"avg": 16.22, "peak": 17.71, "min": 13.0}}, "power_watts_avg": 29.26, "energy_joules_est": 78.46, "sample_count": 26, "duration_seconds": 2.682}, "timestamp": "2026-01-19T14:47:45.453749"}
{"image_index": 203, "image_name": "000000020992.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020992.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1452.23, "latencies_ms": [1452.23], "images_per_second": 0.689, "prompt_tokens": 1099, "response_tokens_est": 15, "n_tiles": 1, "output_text": " A woman with a black scarf is eating a hot dog at night.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26843.2, "ram_available_mb": 98928.9, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 26843.2, "ram_available_mb": 98928.9, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.13, "peak": 40.56, "min": 17.34}, "VIN": {"avg": 76.32, "peak": 119.85, "min": 29.95}, "VIN_SYS_5V0": {"avg": 15.24, "peak": 16.15, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 16.37, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 32.13, "energy_joules_est": 46.68, "sample_count": 14, "duration_seconds": 1.453}, "timestamp": "2026-01-19T14:47:46.920870"}
{"image_index": 203, "image_name": "000000020992.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020992.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2539.07, "latencies_ms": [2539.07], "images_per_second": 0.394, "prompt_tokens": 1113, "response_tokens_est": 57, "n_tiles": 1, "output_text": " 1. person: 1\n2. hand: 1\n3. food: 1\n4. mouth: 1\n5. nose: 1\n6. eyes: 1\n7. hair: 1\n8. scarf: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26843.2, "ram_available_mb": 98928.9, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 26844.0, "ram_available_mb": 98928.2, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.46, "peak": 41.34, "min": 18.52}, "VIN": {"avg": 75.73, "peak": 120.66, "min": 29.76}, "VIN_SYS_5V0": {"avg": 15.2, "peak": 16.26, "min": 13.29}, "VDD_CPU_SOC_MSS": {"avg": 16.28, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 28.46, "energy_joules_est": 72.27, "sample_count": 25, "duration_seconds": 2.539}, "timestamp": "2026-01-19T14:47:49.509349"}
{"image_index": 203, "image_name": "000000020992.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020992.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1959.302, "latencies_ms": [1959.302], "images_per_second": 0.51, "prompt_tokens": 1117, "response_tokens_est": 34, "n_tiles": 1, "output_text": " The woman is in the foreground, with the hot dog in her mouth, and the background is blurred, indicating that the focus is on the woman and the hot dog.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26844.0, "ram_available_mb": 98928.2, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26844.0, "ram_available_mb": 98928.2, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.36, "peak": 40.16, "min": 16.16}, "VIN": {"avg": 72.34, "peak": 106.63, "min": 28.7}, "VIN_SYS_5V0": {"avg": 15.03, "peak": 16.15, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 16.12, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 29.36, "energy_joules_est": 57.53, "sample_count": 19, "duration_seconds": 1.96}, "timestamp": "2026-01-19T14:47:51.488666"}
{"image_index": 203, "image_name": "000000020992.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020992.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1325.364, "latencies_ms": [1325.364], "images_per_second": 0.755, "prompt_tokens": 1111, "response_tokens_est": 10, "n_tiles": 1, "output_text": " A woman is eating a hot dog at night.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26844.0, "ram_available_mb": 98928.2, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.4, "ram_used_mb": 26844.0, "ram_available_mb": 98928.2, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.61, "peak": 40.57, "min": 17.74}, "VIN": {"avg": 82.9, "peak": 120.52, "min": 27.84}, "VIN_SYS_5V0": {"avg": 15.23, "peak": 16.15, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 16.41, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 32.61, "energy_joules_est": 43.23, "sample_count": 13, "duration_seconds": 1.326}, "timestamp": "2026-01-19T14:47:52.849772"}
{"image_index": 203, "image_name": "000000020992.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020992.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2398.707, "latencies_ms": [2398.707], "images_per_second": 0.417, "prompt_tokens": 1109, "response_tokens_est": 51, "n_tiles": 1, "output_text": " The image is taken at night, with the subject illuminated by a streetlight. The subject is wearing a black jacket and has a scarf wrapped around their neck. The background is blurred, but it appears to be a street with buildings and other people.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26844.0, "ram_available_mb": 98928.2, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26844.0, "ram_available_mb": 98928.2, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.84, "peak": 41.34, "min": 18.14}, "VIN": {"avg": 60.19, "peak": 100.56, "min": 28.68}, "VIN_SYS_5V0": {"avg": 15.06, "peak": 16.15, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 16.04, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 28.84, "energy_joules_est": 69.19, "sample_count": 24, "duration_seconds": 2.399}, "timestamp": "2026-01-19T14:47:55.331831"}
{"image_index": 204, "image_name": "000000021167.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021167.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1769.462, "latencies_ms": [1769.462], "images_per_second": 0.565, "prompt_tokens": 1100, "response_tokens_est": 27, "n_tiles": 1, "output_text": " A man and a woman are standing in a room, the man is holding a glass of wine and the woman is wearing a dress.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 26844.0, "ram_available_mb": 98928.2, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26844.2, "ram_available_mb": 98928.0, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.99, "peak": 40.18, "min": 15.38}, "VIN": {"avg": 77.64, "peak": 128.66, "min": 33.67}, "VIN_SYS_5V0": {"avg": 14.96, "peak": 16.26, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 16.0, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 29.99, "energy_joules_est": 53.08, "sample_count": 17, "duration_seconds": 1.77}, "timestamp": "2026-01-19T14:47:57.113399"}
{"image_index": 204, "image_name": "000000021167.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021167.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2509.766, "latencies_ms": [2509.766], "images_per_second": 0.398, "prompt_tokens": 1114, "response_tokens_est": 56, "n_tiles": 1, "output_text": " 1. man: 1\n2. woman: 1\n3. glass: 1\n4. curtain: 1\n5. door: 1\n6. wall: 1\n7. shelf: 1\n8. vase: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26844.2, "ram_available_mb": 98928.0, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26844.2, "ram_available_mb": 98928.0, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.67, "peak": 40.57, "min": 18.14}, "VIN": {"avg": 74.07, "peak": 123.62, "min": 29.01}, "VIN_SYS_5V0": {"avg": 15.19, "peak": 16.26, "min": 13.39}, "VDD_CPU_SOC_MSS": {"avg": 16.25, "peak": 17.71, "min": 14.57}}, "power_watts_avg": 27.67, "energy_joules_est": 69.45, "sample_count": 25, "duration_seconds": 2.51}, "timestamp": "2026-01-19T14:47:59.698622"}
{"image_index": 204, "image_name": "000000021167.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021167.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3245.222, "latencies_ms": [3245.222], "images_per_second": 0.308, "prompt_tokens": 1118, "response_tokens_est": 79, "n_tiles": 1, "output_text": " The man is standing to the right of the woman, and the woman is standing to the left of the man. The man is holding a glass of wine in his right hand, and the woman is holding a glass of wine in her left hand. The man is wearing a black suit with a white shirt and a black tie, and the woman is wearing a black dress with a long sleeve.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 26844.2, "ram_available_mb": 98928.0, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 26844.2, "ram_available_mb": 98928.0, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 24.96, "peak": 39.78, "min": 15.77}, "VIN": {"avg": 66.57, "peak": 113.1, "min": 29.41}, "VIN_SYS_5V0": {"avg": 14.73, "peak": 15.95, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.69, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 24.96, "energy_joules_est": 81.01, "sample_count": 32, "duration_seconds": 3.246}, "timestamp": "2026-01-19T14:48:03.032419"}
{"image_index": 204, "image_name": "000000021167.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021167.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1770.499, "latencies_ms": [1770.499], "images_per_second": 0.565, "prompt_tokens": 1112, "response_tokens_est": 27, "n_tiles": 1, "output_text": " A man and a woman are standing in a room, the man is holding a glass of wine and the woman is looking at him.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26844.2, "ram_available_mb": 98928.0, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26844.2, "ram_available_mb": 98928.0, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.15, "peak": 39.78, "min": 14.98}, "VIN": {"avg": 72.21, "peak": 119.47, "min": 29.22}, "VIN_SYS_5V0": {"avg": 14.79, "peak": 16.15, "min": 11.87}, "VDD_CPU_SOC_MSS": {"avg": 15.86, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 29.15, "energy_joules_est": 51.62, "sample_count": 18, "duration_seconds": 1.771}, "timestamp": "2026-01-19T14:48:04.898767"}
{"image_index": 204, "image_name": "000000021167.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021167.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2322.514, "latencies_ms": [2322.514], "images_per_second": 0.431, "prompt_tokens": 1110, "response_tokens_est": 47, "n_tiles": 1, "output_text": " The image is taken in a dimly lit room with a warm and inviting atmosphere. The man is wearing a black suit with a white shirt and a black tie, while the woman is dressed in a black dress with a long sleeve.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26844.2, "ram_available_mb": 98928.0, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26846.0, "ram_available_mb": 98926.2, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.66, "peak": 40.16, "min": 16.95}, "VIN": {"avg": 71.34, "peak": 119.44, "min": 29.22}, "VIN_SYS_5V0": {"avg": 14.81, "peak": 16.05, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.85, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 27.66, "energy_joules_est": 64.25, "sample_count": 23, "duration_seconds": 2.323}, "timestamp": "2026-01-19T14:48:07.291048"}
{"image_index": 205, "image_name": "000000021465.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021465.jpg", "image_width": 500, "image_height": 281, "image_resolution": "500x281", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 3226.985, "latencies_ms": [3226.985], "images_per_second": 0.31, "prompt_tokens": 1099, "response_tokens_est": 83, "n_tiles": 1, "output_text": " The image captures a scene of a blue wooden cabinet with a white top, adorned with various items such as a silver teapot, a vase, and several small cups, all arranged on its shelves. The cabinet is situated in a room with a concrete floor, and in the background, there are other furniture pieces, including a chair and a table, suggesting a setting that could be a store or a home.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 26846.0, "ram_available_mb": 98926.2, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 26846.0, "ram_available_mb": 98926.2, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 25.43, "peak": 40.57, "min": 15.77}, "VIN": {"avg": 62.78, "peak": 106.11, "min": 28.58}, "VIN_SYS_5V0": {"avg": 14.92, "peak": 16.05, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.91, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 25.43, "energy_joules_est": 82.07, "sample_count": 32, "duration_seconds": 3.227}, "timestamp": "2026-01-19T14:48:10.599114"}
{"image_index": 205, "image_name": "000000021465.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021465.jpg", "image_width": 500, "image_height": 281, "image_resolution": "500x281", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2175.595, "latencies_ms": [2175.595], "images_per_second": 0.46, "prompt_tokens": 1113, "response_tokens_est": 42, "n_tiles": 1, "output_text": " 1. silver teapot\n2. silver vase\n3. silver pitcher\n4. silver cup\n5. silver bowl\n6. silver spoon\n7. silver knife\n8. silver fork", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26846.0, "ram_available_mb": 98926.2, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26845.7, "ram_available_mb": 98926.5, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.33, "peak": 40.16, "min": 15.38}, "VIN": {"avg": 66.69, "peak": 123.45, "min": 29.78}, "VIN_SYS_5V0": {"avg": 14.92, "peak": 16.05, "min": 12.15}, "VDD_CPU_SOC_MSS": {"avg": 15.92, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 28.33, "energy_joules_est": 61.64, "sample_count": 21, "duration_seconds": 2.176}, "timestamp": "2026-01-19T14:48:12.786485"}
{"image_index": 205, "image_name": "000000021465.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021465.jpg", "image_width": 500, "image_height": 281, "image_resolution": "500x281", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2625.219, "latencies_ms": [2625.219], "images_per_second": 0.381, "prompt_tokens": 1117, "response_tokens_est": 60, "n_tiles": 1, "output_text": " The blue cabinet is positioned in the foreground, with the green and white table to its right. The silver teapot is placed on the left side of the cabinet, while the white vase is situated on the right side. The green and white table is located in the background, behind the cabinet.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26845.7, "ram_available_mb": 98926.5, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26845.4, "ram_available_mb": 98926.7, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.24, "peak": 40.56, "min": 17.73}, "VIN": {"avg": 72.89, "peak": 130.59, "min": 29.27}, "VIN_SYS_5V0": {"avg": 15.11, "peak": 16.26, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 16.22, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 27.24, "energy_joules_est": 71.52, "sample_count": 26, "duration_seconds": 2.626}, "timestamp": "2026-01-19T14:48:15.471277"}
{"image_index": 205, "image_name": "000000021465.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021465.jpg", "image_width": 500, "image_height": 281, "image_resolution": "500x281", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3273.438, "latencies_ms": [3273.438], "images_per_second": 0.305, "prompt_tokens": 1111, "response_tokens_est": 84, "n_tiles": 1, "output_text": " The image captures a scene of a vintage blue cabinet, adorned with various items, including a silver teapot, a vase, and a few cups, all arranged on its shelves. The cabinet is situated in a room with a wooden floor, and a green table can be seen in the background. The overall setting suggests a space that is both functional and aesthetically pleasing, with the vintage cabinet serving as a focal point.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26845.4, "ram_available_mb": 98926.7, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 26847.7, "ram_available_mb": 98924.5, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 25.42, "peak": 40.16, "min": 15.77}, "VIN": {"avg": 59.54, "peak": 125.54, "min": 30.1}, "VIN_SYS_5V0": {"avg": 14.94, "peak": 16.05, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.87, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 25.42, "energy_joules_est": 83.22, "sample_count": 32, "duration_seconds": 3.274}, "timestamp": "2026-01-19T14:48:18.800075"}
{"image_index": 205, "image_name": "000000021465.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021465.jpg", "image_width": 500, "image_height": 281, "image_resolution": "500x281", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2894.959, "latencies_ms": [2894.959], "images_per_second": 0.345, "prompt_tokens": 1109, "response_tokens_est": 70, "n_tiles": 1, "output_text": " The image depicts a blue wooden cabinet with a white top, situated in a room with a concrete floor. The cabinet is adorned with various items, including a silver teapot, a silver vase, and several small metal cups. The lighting in the room is natural, coming from a window out of frame, and the weather appears to be clear.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26847.7, "ram_available_mb": 98924.5, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26847.7, "ram_available_mb": 98924.5, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.0, "peak": 40.18, "min": 15.77}, "VIN": {"avg": 59.73, "peak": 97.2, "min": 29.27}, "VIN_SYS_5V0": {"avg": 14.93, "peak": 16.05, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.86, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 26.0, "energy_joules_est": 75.28, "sample_count": 29, "duration_seconds": 2.895}, "timestamp": "2026-01-19T14:48:21.797174"}
{"image_index": 206, "image_name": "000000021503.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021503.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1412.093, "latencies_ms": [1412.093], "images_per_second": 0.708, "prompt_tokens": 1099, "response_tokens_est": 13, "n_tiles": 1, "output_text": " A plate of bread with butter on it is on a table.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 26847.7, "ram_available_mb": 98924.5, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 26847.7, "ram_available_mb": 98924.5, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.15, "peak": 40.18, "min": 14.98}, "VIN": {"avg": 70.07, "peak": 90.42, "min": 29.09}, "VIN_SYS_5V0": {"avg": 14.74, "peak": 16.05, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.83, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 31.15, "energy_joules_est": 44.0, "sample_count": 14, "duration_seconds": 1.412}, "timestamp": "2026-01-19T14:48:23.257115"}
{"image_index": 206, "image_name": "000000021503.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021503.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1945.554, "latencies_ms": [1945.554], "images_per_second": 0.514, "prompt_tokens": 1113, "response_tokens_est": 34, "n_tiles": 1, "output_text": " bread: 6, butter: 6, plate: 1, remote: 1, keyboard: 1, jar: 1, table: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26847.7, "ram_available_mb": 98924.5, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26847.7, "ram_available_mb": 98924.5, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.73, "peak": 40.97, "min": 20.49}, "VIN": {"avg": 70.61, "peak": 128.05, "min": 29.77}, "VIN_SYS_5V0": {"avg": 15.16, "peak": 16.15, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 16.08, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 30.73, "energy_joules_est": 59.79, "sample_count": 19, "duration_seconds": 1.946}, "timestamp": "2026-01-19T14:48:25.241068"}
{"image_index": 206, "image_name": "000000021503.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021503.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2050.16, "latencies_ms": [2050.16], "images_per_second": 0.488, "prompt_tokens": 1117, "response_tokens_est": 38, "n_tiles": 1, "output_text": " The plate of bread and butter is located in the foreground, with the keyboard and remote control in the background. The butter is spread on the bread slices, which are placed on the plate.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26847.7, "ram_available_mb": 98924.5, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26847.5, "ram_available_mb": 98924.7, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.43, "peak": 40.56, "min": 17.73}, "VIN": {"avg": 79.86, "peak": 124.61, "min": 30.25}, "VIN_SYS_5V0": {"avg": 15.17, "peak": 16.15, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 16.22, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 29.43, "energy_joules_est": 60.34, "sample_count": 20, "duration_seconds": 2.05}, "timestamp": "2026-01-19T14:48:27.312275"}
{"image_index": 206, "image_name": "000000021503.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021503.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1502.559, "latencies_ms": [1502.559], "images_per_second": 0.666, "prompt_tokens": 1111, "response_tokens_est": 17, "n_tiles": 1, "output_text": " A plate of cheese and bread is on a desk with a keyboard and a remote.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26847.5, "ram_available_mb": 98924.7, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 26847.5, "ram_available_mb": 98924.7, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.86, "peak": 40.57, "min": 18.14}, "VIN": {"avg": 71.39, "peak": 126.05, "min": 29.38}, "VIN_SYS_5V0": {"avg": 15.21, "peak": 16.15, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 16.3, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 31.86, "energy_joules_est": 47.88, "sample_count": 15, "duration_seconds": 1.503}, "timestamp": "2026-01-19T14:48:28.874555"}
{"image_index": 206, "image_name": "000000021503.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021503.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2368.377, "latencies_ms": [2368.377], "images_per_second": 0.422, "prompt_tokens": 1109, "response_tokens_est": 50, "n_tiles": 1, "output_text": " The image features a plate with six slices of bread, each topped with a layer of butter, placed on a white plate. The lighting in the image is natural, coming from a window in the background, and the bread appears to be freshly baked.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26847.5, "ram_available_mb": 98924.7, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26847.5, "ram_available_mb": 98924.7, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.57, "peak": 40.57, "min": 20.1}, "VIN": {"avg": 68.1, "peak": 107.15, "min": 33.27}, "VIN_SYS_5V0": {"avg": 15.03, "peak": 16.15, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 16.06, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 28.57, "energy_joules_est": 67.68, "sample_count": 23, "duration_seconds": 2.369}, "timestamp": "2026-01-19T14:48:31.251940"}
{"image_index": 207, "image_name": "000000021604.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021604.jpg", "image_width": 512, "image_height": 640, "image_resolution": "512x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2144.618, "latencies_ms": [2144.618], "images_per_second": 0.466, "prompt_tokens": 1432, "response_tokens_est": 26, "n_tiles": 1, "output_text": " A man in a suit and tie is adjusting his tie, which has a pattern of red lights along the length of the tie.", "error": null, "sys_before": {"cpu_percent": 14.3, "ram_used_mb": 26847.5, "ram_available_mb": 98924.7, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 26847.7, "ram_available_mb": 98924.4, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.2, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.29, "peak": 40.16, "min": 16.95}, "VIN": {"avg": 73.92, "peak": 117.65, "min": 29.07}, "VIN_SYS_5V0": {"avg": 15.59, "peak": 16.86, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 16.72, "peak": 18.11, "min": 14.18}}, "power_watts_avg": 31.29, "energy_joules_est": 67.12, "sample_count": 21, "duration_seconds": 2.145}, "timestamp": "2026-01-19T14:48:33.441165"}
{"image_index": 207, "image_name": "000000021604.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021604.jpg", "image_width": 512, "image_height": 640, "image_resolution": "512x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2384.932, "latencies_ms": [2384.932], "images_per_second": 0.419, "prompt_tokens": 1446, "response_tokens_est": 35, "n_tiles": 1, "output_text": " tie: 1, glasses: 1, suit: 1, shirt: 1, tie clip: 1, tie: 1, man: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26847.7, "ram_available_mb": 98924.4, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26847.7, "ram_available_mb": 98924.4, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.59, "peak": 40.57, "min": 18.52}, "VIN": {"avg": 75.95, "peak": 118.73, "min": 28.73}, "VIN_SYS_5V0": {"avg": 15.29, "peak": 16.66, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 16.32, "peak": 17.71, "min": 13.79}}, "power_watts_avg": 30.59, "energy_joules_est": 72.98, "sample_count": 24, "duration_seconds": 2.386}, "timestamp": "2026-01-19T14:48:35.925542"}
{"image_index": 207, "image_name": "000000021604.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021604.jpg", "image_width": 512, "image_height": 640, "image_resolution": "512x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3242.376, "latencies_ms": [3242.376], "images_per_second": 0.308, "prompt_tokens": 1450, "response_tokens_est": 66, "n_tiles": 1, "output_text": " The man is positioned in the foreground of the image, with the tie being the central object. The tie is located in the middle ground, with the man's face and glasses being the closest objects to the camera. The background is a dark, neutral color, providing a contrast to the man's tie and making it stand out.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26847.7, "ram_available_mb": 98924.4, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26847.7, "ram_available_mb": 98924.4, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.63, "peak": 39.78, "min": 16.56}, "VIN": {"avg": 69.58, "peak": 130.22, "min": 28.77}, "VIN_SYS_5V0": {"avg": 15.01, "peak": 16.56, "min": 11.77}, "VDD_CPU_SOC_MSS": {"avg": 16.04, "peak": 17.71, "min": 13.0}}, "power_watts_avg": 27.63, "energy_joules_est": 89.6, "sample_count": 32, "duration_seconds": 3.243}, "timestamp": "2026-01-19T14:48:39.243609"}
{"image_index": 207, "image_name": "000000021604.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021604.jpg", "image_width": 512, "image_height": 640, "image_resolution": "512x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1763.969, "latencies_ms": [1763.969], "images_per_second": 0.567, "prompt_tokens": 1444, "response_tokens_est": 12, "n_tiles": 1, "output_text": " A man in a suit and tie is adjusting his tie.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26847.7, "ram_available_mb": 98924.4, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.3, "ram_used_mb": 26847.7, "ram_available_mb": 98924.4, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.32, "peak": 40.18, "min": 15.77}, "VIN": {"avg": 77.5, "peak": 124.36, "min": 27.7}, "VIN_SYS_5V0": {"avg": 15.19, "peak": 16.56, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 16.27, "peak": 17.71, "min": 13.39}}, "power_watts_avg": 32.32, "energy_joules_est": 57.02, "sample_count": 18, "duration_seconds": 1.764}, "timestamp": "2026-01-19T14:48:41.108657"}
{"image_index": 207, "image_name": "000000021604.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021604.jpg", "image_width": 512, "image_height": 640, "image_resolution": "512x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2291.096, "latencies_ms": [2291.096], "images_per_second": 0.436, "prompt_tokens": 1442, "response_tokens_est": 31, "n_tiles": 1, "output_text": " The man is wearing a black suit and tie, with a red tie that has a pattern of lights. The lighting is dim and the background is dark.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26847.7, "ram_available_mb": 98924.4, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26848.2, "ram_available_mb": 98923.9, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.19, "peak": 40.57, "min": 19.7}, "VIN": {"avg": 75.35, "peak": 117.96, "min": 27.32}, "VIN_SYS_5V0": {"avg": 15.11, "peak": 16.56, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 16.21, "peak": 17.71, "min": 13.0}}, "power_watts_avg": 31.19, "energy_joules_est": 71.47, "sample_count": 23, "duration_seconds": 2.291}, "timestamp": "2026-01-19T14:48:43.499879"}
{"image_index": 208, "image_name": "000000021839.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021839.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1656.206, "latencies_ms": [1656.206], "images_per_second": 0.604, "prompt_tokens": 1100, "response_tokens_est": 23, "n_tiles": 1, "output_text": " A man is walking across the street in front of a building with a sign that says \"TAFARINA\".", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 26848.2, "ram_available_mb": 98923.9, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 26848.2, "ram_available_mb": 98923.9, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.22, "peak": 40.56, "min": 16.56}, "VIN": {"avg": 68.95, "peak": 101.12, "min": 27.6}, "VIN_SYS_5V0": {"avg": 14.77, "peak": 16.05, "min": 11.87}, "VDD_CPU_SOC_MSS": {"avg": 15.91, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 30.22, "energy_joules_est": 50.06, "sample_count": 17, "duration_seconds": 1.657}, "timestamp": "2026-01-19T14:48:45.266930"}
{"image_index": 208, "image_name": "000000021839.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021839.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2699.588, "latencies_ms": [2699.588], "images_per_second": 0.37, "prompt_tokens": 1114, "response_tokens_est": 63, "n_tiles": 1, "output_text": " 1. person: 1\n2. traffic light: 1\n3. street light: 1\n4. building: 1\n5. sign: 1\n6. car: 1\n7. person's handbag: 1\n8. person's hand: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26848.2, "ram_available_mb": 98923.9, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26848.2, "ram_available_mb": 98924.0, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.73, "peak": 40.16, "min": 16.95}, "VIN": {"avg": 71.47, "peak": 128.34, "min": 29.76}, "VIN_SYS_5V0": {"avg": 14.84, "peak": 16.05, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.87, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 26.73, "energy_joules_est": 72.17, "sample_count": 27, "duration_seconds": 2.7}, "timestamp": "2026-01-19T14:48:48.071332"}
{"image_index": 208, "image_name": "000000021839.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021839.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2033.843, "latencies_ms": [2033.843], "images_per_second": 0.492, "prompt_tokens": 1118, "response_tokens_est": 37, "n_tiles": 1, "output_text": " The person is in the foreground, walking on the sidewalk, while the building is in the background. The traffic light is also in the background, and the street is in the foreground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26848.2, "ram_available_mb": 98924.0, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26848.2, "ram_available_mb": 98924.0, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.56, "peak": 39.78, "min": 14.98}, "VIN": {"avg": 75.54, "peak": 121.47, "min": 29.66}, "VIN_SYS_5V0": {"avg": 14.83, "peak": 16.05, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.87, "peak": 17.32, "min": 12.61}}, "power_watts_avg": 28.56, "energy_joules_est": 58.1, "sample_count": 20, "duration_seconds": 2.034}, "timestamp": "2026-01-19T14:48:50.154781"}
{"image_index": 208, "image_name": "000000021839.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021839.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1697.038, "latencies_ms": [1697.038], "images_per_second": 0.589, "prompt_tokens": 1112, "response_tokens_est": 25, "n_tiles": 1, "output_text": " A man is walking across the street at night in front of a building with a sign that says \"TAFARINA\".", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26848.2, "ram_available_mb": 98924.0, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26848.2, "ram_available_mb": 98924.0, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.22, "peak": 39.39, "min": 16.95}, "VIN": {"avg": 75.31, "peak": 118.46, "min": 29.43}, "VIN_SYS_5V0": {"avg": 15.08, "peak": 16.15, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 16.14, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 30.22, "energy_joules_est": 51.3, "sample_count": 17, "duration_seconds": 1.698}, "timestamp": "2026-01-19T14:48:51.932527"}
{"image_index": 208, "image_name": "000000021839.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021839.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1894.46, "latencies_ms": [1894.46], "images_per_second": 0.528, "prompt_tokens": 1110, "response_tokens_est": 32, "n_tiles": 1, "output_text": " The image is taken at night, with the sky being dark blue and the street being lit by streetlights. The building is white and has a curved facade.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26848.2, "ram_available_mb": 98924.0, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26848.2, "ram_available_mb": 98924.0, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.55, "peak": 40.18, "min": 17.35}, "VIN": {"avg": 65.42, "peak": 92.39, "min": 29.19}, "VIN_SYS_5V0": {"avg": 14.89, "peak": 16.15, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.99, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 29.55, "energy_joules_est": 55.99, "sample_count": 19, "duration_seconds": 1.895}, "timestamp": "2026-01-19T14:48:53.913775"}
{"image_index": 209, "image_name": "000000021879.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021879.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2525.649, "latencies_ms": [2525.649], "images_per_second": 0.396, "prompt_tokens": 1099, "response_tokens_est": 56, "n_tiles": 1, "output_text": " In the image, a young girl is skillfully riding a wave on a blue surfboard, while a man in a black wetsuit and a woman in a black bikini are also enjoying the water, with a man in a red and black wetsuit swimming nearby.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 26848.2, "ram_available_mb": 98924.0, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26848.2, "ram_available_mb": 98924.0, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.19, "peak": 40.57, "min": 16.56}, "VIN": {"avg": 68.33, "peak": 131.41, "min": 28.82}, "VIN_SYS_5V0": {"avg": 14.84, "peak": 16.05, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.87, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 27.19, "energy_joules_est": 68.69, "sample_count": 25, "duration_seconds": 2.526}, "timestamp": "2026-01-19T14:48:56.516176"}
{"image_index": 209, "image_name": "000000021879.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021879.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2621.434, "latencies_ms": [2621.434], "images_per_second": 0.381, "prompt_tokens": 1113, "response_tokens_est": 59, "n_tiles": 1, "output_text": " 1. girl: 1\n2. surfboard: 1\n3. wave: 1\n4. person: 2\n5. surfboard: 2\n6. person: 1\n7. surfboard: 1\n8. person: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26848.2, "ram_available_mb": 98924.0, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 26849.0, "ram_available_mb": 98923.2, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.72, "peak": 40.18, "min": 15.77}, "VIN": {"avg": 63.19, "peak": 114.13, "min": 27.17}, "VIN_SYS_5V0": {"avg": 14.84, "peak": 16.05, "min": 12.25}, "VDD_CPU_SOC_MSS": {"avg": 15.96, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 26.72, "energy_joules_est": 70.05, "sample_count": 26, "duration_seconds": 2.622}, "timestamp": "2026-01-19T14:48:59.224566"}
{"image_index": 209, "image_name": "000000021879.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021879.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2522.084, "latencies_ms": [2522.084], "images_per_second": 0.396, "prompt_tokens": 1117, "response_tokens_est": 53, "n_tiles": 1, "output_text": " The main object, a girl on a surfboard, is in the foreground, with the ocean waves and other surfers in the background. The girl is positioned to the left of the image, while the surfers are scattered across the right side of the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26849.0, "ram_available_mb": 98923.2, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26849.0, "ram_available_mb": 98923.2, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.82, "peak": 40.18, "min": 14.98}, "VIN": {"avg": 68.03, "peak": 127.06, "min": 28.65}, "VIN_SYS_5V0": {"avg": 14.79, "peak": 15.95, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 15.72, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 26.82, "energy_joules_est": 67.65, "sample_count": 25, "duration_seconds": 2.522}, "timestamp": "2026-01-19T14:49:01.809791"}
{"image_index": 209, "image_name": "000000021879.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021879.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2284.297, "latencies_ms": [2284.297], "images_per_second": 0.438, "prompt_tokens": 1111, "response_tokens_est": 46, "n_tiles": 1, "output_text": " In the image, a young girl is surfing on a blue surfboard in the ocean. She is wearing a bikini and is riding a wave. There are other people in the background, some of them are also surfing.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26849.0, "ram_available_mb": 98923.2, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 26849.0, "ram_available_mb": 98923.2, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.58, "peak": 40.16, "min": 15.38}, "VIN": {"avg": 73.6, "peak": 128.56, "min": 29.88}, "VIN_SYS_5V0": {"avg": 14.86, "peak": 16.05, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.95, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 27.58, "energy_joules_est": 63.01, "sample_count": 23, "duration_seconds": 2.285}, "timestamp": "2026-01-19T14:49:04.198450"}
{"image_index": 209, "image_name": "000000021879.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021879.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2354.695, "latencies_ms": [2354.695], "images_per_second": 0.425, "prompt_tokens": 1109, "response_tokens_est": 49, "n_tiles": 1, "output_text": " The image captures a vibrant scene of a young girl surfing on a blue surfboard in the ocean. The water is a deep blue, reflecting the clear sky above. The sunlight filters through the water, creating a shimmering effect on the waves.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26849.0, "ram_available_mb": 98923.2, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 26848.9, "ram_available_mb": 98923.2, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.7, "peak": 40.16, "min": 15.38}, "VIN": {"avg": 74.2, "peak": 122.14, "min": 28.99}, "VIN_SYS_5V0": {"avg": 14.86, "peak": 16.05, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.99, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 27.7, "energy_joules_est": 65.24, "sample_count": 23, "duration_seconds": 2.355}, "timestamp": "2026-01-19T14:49:06.580671"}
{"image_index": 210, "image_name": "000000021903.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021903.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1786.891, "latencies_ms": [1786.891], "images_per_second": 0.56, "prompt_tokens": 1099, "response_tokens_est": 28, "n_tiles": 1, "output_text": " A man in a white shirt and beige pants is feeding an elephant with a stick in a fenced area with trees in the background.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 26848.9, "ram_available_mb": 98923.2, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26848.9, "ram_available_mb": 98923.2, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.98, "peak": 40.18, "min": 16.95}, "VIN": {"avg": 74.48, "peak": 125.15, "min": 29.93}, "VIN_SYS_5V0": {"avg": 15.1, "peak": 16.15, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 16.21, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 29.98, "energy_joules_est": 53.59, "sample_count": 18, "duration_seconds": 1.787}, "timestamp": "2026-01-19T14:49:08.454726"}
{"image_index": 210, "image_name": "000000021903.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021903.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1694.315, "latencies_ms": [1694.315], "images_per_second": 0.59, "prompt_tokens": 1113, "response_tokens_est": 24, "n_tiles": 1, "output_text": " elephant: 1, man: 1, fence: 1, tree: 1, person: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26848.9, "ram_available_mb": 98923.2, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 26848.9, "ram_available_mb": 98923.2, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.59, "peak": 40.56, "min": 17.35}, "VIN": {"avg": 71.3, "peak": 104.04, "min": 29.01}, "VIN_SYS_5V0": {"avg": 14.9, "peak": 16.05, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.91, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 30.59, "energy_joules_est": 51.84, "sample_count": 17, "duration_seconds": 1.695}, "timestamp": "2026-01-19T14:49:10.215833"}
{"image_index": 210, "image_name": "000000021903.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021903.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2514.112, "latencies_ms": [2514.112], "images_per_second": 0.398, "prompt_tokens": 1117, "response_tokens_est": 55, "n_tiles": 1, "output_text": " The man is standing on the left side of the image, while the elephant is on the right side. The man is in the foreground, while the elephant is in the background. The man is holding the food out to the elephant, which is reaching out to take it.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26848.9, "ram_available_mb": 98923.2, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 26848.9, "ram_available_mb": 98923.2, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.53, "peak": 40.56, "min": 18.13}, "VIN": {"avg": 69.58, "peak": 125.4, "min": 30.1}, "VIN_SYS_5V0": {"avg": 14.92, "peak": 16.05, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 16.03, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 27.53, "energy_joules_est": 69.22, "sample_count": 25, "duration_seconds": 2.514}, "timestamp": "2026-01-19T14:49:12.808204"}
{"image_index": 210, "image_name": "000000021903.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021903.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1513.454, "latencies_ms": [1513.454], "images_per_second": 0.661, "prompt_tokens": 1111, "response_tokens_est": 17, "n_tiles": 1, "output_text": " A man is feeding an elephant in a fenced area with trees in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26848.9, "ram_available_mb": 98923.2, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 26848.9, "ram_available_mb": 98923.2, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.07, "peak": 40.57, "min": 15.38}, "VIN": {"avg": 74.86, "peak": 108.04, "min": 28.77}, "VIN_SYS_5V0": {"avg": 14.94, "peak": 16.05, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 16.01, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 31.07, "energy_joules_est": 47.04, "sample_count": 15, "duration_seconds": 1.514}, "timestamp": "2026-01-19T14:49:14.364375"}
{"image_index": 210, "image_name": "000000021903.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021903.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1505.894, "latencies_ms": [1505.894], "images_per_second": 0.664, "prompt_tokens": 1109, "response_tokens_est": 17, "n_tiles": 1, "output_text": " The elephant is gray, the man is wearing white, and the sky is blue.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26848.9, "ram_available_mb": 98923.2, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 26849.2, "ram_available_mb": 98923.0, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.62, "peak": 40.95, "min": 20.49}, "VIN": {"avg": 77.42, "peak": 123.89, "min": 29.7}, "VIN_SYS_5V0": {"avg": 15.13, "peak": 16.15, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 16.22, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 32.62, "energy_joules_est": 49.13, "sample_count": 15, "duration_seconds": 1.506}, "timestamp": "2026-01-19T14:49:15.930669"}
{"image_index": 211, "image_name": "000000022192.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022192.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1631.895, "latencies_ms": [1631.895], "images_per_second": 0.613, "prompt_tokens": 1099, "response_tokens_est": 21, "n_tiles": 1, "output_text": " A brown dog sits on a bed covered in clothes and other items, with a box of tissues nearby.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26849.2, "ram_available_mb": 98923.0, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 26849.2, "ram_available_mb": 98923.0, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.73, "peak": 40.56, "min": 20.1}, "VIN": {"avg": 71.93, "peak": 119.65, "min": 30.44}, "VIN_SYS_5V0": {"avg": 14.95, "peak": 16.05, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 16.04, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 31.73, "energy_joules_est": 51.79, "sample_count": 16, "duration_seconds": 1.632}, "timestamp": "2026-01-19T14:49:17.597937"}
{"image_index": 211, "image_name": "000000022192.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022192.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1956.165, "latencies_ms": [1956.165], "images_per_second": 0.511, "prompt_tokens": 1113, "response_tokens_est": 34, "n_tiles": 1, "output_text": " dog: 1, box: 1, clothes: 1, bag: 1, blanket: 1, pillow: 1, curtain: 1", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 26849.2, "ram_available_mb": 98923.0, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26849.2, "ram_available_mb": 98923.0, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.27, "peak": 40.95, "min": 19.7}, "VIN": {"avg": 73.5, "peak": 114.41, "min": 29.46}, "VIN_SYS_5V0": {"avg": 15.11, "peak": 16.15, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 16.22, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 30.27, "energy_joules_est": 59.23, "sample_count": 19, "duration_seconds": 1.957}, "timestamp": "2026-01-19T14:49:19.583261"}
{"image_index": 211, "image_name": "000000022192.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022192.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2274.43, "latencies_ms": [2274.43], "images_per_second": 0.44, "prompt_tokens": 1117, "response_tokens_est": 46, "n_tiles": 1, "output_text": " The dog is sitting on the bed, which is in the foreground of the image. The bed is covered with clothes and a box, which are in the middle of the image. The curtain is in the background of the image.", "error": null, "sys_before": {"cpu_percent": 40.0, "ram_used_mb": 26849.2, "ram_available_mb": 98923.0, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26849.2, "ram_available_mb": 98923.0, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.6, "peak": 40.56, "min": 17.74}, "VIN": {"avg": 70.02, "peak": 106.88, "min": 29.06}, "VIN_SYS_5V0": {"avg": 15.13, "peak": 16.15, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 16.09, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 28.6, "energy_joules_est": 65.06, "sample_count": 22, "duration_seconds": 2.275}, "timestamp": "2026-01-19T14:49:21.873183"}
{"image_index": 211, "image_name": "000000022192.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022192.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1460.42, "latencies_ms": [1460.42], "images_per_second": 0.685, "prompt_tokens": 1111, "response_tokens_est": 15, "n_tiles": 1, "output_text": " A brown dog sits on a bed with clothes and a box on it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26849.2, "ram_available_mb": 98923.0, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 26849.2, "ram_available_mb": 98923.0, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.49, "peak": 40.56, "min": 17.34}, "VIN": {"avg": 78.21, "peak": 120.58, "min": 28.71}, "VIN_SYS_5V0": {"avg": 15.16, "peak": 16.15, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 16.19, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 31.49, "energy_joules_est": 46.0, "sample_count": 15, "duration_seconds": 1.461}, "timestamp": "2026-01-19T14:49:23.437424"}
{"image_index": 211, "image_name": "000000022192.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022192.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1548.252, "latencies_ms": [1548.252], "images_per_second": 0.646, "prompt_tokens": 1109, "response_tokens_est": 18, "n_tiles": 1, "output_text": " The dog is brown and white, the bed is white, and the curtains are white.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26849.2, "ram_available_mb": 98923.0, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 26849.2, "ram_available_mb": 98923.0, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.15, "peak": 40.97, "min": 18.92}, "VIN": {"avg": 70.05, "peak": 114.44, "min": 28.87}, "VIN_SYS_5V0": {"avg": 14.88, "peak": 16.05, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.93, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 32.15, "energy_joules_est": 49.78, "sample_count": 15, "duration_seconds": 1.549}, "timestamp": "2026-01-19T14:49:24.996748"}
{"image_index": 212, "image_name": "000000022371.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022371.jpg", "image_width": 425, "image_height": 282, "image_resolution": "425x282", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2040.168, "latencies_ms": [2040.168], "images_per_second": 0.49, "prompt_tokens": 1099, "response_tokens_est": 38, "n_tiles": 1, "output_text": " A man wearing a blue tie and a white shirt is sitting at a desk with a laptop and a pen in his hand, and he is looking at the laptop screen with a thoughtful expression.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26849.2, "ram_available_mb": 98923.0, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26849.2, "ram_available_mb": 98923.0, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.1, "peak": 40.57, "min": 20.1}, "VIN": {"avg": 74.93, "peak": 126.61, "min": 29.96}, "VIN_SYS_5V0": {"avg": 15.23, "peak": 16.26, "min": 13.39}, "VDD_CPU_SOC_MSS": {"avg": 16.32, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 30.1, "energy_joules_est": 61.42, "sample_count": 20, "duration_seconds": 2.041}, "timestamp": "2026-01-19T14:49:27.086318"}
{"image_index": 212, "image_name": "000000022371.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022371.jpg", "image_width": 425, "image_height": 282, "image_resolution": "425x282", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1956.035, "latencies_ms": [1956.035], "images_per_second": 0.511, "prompt_tokens": 1113, "response_tokens_est": 34, "n_tiles": 1, "output_text": " laptop: 1, pen: 1, paper: 1, glasses: 1, shirt: 1, tie: 1, man: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26849.2, "ram_available_mb": 98923.0, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26849.2, "ram_available_mb": 98923.0, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.67, "peak": 40.56, "min": 16.95}, "VIN": {"avg": 69.84, "peak": 107.29, "min": 29.08}, "VIN_SYS_5V0": {"avg": 15.08, "peak": 16.15, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 16.12, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 29.67, "energy_joules_est": 58.06, "sample_count": 19, "duration_seconds": 1.957}, "timestamp": "2026-01-19T14:49:29.055382"}
{"image_index": 212, "image_name": "000000022371.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022371.jpg", "image_width": 425, "image_height": 282, "image_resolution": "425x282", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2576.96, "latencies_ms": [2576.96], "images_per_second": 0.388, "prompt_tokens": 1117, "response_tokens_est": 58, "n_tiles": 1, "output_text": " The man is seated at a desk with a laptop in front of him, which is to his left. The laptop is positioned in the foreground of the image, while the man is in the background. The man is also holding a pen in his right hand, which is near the laptop.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26849.2, "ram_available_mb": 98923.0, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26849.2, "ram_available_mb": 98923.0, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.66, "peak": 40.56, "min": 18.14}, "VIN": {"avg": 74.63, "peak": 127.41, "min": 29.29}, "VIN_SYS_5V0": {"avg": 15.17, "peak": 16.15, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 16.16, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 27.66, "energy_joules_est": 71.3, "sample_count": 25, "duration_seconds": 2.578}, "timestamp": "2026-01-19T14:49:31.656078"}
{"image_index": 212, "image_name": "000000022371.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022371.jpg", "image_width": 425, "image_height": 282, "image_resolution": "425x282", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2033.076, "latencies_ms": [2033.076], "images_per_second": 0.492, "prompt_tokens": 1111, "response_tokens_est": 37, "n_tiles": 1, "output_text": " A man wearing a blue tie and a white shirt is sitting at a desk with a laptop and a pen in his hand. He is looking at the laptop screen with a thoughtful expression.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26849.2, "ram_available_mb": 98923.0, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26849.2, "ram_available_mb": 98923.0, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.04, "peak": 40.56, "min": 16.56}, "VIN": {"avg": 73.3, "peak": 120.8, "min": 29.02}, "VIN_SYS_5V0": {"avg": 15.14, "peak": 16.15, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 16.14, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 29.04, "energy_joules_est": 59.05, "sample_count": 20, "duration_seconds": 2.033}, "timestamp": "2026-01-19T14:49:33.740451"}
{"image_index": 212, "image_name": "000000022371.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022371.jpg", "image_width": 425, "image_height": 282, "image_resolution": "425x282", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1602.335, "latencies_ms": [1602.335], "images_per_second": 0.624, "prompt_tokens": 1109, "response_tokens_est": 20, "n_tiles": 1, "output_text": " The man is wearing a blue tie and a white shirt. The background is a light blue color.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26849.2, "ram_available_mb": 98923.0, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 26849.2, "ram_available_mb": 98923.0, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.9, "peak": 40.18, "min": 16.95}, "VIN": {"avg": 72.78, "peak": 126.49, "min": 28.69}, "VIN_SYS_5V0": {"avg": 14.96, "peak": 16.05, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.99, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 30.9, "energy_joules_est": 49.52, "sample_count": 16, "duration_seconds": 1.603}, "timestamp": "2026-01-19T14:49:35.401496"}
{"image_index": 213, "image_name": "000000022396.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022396.jpg", "image_width": 640, "image_height": 495, "image_resolution": "640x495", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1867.038, "latencies_ms": [1867.038], "images_per_second": 0.536, "prompt_tokens": 1432, "response_tokens_est": 15, "n_tiles": 1, "output_text": " A large airplane is flying in the sky with a moon in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26849.2, "ram_available_mb": 98923.0, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.5, "ram_used_mb": 26849.2, "ram_available_mb": 98923.0, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.2, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.98, "peak": 40.56, "min": 19.32}, "VIN": {"avg": 78.54, "peak": 123.76, "min": 29.09}, "VIN_SYS_5V0": {"avg": 15.44, "peak": 16.76, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 16.53, "peak": 17.71, "min": 13.39}}, "power_watts_avg": 32.98, "energy_joules_est": 61.59, "sample_count": 18, "duration_seconds": 1.867}, "timestamp": "2026-01-19T14:49:37.283118"}
{"image_index": 213, "image_name": "000000022396.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022396.jpg", "image_width": 640, "image_height": 495, "image_resolution": "640x495", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1535.776, "latencies_ms": [1535.776], "images_per_second": 0.651, "prompt_tokens": 1446, "response_tokens_est": 4, "n_tiles": 1, "output_text": " airplane: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26849.2, "ram_available_mb": 98923.0, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 1.8, "ram_used_mb": 26849.2, "ram_available_mb": 98923.0, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 35.16, "peak": 40.56, "min": 21.67}, "VIN": {"avg": 83.1, "peak": 132.34, "min": 30.07}, "VIN_SYS_5V0": {"avg": 15.69, "peak": 16.76, "min": 13.49}, "VDD_CPU_SOC_MSS": {"avg": 16.85, "peak": 18.11, "min": 14.57}}, "power_watts_avg": 35.16, "energy_joules_est": 54.01, "sample_count": 15, "duration_seconds": 1.536}, "timestamp": "2026-01-19T14:49:38.853121"}
{"image_index": 213, "image_name": "000000022396.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022396.jpg", "image_width": 640, "image_height": 495, "image_resolution": "640x495", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2439.936, "latencies_ms": [2439.936], "images_per_second": 0.41, "prompt_tokens": 1450, "response_tokens_est": 37, "n_tiles": 1, "output_text": " The airplane is in the background, flying high in the sky, while the moon is in the foreground, closer to the camera. The airplane is flying to the right of the moon.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26849.2, "ram_available_mb": 98923.0, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 26849.2, "ram_available_mb": 98923.0, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.2, "peak": 41.34, "min": 20.1}, "VIN": {"avg": 71.48, "peak": 121.46, "min": 30.36}, "VIN_SYS_5V0": {"avg": 15.42, "peak": 16.76, "min": 13.39}, "VDD_CPU_SOC_MSS": {"avg": 16.45, "peak": 17.71, "min": 14.57}}, "power_watts_avg": 32.2, "energy_joules_est": 78.58, "sample_count": 24, "duration_seconds": 2.44}, "timestamp": "2026-01-19T14:49:41.354255"}
{"image_index": 213, "image_name": "000000022396.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022396.jpg", "image_width": 640, "image_height": 495, "image_resolution": "640x495", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1846.548, "latencies_ms": [1846.548], "images_per_second": 0.542, "prompt_tokens": 1444, "response_tokens_est": 15, "n_tiles": 1, "output_text": " A plane is flying in the sky and a moon is in the sky.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26849.2, "ram_available_mb": 98923.0, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 26849.2, "ram_available_mb": 98923.0, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.69, "peak": 40.18, "min": 16.95}, "VIN": {"avg": 77.56, "peak": 117.46, "min": 28.81}, "VIN_SYS_5V0": {"avg": 15.31, "peak": 16.56, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 16.49, "peak": 17.71, "min": 13.39}}, "power_watts_avg": 32.69, "energy_joules_est": 60.37, "sample_count": 18, "duration_seconds": 1.847}, "timestamp": "2026-01-19T14:49:43.235049"}
{"image_index": 213, "image_name": "000000022396.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022396.jpg", "image_width": 640, "image_height": 495, "image_resolution": "640x495", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1925.998, "latencies_ms": [1925.998], "images_per_second": 0.519, "prompt_tokens": 1442, "response_tokens_est": 18, "n_tiles": 1, "output_text": " The airplane is white with red and blue on the tail, and the moon is orange.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26849.2, "ram_available_mb": 98923.0, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.5, "ram_used_mb": 26849.2, "ram_available_mb": 98923.0, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 33.34, "peak": 40.56, "min": 21.28}, "VIN": {"avg": 77.02, "peak": 121.21, "min": 28.63}, "VIN_SYS_5V0": {"avg": 15.44, "peak": 16.66, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 16.6, "peak": 17.71, "min": 14.18}}, "power_watts_avg": 33.34, "energy_joules_est": 64.22, "sample_count": 19, "duration_seconds": 1.926}, "timestamp": "2026-01-19T14:49:45.206722"}
{"image_index": 214, "image_name": "000000022479.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022479.jpg", "image_width": 640, "image_height": 605, "image_resolution": "640x605", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2161.233, "latencies_ms": [2161.233], "images_per_second": 0.463, "prompt_tokens": 1432, "response_tokens_est": 26, "n_tiles": 1, "output_text": " A young man wearing a tie-dye shirt and black pants is performing a trick on a skateboard in a skate park.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 26849.2, "ram_available_mb": 98923.0, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 26849.4, "ram_available_mb": 98922.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.2, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.85, "peak": 40.56, "min": 20.1}, "VIN": {"avg": 74.34, "peak": 119.94, "min": 30.02}, "VIN_SYS_5V0": {"avg": 15.48, "peak": 16.76, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 16.52, "peak": 17.71, "min": 13.79}}, "power_watts_avg": 31.85, "energy_joules_est": 68.85, "sample_count": 21, "duration_seconds": 2.162}, "timestamp": "2026-01-19T14:49:47.405641"}
{"image_index": 214, "image_name": "000000022479.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022479.jpg", "image_width": 640, "image_height": 605, "image_resolution": "640x605", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3092.77, "latencies_ms": [3092.77], "images_per_second": 0.323, "prompt_tokens": 1446, "response_tokens_est": 61, "n_tiles": 1, "output_text": " 1. skateboard: 1\n2. person: 1\n3. palm tree: 2\n4. building: 1\n5. playground equipment: 1\n6. skateboard ramp: 1\n7. clouds: 1\n8. sky: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26849.4, "ram_available_mb": 98922.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26849.4, "ram_available_mb": 98922.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.59, "peak": 40.56, "min": 18.52}, "VIN": {"avg": 71.22, "peak": 117.86, "min": 30.01}, "VIN_SYS_5V0": {"avg": 15.24, "peak": 16.66, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 16.26, "peak": 17.71, "min": 13.79}}, "power_watts_avg": 28.59, "energy_joules_est": 88.43, "sample_count": 30, "duration_seconds": 3.093}, "timestamp": "2026-01-19T14:49:50.532511"}
{"image_index": 214, "image_name": "000000022479.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022479.jpg", "image_width": 640, "image_height": 605, "image_resolution": "640x605", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2635.351, "latencies_ms": [2635.351], "images_per_second": 0.379, "prompt_tokens": 1450, "response_tokens_est": 42, "n_tiles": 1, "output_text": " The skateboarder is in the foreground, performing a trick in the air, while the skate park and palm trees are in the background. The skateboarder is closer to the camera than the palm trees.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26849.4, "ram_available_mb": 98922.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26849.4, "ram_available_mb": 98922.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.3, "peak": 40.16, "min": 16.56}, "VIN": {"avg": 72.37, "peak": 119.79, "min": 30.51}, "VIN_SYS_5V0": {"avg": 15.18, "peak": 16.66, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 16.19, "peak": 17.71, "min": 13.79}}, "power_watts_avg": 29.3, "energy_joules_est": 77.23, "sample_count": 26, "duration_seconds": 2.636}, "timestamp": "2026-01-19T14:49:53.232699"}
{"image_index": 214, "image_name": "000000022479.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022479.jpg", "image_width": 640, "image_height": 605, "image_resolution": "640x605", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1937.244, "latencies_ms": [1937.244], "images_per_second": 0.516, "prompt_tokens": 1444, "response_tokens_est": 18, "n_tiles": 1, "output_text": " A young man wearing a tie dye shirt is skateboarding on a ramp in a park.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26849.4, "ram_available_mb": 98922.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.4, "ram_used_mb": 26849.4, "ram_available_mb": 98922.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.09, "peak": 40.16, "min": 16.56}, "VIN": {"avg": 75.58, "peak": 123.66, "min": 29.45}, "VIN_SYS_5V0": {"avg": 15.2, "peak": 16.56, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 16.31, "peak": 17.71, "min": 13.0}}, "power_watts_avg": 32.09, "energy_joules_est": 62.18, "sample_count": 19, "duration_seconds": 1.938}, "timestamp": "2026-01-19T14:49:55.219100"}
{"image_index": 214, "image_name": "000000022479.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022479.jpg", "image_width": 640, "image_height": 605, "image_resolution": "640x605", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2176.795, "latencies_ms": [2176.795], "images_per_second": 0.459, "prompt_tokens": 1442, "response_tokens_est": 27, "n_tiles": 1, "output_text": " The skateboarder is wearing a tie-dye shirt and black pants. The skate park is surrounded by palm trees and buildings.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26849.4, "ram_available_mb": 98922.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 26849.6, "ram_available_mb": 98922.5, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.24, "peak": 40.56, "min": 20.1}, "VIN": {"avg": 72.39, "peak": 118.43, "min": 34.86}, "VIN_SYS_5V0": {"avg": 15.35, "peak": 16.66, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 16.5, "peak": 17.71, "min": 13.79}}, "power_watts_avg": 32.24, "energy_joules_est": 70.19, "sample_count": 21, "duration_seconds": 2.177}, "timestamp": "2026-01-19T14:49:57.403989"}
{"image_index": 215, "image_name": "000000022589.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022589.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1696.447, "latencies_ms": [1696.447], "images_per_second": 0.589, "prompt_tokens": 1099, "response_tokens_est": 25, "n_tiles": 1, "output_text": " In the image, a sheep with a fluffy coat is standing in a grassy field, surrounded by trees and a wire fence.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26849.6, "ram_available_mb": 98922.5, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26849.6, "ram_available_mb": 98922.5, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.31, "peak": 40.18, "min": 20.11}, "VIN": {"avg": 70.7, "peak": 103.88, "min": 28.82}, "VIN_SYS_5V0": {"avg": 15.32, "peak": 16.26, "min": 13.39}, "VDD_CPU_SOC_MSS": {"avg": 16.35, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 31.31, "energy_joules_est": 53.13, "sample_count": 17, "duration_seconds": 1.697}, "timestamp": "2026-01-19T14:49:59.180998"}
{"image_index": 215, "image_name": "000000022589.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022589.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2704.341, "latencies_ms": [2704.341], "images_per_second": 0.37, "prompt_tokens": 1113, "response_tokens_est": 56, "n_tiles": 1, "output_text": " 1. sheep: 1\n2. wire: 1\n3. grass: 1\n4. trees: 1\n5. fence: 1\n6. wire: 1\n7. grass: 1\n8. trees: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26849.4, "ram_available_mb": 98922.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 26849.4, "ram_available_mb": 98922.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.66, "peak": 40.56, "min": 18.14}, "VIN": {"avg": 68.67, "peak": 119.67, "min": 32.89}, "VIN_SYS_5V0": {"avg": 14.7, "peak": 16.05, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.6, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 26.66, "energy_joules_est": 72.11, "sample_count": 26, "duration_seconds": 2.705}, "timestamp": "2026-01-19T14:50:01.894573"}
{"image_index": 215, "image_name": "000000022589.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022589.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1893.689, "latencies_ms": [1893.689], "images_per_second": 0.528, "prompt_tokens": 1117, "response_tokens_est": 31, "n_tiles": 1, "output_text": " The sheep is in the foreground, with the wire fence in front of it. The sheep is behind the fence, and the trees are in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26849.4, "ram_available_mb": 98922.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26849.4, "ram_available_mb": 98922.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.44, "peak": 39.78, "min": 16.95}, "VIN": {"avg": 68.21, "peak": 104.75, "min": 28.12}, "VIN_SYS_5V0": {"avg": 15.11, "peak": 16.15, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 16.08, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 29.44, "energy_joules_est": 55.76, "sample_count": 19, "duration_seconds": 1.894}, "timestamp": "2026-01-19T14:50:03.881683"}
{"image_index": 215, "image_name": "000000022589.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022589.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1629.561, "latencies_ms": [1629.561], "images_per_second": 0.614, "prompt_tokens": 1111, "response_tokens_est": 21, "n_tiles": 1, "output_text": " A sheep with a fluffy coat is standing in a fenced-in area with trees in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26849.4, "ram_available_mb": 98922.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 26849.4, "ram_available_mb": 98922.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.68, "peak": 40.16, "min": 16.16}, "VIN": {"avg": 75.88, "peak": 126.6, "min": 28.76}, "VIN_SYS_5V0": {"avg": 14.78, "peak": 16.05, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.75, "peak": 17.32, "min": 12.61}}, "power_watts_avg": 30.68, "energy_joules_est": 50.01, "sample_count": 16, "duration_seconds": 1.63}, "timestamp": "2026-01-19T14:50:05.557346"}
{"image_index": 215, "image_name": "000000022589.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022589.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1504.849, "latencies_ms": [1504.849], "images_per_second": 0.665, "prompt_tokens": 1109, "response_tokens_est": 17, "n_tiles": 1, "output_text": " The sheep is white and fluffy, and the photo was taken on a sunny day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26849.4, "ram_available_mb": 98922.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 26849.4, "ram_available_mb": 98922.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.27, "peak": 40.95, "min": 19.32}, "VIN": {"avg": 80.26, "peak": 126.15, "min": 28.98}, "VIN_SYS_5V0": {"avg": 15.07, "peak": 16.15, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 16.14, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 32.27, "energy_joules_est": 48.57, "sample_count": 15, "duration_seconds": 1.505}, "timestamp": "2026-01-19T14:50:07.128261"}
{"image_index": 216, "image_name": "000000022623.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022623.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2060.898, "latencies_ms": [2060.898], "images_per_second": 0.485, "prompt_tokens": 1099, "response_tokens_est": 37, "n_tiles": 1, "output_text": " The image shows a close-up of a silver electronic device with a red button and a white button, with the word \"WOW\" written in red on the bottom right corner.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26849.4, "ram_available_mb": 98922.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26849.4, "ram_available_mb": 98922.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.7, "peak": 40.95, "min": 20.1}, "VIN": {"avg": 66.51, "peak": 104.5, "min": 28.91}, "VIN_SYS_5V0": {"avg": 14.94, "peak": 16.05, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.92, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 29.7, "energy_joules_est": 61.23, "sample_count": 20, "duration_seconds": 2.062}, "timestamp": "2026-01-19T14:50:09.223701"}
{"image_index": 216, "image_name": "000000022623.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022623.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2834.508, "latencies_ms": [2834.508], "images_per_second": 0.353, "prompt_tokens": 1113, "response_tokens_est": 68, "n_tiles": 1, "output_text": " 1. white button: 2\n2. red button: 1\n3. silver button: 1\n4. black button: 1\n5. white arrow button: 1\n6. white circle button: 1\n7. silver circle button: 1\n8. red circle button: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26849.4, "ram_available_mb": 98922.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 26849.4, "ram_available_mb": 98922.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.59, "peak": 40.18, "min": 17.73}, "VIN": {"avg": 63.04, "peak": 123.79, "min": 28.11}, "VIN_SYS_5V0": {"avg": 15.04, "peak": 16.15, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 16.07, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 26.59, "energy_joules_est": 75.38, "sample_count": 28, "duration_seconds": 2.835}, "timestamp": "2026-01-19T14:50:12.141367"}
{"image_index": 216, "image_name": "000000022623.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022623.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2306.498, "latencies_ms": [2306.498], "images_per_second": 0.434, "prompt_tokens": 1117, "response_tokens_est": 47, "n_tiles": 1, "output_text": " The main objects are located in the foreground of the image, with the left side of the device being closer to the camera than the right side. The device is positioned on a dark surface, which is in the background of the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26849.4, "ram_available_mb": 98922.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26849.4, "ram_available_mb": 98922.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.51, "peak": 40.16, "min": 14.98}, "VIN": {"avg": 71.5, "peak": 106.63, "min": 29.06}, "VIN_SYS_5V0": {"avg": 14.85, "peak": 16.05, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.92, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 27.51, "energy_joules_est": 63.46, "sample_count": 23, "duration_seconds": 2.307}, "timestamp": "2026-01-19T14:50:14.542638"}
{"image_index": 216, "image_name": "000000022623.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022623.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2581.849, "latencies_ms": [2581.849], "images_per_second": 0.387, "prompt_tokens": 1111, "response_tokens_est": 57, "n_tiles": 1, "output_text": " The image captures a close-up view of a silver electronic device, possibly a portable media player, with a prominent red button and a circular button with a metallic finish. The device is placed on a dark surface, and the background is blurred, drawing focus to the device's features.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26849.4, "ram_available_mb": 98922.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26849.4, "ram_available_mb": 98922.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.9, "peak": 40.16, "min": 15.38}, "VIN": {"avg": 64.34, "peak": 88.9, "min": 32.01}, "VIN_SYS_5V0": {"avg": 14.82, "peak": 15.95, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.76, "peak": 16.92, "min": 13.0}}, "power_watts_avg": 26.9, "energy_joules_est": 69.47, "sample_count": 25, "duration_seconds": 2.582}, "timestamp": "2026-01-19T14:50:17.132534"}
{"image_index": 216, "image_name": "000000022623.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022623.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1628.601, "latencies_ms": [1628.601], "images_per_second": 0.614, "prompt_tokens": 1109, "response_tokens_est": 22, "n_tiles": 1, "output_text": " The device is silver with a red button and a white button. The device is illuminated by a light source.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26849.4, "ram_available_mb": 98922.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26849.4, "ram_available_mb": 98922.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.1, "peak": 39.77, "min": 17.34}, "VIN": {"avg": 72.6, "peak": 109.94, "min": 30.28}, "VIN_SYS_5V0": {"avg": 15.27, "peak": 16.26, "min": 13.29}, "VDD_CPU_SOC_MSS": {"avg": 16.31, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 31.1, "energy_joules_est": 50.66, "sample_count": 16, "duration_seconds": 1.629}, "timestamp": "2026-01-19T14:50:18.801717"}
{"image_index": 217, "image_name": "000000022705.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022705.jpg", "image_width": 482, "image_height": 640, "image_resolution": "482x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2034.718, "latencies_ms": [2034.718], "images_per_second": 0.491, "prompt_tokens": 1432, "response_tokens_est": 21, "n_tiles": 1, "output_text": " A woman wearing a black dress and black heels is standing in a kitchen holding a glass of orange juice.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 26849.4, "ram_available_mb": 98922.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 26849.4, "ram_available_mb": 98922.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.2, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.15, "peak": 40.57, "min": 19.32}, "VIN": {"avg": 74.33, "peak": 121.84, "min": 28.9}, "VIN_SYS_5V0": {"avg": 15.45, "peak": 16.86, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 16.52, "peak": 18.11, "min": 13.79}}, "power_watts_avg": 32.15, "energy_joules_est": 65.43, "sample_count": 20, "duration_seconds": 2.035}, "timestamp": "2026-01-19T14:50:20.877289"}
{"image_index": 217, "image_name": "000000022705.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022705.jpg", "image_width": 482, "image_height": 640, "image_resolution": "482x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2953.767, "latencies_ms": [2953.767], "images_per_second": 0.339, "prompt_tokens": 1446, "response_tokens_est": 56, "n_tiles": 1, "output_text": " 1. woman: 1\n2. glass: 1\n3. bottle: 1\n4. refrigerator: 1\n5. cabinet: 1\n6. counter: 1\n7. floor: 1\n8. wall: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26849.4, "ram_available_mb": 98922.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26849.4, "ram_available_mb": 98922.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.03, "peak": 40.56, "min": 18.52}, "VIN": {"avg": 70.29, "peak": 123.5, "min": 28.95}, "VIN_SYS_5V0": {"avg": 15.26, "peak": 16.66, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 16.25, "peak": 17.71, "min": 13.79}}, "power_watts_avg": 29.03, "energy_joules_est": 85.76, "sample_count": 29, "duration_seconds": 2.954}, "timestamp": "2026-01-19T14:50:23.897988"}
{"image_index": 217, "image_name": "000000022705.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022705.jpg", "image_width": 482, "image_height": 640, "image_resolution": "482x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2532.904, "latencies_ms": [2532.904], "images_per_second": 0.395, "prompt_tokens": 1450, "response_tokens_est": 40, "n_tiles": 1, "output_text": " The woman is standing in the foreground of the image, with the refrigerator in the background. The woman is holding a glass of orange juice in her right hand, and the refrigerator is to her left.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26849.4, "ram_available_mb": 98922.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26849.4, "ram_available_mb": 98922.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.77, "peak": 40.16, "min": 16.16}, "VIN": {"avg": 71.99, "peak": 117.14, "min": 28.84}, "VIN_SYS_5V0": {"avg": 15.14, "peak": 16.56, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 16.2, "peak": 17.71, "min": 13.0}}, "power_watts_avg": 29.77, "energy_joules_est": 75.42, "sample_count": 25, "duration_seconds": 2.533}, "timestamp": "2026-01-19T14:50:26.486357"}
{"image_index": 217, "image_name": "000000022705.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022705.jpg", "image_width": 482, "image_height": 640, "image_resolution": "482x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2011.0, "latencies_ms": [2011.0], "images_per_second": 0.497, "prompt_tokens": 1444, "response_tokens_est": 20, "n_tiles": 1, "output_text": " A woman wearing a black dress and heels is standing in a kitchen holding a glass of champagne.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26849.4, "ram_available_mb": 98922.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 26849.4, "ram_available_mb": 98922.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.83, "peak": 40.16, "min": 16.95}, "VIN": {"avg": 73.9, "peak": 128.54, "min": 28.56}, "VIN_SYS_5V0": {"avg": 15.23, "peak": 16.66, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 16.34, "peak": 17.71, "min": 13.39}}, "power_watts_avg": 31.83, "energy_joules_est": 64.04, "sample_count": 20, "duration_seconds": 2.012}, "timestamp": "2026-01-19T14:50:28.571732"}
{"image_index": 217, "image_name": "000000022705.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022705.jpg", "image_width": 482, "image_height": 640, "image_resolution": "482x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2157.085, "latencies_ms": [2157.085], "images_per_second": 0.464, "prompt_tokens": 1442, "response_tokens_est": 25, "n_tiles": 1, "output_text": " The woman is wearing a black dress with sparkles and black shoes. The kitchen has wooden cabinets and a stainless steel refrigerator.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26849.4, "ram_available_mb": 98922.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 26849.4, "ram_available_mb": 98922.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.7, "peak": 40.56, "min": 18.14}, "VIN": {"avg": 72.19, "peak": 117.26, "min": 30.26}, "VIN_SYS_5V0": {"avg": 15.16, "peak": 16.56, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 16.23, "peak": 17.71, "min": 13.0}}, "power_watts_avg": 31.7, "energy_joules_est": 68.39, "sample_count": 21, "duration_seconds": 2.158}, "timestamp": "2026-01-19T14:50:30.748805"}
{"image_index": 218, "image_name": "000000022755.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022755.jpg", "image_width": 640, "image_height": 479, "image_resolution": "640x479", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1797.639, "latencies_ms": [1797.639], "images_per_second": 0.556, "prompt_tokens": 1099, "response_tokens_est": 28, "n_tiles": 1, "output_text": " The image features a round mirror reflecting a yellow school bus and a car on a street, with a building and a sign in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26849.4, "ram_available_mb": 98922.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26849.4, "ram_available_mb": 98922.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.55, "peak": 40.95, "min": 18.91}, "VIN": {"avg": 69.84, "peak": 97.73, "min": 29.36}, "VIN_SYS_5V0": {"avg": 15.06, "peak": 16.15, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 16.18, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 30.55, "energy_joules_est": 54.93, "sample_count": 18, "duration_seconds": 1.798}, "timestamp": "2026-01-19T14:50:32.629395"}
{"image_index": 218, "image_name": "000000022755.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022755.jpg", "image_width": 640, "image_height": 479, "image_resolution": "640x479", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2010.157, "latencies_ms": [2010.157], "images_per_second": 0.497, "prompt_tokens": 1113, "response_tokens_est": 35, "n_tiles": 1, "output_text": " mirror: 1\nbus: 1\npole: 1\ntraffic light: 1\nbuilding: 1\nsign: 1\nsky: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26849.4, "ram_available_mb": 98922.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26849.4, "ram_available_mb": 98922.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.07, "peak": 40.18, "min": 17.35}, "VIN": {"avg": 74.28, "peak": 116.53, "min": 29.99}, "VIN_SYS_5V0": {"avg": 14.82, "peak": 16.05, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.98, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 29.07, "energy_joules_est": 58.44, "sample_count": 20, "duration_seconds": 2.01}, "timestamp": "2026-01-19T14:50:34.717537"}
{"image_index": 218, "image_name": "000000022755.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022755.jpg", "image_width": 640, "image_height": 479, "image_resolution": "640x479", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2094.781, "latencies_ms": [2094.781], "images_per_second": 0.477, "prompt_tokens": 1117, "response_tokens_est": 38, "n_tiles": 1, "output_text": " The mirror is positioned in the foreground, reflecting the yellow school bus in the background. The bus is located to the left of the mirror, while the traffic light is situated to the right.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26849.4, "ram_available_mb": 98922.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26849.4, "ram_available_mb": 98922.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.53, "peak": 40.16, "min": 16.56}, "VIN": {"avg": 63.23, "peak": 84.92, "min": 29.13}, "VIN_SYS_5V0": {"avg": 14.79, "peak": 16.05, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.9, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 28.53, "energy_joules_est": 59.78, "sample_count": 21, "duration_seconds": 2.095}, "timestamp": "2026-01-19T14:50:36.891075"}
{"image_index": 218, "image_name": "000000022755.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022755.jpg", "image_width": 640, "image_height": 479, "image_resolution": "640x479", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2386.303, "latencies_ms": [2386.303], "images_per_second": 0.419, "prompt_tokens": 1111, "response_tokens_est": 50, "n_tiles": 1, "output_text": " The image captures a scene of a yellow school bus parked on the side of a road, with its side mirror reflecting the surroundings. The background features a storefront with a sign that reads \"Harley-Davidson\" and a cloudy sky overhead.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26849.4, "ram_available_mb": 98922.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26849.4, "ram_available_mb": 98922.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.83, "peak": 40.56, "min": 16.16}, "VIN": {"avg": 67.41, "peak": 129.05, "min": 29.67}, "VIN_SYS_5V0": {"avg": 14.89, "peak": 16.05, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.97, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 27.83, "energy_joules_est": 66.42, "sample_count": 23, "duration_seconds": 2.387}, "timestamp": "2026-01-19T14:50:39.293233"}
{"image_index": 218, "image_name": "000000022755.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022755.jpg", "image_width": 640, "image_height": 479, "image_resolution": "640x479", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1852.082, "latencies_ms": [1852.082], "images_per_second": 0.54, "prompt_tokens": 1109, "response_tokens_est": 30, "n_tiles": 1, "output_text": " The image features a round mirror with a yellow school bus reflected in it, set against a backdrop of a cloudy sky and a building with a sign.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26849.4, "ram_available_mb": 98922.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26849.4, "ram_available_mb": 98922.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.07, "peak": 40.56, "min": 16.95}, "VIN": {"avg": 74.21, "peak": 120.64, "min": 28.74}, "VIN_SYS_5V0": {"avg": 15.19, "peak": 16.15, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 16.32, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 30.07, "energy_joules_est": 55.7, "sample_count": 18, "duration_seconds": 1.852}, "timestamp": "2026-01-19T14:50:41.174987"}
{"image_index": 219, "image_name": "000000022892.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022892.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1577.608, "latencies_ms": [1577.608], "images_per_second": 0.634, "prompt_tokens": 1099, "response_tokens_est": 19, "n_tiles": 1, "output_text": " A gray cat is sitting on a table next to a window, looking at a dog outside.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26849.4, "ram_available_mb": 98922.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26849.4, "ram_available_mb": 98922.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.27, "peak": 40.56, "min": 18.13}, "VIN": {"avg": 77.14, "peak": 124.53, "min": 28.39}, "VIN_SYS_5V0": {"avg": 15.04, "peak": 16.15, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 16.17, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 31.27, "energy_joules_est": 49.35, "sample_count": 16, "duration_seconds": 1.578}, "timestamp": "2026-01-19T14:50:42.837630"}
{"image_index": 219, "image_name": "000000022892.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022892.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1771.544, "latencies_ms": [1771.544], "images_per_second": 0.564, "prompt_tokens": 1113, "response_tokens_est": 26, "n_tiles": 1, "output_text": " cat: 1, dog: 1, potted plant: 2, window: 1, table: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26849.4, "ram_available_mb": 98922.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26849.4, "ram_available_mb": 98922.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.31, "peak": 40.57, "min": 18.53}, "VIN": {"avg": 74.21, "peak": 126.56, "min": 28.72}, "VIN_SYS_5V0": {"avg": 14.83, "peak": 16.05, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.9, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 30.31, "energy_joules_est": 53.72, "sample_count": 18, "duration_seconds": 1.772}, "timestamp": "2026-01-19T14:50:44.716409"}
{"image_index": 219, "image_name": "000000022892.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022892.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2037.032, "latencies_ms": [2037.032], "images_per_second": 0.491, "prompt_tokens": 1117, "response_tokens_est": 36, "n_tiles": 1, "output_text": " The cat is in the foreground, looking out the window, while the dog is in the background, looking out the window. The cat is closer to the camera than the dog.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26849.4, "ram_available_mb": 98922.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26849.4, "ram_available_mb": 98922.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.03, "peak": 40.16, "min": 16.95}, "VIN": {"avg": 72.51, "peak": 125.67, "min": 29.32}, "VIN_SYS_5V0": {"avg": 14.78, "peak": 16.05, "min": 11.77}, "VDD_CPU_SOC_MSS": {"avg": 15.79, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 29.03, "energy_joules_est": 59.14, "sample_count": 20, "duration_seconds": 2.037}, "timestamp": "2026-01-19T14:50:46.804539"}
{"image_index": 219, "image_name": "000000022892.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022892.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1647.064, "latencies_ms": [1647.064], "images_per_second": 0.607, "prompt_tokens": 1111, "response_tokens_est": 21, "n_tiles": 1, "output_text": " A gray cat is sitting on a table next to a window, looking out at a dog standing outside.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26849.4, "ram_available_mb": 98922.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 26849.4, "ram_available_mb": 98922.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.83, "peak": 40.18, "min": 16.95}, "VIN": {"avg": 69.33, "peak": 90.67, "min": 30.01}, "VIN_SYS_5V0": {"avg": 14.93, "peak": 16.15, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 16.07, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 30.83, "energy_joules_est": 50.79, "sample_count": 16, "duration_seconds": 1.648}, "timestamp": "2026-01-19T14:50:48.464454"}
{"image_index": 219, "image_name": "000000022892.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022892.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2480.991, "latencies_ms": [2480.991], "images_per_second": 0.403, "prompt_tokens": 1109, "response_tokens_est": 52, "n_tiles": 1, "output_text": " The image features a gray cat and a brown dog, both looking out of a window. The cat is sitting on a wooden table, while the dog is standing outside. The lighting is natural, coming from the window, and the weather appears to be clear.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26849.4, "ram_available_mb": 98922.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26849.4, "ram_available_mb": 98922.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.15, "peak": 40.97, "min": 19.31}, "VIN": {"avg": 74.12, "peak": 125.91, "min": 29.62}, "VIN_SYS_5V0": {"avg": 15.1, "peak": 16.15, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 16.13, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 28.15, "energy_joules_est": 69.85, "sample_count": 24, "duration_seconds": 2.481}, "timestamp": "2026-01-19T14:50:50.966712"}
{"image_index": 220, "image_name": "000000022935.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022935.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2067.431, "latencies_ms": [2067.431], "images_per_second": 0.484, "prompt_tokens": 1099, "response_tokens_est": 39, "n_tiles": 1, "output_text": " A female soccer player in a blue jersey with the word \"Acronis\" on it is dribbling a soccer ball while another player in a yellow jersey is in the background.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 26849.4, "ram_available_mb": 98922.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26849.4, "ram_available_mb": 98922.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.04, "peak": 40.18, "min": 16.55}, "VIN": {"avg": 72.44, "peak": 125.06, "min": 30.72}, "VIN_SYS_5V0": {"avg": 15.15, "peak": 16.15, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 16.18, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 29.04, "energy_joules_est": 60.06, "sample_count": 20, "duration_seconds": 2.068}, "timestamp": "2026-01-19T14:50:53.059025"}
{"image_index": 220, "image_name": "000000022935.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022935.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2668.78, "latencies_ms": [2668.78], "images_per_second": 0.375, "prompt_tokens": 1113, "response_tokens_est": 62, "n_tiles": 1, "output_text": " 1. person: 1\n2. jersey: 1\n3. ball: 1\n4. headband: 1\n5. shorts: 1\n6. socks: 1\n7. jersey logo: 1\n8. jersey number: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26849.4, "ram_available_mb": 98922.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26849.4, "ram_available_mb": 98922.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.41, "peak": 40.57, "min": 18.14}, "VIN": {"avg": 76.75, "peak": 126.0, "min": 28.95}, "VIN_SYS_5V0": {"avg": 15.19, "peak": 16.26, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 16.22, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 27.41, "energy_joules_est": 73.16, "sample_count": 26, "duration_seconds": 2.669}, "timestamp": "2026-01-19T14:50:55.744368"}
{"image_index": 220, "image_name": "000000022935.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022935.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2349.21, "latencies_ms": [2349.21], "images_per_second": 0.426, "prompt_tokens": 1117, "response_tokens_est": 50, "n_tiles": 1, "output_text": " The main object, a woman, is in the foreground, holding a ball. The background is blurred, indicating that the focus is on the woman. The woman is positioned to the left of the frame, and the ball is in her right hand.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26849.4, "ram_available_mb": 98922.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 26849.4, "ram_available_mb": 98922.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.16, "peak": 40.18, "min": 17.35}, "VIN": {"avg": 70.03, "peak": 103.4, "min": 29.84}, "VIN_SYS_5V0": {"avg": 15.12, "peak": 16.15, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 16.26, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 28.16, "energy_joules_est": 66.17, "sample_count": 23, "duration_seconds": 2.35}, "timestamp": "2026-01-19T14:50:58.144639"}
{"image_index": 220, "image_name": "000000022935.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022935.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1818.422, "latencies_ms": [1818.422], "images_per_second": 0.55, "prompt_tokens": 1111, "response_tokens_est": 29, "n_tiles": 1, "output_text": " A female athlete in a blue jersey is playing a game of soccer, while another player in a yellow jersey is standing in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26849.4, "ram_available_mb": 98922.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26849.4, "ram_available_mb": 98922.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.94, "peak": 40.57, "min": 16.16}, "VIN": {"avg": 74.01, "peak": 123.81, "min": 29.62}, "VIN_SYS_5V0": {"avg": 15.01, "peak": 16.05, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 16.12, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 29.94, "energy_joules_est": 54.45, "sample_count": 18, "duration_seconds": 1.819}, "timestamp": "2026-01-19T14:51:00.025590"}
{"image_index": 220, "image_name": "000000022935.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022935.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3138.613, "latencies_ms": [3138.613], "images_per_second": 0.319, "prompt_tokens": 1109, "response_tokens_est": 79, "n_tiles": 1, "output_text": " The image captures a dynamic moment on a grassy field, where a female athlete in a vibrant blue jersey is in the midst of a powerful kick, her body language suggesting a sense of urgency and athleticism. The lighting is natural and soft, casting a warm glow on the scene, and the colors are vivid, with the blue of the jersey standing out against the green of the field.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26849.4, "ram_available_mb": 98922.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 26849.4, "ram_available_mb": 98922.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 25.8, "peak": 40.18, "min": 17.34}, "VIN": {"avg": 59.23, "peak": 124.43, "min": 27.81}, "VIN_SYS_5V0": {"avg": 14.89, "peak": 16.05, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.83, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 25.8, "energy_joules_est": 80.98, "sample_count": 31, "duration_seconds": 3.139}, "timestamp": "2026-01-19T14:51:03.250263"}
{"image_index": 221, "image_name": "000000022969.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022969.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1527.029, "latencies_ms": [1527.029], "images_per_second": 0.655, "prompt_tokens": 1099, "response_tokens_est": 17, "n_tiles": 1, "output_text": " Two giraffes are standing in a fenced-in area with trees in the background.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 26849.4, "ram_available_mb": 98922.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 26849.6, "ram_available_mb": 98922.6, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.67, "peak": 40.16, "min": 14.98}, "VIN": {"avg": 77.89, "peak": 127.99, "min": 29.13}, "VIN_SYS_5V0": {"avg": 14.75, "peak": 16.05, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.88, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 30.67, "energy_joules_est": 46.86, "sample_count": 15, "duration_seconds": 1.528}, "timestamp": "2026-01-19T14:51:04.822662"}
{"image_index": 221, "image_name": "000000022969.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022969.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2625.746, "latencies_ms": [2625.746], "images_per_second": 0.381, "prompt_tokens": 1113, "response_tokens_est": 60, "n_tiles": 1, "output_text": " 1. giraffe: 2\n2. fence: 1\n3. tree: 1\n4. grass: 1\n5. dirt: 1\n6. dirt patch: 1\n7. fence post: 1\n8. giraffe's tail: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26849.6, "ram_available_mb": 98922.6, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26849.6, "ram_available_mb": 98922.6, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.64, "peak": 40.95, "min": 18.14}, "VIN": {"avg": 72.88, "peak": 118.58, "min": 29.7}, "VIN_SYS_5V0": {"avg": 15.04, "peak": 16.15, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 15.98, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 27.64, "energy_joules_est": 72.6, "sample_count": 26, "duration_seconds": 2.627}, "timestamp": "2026-01-19T14:51:07.522567"}
{"image_index": 221, "image_name": "000000022969.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022969.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2307.36, "latencies_ms": [2307.36], "images_per_second": 0.433, "prompt_tokens": 1117, "response_tokens_est": 47, "n_tiles": 1, "output_text": " The giraffe on the left is positioned closer to the camera than the one on the right, which is farther away. The giraffe on the right is standing near the fence, while the one on the left is bending down to eat grass.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26849.6, "ram_available_mb": 98922.6, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26849.6, "ram_available_mb": 98922.6, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.68, "peak": 40.16, "min": 15.38}, "VIN": {"avg": 69.45, "peak": 126.86, "min": 27.91}, "VIN_SYS_5V0": {"avg": 14.85, "peak": 16.05, "min": 12.15}, "VDD_CPU_SOC_MSS": {"avg": 15.85, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 27.68, "energy_joules_est": 63.89, "sample_count": 23, "duration_seconds": 2.308}, "timestamp": "2026-01-19T14:51:09.915116"}
{"image_index": 221, "image_name": "000000022969.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022969.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1579.423, "latencies_ms": [1579.423], "images_per_second": 0.633, "prompt_tokens": 1111, "response_tokens_est": 19, "n_tiles": 1, "output_text": " Two giraffes are in a fenced-in area, one eating grass and the other standing.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26849.6, "ram_available_mb": 98922.6, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26853.6, "ram_available_mb": 98918.6, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.38, "peak": 40.18, "min": 15.38}, "VIN": {"avg": 72.3, "peak": 126.17, "min": 32.95}, "VIN_SYS_5V0": {"avg": 14.76, "peak": 16.05, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.97, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 30.38, "energy_joules_est": 48.0, "sample_count": 16, "duration_seconds": 1.58}, "timestamp": "2026-01-19T14:51:11.583933"}
{"image_index": 221, "image_name": "000000022969.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022969.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1811.538, "latencies_ms": [1811.538], "images_per_second": 0.552, "prompt_tokens": 1109, "response_tokens_est": 28, "n_tiles": 1, "output_text": " The giraffes are brown and white, and the grass is green. The giraffes are standing in a fenced area with trees in the background.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26853.6, "ram_available_mb": 98918.6, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 9.6, "ram_used_mb": 26927.4, "ram_available_mb": 98844.8, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.48, "peak": 40.16, "min": 18.91}, "VIN": {"avg": 78.75, "peak": 124.35, "min": 33.07}, "VIN_SYS_5V0": {"avg": 14.98, "peak": 16.15, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 17.26, "peak": 18.89, "min": 13.79}}, "power_watts_avg": 30.48, "energy_joules_est": 55.22, "sample_count": 18, "duration_seconds": 1.812}, "timestamp": "2026-01-19T14:51:13.460087"}
{"image_index": 222, "image_name": "000000023023.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023023.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2094.832, "latencies_ms": [2094.832], "images_per_second": 0.477, "prompt_tokens": 1432, "response_tokens_est": 23, "n_tiles": 1, "output_text": " A suitcase, a plastic bag, and a black bag are placed on the floor in front of a curtain.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 26927.4, "ram_available_mb": 98844.8, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 10.6, "ram_used_mb": 26943.1, "ram_available_mb": 98829.1, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.2, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.14, "peak": 40.16, "min": 16.95}, "VIN": {"avg": 78.73, "peak": 133.5, "min": 31.89}, "VIN_SYS_5V0": {"avg": 15.27, "peak": 16.66, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 18.95, "peak": 20.47, "min": 15.76}}, "power_watts_avg": 31.14, "energy_joules_est": 65.26, "sample_count": 21, "duration_seconds": 2.096}, "timestamp": "2026-01-19T14:51:15.648972"}
{"image_index": 222, "image_name": "000000023023.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023023.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1584.858, "latencies_ms": [1584.858], "images_per_second": 0.631, "prompt_tokens": 1446, "response_tokens_est": 5, "n_tiles": 1, "output_text": " suitcase: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26943.1, "ram_available_mb": 98829.1, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 9.2, "ram_used_mb": 26937.2, "ram_available_mb": 98835.0, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 33.31, "peak": 40.56, "min": 17.35}, "VIN": {"avg": 76.19, "peak": 116.72, "min": 32.6}, "VIN_SYS_5V0": {"avg": 15.13, "peak": 16.56, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 18.77, "peak": 20.07, "min": 15.76}}, "power_watts_avg": 33.31, "energy_joules_est": 52.81, "sample_count": 16, "duration_seconds": 1.585}, "timestamp": "2026-01-19T14:51:17.318925"}
{"image_index": 222, "image_name": "000000023023.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023023.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2184.54, "latencies_ms": [2184.54], "images_per_second": 0.458, "prompt_tokens": 1450, "response_tokens_est": 27, "n_tiles": 1, "output_text": " The suitcase is on the left, the backpack is on the right, and the trash bag is in front of the suitcase.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26937.2, "ram_available_mb": 98835.0, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 10.7, "ram_used_mb": 26953.5, "ram_available_mb": 98818.7, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 33.03, "peak": 41.34, "min": 23.24}, "VIN": {"avg": 77.15, "peak": 129.27, "min": 34.59}, "VIN_SYS_5V0": {"avg": 15.3, "peak": 16.66, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 19.07, "peak": 20.47, "min": 16.14}}, "power_watts_avg": 33.03, "energy_joules_est": 72.17, "sample_count": 21, "duration_seconds": 2.185}, "timestamp": "2026-01-19T14:51:19.512185"}
{"image_index": 222, "image_name": "000000023023.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023023.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1980.546, "latencies_ms": [1980.546], "images_per_second": 0.505, "prompt_tokens": 1444, "response_tokens_est": 21, "n_tiles": 1, "output_text": " A suitcase, a plastic bag, and a backpack are on the floor in front of a curtain.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26953.5, "ram_available_mb": 98818.7, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 10.1, "ram_used_mb": 26966.2, "ram_available_mb": 98806.0, "ram_percent": 21.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.57, "peak": 40.56, "min": 19.31}, "VIN": {"avg": 77.0, "peak": 121.92, "min": 30.54}, "VIN_SYS_5V0": {"avg": 15.54, "peak": 16.76, "min": 13.29}, "VDD_CPU_SOC_MSS": {"avg": 19.27, "peak": 20.47, "min": 16.93}}, "power_watts_avg": 32.57, "energy_joules_est": 64.52, "sample_count": 20, "duration_seconds": 1.981}, "timestamp": "2026-01-19T14:51:21.595995"}
{"image_index": 222, "image_name": "000000023023.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023023.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2091.445, "latencies_ms": [2091.445], "images_per_second": 0.478, "prompt_tokens": 1442, "response_tokens_est": 23, "n_tiles": 1, "output_text": " The image is in black and white, and the lighting is dim. The materials of the objects are not clear.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26966.2, "ram_available_mb": 98806.0, "ram_percent": 21.4}, "sys_after": {"cpu_percent": 12.4, "ram_used_mb": 27024.9, "ram_available_mb": 98747.3, "ram_percent": 21.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.55, "peak": 40.56, "min": 17.74}, "VIN": {"avg": 78.28, "peak": 121.0, "min": 31.82}, "VIN_SYS_5V0": {"avg": 15.15, "peak": 16.56, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 19.27, "peak": 20.86, "min": 15.76}}, "power_watts_avg": 31.55, "energy_joules_est": 66.0, "sample_count": 21, "duration_seconds": 2.092}, "timestamp": "2026-01-19T14:51:23.782366"}
{"image_index": 223, "image_name": "000000023034.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023034.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1809.935, "latencies_ms": [1809.935], "images_per_second": 0.553, "prompt_tokens": 1099, "response_tokens_est": 27, "n_tiles": 1, "output_text": " A man in a blue shirt and a red bandana is standing on a rocky trail in the woods, observing two people riding horses.", "error": null, "sys_before": {"cpu_percent": 18.8, "ram_used_mb": 27024.9, "ram_available_mb": 98747.3, "ram_percent": 21.5}, "sys_after": {"cpu_percent": 17.4, "ram_used_mb": 27406.2, "ram_available_mb": 98366.0, "ram_percent": 21.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.94, "peak": 40.16, "min": 17.35}, "VIN": {"avg": 73.92, "peak": 110.02, "min": 33.68}, "VIN_SYS_5V0": {"avg": 14.93, "peak": 16.05, "min": 11.87}, "VDD_CPU_SOC_MSS": {"avg": 19.49, "peak": 20.87, "min": 15.36}}, "power_watts_avg": 29.94, "energy_joules_est": 54.22, "sample_count": 18, "duration_seconds": 1.811}, "timestamp": "2026-01-19T14:51:25.659558"}
{"image_index": 223, "image_name": "000000023034.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023034.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2537.088, "latencies_ms": [2537.088], "images_per_second": 0.394, "prompt_tokens": 1113, "response_tokens_est": 56, "n_tiles": 1, "output_text": " 1. person: 1\n2. horse: 2\n3. saddle: 1\n4. backpack: 1\n5. rock: 1\n6. tree: 1\n7. man: 1\n8. hat: 1", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 27406.2, "ram_available_mb": 98366.0, "ram_percent": 21.8}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 27418.2, "ram_available_mb": 98354.0, "ram_percent": 21.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.58, "peak": 40.57, "min": 17.73}, "VIN": {"avg": 70.42, "peak": 127.2, "min": 33.69}, "VIN_SYS_5V0": {"avg": 15.0, "peak": 16.15, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 16.98, "peak": 18.11, "min": 15.36}}, "power_watts_avg": 27.58, "energy_joules_est": 69.99, "sample_count": 25, "duration_seconds": 2.538}, "timestamp": "2026-01-19T14:51:28.262129"}
{"image_index": 223, "image_name": "000000023034.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023034.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2534.871, "latencies_ms": [2534.871], "images_per_second": 0.394, "prompt_tokens": 1117, "response_tokens_est": 56, "n_tiles": 1, "output_text": " The man in the blue shirt is standing to the right of the man on the horse, and the man on the horse is in the foreground of the image. The man in the blue shirt is also in the foreground, but closer to the camera than the man on the horse.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27418.2, "ram_available_mb": 98354.0, "ram_percent": 21.8}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 27428.6, "ram_available_mb": 98343.6, "ram_percent": 21.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.09, "peak": 40.16, "min": 15.76}, "VIN": {"avg": 65.9, "peak": 107.65, "min": 28.98}, "VIN_SYS_5V0": {"avg": 15.01, "peak": 16.15, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 16.42, "peak": 17.71, "min": 14.57}}, "power_watts_avg": 27.09, "energy_joules_est": 68.68, "sample_count": 25, "duration_seconds": 2.535}, "timestamp": "2026-01-19T14:51:30.858302"}
{"image_index": 223, "image_name": "000000023034.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023034.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1646.628, "latencies_ms": [1646.628], "images_per_second": 0.607, "prompt_tokens": 1111, "response_tokens_est": 21, "n_tiles": 1, "output_text": " A man in a blue shirt and a man in a red bandana are riding horses through a forest.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 27428.6, "ram_available_mb": 98343.6, "ram_percent": 21.8}, "sys_after": {"cpu_percent": 7.9, "ram_used_mb": 27478.2, "ram_available_mb": 98294.0, "ram_percent": 21.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.5, "peak": 40.16, "min": 15.77}, "VIN": {"avg": 73.8, "peak": 123.57, "min": 32.6}, "VIN_SYS_5V0": {"avg": 14.99, "peak": 16.05, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 16.58, "peak": 17.71, "min": 13.79}}, "power_watts_avg": 30.5, "energy_joules_est": 50.23, "sample_count": 16, "duration_seconds": 1.647}, "timestamp": "2026-01-19T14:51:32.526598"}
{"image_index": 223, "image_name": "000000023034.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023034.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2634.192, "latencies_ms": [2634.192], "images_per_second": 0.38, "prompt_tokens": 1109, "response_tokens_est": 58, "n_tiles": 1, "output_text": " The image features a man wearing a blue shirt and a red bandana, standing on a rocky trail surrounded by trees. The lighting is natural, and the colors are vibrant, with the blue of the man's shirt standing out against the green of the trees and the brown of the rocks.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27478.2, "ram_available_mb": 98294.0, "ram_percent": 21.8}, "sys_after": {"cpu_percent": 25.1, "ram_used_mb": 27513.3, "ram_available_mb": 98258.8, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.41, "peak": 40.95, "min": 18.13}, "VIN": {"avg": 71.53, "peak": 109.04, "min": 30.94}, "VIN_SYS_5V0": {"avg": 15.28, "peak": 16.26, "min": 13.29}, "VDD_CPU_SOC_MSS": {"avg": 19.0, "peak": 23.22, "min": 16.14}}, "power_watts_avg": 27.41, "energy_joules_est": 72.22, "sample_count": 26, "duration_seconds": 2.635}, "timestamp": "2026-01-19T14:51:35.227808"}
{"image_index": 224, "image_name": "000000023126.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023126.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1732.556, "latencies_ms": [1732.556], "images_per_second": 0.577, "prompt_tokens": 1099, "response_tokens_est": 25, "n_tiles": 1, "output_text": " A man wearing a sweater with the word \"Russia\" on it is riding a horse in a black and white photo.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 27514.1, "ram_available_mb": 98258.1, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 27521.0, "ram_available_mb": 98251.2, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.08, "peak": 40.18, "min": 15.77}, "VIN": {"avg": 76.93, "peak": 126.3, "min": 30.01}, "VIN_SYS_5V0": {"avg": 15.03, "peak": 16.15, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 16.65, "peak": 17.71, "min": 15.36}}, "power_watts_avg": 30.08, "energy_joules_est": 52.13, "sample_count": 17, "duration_seconds": 1.733}, "timestamp": "2026-01-19T14:51:36.996455"}
{"image_index": 224, "image_name": "000000023126.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023126.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2701.821, "latencies_ms": [2701.821], "images_per_second": 0.37, "prompt_tokens": 1113, "response_tokens_est": 63, "n_tiles": 1, "output_text": " 1. man: 1\n2. horse: 1\n3. saddle: 1\n4. bridle: 1\n5. reins: 1\n6. man's hand: 1\n7. man's leg: 1\n8. man's foot: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27521.0, "ram_available_mb": 98251.2, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 27511.7, "ram_available_mb": 98260.5, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.1, "peak": 40.56, "min": 17.34}, "VIN": {"avg": 68.46, "peak": 90.49, "min": 27.85}, "VIN_SYS_5V0": {"avg": 15.04, "peak": 16.15, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 16.3, "peak": 17.71, "min": 14.18}}, "power_watts_avg": 27.1, "energy_joules_est": 73.23, "sample_count": 27, "duration_seconds": 2.702}, "timestamp": "2026-01-19T14:51:39.803876"}
{"image_index": 224, "image_name": "000000023126.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023126.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2438.767, "latencies_ms": [2438.767], "images_per_second": 0.41, "prompt_tokens": 1117, "response_tokens_est": 51, "n_tiles": 1, "output_text": " The man is positioned in the foreground, riding a horse that is in motion, while the background features a blurred building. The man is holding the reins of the horse, which is moving forward, and the horse is positioned in the center of the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27511.7, "ram_available_mb": 98260.5, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 18.8, "ram_used_mb": 27547.1, "ram_available_mb": 98225.0, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.18, "peak": 39.77, "min": 14.59}, "VIN": {"avg": 70.88, "peak": 94.79, "min": 29.16}, "VIN_SYS_5V0": {"avg": 14.86, "peak": 16.15, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 18.0, "peak": 24.01, "min": 13.39}}, "power_watts_avg": 27.18, "energy_joules_est": 66.3, "sample_count": 24, "duration_seconds": 2.439}, "timestamp": "2026-01-19T14:51:42.295819"}
{"image_index": 224, "image_name": "000000023126.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023126.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1619.403, "latencies_ms": [1619.403], "images_per_second": 0.618, "prompt_tokens": 1111, "response_tokens_est": 21, "n_tiles": 1, "output_text": " A man is riding a horse in a race, wearing a sweater with the word Russia on it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27547.1, "ram_available_mb": 98225.0, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 27540.5, "ram_available_mb": 98231.7, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.75, "peak": 40.56, "min": 16.16}, "VIN": {"avg": 76.25, "peak": 105.92, "min": 29.27}, "VIN_SYS_5V0": {"avg": 15.04, "peak": 16.15, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 16.19, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 30.75, "energy_joules_est": 49.81, "sample_count": 16, "duration_seconds": 1.62}, "timestamp": "2026-01-19T14:51:43.968916"}
{"image_index": 224, "image_name": "000000023126.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023126.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2222.254, "latencies_ms": [2222.254], "images_per_second": 0.45, "prompt_tokens": 1109, "response_tokens_est": 44, "n_tiles": 1, "output_text": " The image is in black and white, with a blurred background that suggests motion. The lighting is natural, likely from the sun, and the material of the horse and rider is not clearly visible due to the motion blur.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27540.5, "ram_available_mb": 98231.7, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 27537.8, "ram_available_mb": 98234.3, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.83, "peak": 40.97, "min": 18.91}, "VIN": {"avg": 72.59, "peak": 125.9, "min": 30.26}, "VIN_SYS_5V0": {"avg": 14.94, "peak": 16.15, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 16.11, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 28.83, "energy_joules_est": 64.08, "sample_count": 22, "duration_seconds": 2.223}, "timestamp": "2026-01-19T14:51:46.263258"}
{"image_index": 225, "image_name": "000000023230.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023230.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1476.097, "latencies_ms": [1476.097], "images_per_second": 0.677, "prompt_tokens": 1099, "response_tokens_est": 16, "n_tiles": 1, "output_text": " A group of geese are swimming in a pond surrounded by tall grass and trees.", "error": null, "sys_before": {"cpu_percent": 14.3, "ram_used_mb": 27537.8, "ram_available_mb": 98234.4, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 27539.2, "ram_available_mb": 98232.9, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.18, "peak": 40.18, "min": 16.16}, "VIN": {"avg": 73.99, "peak": 122.27, "min": 29.43}, "VIN_SYS_5V0": {"avg": 14.87, "peak": 16.05, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 16.06, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 31.18, "energy_joules_est": 46.03, "sample_count": 15, "duration_seconds": 1.476}, "timestamp": "2026-01-19T14:51:47.843359"}
{"image_index": 225, "image_name": "000000023230.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023230.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1193.164, "latencies_ms": [1193.164], "images_per_second": 0.838, "prompt_tokens": 1113, "response_tokens_est": 4, "n_tiles": 1, "output_text": " goose: 4", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27539.2, "ram_available_mb": 98232.9, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 2.5, "ram_used_mb": 27541.4, "ram_available_mb": 98230.8, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 33.03, "peak": 40.57, "min": 19.32}, "VIN": {"avg": 78.49, "peak": 120.93, "min": 30.52}, "VIN_SYS_5V0": {"avg": 14.68, "peak": 16.05, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.78, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 33.03, "energy_joules_est": 39.43, "sample_count": 12, "duration_seconds": 1.194}, "timestamp": "2026-01-19T14:51:49.110501"}
{"image_index": 225, "image_name": "000000023230.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023230.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2317.14, "latencies_ms": [2317.14], "images_per_second": 0.432, "prompt_tokens": 1117, "response_tokens_est": 47, "n_tiles": 1, "output_text": " The geese are positioned in the middle of the pond, with the vegetation on the left side of the image and the trees on the right side. The geese are relatively close to the camera, while the vegetation and trees are farther away.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27541.4, "ram_available_mb": 98230.8, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 27540.6, "ram_available_mb": 98231.6, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.53, "peak": 40.95, "min": 18.52}, "VIN": {"avg": 70.67, "peak": 120.63, "min": 28.87}, "VIN_SYS_5V0": {"avg": 14.92, "peak": 16.15, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 16.02, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 29.53, "energy_joules_est": 68.44, "sample_count": 23, "duration_seconds": 2.318}, "timestamp": "2026-01-19T14:51:51.504186"}
{"image_index": 225, "image_name": "000000023230.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023230.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1497.053, "latencies_ms": [1497.053], "images_per_second": 0.668, "prompt_tokens": 1111, "response_tokens_est": 16, "n_tiles": 1, "output_text": " A group of geese are swimming in a lake surrounded by tall grass and trees.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 27540.6, "ram_available_mb": 98231.6, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 27537.5, "ram_available_mb": 98234.7, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.97, "peak": 40.18, "min": 15.77}, "VIN": {"avg": 73.06, "peak": 107.49, "min": 29.08}, "VIN_SYS_5V0": {"avg": 14.83, "peak": 16.05, "min": 12.15}, "VDD_CPU_SOC_MSS": {"avg": 16.01, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 30.97, "energy_joules_est": 46.38, "sample_count": 15, "duration_seconds": 1.497}, "timestamp": "2026-01-19T14:51:53.076645"}
{"image_index": 225, "image_name": "000000023230.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023230.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3000.982, "latencies_ms": [3000.982], "images_per_second": 0.333, "prompt_tokens": 1109, "response_tokens_est": 74, "n_tiles": 1, "output_text": " The image features a serene scene of a group of geese swimming in a body of water, with the geese displaying a variety of colors including shades of brown, white, and black. The lighting in the image is natural and bright, suggesting that the photo was taken during the daytime. The water appears calm and reflective, with the geese creating ripples as they move through it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27537.5, "ram_available_mb": 98234.7, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 27537.7, "ram_available_mb": 98234.4, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.67, "peak": 40.56, "min": 19.3}, "VIN": {"avg": 63.89, "peak": 116.36, "min": 29.85}, "VIN_SYS_5V0": {"avg": 14.91, "peak": 16.05, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.91, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 26.67, "energy_joules_est": 80.04, "sample_count": 29, "duration_seconds": 3.001}, "timestamp": "2026-01-19T14:51:56.095387"}
{"image_index": 226, "image_name": "000000023272.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023272.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1804.319, "latencies_ms": [1804.319], "images_per_second": 0.554, "prompt_tokens": 1099, "response_tokens_est": 29, "n_tiles": 1, "output_text": " A cat is sitting on the hood of a black Mercedes-Benz car, which is parked in front of a house with a garden.", "error": null, "sys_before": {"cpu_percent": 22.2, "ram_used_mb": 27537.7, "ram_available_mb": 98234.4, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 27515.1, "ram_available_mb": 98257.1, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.98, "peak": 39.78, "min": 16.95}, "VIN": {"avg": 74.3, "peak": 128.92, "min": 28.82}, "VIN_SYS_5V0": {"avg": 15.18, "peak": 16.15, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 16.49, "peak": 17.71, "min": 14.18}}, "power_watts_avg": 29.98, "energy_joules_est": 54.11, "sample_count": 18, "duration_seconds": 1.805}, "timestamp": "2026-01-19T14:51:57.982487"}
{"image_index": 226, "image_name": "000000023272.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023272.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2550.666, "latencies_ms": [2550.666], "images_per_second": 0.392, "prompt_tokens": 1113, "response_tokens_est": 56, "n_tiles": 1, "output_text": " 1. cat: 1\n2. car: 1\n3. house: 1\n4. window: 1\n5. door: 1\n6. fence: 1\n7. plant: 1\n8. flowers: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27515.1, "ram_available_mb": 98257.1, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 27493.0, "ram_available_mb": 98279.2, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.33, "peak": 40.18, "min": 16.95}, "VIN": {"avg": 68.21, "peak": 106.79, "min": 29.23}, "VIN_SYS_5V0": {"avg": 14.96, "peak": 16.05, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 16.0, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 27.33, "energy_joules_est": 69.72, "sample_count": 25, "duration_seconds": 2.551}, "timestamp": "2026-01-19T14:52:00.574243"}
{"image_index": 226, "image_name": "000000023272.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023272.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2648.122, "latencies_ms": [2648.122], "images_per_second": 0.378, "prompt_tokens": 1117, "response_tokens_est": 61, "n_tiles": 1, "output_text": " The cat is sitting on the hood of the car, which is in the foreground of the image. The car is parked in front of a house, which is in the background of the image. The cat is positioned to the left of the car, and the house is to the right of the car.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27493.0, "ram_available_mb": 98279.2, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 27491.1, "ram_available_mb": 98281.1, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.99, "peak": 40.57, "min": 16.16}, "VIN": {"avg": 76.26, "peak": 126.97, "min": 29.81}, "VIN_SYS_5V0": {"avg": 15.05, "peak": 16.15, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 16.17, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 26.99, "energy_joules_est": 71.48, "sample_count": 26, "duration_seconds": 2.648}, "timestamp": "2026-01-19T14:52:03.275392"}
{"image_index": 226, "image_name": "000000023272.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023272.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1568.273, "latencies_ms": [1568.273], "images_per_second": 0.638, "prompt_tokens": 1111, "response_tokens_est": 19, "n_tiles": 1, "output_text": " A cat is sitting on top of a car, which is parked in front of a house.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 27491.1, "ram_available_mb": 98281.1, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 27490.7, "ram_available_mb": 98281.4, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.68, "peak": 40.57, "min": 16.16}, "VIN": {"avg": 71.24, "peak": 108.24, "min": 28.75}, "VIN_SYS_5V0": {"avg": 14.95, "peak": 16.15, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 16.12, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 30.68, "energy_joules_est": 48.13, "sample_count": 16, "duration_seconds": 1.569}, "timestamp": "2026-01-19T14:52:04.946851"}
{"image_index": 226, "image_name": "000000023272.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023272.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1672.85, "latencies_ms": [1672.85], "images_per_second": 0.598, "prompt_tokens": 1109, "response_tokens_est": 23, "n_tiles": 1, "output_text": " The car is black and the cat is orange and white. The cat is sitting on the hood of the car.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 27490.7, "ram_available_mb": 98281.4, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 27491.0, "ram_available_mb": 98281.2, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.77, "peak": 40.56, "min": 18.14}, "VIN": {"avg": 71.23, "peak": 104.8, "min": 29.44}, "VIN_SYS_5V0": {"avg": 14.8, "peak": 16.05, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.86, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 30.77, "energy_joules_est": 51.48, "sample_count": 17, "duration_seconds": 1.673}, "timestamp": "2026-01-19T14:52:06.717044"}
{"image_index": 227, "image_name": "000000023359.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023359.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1848.903, "latencies_ms": [1848.903], "images_per_second": 0.541, "prompt_tokens": 1099, "response_tokens_est": 29, "n_tiles": 1, "output_text": " A man is snowboarding in the air with his arms outstretched, wearing a brown jacket and yellow pants, against a clear blue sky.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 27491.0, "ram_available_mb": 98281.2, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 27491.1, "ram_available_mb": 98281.0, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.96, "peak": 40.18, "min": 17.35}, "VIN": {"avg": 70.8, "peak": 106.47, "min": 29.83}, "VIN_SYS_5V0": {"avg": 14.72, "peak": 15.95, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.94, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 29.96, "energy_joules_est": 55.42, "sample_count": 18, "duration_seconds": 1.85}, "timestamp": "2026-01-19T14:52:08.599402"}
{"image_index": 227, "image_name": "000000023359.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023359.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2547.641, "latencies_ms": [2547.641], "images_per_second": 0.393, "prompt_tokens": 1113, "response_tokens_est": 57, "n_tiles": 1, "output_text": " snowboard: 1, snowboarder: 1, snowboarder's pants: 1, snowboarder's jacket: 1, snowboarder's hat: 1, snowboarder's gloves: 1, snowboarder's boots: 1", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 27491.1, "ram_available_mb": 98281.0, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 27491.2, "ram_available_mb": 98281.0, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.58, "peak": 40.56, "min": 18.52}, "VIN": {"avg": 67.09, "peak": 97.69, "min": 29.34}, "VIN_SYS_5V0": {"avg": 15.06, "peak": 16.15, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 16.17, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 27.58, "energy_joules_est": 70.28, "sample_count": 25, "duration_seconds": 2.548}, "timestamp": "2026-01-19T14:52:11.205511"}
{"image_index": 227, "image_name": "000000023359.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023359.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2039.454, "latencies_ms": [2039.454], "images_per_second": 0.49, "prompt_tokens": 1117, "response_tokens_est": 36, "n_tiles": 1, "output_text": " The snowboarder is in the foreground, jumping over a snow-covered slope. The snowboarder is in the middle of the image, with the sky in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27491.2, "ram_available_mb": 98281.0, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 8.6, "ram_used_mb": 27609.4, "ram_available_mb": 98162.8, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.86, "peak": 40.16, "min": 16.16}, "VIN": {"avg": 78.62, "peak": 119.31, "min": 33.16}, "VIN_SYS_5V0": {"avg": 14.98, "peak": 16.05, "min": 12.25}, "VDD_CPU_SOC_MSS": {"avg": 16.32, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 28.86, "energy_joules_est": 58.87, "sample_count": 20, "duration_seconds": 2.04}, "timestamp": "2026-01-19T14:52:13.287421"}
{"image_index": 227, "image_name": "000000023359.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023359.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1524.718, "latencies_ms": [1524.718], "images_per_second": 0.656, "prompt_tokens": 1111, "response_tokens_est": 17, "n_tiles": 1, "output_text": " A snowboarder is performing a trick in the air against a clear blue sky.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 27609.4, "ram_available_mb": 98162.8, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 12.2, "ram_used_mb": 27705.6, "ram_available_mb": 98066.6, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.59, "peak": 40.16, "min": 16.95}, "VIN": {"avg": 74.15, "peak": 105.09, "min": 32.25}, "VIN_SYS_5V0": {"avg": 15.27, "peak": 16.15, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 18.89, "peak": 20.07, "min": 16.15}}, "power_watts_avg": 31.59, "energy_joules_est": 48.18, "sample_count": 15, "duration_seconds": 1.525}, "timestamp": "2026-01-19T14:52:14.841774"}
{"image_index": 227, "image_name": "000000023359.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023359.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2036.549, "latencies_ms": [2036.549], "images_per_second": 0.491, "prompt_tokens": 1109, "response_tokens_est": 37, "n_tiles": 1, "output_text": " The snowboarder is wearing a brown jacket and yellow pants, and the snow is white. The sky is clear and blue, and the snowboarder is in mid-air.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27705.6, "ram_available_mb": 98066.6, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 8.8, "ram_used_mb": 27718.9, "ram_available_mb": 98053.3, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.12, "peak": 40.56, "min": 20.5}, "VIN": {"avg": 68.38, "peak": 110.18, "min": 30.33}, "VIN_SYS_5V0": {"avg": 15.18, "peak": 16.26, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 18.23, "peak": 20.07, "min": 15.75}}, "power_watts_avg": 30.12, "energy_joules_est": 61.36, "sample_count": 20, "duration_seconds": 2.037}, "timestamp": "2026-01-19T14:52:16.915781"}
{"image_index": 228, "image_name": "000000023666.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023666.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1448.551, "latencies_ms": [1448.551], "images_per_second": 0.69, "prompt_tokens": 1100, "response_tokens_est": 15, "n_tiles": 1, "output_text": " A small bathroom with a toilet, bathtub, and pipes on the wall.", "error": null, "sys_before": {"cpu_percent": 14.3, "ram_used_mb": 27718.9, "ram_available_mb": 98053.3, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 27718.9, "ram_available_mb": 98053.3, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.94, "peak": 40.16, "min": 17.35}, "VIN": {"avg": 74.67, "peak": 121.07, "min": 30.32}, "VIN_SYS_5V0": {"avg": 15.13, "peak": 16.05, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 16.42, "peak": 17.72, "min": 14.18}}, "power_watts_avg": 31.94, "energy_joules_est": 46.28, "sample_count": 14, "duration_seconds": 1.449}, "timestamp": "2026-01-19T14:52:18.381506"}
{"image_index": 228, "image_name": "000000023666.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023666.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1932.395, "latencies_ms": [1932.395], "images_per_second": 0.517, "prompt_tokens": 1114, "response_tokens_est": 34, "n_tiles": 1, "output_text": " toilet: 1, bathtub: 1, pipe: 1, towel: 1, door: 1, wall: 1, floor: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27718.9, "ram_available_mb": 98053.3, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 27699.2, "ram_available_mb": 98073.0, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.83, "peak": 40.57, "min": 20.49}, "VIN": {"avg": 75.05, "peak": 123.56, "min": 29.95}, "VIN_SYS_5V0": {"avg": 15.29, "peak": 16.26, "min": 13.39}, "VDD_CPU_SOC_MSS": {"avg": 16.43, "peak": 17.71, "min": 14.57}}, "power_watts_avg": 30.83, "energy_joules_est": 59.59, "sample_count": 19, "duration_seconds": 1.933}, "timestamp": "2026-01-19T14:52:20.363531"}
{"image_index": 228, "image_name": "000000023666.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023666.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1844.679, "latencies_ms": [1844.679], "images_per_second": 0.542, "prompt_tokens": 1118, "response_tokens_est": 30, "n_tiles": 1, "output_text": " The toilet is located to the left of the bathtub, which is situated in the background. The toilet is positioned closer to the camera than the bathtub.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27699.2, "ram_available_mb": 98073.0, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 27694.1, "ram_available_mb": 98078.0, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.09, "peak": 40.16, "min": 17.35}, "VIN": {"avg": 68.36, "peak": 83.48, "min": 30.73}, "VIN_SYS_5V0": {"avg": 15.12, "peak": 16.26, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 16.25, "peak": 17.71, "min": 13.79}}, "power_watts_avg": 30.09, "energy_joules_est": 55.51, "sample_count": 18, "duration_seconds": 1.845}, "timestamp": "2026-01-19T14:52:22.234840"}
{"image_index": 228, "image_name": "000000023666.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023666.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1432.925, "latencies_ms": [1432.925], "images_per_second": 0.698, "prompt_tokens": 1112, "response_tokens_est": 15, "n_tiles": 1, "output_text": " A small bathroom with a toilet and a bathtub is shown in the image.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 27694.1, "ram_available_mb": 98078.0, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 27694.2, "ram_available_mb": 98077.9, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.3, "peak": 39.78, "min": 18.91}, "VIN": {"avg": 77.93, "peak": 120.03, "min": 30.06}, "VIN_SYS_5V0": {"avg": 15.27, "peak": 16.26, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 16.56, "peak": 17.71, "min": 14.57}}, "power_watts_avg": 32.3, "energy_joules_est": 46.29, "sample_count": 14, "duration_seconds": 1.433}, "timestamp": "2026-01-19T14:52:23.699619"}
{"image_index": 228, "image_name": "000000023666.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023666.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1343.247, "latencies_ms": [1343.247], "images_per_second": 0.744, "prompt_tokens": 1110, "response_tokens_est": 11, "n_tiles": 1, "output_text": " The bathroom is painted white and has a wooden floor.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 27694.2, "ram_available_mb": 98077.9, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 27694.4, "ram_available_mb": 98077.8, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 33.85, "peak": 40.18, "min": 21.67}, "VIN": {"avg": 80.67, "peak": 127.64, "min": 29.17}, "VIN_SYS_5V0": {"avg": 15.27, "peak": 16.26, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 16.38, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 33.85, "energy_joules_est": 45.48, "sample_count": 13, "duration_seconds": 1.344}, "timestamp": "2026-01-19T14:52:25.061842"}
{"image_index": 229, "image_name": "000000023751.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023751.jpg", "image_width": 430, "image_height": 640, "image_resolution": "430x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1576.075, "latencies_ms": [1576.075], "images_per_second": 0.634, "prompt_tokens": 1100, "response_tokens_est": 19, "n_tiles": 1, "output_text": " A statue of two people holding a kite with a colorful pattern is on top of a building.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 27694.4, "ram_available_mb": 98077.8, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 27694.5, "ram_available_mb": 98077.6, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.52, "peak": 40.95, "min": 21.67}, "VIN": {"avg": 70.8, "peak": 102.88, "min": 27.6}, "VIN_SYS_5V0": {"avg": 15.13, "peak": 16.15, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 16.29, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 32.52, "energy_joules_est": 51.26, "sample_count": 16, "duration_seconds": 1.576}, "timestamp": "2026-01-19T14:52:26.741598"}
{"image_index": 229, "image_name": "000000023751.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023751.jpg", "image_width": 430, "image_height": 640, "image_resolution": "430x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1356.027, "latencies_ms": [1356.027], "images_per_second": 0.737, "prompt_tokens": 1114, "response_tokens_est": 10, "n_tiles": 1, "output_text": " kite: 1\nstatue: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27694.5, "ram_available_mb": 98077.6, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 27705.5, "ram_available_mb": 98066.7, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.83, "peak": 40.18, "min": 18.14}, "VIN": {"avg": 67.43, "peak": 87.58, "min": 30.91}, "VIN_SYS_5V0": {"avg": 14.65, "peak": 16.05, "min": 11.87}, "VDD_CPU_SOC_MSS": {"avg": 15.83, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 31.83, "energy_joules_est": 43.18, "sample_count": 14, "duration_seconds": 1.356}, "timestamp": "2026-01-19T14:52:28.200904"}
{"image_index": 229, "image_name": "000000023751.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023751.jpg", "image_width": 430, "image_height": 640, "image_resolution": "430x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2383.838, "latencies_ms": [2383.838], "images_per_second": 0.419, "prompt_tokens": 1118, "response_tokens_est": 50, "n_tiles": 1, "output_text": " The kite is in the foreground, flying high in the sky, while the statue is in the background, situated on a building. The kite is positioned to the left of the statue, and the statue is located on the right side of the building.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27705.5, "ram_available_mb": 98066.7, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 27723.2, "ram_available_mb": 98049.0, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.61, "peak": 40.56, "min": 20.1}, "VIN": {"avg": 66.4, "peak": 116.3, "min": 29.23}, "VIN_SYS_5V0": {"avg": 14.91, "peak": 16.15, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 16.09, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 28.61, "energy_joules_est": 68.21, "sample_count": 23, "duration_seconds": 2.384}, "timestamp": "2026-01-19T14:52:30.599138"}
{"image_index": 229, "image_name": "000000023751.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023751.jpg", "image_width": 430, "image_height": 640, "image_resolution": "430x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1545.642, "latencies_ms": [1545.642], "images_per_second": 0.647, "prompt_tokens": 1112, "response_tokens_est": 19, "n_tiles": 1, "output_text": " A kite with a colorful pattern is flying high in the sky above a statue of two people.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27723.2, "ram_available_mb": 98049.0, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 27723.6, "ram_available_mb": 98048.5, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.23, "peak": 39.77, "min": 16.95}, "VIN": {"avg": 62.97, "peak": 87.53, "min": 32.27}, "VIN_SYS_5V0": {"avg": 15.29, "peak": 16.36, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 16.45, "peak": 17.71, "min": 14.18}}, "power_watts_avg": 31.23, "energy_joules_est": 48.28, "sample_count": 15, "duration_seconds": 1.546}, "timestamp": "2026-01-19T14:52:32.155498"}
{"image_index": 229, "image_name": "000000023751.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023751.jpg", "image_width": 430, "image_height": 640, "image_resolution": "430x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2121.007, "latencies_ms": [2121.007], "images_per_second": 0.471, "prompt_tokens": 1110, "response_tokens_est": 39, "n_tiles": 1, "output_text": " The kite is colorful and has a long tail, while the statue is made of metal and has a shiny surface. The sky is overcast, and the building is made of glass and steel.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 27723.6, "ram_available_mb": 98048.5, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 9.1, "ram_used_mb": 27505.5, "ram_available_mb": 98266.7, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.34, "peak": 40.57, "min": 18.91}, "VIN": {"avg": 70.83, "peak": 121.69, "min": 30.98}, "VIN_SYS_5V0": {"avg": 15.25, "peak": 16.15, "min": 13.29}, "VDD_CPU_SOC_MSS": {"avg": 16.55, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 29.34, "energy_joules_est": 62.24, "sample_count": 21, "duration_seconds": 2.121}, "timestamp": "2026-01-19T14:52:34.333577"}
{"image_index": 230, "image_name": "000000023781.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023781.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1864.153, "latencies_ms": [1864.153], "images_per_second": 0.536, "prompt_tokens": 1099, "response_tokens_est": 30, "n_tiles": 1, "output_text": " The image displays a variety of fresh vegetables, including strawberries, broccoli, radishes, carrots, and green beans, arranged in a visually appealing manner.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 27505.5, "ram_available_mb": 98266.7, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 27515.1, "ram_available_mb": 98257.1, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.74, "peak": 40.16, "min": 16.16}, "VIN": {"avg": 67.51, "peak": 126.02, "min": 29.28}, "VIN_SYS_5V0": {"avg": 14.98, "peak": 16.05, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 16.29, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 29.74, "energy_joules_est": 55.47, "sample_count": 18, "duration_seconds": 1.865}, "timestamp": "2026-01-19T14:52:36.215343"}
{"image_index": 230, "image_name": "000000023781.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023781.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2026.584, "latencies_ms": [2026.584], "images_per_second": 0.493, "prompt_tokens": 1113, "response_tokens_est": 37, "n_tiles": 1, "output_text": " strawberries: 10, broccoli: 1, radishes: 1, carrots: 1, green beans: 1, asparagus: 1, parsley: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27515.1, "ram_available_mb": 98257.1, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 6.3, "ram_used_mb": 27523.2, "ram_available_mb": 98249.0, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.72, "peak": 40.56, "min": 18.91}, "VIN": {"avg": 74.82, "peak": 118.98, "min": 29.3}, "VIN_SYS_5V0": {"avg": 15.22, "peak": 16.26, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 16.6, "peak": 18.11, "min": 14.57}}, "power_watts_avg": 29.72, "energy_joules_est": 60.25, "sample_count": 20, "duration_seconds": 2.027}, "timestamp": "2026-01-19T14:52:38.294144"}
{"image_index": 230, "image_name": "000000023781.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023781.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2172.991, "latencies_ms": [2172.991], "images_per_second": 0.46, "prompt_tokens": 1117, "response_tokens_est": 42, "n_tiles": 1, "output_text": " The strawberries are located in the left foreground, while the broccoli is situated in the upper left background. The radishes are positioned in the upper right background, and the carrots are located in the lower right foreground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27523.2, "ram_available_mb": 98249.0, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 27522.1, "ram_available_mb": 98250.0, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.85, "peak": 40.57, "min": 16.95}, "VIN": {"avg": 67.92, "peak": 118.63, "min": 29.41}, "VIN_SYS_5V0": {"avg": 15.01, "peak": 16.15, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 16.16, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 28.85, "energy_joules_est": 62.71, "sample_count": 21, "duration_seconds": 2.173}, "timestamp": "2026-01-19T14:52:40.485278"}
{"image_index": 230, "image_name": "000000023781.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023781.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2542.633, "latencies_ms": [2542.633], "images_per_second": 0.393, "prompt_tokens": 1111, "response_tokens_est": 57, "n_tiles": 1, "output_text": " The image captures a vibrant display of fresh produce, including strawberries, broccoli, radishes, carrots, and green beans, arranged in a rustic wooden crate. The setting appears to be a market or a farm stand, where the produce is being sold or displayed for customers to purchase.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27522.1, "ram_available_mb": 98250.0, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 27528.8, "ram_available_mb": 98243.4, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.48, "peak": 40.56, "min": 17.34}, "VIN": {"avg": 69.29, "peak": 96.83, "min": 29.38}, "VIN_SYS_5V0": {"avg": 15.1, "peak": 16.15, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 16.21, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 27.48, "energy_joules_est": 69.88, "sample_count": 25, "duration_seconds": 2.543}, "timestamp": "2026-01-19T14:52:43.084044"}
{"image_index": 230, "image_name": "000000023781.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023781.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2642.647, "latencies_ms": [2642.647], "images_per_second": 0.378, "prompt_tokens": 1109, "response_tokens_est": 60, "n_tiles": 1, "output_text": " The image features a variety of fresh vegetables, including strawberries, broccoli, radishes, carrots, and potatoes, all displayed in a rustic wooden crate. The lighting is natural and bright, highlighting the vibrant colors of the produce. The vegetables are arranged in a way that showcases their freshness and abundance.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 27528.8, "ram_available_mb": 98243.4, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 27536.1, "ram_available_mb": 98236.1, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.85, "peak": 40.18, "min": 16.16}, "VIN": {"avg": 67.55, "peak": 118.06, "min": 29.2}, "VIN_SYS_5V0": {"avg": 14.9, "peak": 16.05, "min": 12.25}, "VDD_CPU_SOC_MSS": {"avg": 15.93, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 26.85, "energy_joules_est": 70.97, "sample_count": 26, "duration_seconds": 2.643}, "timestamp": "2026-01-19T14:52:45.788668"}
{"image_index": 231, "image_name": "000000023899.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023899.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1652.421, "latencies_ms": [1652.421], "images_per_second": 0.605, "prompt_tokens": 1099, "response_tokens_est": 22, "n_tiles": 1, "output_text": " Three people are sitting on a couch playing a video game, with a projector projecting onto the wall behind them.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 27536.1, "ram_available_mb": 98236.1, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 27525.0, "ram_available_mb": 98247.1, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.53, "peak": 40.18, "min": 15.77}, "VIN": {"avg": 72.06, "peak": 123.82, "min": 30.06}, "VIN_SYS_5V0": {"avg": 14.93, "peak": 16.05, "min": 12.25}, "VDD_CPU_SOC_MSS": {"avg": 16.02, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 30.53, "energy_joules_est": 50.47, "sample_count": 16, "duration_seconds": 1.653}, "timestamp": "2026-01-19T14:52:47.457052"}
{"image_index": 231, "image_name": "000000023899.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023899.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1437.541, "latencies_ms": [1437.541], "images_per_second": 0.696, "prompt_tokens": 1113, "response_tokens_est": 14, "n_tiles": 1, "output_text": " projector: 1, couch: 1, person: 3", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27525.0, "ram_available_mb": 98247.1, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 27533.7, "ram_available_mb": 98238.5, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.92, "peak": 40.95, "min": 20.11}, "VIN": {"avg": 79.16, "peak": 117.96, "min": 28.57}, "VIN_SYS_5V0": {"avg": 15.26, "peak": 16.15, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 16.37, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 32.92, "energy_joules_est": 47.33, "sample_count": 14, "duration_seconds": 1.438}, "timestamp": "2026-01-19T14:52:48.918438"}
{"image_index": 231, "image_name": "000000023899.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023899.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3081.495, "latencies_ms": [3081.495], "images_per_second": 0.325, "prompt_tokens": 1117, "response_tokens_est": 74, "n_tiles": 1, "output_text": " The main objects are positioned in the foreground, with the person on the left being closest to the camera, the person in the middle being slightly farther away, and the person on the right being the farthest from the camera. The projector is located above the couch, and the person on the right is holding a game controller, which is positioned in the middle of the couch.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27533.7, "ram_available_mb": 98238.5, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 27548.1, "ram_available_mb": 98224.0, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.61, "peak": 40.95, "min": 18.13}, "VIN": {"avg": 64.12, "peak": 96.76, "min": 28.85}, "VIN_SYS_5V0": {"avg": 14.99, "peak": 16.15, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 15.98, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 26.61, "energy_joules_est": 82.02, "sample_count": 30, "duration_seconds": 3.082}, "timestamp": "2026-01-19T14:52:52.046349"}
{"image_index": 231, "image_name": "000000023899.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023899.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1397.178, "latencies_ms": [1397.178], "images_per_second": 0.716, "prompt_tokens": 1111, "response_tokens_est": 12, "n_tiles": 1, "output_text": " Three men are sitting on a couch playing a video game.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27548.1, "ram_available_mb": 98224.0, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 6.8, "ram_used_mb": 27542.9, "ram_available_mb": 98229.3, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.4, "peak": 40.18, "min": 15.77}, "VIN": {"avg": 71.83, "peak": 121.04, "min": 30.08}, "VIN_SYS_5V0": {"avg": 14.96, "peak": 16.15, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 16.23, "peak": 18.11, "min": 13.39}}, "power_watts_avg": 31.4, "energy_joules_est": 43.88, "sample_count": 14, "duration_seconds": 1.398}, "timestamp": "2026-01-19T14:52:53.505297"}
{"image_index": 231, "image_name": "000000023899.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023899.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2040.629, "latencies_ms": [2040.629], "images_per_second": 0.49, "prompt_tokens": 1109, "response_tokens_est": 32, "n_tiles": 1, "output_text": " The image is a black and white photo with a blue and white projector on a black table. The room is dimly lit with a window in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27542.9, "ram_available_mb": 98229.3, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 27541.8, "ram_available_mb": 98230.4, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.53, "peak": 40.97, "min": 18.91}, "VIN": {"avg": 73.21, "peak": 128.19, "min": 28.09}, "VIN_SYS_5V0": {"avg": 14.74, "peak": 16.15, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.81, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 29.53, "energy_joules_est": 60.27, "sample_count": 20, "duration_seconds": 2.041}, "timestamp": "2026-01-19T14:52:55.596068"}
{"image_index": 232, "image_name": "000000023937.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023937.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1928.967, "latencies_ms": [1928.967], "images_per_second": 0.518, "prompt_tokens": 1099, "response_tokens_est": 33, "n_tiles": 1, "output_text": " In the image, a group of sheep are resting in a lush green field, with a tree providing shade and a distant herd of cows grazing peacefully in the background.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 27541.8, "ram_available_mb": 98230.4, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 27551.5, "ram_available_mb": 98220.7, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.34, "peak": 40.56, "min": 15.77}, "VIN": {"avg": 75.08, "peak": 125.68, "min": 28.67}, "VIN_SYS_5V0": {"avg": 14.91, "peak": 16.05, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.93, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 29.34, "energy_joules_est": 56.6, "sample_count": 19, "duration_seconds": 1.929}, "timestamp": "2026-01-19T14:52:57.578301"}
{"image_index": 232, "image_name": "000000023937.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023937.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1186.523, "latencies_ms": [1186.523], "images_per_second": 0.843, "prompt_tokens": 1113, "response_tokens_est": 4, "n_tiles": 1, "output_text": " sheep: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27551.5, "ram_available_mb": 98220.7, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 27551.2, "ram_available_mb": 98220.9, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.6, "peak": 40.57, "min": 17.74}, "VIN": {"avg": 81.21, "peak": 129.44, "min": 29.01}, "VIN_SYS_5V0": {"avg": 14.97, "peak": 16.15, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 16.11, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 32.6, "energy_joules_est": 38.71, "sample_count": 12, "duration_seconds": 1.187}, "timestamp": "2026-01-19T14:52:58.829430"}
{"image_index": 232, "image_name": "000000023937.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023937.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2121.445, "latencies_ms": [2121.445], "images_per_second": 0.471, "prompt_tokens": 1117, "response_tokens_est": 38, "n_tiles": 1, "output_text": " The sheep are positioned in the foreground, with the tree trunk to their right. The cows are located in the background, with the tree trunk serving as a boundary between the foreground and background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27551.2, "ram_available_mb": 98220.9, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 27554.7, "ram_available_mb": 98217.4, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.47, "peak": 41.36, "min": 18.91}, "VIN": {"avg": 72.79, "peak": 126.86, "min": 30.48}, "VIN_SYS_5V0": {"avg": 14.94, "peak": 16.15, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 16.07, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 30.47, "energy_joules_est": 64.65, "sample_count": 21, "duration_seconds": 2.122}, "timestamp": "2026-01-19T14:53:01.019434"}
{"image_index": 232, "image_name": "000000023937.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023937.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2786.512, "latencies_ms": [2786.512], "images_per_second": 0.359, "prompt_tokens": 1111, "response_tokens_est": 65, "n_tiles": 1, "output_text": " In a serene rural setting, a group of sheep are peacefully grazing in a lush green field. The sheep, varying in shades of white and brown, are scattered across the grassy expanse, some lying down while others are standing. In the background, a tree stands tall, adding a touch of nature's beauty to the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27554.7, "ram_available_mb": 98217.4, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 27540.1, "ram_available_mb": 98232.1, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.57, "peak": 40.18, "min": 16.16}, "VIN": {"avg": 68.99, "peak": 113.16, "min": 30.09}, "VIN_SYS_5V0": {"avg": 14.91, "peak": 16.05, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.94, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 26.57, "energy_joules_est": 74.05, "sample_count": 27, "duration_seconds": 2.787}, "timestamp": "2026-01-19T14:53:03.836028"}
{"image_index": 232, "image_name": "000000023937.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023937.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2679.255, "latencies_ms": [2679.255], "images_per_second": 0.373, "prompt_tokens": 1109, "response_tokens_est": 62, "n_tiles": 1, "output_text": " The image features a lush green field with a tree trunk in the foreground, and a herd of cows grazing peacefully in the background. The lighting is natural and bright, suggesting it is daytime. The colors are vibrant and the materials appear to be natural, with the grass being green and the tree trunk being brown.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27540.1, "ram_available_mb": 98232.1, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 27546.0, "ram_available_mb": 98226.2, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.94, "peak": 40.57, "min": 16.16}, "VIN": {"avg": 71.76, "peak": 106.61, "min": 29.9}, "VIN_SYS_5V0": {"avg": 15.07, "peak": 16.15, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 16.19, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 26.94, "energy_joules_est": 72.19, "sample_count": 26, "duration_seconds": 2.68}, "timestamp": "2026-01-19T14:53:06.540500"}
{"image_index": 233, "image_name": "000000024021.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024021.jpg", "image_width": 640, "image_height": 390, "image_resolution": "640x390", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1738.27, "latencies_ms": [1738.27], "images_per_second": 0.575, "prompt_tokens": 1099, "response_tokens_est": 26, "n_tiles": 1, "output_text": " A black and white photograph of a large group of people, including both adults and children, posing together in front of a building.", "error": null, "sys_before": {"cpu_percent": 14.3, "ram_used_mb": 27546.0, "ram_available_mb": 98226.2, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 27549.3, "ram_available_mb": 98222.9, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.4, "peak": 40.57, "min": 16.56}, "VIN": {"avg": 70.97, "peak": 115.25, "min": 29.11}, "VIN_SYS_5V0": {"avg": 15.15, "peak": 16.15, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 16.33, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 30.4, "energy_joules_est": 52.86, "sample_count": 17, "duration_seconds": 1.739}, "timestamp": "2026-01-19T14:53:08.317443"}
{"image_index": 233, "image_name": "000000024021.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024021.jpg", "image_width": 640, "image_height": 390, "image_resolution": "640x390", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3626.849, "latencies_ms": [3626.849], "images_per_second": 0.276, "prompt_tokens": 1113, "response_tokens_est": 99, "n_tiles": 1, "output_text": " 1. group of boys: 100\n2. boys in suits: 10\n3. boys in school uniforms: 10\n4. boys in ties: 10\n5. boys in ties and suits: 10\n6. boys in ties and school uniforms: 10\n7. boys in ties and suits and school uniforms: 10\n8. boys in ties and suits and school uniforms and ties: 10", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27549.3, "ram_available_mb": 98222.9, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 27550.2, "ram_available_mb": 98222.0, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 25.36, "peak": 40.56, "min": 18.91}, "VIN": {"avg": 66.35, "peak": 102.29, "min": 29.12}, "VIN_SYS_5V0": {"avg": 15.11, "peak": 16.15, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 16.13, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 25.36, "energy_joules_est": 91.99, "sample_count": 35, "duration_seconds": 3.627}, "timestamp": "2026-01-19T14:53:11.962083"}
{"image_index": 233, "image_name": "000000024021.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024021.jpg", "image_width": 640, "image_height": 390, "image_resolution": "640x390", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2256.19, "latencies_ms": [2256.19], "images_per_second": 0.443, "prompt_tokens": 1117, "response_tokens_est": 46, "n_tiles": 1, "output_text": " The group of boys is positioned in the foreground of the image, with the building in the background. The boys are arranged in a grid-like pattern, with some standing and others sitting, creating a sense of depth and perspective.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27550.2, "ram_available_mb": 98222.0, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 27557.1, "ram_available_mb": 98215.1, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.35, "peak": 40.56, "min": 16.56}, "VIN": {"avg": 70.93, "peak": 119.74, "min": 29.97}, "VIN_SYS_5V0": {"avg": 15.19, "peak": 16.15, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 16.32, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 28.35, "energy_joules_est": 63.97, "sample_count": 22, "duration_seconds": 2.256}, "timestamp": "2026-01-19T14:53:14.257828"}
{"image_index": 233, "image_name": "000000024021.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024021.jpg", "image_width": 640, "image_height": 390, "image_resolution": "640x390", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1656.002, "latencies_ms": [1656.002], "images_per_second": 0.604, "prompt_tokens": 1111, "response_tokens_est": 22, "n_tiles": 1, "output_text": " A large group of people, both boys and men, are posing for a picture in front of a building.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27557.1, "ram_available_mb": 98215.1, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 27556.0, "ram_available_mb": 98216.2, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.9, "peak": 40.57, "min": 16.56}, "VIN": {"avg": 77.05, "peak": 124.81, "min": 28.88}, "VIN_SYS_5V0": {"avg": 15.09, "peak": 16.15, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 16.17, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 30.9, "energy_joules_est": 51.19, "sample_count": 16, "duration_seconds": 1.656}, "timestamp": "2026-01-19T14:53:15.931416"}
{"image_index": 233, "image_name": "000000024021.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024021.jpg", "image_width": 640, "image_height": 390, "image_resolution": "640x390", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1940.561, "latencies_ms": [1940.561], "images_per_second": 0.515, "prompt_tokens": 1109, "response_tokens_est": 34, "n_tiles": 1, "output_text": " The image is in black and white, and the lighting is even, with no shadows or highlights. The material of the photograph is paper, and the weather is clear.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 27556.0, "ram_available_mb": 98216.2, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 27561.8, "ram_available_mb": 98210.4, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.31, "peak": 40.56, "min": 20.1}, "VIN": {"avg": 74.96, "peak": 123.77, "min": 28.9}, "VIN_SYS_5V0": {"avg": 15.19, "peak": 16.15, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 16.35, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 30.31, "energy_joules_est": 58.83, "sample_count": 19, "duration_seconds": 1.941}, "timestamp": "2026-01-19T14:53:17.920289"}
{"image_index": 234, "image_name": "000000024027.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024027.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1670.958, "latencies_ms": [1670.958], "images_per_second": 0.598, "prompt_tokens": 1099, "response_tokens_est": 23, "n_tiles": 1, "output_text": " A colorful kite with a long tail is flying high in the sky, with a building and trees in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27561.8, "ram_available_mb": 98210.4, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 6.5, "ram_used_mb": 27535.0, "ram_available_mb": 98237.2, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.45, "peak": 40.57, "min": 17.34}, "VIN": {"avg": 74.23, "peak": 115.26, "min": 27.94}, "VIN_SYS_5V0": {"avg": 14.99, "peak": 16.15, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 16.42, "peak": 18.11, "min": 13.79}}, "power_watts_avg": 30.45, "energy_joules_est": 50.89, "sample_count": 17, "duration_seconds": 1.671}, "timestamp": "2026-01-19T14:53:19.694029"}
{"image_index": 234, "image_name": "000000024027.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024027.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2666.932, "latencies_ms": [2666.932], "images_per_second": 0.375, "prompt_tokens": 1113, "response_tokens_est": 61, "n_tiles": 1, "output_text": " 1. Kite: 1\n2. Sky: 2\n3. Clouds: 2\n4. Trees: 2\n5. Buildings: 1\n6. People: 1\n7. Kite string: 1\n8. Kite tail: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27535.0, "ram_available_mb": 98237.2, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 27546.1, "ram_available_mb": 98226.1, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.18, "peak": 40.18, "min": 17.74}, "VIN": {"avg": 73.74, "peak": 131.37, "min": 28.64}, "VIN_SYS_5V0": {"avg": 14.93, "peak": 16.05, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 16.01, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 27.18, "energy_joules_est": 72.5, "sample_count": 26, "duration_seconds": 2.667}, "timestamp": "2026-01-19T14:53:22.388611"}
{"image_index": 234, "image_name": "000000024027.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024027.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2098.783, "latencies_ms": [2098.783], "images_per_second": 0.476, "prompt_tokens": 1117, "response_tokens_est": 40, "n_tiles": 1, "output_text": " The kite is in the foreground, flying high in the sky, while the buildings are in the background. The kite is positioned to the left of the buildings, and the sky is to the right.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27546.1, "ram_available_mb": 98226.1, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 27554.0, "ram_available_mb": 98218.2, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.65, "peak": 40.57, "min": 16.55}, "VIN": {"avg": 68.99, "peak": 121.8, "min": 30.07}, "VIN_SYS_5V0": {"avg": 15.08, "peak": 16.26, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 16.18, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 28.65, "energy_joules_est": 60.14, "sample_count": 21, "duration_seconds": 2.099}, "timestamp": "2026-01-19T14:53:24.580890"}
{"image_index": 234, "image_name": "000000024027.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024027.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1630.935, "latencies_ms": [1630.935], "images_per_second": 0.613, "prompt_tokens": 1111, "response_tokens_est": 20, "n_tiles": 1, "output_text": " A colorful kite is flying high in the sky over a park with trees and buildings in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27554.0, "ram_available_mb": 98218.2, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 27552.1, "ram_available_mb": 98220.1, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.53, "peak": 40.18, "min": 15.77}, "VIN": {"avg": 81.15, "peak": 132.6, "min": 27.58}, "VIN_SYS_5V0": {"avg": 14.76, "peak": 16.05, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.8, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 30.53, "energy_joules_est": 49.8, "sample_count": 16, "duration_seconds": 1.631}, "timestamp": "2026-01-19T14:53:26.255818"}
{"image_index": 234, "image_name": "000000024027.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024027.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2307.542, "latencies_ms": [2307.542], "images_per_second": 0.433, "prompt_tokens": 1109, "response_tokens_est": 46, "n_tiles": 1, "output_text": " The kite is a vibrant mix of colors, with a long tail that trails behind it. The sky is a clear blue with fluffy white clouds, and the kite is flying high above a park with trees and buildings in the background.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 27552.1, "ram_available_mb": 98220.1, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 27561.2, "ram_available_mb": 98211.0, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.26, "peak": 40.56, "min": 18.13}, "VIN": {"avg": 67.07, "peak": 105.39, "min": 28.89}, "VIN_SYS_5V0": {"avg": 14.92, "peak": 15.95, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 15.82, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 28.26, "energy_joules_est": 65.22, "sample_count": 23, "duration_seconds": 2.308}, "timestamp": "2026-01-19T14:53:28.647667"}
{"image_index": 235, "image_name": "000000024144.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024144.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1570.96, "latencies_ms": [1570.96], "images_per_second": 0.637, "prompt_tokens": 1099, "response_tokens_est": 19, "n_tiles": 1, "output_text": " A large pizza with a generous amount of cheese and tomato sauce is placed in a cardboard box.", "error": null, "sys_before": {"cpu_percent": 11.8, "ram_used_mb": 27561.2, "ram_available_mb": 98211.0, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 27566.0, "ram_available_mb": 98206.2, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.46, "peak": 40.18, "min": 15.38}, "VIN": {"avg": 77.44, "peak": 128.2, "min": 28.86}, "VIN_SYS_5V0": {"avg": 14.75, "peak": 16.05, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.85, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 30.46, "energy_joules_est": 47.87, "sample_count": 16, "duration_seconds": 1.572}, "timestamp": "2026-01-19T14:53:30.323996"}
{"image_index": 235, "image_name": "000000024144.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024144.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2016.225, "latencies_ms": [2016.225], "images_per_second": 0.496, "prompt_tokens": 1113, "response_tokens_est": 36, "n_tiles": 1, "output_text": " pizza: 1, box: 1, pizza box: 1, cheese: 1, sauce: 1, pepperoni: 1, crust: 1", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 27566.0, "ram_available_mb": 98206.2, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 27568.3, "ram_available_mb": 98203.9, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.53, "peak": 40.57, "min": 18.53}, "VIN": {"avg": 67.64, "peak": 128.16, "min": 29.18}, "VIN_SYS_5V0": {"avg": 14.83, "peak": 16.05, "min": 11.87}, "VDD_CPU_SOC_MSS": {"avg": 15.83, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 29.53, "energy_joules_est": 59.55, "sample_count": 20, "duration_seconds": 2.017}, "timestamp": "2026-01-19T14:53:32.404017"}
{"image_index": 235, "image_name": "000000024144.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024144.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1986.419, "latencies_ms": [1986.419], "images_per_second": 0.503, "prompt_tokens": 1117, "response_tokens_est": 35, "n_tiles": 1, "output_text": " The pizza is in the foreground, with the cardboard box in the background. The pizza is on the left side of the box, and the box is on the right side.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 27568.3, "ram_available_mb": 98203.9, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 27574.0, "ram_available_mb": 98198.2, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.09, "peak": 40.56, "min": 16.95}, "VIN": {"avg": 69.96, "peak": 106.36, "min": 29.67}, "VIN_SYS_5V0": {"avg": 14.89, "peak": 16.05, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.96, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 29.09, "energy_joules_est": 57.8, "sample_count": 20, "duration_seconds": 1.987}, "timestamp": "2026-01-19T14:53:34.488882"}
{"image_index": 235, "image_name": "000000024144.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024144.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1527.074, "latencies_ms": [1527.074], "images_per_second": 0.655, "prompt_tokens": 1111, "response_tokens_est": 17, "n_tiles": 1, "output_text": " A large pizza with cheese and tomato sauce is in a cardboard box on a table.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27574.0, "ram_available_mb": 98198.2, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 27571.9, "ram_available_mb": 98200.3, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.15, "peak": 40.18, "min": 16.16}, "VIN": {"avg": 72.8, "peak": 117.11, "min": 28.21}, "VIN_SYS_5V0": {"avg": 14.73, "peak": 16.05, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.88, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 31.15, "energy_joules_est": 47.58, "sample_count": 15, "duration_seconds": 1.528}, "timestamp": "2026-01-19T14:53:36.056799"}
{"image_index": 235, "image_name": "000000024144.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024144.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1899.821, "latencies_ms": [1899.821], "images_per_second": 0.526, "prompt_tokens": 1109, "response_tokens_est": 31, "n_tiles": 1, "output_text": " The pizza is in a cardboard box with a white and brown color scheme. The lighting is natural and the pizza is in a room with a black background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27571.9, "ram_available_mb": 98200.3, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 27585.9, "ram_available_mb": 98186.3, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.34, "peak": 40.97, "min": 19.7}, "VIN": {"avg": 70.65, "peak": 114.04, "min": 29.95}, "VIN_SYS_5V0": {"avg": 15.0, "peak": 16.15, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 16.04, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 30.34, "energy_joules_est": 57.65, "sample_count": 19, "duration_seconds": 1.9}, "timestamp": "2026-01-19T14:53:38.030800"}
{"image_index": 236, "image_name": "000000024243.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024243.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1748.448, "latencies_ms": [1748.448], "images_per_second": 0.572, "prompt_tokens": 1099, "response_tokens_est": 26, "n_tiles": 1, "output_text": " A woman sits on the edge of an open refrigerator, talking on her cell phone, while a man sits on the curb nearby.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27585.9, "ram_available_mb": 98186.3, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 27590.6, "ram_available_mb": 98181.6, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.4, "peak": 40.57, "min": 16.56}, "VIN": {"avg": 73.69, "peak": 107.83, "min": 29.88}, "VIN_SYS_5V0": {"avg": 14.89, "peak": 16.05, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.93, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 30.4, "energy_joules_est": 53.17, "sample_count": 17, "duration_seconds": 1.749}, "timestamp": "2026-01-19T14:53:39.816950"}
{"image_index": 236, "image_name": "000000024243.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024243.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2522.608, "latencies_ms": [2522.608], "images_per_second": 0.396, "prompt_tokens": 1113, "response_tokens_est": 56, "n_tiles": 1, "output_text": " 1. person: 2\n2. jacket: 1\n3. pants: 1\n4. shoes: 1\n5. refrigerator: 1\n6. cup: 2\n7. glass: 1\n8. door: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27590.6, "ram_available_mb": 98181.6, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 27592.9, "ram_available_mb": 98179.3, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.75, "peak": 40.56, "min": 18.14}, "VIN": {"avg": 73.98, "peak": 117.61, "min": 28.73}, "VIN_SYS_5V0": {"avg": 15.13, "peak": 16.15, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 16.13, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 27.75, "energy_joules_est": 70.02, "sample_count": 25, "duration_seconds": 2.523}, "timestamp": "2026-01-19T14:53:42.414850"}
{"image_index": 236, "image_name": "000000024243.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024243.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2309.094, "latencies_ms": [2309.094], "images_per_second": 0.433, "prompt_tokens": 1117, "response_tokens_est": 48, "n_tiles": 1, "output_text": " The woman is sitting on the open door of the refrigerator, which is located on the right side of the image. The refrigerator is situated on the left side of the image, and the woman is positioned closer to the camera than the refrigerator.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27592.9, "ram_available_mb": 98179.3, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 27554.7, "ram_available_mb": 98217.5, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.68, "peak": 40.56, "min": 15.38}, "VIN": {"avg": 64.71, "peak": 114.39, "min": 29.09}, "VIN_SYS_5V0": {"avg": 14.97, "peak": 16.15, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 16.24, "peak": 17.71, "min": 13.39}}, "power_watts_avg": 27.68, "energy_joules_est": 63.93, "sample_count": 23, "duration_seconds": 2.31}, "timestamp": "2026-01-19T14:53:44.808356"}
{"image_index": 236, "image_name": "000000024243.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024243.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1696.286, "latencies_ms": [1696.286], "images_per_second": 0.59, "prompt_tokens": 1111, "response_tokens_est": 24, "n_tiles": 1, "output_text": " A woman sits on the edge of an open refrigerator on a city street, while a man sits on a bench nearby.", "error": null, "sys_before": {"cpu_percent": 40.0, "ram_used_mb": 27554.7, "ram_available_mb": 98217.5, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 27556.2, "ram_available_mb": 98215.9, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.19, "peak": 40.56, "min": 15.77}, "VIN": {"avg": 72.3, "peak": 112.45, "min": 28.83}, "VIN_SYS_5V0": {"avg": 14.89, "peak": 16.05, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 16.0, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 30.19, "energy_joules_est": 51.22, "sample_count": 17, "duration_seconds": 1.697}, "timestamp": "2026-01-19T14:53:46.585179"}
{"image_index": 236, "image_name": "000000024243.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024243.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2486.328, "latencies_ms": [2486.328], "images_per_second": 0.402, "prompt_tokens": 1109, "response_tokens_est": 54, "n_tiles": 1, "output_text": " The image depicts a woman sitting on an open refrigerator door, with a man smoking a cigarette nearby. The scene is set on a street with a sidewalk and a building in the background. The lighting appears to be natural daylight, and the weather seems to be overcast.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27556.2, "ram_available_mb": 98215.9, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 27559.5, "ram_available_mb": 98212.7, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.83, "peak": 40.57, "min": 17.74}, "VIN": {"avg": 69.42, "peak": 111.0, "min": 29.15}, "VIN_SYS_5V0": {"avg": 14.93, "peak": 16.05, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.93, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 27.83, "energy_joules_est": 69.21, "sample_count": 24, "duration_seconds": 2.487}, "timestamp": "2026-01-19T14:53:49.086012"}
{"image_index": 237, "image_name": "000000024567.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024567.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1691.556, "latencies_ms": [1691.556], "images_per_second": 0.591, "prompt_tokens": 1100, "response_tokens_est": 25, "n_tiles": 1, "output_text": " A man wearing a straw hat and a green shirt is sitting at a white table with a tray of hot dogs on it.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 27559.5, "ram_available_mb": 98212.7, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 27569.1, "ram_available_mb": 98203.0, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.38, "peak": 39.77, "min": 17.35}, "VIN": {"avg": 80.68, "peak": 128.21, "min": 29.95}, "VIN_SYS_5V0": {"avg": 15.19, "peak": 16.15, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 16.42, "peak": 17.71, "min": 14.18}}, "power_watts_avg": 30.38, "energy_joules_est": 51.4, "sample_count": 17, "duration_seconds": 1.692}, "timestamp": "2026-01-19T14:53:50.866923"}
{"image_index": 237, "image_name": "000000024567.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024567.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2576.906, "latencies_ms": [2576.906], "images_per_second": 0.388, "prompt_tokens": 1114, "response_tokens_est": 58, "n_tiles": 1, "output_text": " 1. man: 1\n2. hat: 1\n3. shirt: 1\n4. chair: 1\n5. hotdogs: 12\n6. foil: 1\n7. grass: 1\n8. white: 1", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 27569.1, "ram_available_mb": 98203.0, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 8.2, "ram_used_mb": 27541.6, "ram_available_mb": 98230.5, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.39, "peak": 40.18, "min": 17.74}, "VIN": {"avg": 69.48, "peak": 123.94, "min": 31.31}, "VIN_SYS_5V0": {"avg": 15.02, "peak": 16.15, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 16.38, "peak": 18.11, "min": 13.39}}, "power_watts_avg": 27.39, "energy_joules_est": 70.59, "sample_count": 25, "duration_seconds": 2.577}, "timestamp": "2026-01-19T14:53:53.475539"}
{"image_index": 237, "image_name": "000000024567.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024567.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2101.431, "latencies_ms": [2101.431], "images_per_second": 0.476, "prompt_tokens": 1118, "response_tokens_est": 40, "n_tiles": 1, "output_text": " The man is sitting on the left side of the image, with the hot dogs in the center. The hot dogs are placed on a white table, which is positioned in the foreground of the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27541.6, "ram_available_mb": 98230.5, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 27538.4, "ram_available_mb": 98233.7, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.52, "peak": 40.18, "min": 16.56}, "VIN": {"avg": 65.12, "peak": 114.7, "min": 28.45}, "VIN_SYS_5V0": {"avg": 15.06, "peak": 16.26, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 16.2, "peak": 17.71, "min": 14.18}}, "power_watts_avg": 28.52, "energy_joules_est": 59.95, "sample_count": 21, "duration_seconds": 2.102}, "timestamp": "2026-01-19T14:53:55.661862"}
{"image_index": 237, "image_name": "000000024567.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024567.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1523.371, "latencies_ms": [1523.371], "images_per_second": 0.656, "prompt_tokens": 1112, "response_tokens_est": 18, "n_tiles": 1, "output_text": " A man wearing a straw hat is sitting at a table with a tray of hot dogs.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27538.4, "ram_available_mb": 98233.7, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 27554.0, "ram_available_mb": 98218.2, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.88, "peak": 39.39, "min": 16.16}, "VIN": {"avg": 79.67, "peak": 122.57, "min": 29.52}, "VIN_SYS_5V0": {"avg": 14.94, "peak": 16.05, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 16.09, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 30.88, "energy_joules_est": 47.05, "sample_count": 15, "duration_seconds": 1.524}, "timestamp": "2026-01-19T14:53:57.238327"}
{"image_index": 237, "image_name": "000000024567.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024567.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3056.759, "latencies_ms": [3056.759], "images_per_second": 0.327, "prompt_tokens": 1110, "response_tokens_est": 73, "n_tiles": 1, "output_text": " The image features a man wearing a straw hat and a green shirt, sitting at a white table with a tray of hot dogs. The hot dogs are placed on a piece of aluminum foil, and the man is holding a red hot dog in his hand. The lighting in the image is bright and natural, suggesting that the photo was taken outdoors on a sunny day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27554.0, "ram_available_mb": 98218.2, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 27554.6, "ram_available_mb": 98217.5, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.16, "peak": 40.56, "min": 17.73}, "VIN": {"avg": 66.74, "peak": 103.1, "min": 28.76}, "VIN_SYS_5V0": {"avg": 14.9, "peak": 16.15, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.95, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 26.16, "energy_joules_est": 79.98, "sample_count": 30, "duration_seconds": 3.057}, "timestamp": "2026-01-19T14:54:00.352556"}
{"image_index": 238, "image_name": "000000024610.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024610.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1949.931, "latencies_ms": [1949.931], "images_per_second": 0.513, "prompt_tokens": 1099, "response_tokens_est": 34, "n_tiles": 1, "output_text": " The image depicts a room with a desk, a chair, a bookshelf, and a couch, with a laptop on the desk and a star hanging on the wall.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27554.6, "ram_available_mb": 98217.5, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 6.3, "ram_used_mb": 27538.5, "ram_available_mb": 98233.7, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.17, "peak": 40.16, "min": 15.38}, "VIN": {"avg": 72.04, "peak": 123.0, "min": 29.12}, "VIN_SYS_5V0": {"avg": 15.02, "peak": 16.05, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 16.27, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 29.17, "energy_joules_est": 56.89, "sample_count": 19, "duration_seconds": 1.95}, "timestamp": "2026-01-19T14:54:02.333870"}
{"image_index": 238, "image_name": "000000024610.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024610.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2157.609, "latencies_ms": [2157.609], "images_per_second": 0.463, "prompt_tokens": 1113, "response_tokens_est": 42, "n_tiles": 1, "output_text": " chair: 1, laptop: 1, bookshelf: 1, books: 1, blanket: 1, bookshelf: 1, books: 1, bookshelf: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27538.5, "ram_available_mb": 98233.7, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 27536.7, "ram_available_mb": 98235.5, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.98, "peak": 40.56, "min": 17.73}, "VIN": {"avg": 75.23, "peak": 127.86, "min": 29.5}, "VIN_SYS_5V0": {"avg": 15.16, "peak": 16.15, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 16.31, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 28.98, "energy_joules_est": 62.54, "sample_count": 21, "duration_seconds": 2.158}, "timestamp": "2026-01-19T14:54:04.521300"}
{"image_index": 238, "image_name": "000000024610.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024610.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2280.954, "latencies_ms": [2280.954], "images_per_second": 0.438, "prompt_tokens": 1117, "response_tokens_est": 45, "n_tiles": 1, "output_text": " The laptop is on the left side of the room, the bookshelf is in the middle, and the couch is on the right side. The laptop is near the bookshelf, and the couch is near the bookshelf.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27536.7, "ram_available_mb": 98235.5, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 27533.0, "ram_available_mb": 98239.2, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.42, "peak": 40.18, "min": 17.73}, "VIN": {"avg": 71.01, "peak": 125.2, "min": 30.5}, "VIN_SYS_5V0": {"avg": 15.11, "peak": 16.15, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 16.12, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 28.42, "energy_joules_est": 64.84, "sample_count": 22, "duration_seconds": 2.281}, "timestamp": "2026-01-19T14:54:06.817213"}
{"image_index": 238, "image_name": "000000024610.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024610.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1421.107, "latencies_ms": [1421.107], "images_per_second": 0.704, "prompt_tokens": 1111, "response_tokens_est": 14, "n_tiles": 1, "output_text": " A room with a desk, bookshelf, and couch is shown.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27533.0, "ram_available_mb": 98239.2, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 27538.4, "ram_available_mb": 98233.8, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.16, "peak": 40.57, "min": 17.35}, "VIN": {"avg": 77.09, "peak": 125.43, "min": 29.54}, "VIN_SYS_5V0": {"avg": 15.23, "peak": 16.15, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 16.31, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 32.16, "energy_joules_est": 45.71, "sample_count": 14, "duration_seconds": 1.421}, "timestamp": "2026-01-19T14:54:08.279026"}
{"image_index": 238, "image_name": "000000024610.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024610.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1610.115, "latencies_ms": [1610.115], "images_per_second": 0.621, "prompt_tokens": 1109, "response_tokens_est": 21, "n_tiles": 1, "output_text": " The room is lit by a single light bulb, and the walls are painted a light beige color.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27538.4, "ram_available_mb": 98233.8, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 27552.2, "ram_available_mb": 98220.0, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.3, "peak": 40.97, "min": 21.67}, "VIN": {"avg": 72.53, "peak": 117.03, "min": 29.26}, "VIN_SYS_5V0": {"avg": 15.13, "peak": 16.15, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 16.22, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 32.3, "energy_joules_est": 52.03, "sample_count": 16, "duration_seconds": 1.611}, "timestamp": "2026-01-19T14:54:09.947182"}
{"image_index": 239, "image_name": "000000024919.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024919.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1642.616, "latencies_ms": [1642.616], "images_per_second": 0.609, "prompt_tokens": 1099, "response_tokens_est": 22, "n_tiles": 1, "output_text": " In the image, there are two elephants standing in a grassy field, with one of them wearing a hat.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 27552.2, "ram_available_mb": 98220.0, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 27553.9, "ram_available_mb": 98218.3, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.59, "peak": 40.97, "min": 19.32}, "VIN": {"avg": 73.43, "peak": 122.03, "min": 29.3}, "VIN_SYS_5V0": {"avg": 15.0, "peak": 16.05, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 16.19, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 31.59, "energy_joules_est": 51.9, "sample_count": 16, "duration_seconds": 1.643}, "timestamp": "2026-01-19T14:54:11.627543"}
{"image_index": 239, "image_name": "000000024919.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024919.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1183.33, "latencies_ms": [1183.33], "images_per_second": 0.845, "prompt_tokens": 1113, "response_tokens_est": 4, "n_tiles": 1, "output_text": " elephant: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27553.9, "ram_available_mb": 98218.3, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 27557.8, "ram_available_mb": 98214.3, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 33.35, "peak": 40.56, "min": 19.7}, "VIN": {"avg": 72.79, "peak": 96.19, "min": 29.82}, "VIN_SYS_5V0": {"avg": 15.07, "peak": 16.15, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 16.27, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 33.35, "energy_joules_est": 39.48, "sample_count": 12, "duration_seconds": 1.184}, "timestamp": "2026-01-19T14:54:12.879808"}
{"image_index": 239, "image_name": "000000024919.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024919.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3166.103, "latencies_ms": [3166.103], "images_per_second": 0.316, "prompt_tokens": 1117, "response_tokens_est": 76, "n_tiles": 1, "output_text": " The two elephants are positioned in the foreground of the image, with the one on the left slightly closer to the camera than the one on the right. The elephants are facing each other, with the one on the left appearing to be slightly larger in size. The background of the image features a hazy, green landscape with trees and bushes, suggesting a natural habitat for the elephants.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27557.8, "ram_available_mb": 98214.3, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 27538.7, "ram_available_mb": 98233.5, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.82, "peak": 40.97, "min": 17.73}, "VIN": {"avg": 65.49, "peak": 119.76, "min": 29.7}, "VIN_SYS_5V0": {"avg": 14.92, "peak": 16.15, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.94, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 26.82, "energy_joules_est": 84.92, "sample_count": 31, "duration_seconds": 3.166}, "timestamp": "2026-01-19T14:54:16.105286"}
{"image_index": 239, "image_name": "000000024919.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024919.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2765.736, "latencies_ms": [2765.736], "images_per_second": 0.362, "prompt_tokens": 1111, "response_tokens_est": 65, "n_tiles": 1, "output_text": " In the heart of a verdant savannah, two majestic elephants engage in a tender moment, their trunks entwined in a display of affection. The lush greenery of the surrounding landscape provides a serene backdrop to this intimate interaction, as the elephants stand amidst the tall grass, their massive forms contrasting with the delicate flora.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27538.7, "ram_available_mb": 98233.5, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 27535.4, "ram_available_mb": 98236.8, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.54, "peak": 40.18, "min": 15.38}, "VIN": {"avg": 64.98, "peak": 112.6, "min": 29.87}, "VIN_SYS_5V0": {"avg": 14.96, "peak": 16.05, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.94, "peak": 17.32, "min": 13.4}}, "power_watts_avg": 26.54, "energy_joules_est": 73.41, "sample_count": 27, "duration_seconds": 2.766}, "timestamp": "2026-01-19T14:54:18.897847"}
{"image_index": 239, "image_name": "000000024919.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024919.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1521.55, "latencies_ms": [1521.55], "images_per_second": 0.657, "prompt_tokens": 1109, "response_tokens_est": 17, "n_tiles": 1, "output_text": " The elephants are brown, the sky is overcast, and the vegetation is green.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27535.4, "ram_available_mb": 98236.8, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 27539.3, "ram_available_mb": 98232.9, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.2, "peak": 40.16, "min": 16.16}, "VIN": {"avg": 68.82, "peak": 81.98, "min": 30.41}, "VIN_SYS_5V0": {"avg": 15.09, "peak": 16.15, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 16.25, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 31.2, "energy_joules_est": 47.48, "sample_count": 15, "duration_seconds": 1.522}, "timestamp": "2026-01-19T14:54:20.461489"}
{"image_index": 240, "image_name": "000000025057.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025057.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2026.136, "latencies_ms": [2026.136], "images_per_second": 0.494, "prompt_tokens": 1099, "response_tokens_est": 37, "n_tiles": 1, "output_text": " A shirtless man wearing a white baseball cap and sunglasses is holding a white frisbee in his right hand and a green bottle in his left hand while walking on a grassy field.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 27539.3, "ram_available_mb": 98232.9, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 27542.5, "ram_available_mb": 98229.7, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.9, "peak": 40.97, "min": 19.7}, "VIN": {"avg": 68.68, "peak": 119.83, "min": 28.02}, "VIN_SYS_5V0": {"avg": 15.08, "peak": 16.15, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 16.12, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 29.9, "energy_joules_est": 60.6, "sample_count": 20, "duration_seconds": 2.027}, "timestamp": "2026-01-19T14:54:22.554204"}
{"image_index": 240, "image_name": "000000025057.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025057.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2588.582, "latencies_ms": [2588.582], "images_per_second": 0.386, "prompt_tokens": 1113, "response_tokens_est": 58, "n_tiles": 1, "output_text": " 1. man: 1\n2. shorts: 1\n3. cap: 1\n4. frisbee: 1\n5. bottle: 1\n6. grass: 1\n7. trees: 1\n8. sky: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27542.5, "ram_available_mb": 98229.7, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 27549.2, "ram_available_mb": 98223.0, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.28, "peak": 40.57, "min": 16.56}, "VIN": {"avg": 64.1, "peak": 117.6, "min": 32.16}, "VIN_SYS_5V0": {"avg": 14.99, "peak": 16.15, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 16.0, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 27.28, "energy_joules_est": 70.63, "sample_count": 25, "duration_seconds": 2.589}, "timestamp": "2026-01-19T14:54:25.149546"}
{"image_index": 240, "image_name": "000000025057.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025057.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2443.167, "latencies_ms": [2443.167], "images_per_second": 0.409, "prompt_tokens": 1117, "response_tokens_est": 53, "n_tiles": 1, "output_text": " The man is in the foreground, holding a frisbee and wearing a cap. The frisbee is in his hand, and he is standing on a grassy field. The background shows a person walking in the distance, and there are trees behind the man.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27549.2, "ram_available_mb": 98223.0, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 27558.7, "ram_available_mb": 98213.4, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.69, "peak": 40.56, "min": 16.95}, "VIN": {"avg": 73.9, "peak": 125.58, "min": 28.63}, "VIN_SYS_5V0": {"avg": 15.16, "peak": 16.26, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 16.32, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 27.69, "energy_joules_est": 67.66, "sample_count": 24, "duration_seconds": 2.443}, "timestamp": "2026-01-19T14:54:27.648330"}
{"image_index": 240, "image_name": "000000025057.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025057.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1499.577, "latencies_ms": [1499.577], "images_per_second": 0.667, "prompt_tokens": 1111, "response_tokens_est": 16, "n_tiles": 1, "output_text": " A man is playing frisbee in a field with trees in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27558.7, "ram_available_mb": 98213.4, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 27557.0, "ram_available_mb": 98215.2, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.15, "peak": 40.16, "min": 16.16}, "VIN": {"avg": 71.77, "peak": 105.84, "min": 28.69}, "VIN_SYS_5V0": {"avg": 14.99, "peak": 16.05, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 16.12, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 31.15, "energy_joules_est": 46.72, "sample_count": 15, "duration_seconds": 1.5}, "timestamp": "2026-01-19T14:54:29.213861"}
{"image_index": 240, "image_name": "000000025057.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025057.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2181.19, "latencies_ms": [2181.19], "images_per_second": 0.458, "prompt_tokens": 1109, "response_tokens_est": 42, "n_tiles": 1, "output_text": " The image features a man in a white cap and shorts, holding a frisbee in his hand, with a clear blue sky in the background. The grass is green, and the man is wearing sunglasses.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27557.0, "ram_available_mb": 98215.2, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 27574.1, "ram_available_mb": 98198.1, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.38, "peak": 40.56, "min": 20.11}, "VIN": {"avg": 73.71, "peak": 122.75, "min": 32.56}, "VIN_SYS_5V0": {"avg": 15.03, "peak": 16.05, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 16.05, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 29.38, "energy_joules_est": 64.09, "sample_count": 21, "duration_seconds": 2.182}, "timestamp": "2026-01-19T14:54:31.399358"}
{"image_index": 241, "image_name": "000000025096.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025096.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1714.409, "latencies_ms": [1714.409], "images_per_second": 0.583, "prompt_tokens": 1100, "response_tokens_est": 26, "n_tiles": 1, "output_text": " A young boy wearing a blue sports jersey is cutting a chocolate cake with a knife on a table with a colorful tablecloth.", "error": null, "sys_before": {"cpu_percent": 14.3, "ram_used_mb": 27574.1, "ram_available_mb": 98198.1, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 27575.4, "ram_available_mb": 98196.8, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.61, "peak": 39.77, "min": 18.91}, "VIN": {"avg": 64.33, "peak": 104.64, "min": 28.66}, "VIN_SYS_5V0": {"avg": 15.32, "peak": 16.26, "min": 13.49}, "VDD_CPU_SOC_MSS": {"avg": 16.49, "peak": 17.71, "min": 14.96}}, "power_watts_avg": 30.61, "energy_joules_est": 52.49, "sample_count": 17, "duration_seconds": 1.715}, "timestamp": "2026-01-19T14:54:33.177618"}
{"image_index": 241, "image_name": "000000025096.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025096.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2552.577, "latencies_ms": [2552.577], "images_per_second": 0.392, "prompt_tokens": 1114, "response_tokens_est": 57, "n_tiles": 1, "output_text": " 1. boy: 1\n2. knife: 1\n3. plate: 1\n4. cake: 1\n5. tablecloth: 1\n6. wall: 1\n7. chair: 1\n8. person: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27575.4, "ram_available_mb": 98196.8, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 27584.7, "ram_available_mb": 98187.4, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.4, "peak": 40.57, "min": 18.14}, "VIN": {"avg": 67.66, "peak": 84.16, "min": 29.26}, "VIN_SYS_5V0": {"avg": 15.07, "peak": 16.26, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 16.08, "peak": 17.71, "min": 13.39}}, "power_watts_avg": 27.4, "energy_joules_est": 69.95, "sample_count": 25, "duration_seconds": 2.553}, "timestamp": "2026-01-19T14:54:35.775968"}
{"image_index": 241, "image_name": "000000025096.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025096.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2095.965, "latencies_ms": [2095.965], "images_per_second": 0.477, "prompt_tokens": 1118, "response_tokens_est": 40, "n_tiles": 1, "output_text": " The boy is in the foreground, leaning over the table with a knife in his hand. The cake is in the middle of the table, and the plate is on the right side of the table.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27584.7, "ram_available_mb": 98187.4, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 27584.2, "ram_available_mb": 98188.0, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.5, "peak": 39.78, "min": 16.16}, "VIN": {"avg": 72.11, "peak": 132.28, "min": 27.77}, "VIN_SYS_5V0": {"avg": 15.04, "peak": 16.15, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 16.14, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 28.5, "energy_joules_est": 59.74, "sample_count": 21, "duration_seconds": 2.096}, "timestamp": "2026-01-19T14:54:37.962764"}
{"image_index": 241, "image_name": "000000025096.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025096.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1810.41, "latencies_ms": [1810.41], "images_per_second": 0.552, "prompt_tokens": 1112, "response_tokens_est": 29, "n_tiles": 1, "output_text": " A young boy wearing a blue shirt is cutting a cake with a knife. The cake is decorated with chocolate and has a toy car on top.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27584.2, "ram_available_mb": 98188.0, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 27591.3, "ram_available_mb": 98180.8, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.37, "peak": 39.39, "min": 15.77}, "VIN": {"avg": 69.6, "peak": 116.64, "min": 30.6}, "VIN_SYS_5V0": {"avg": 14.9, "peak": 16.05, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 16.01, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 29.37, "energy_joules_est": 53.18, "sample_count": 18, "duration_seconds": 1.811}, "timestamp": "2026-01-19T14:54:39.840938"}
{"image_index": 241, "image_name": "000000025096.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025096.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2162.212, "latencies_ms": [2162.212], "images_per_second": 0.462, "prompt_tokens": 1110, "response_tokens_est": 42, "n_tiles": 1, "output_text": " The boy is wearing a blue shirt and is cutting a cake with a knife. The cake is decorated with chocolate and has a chocolate roll on top. The cake is on a table with a colorful tablecloth.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27591.3, "ram_available_mb": 98180.8, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 6.3, "ram_used_mb": 27555.0, "ram_available_mb": 98217.2, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.7, "peak": 40.16, "min": 17.34}, "VIN": {"avg": 68.71, "peak": 106.65, "min": 28.79}, "VIN_SYS_5V0": {"avg": 15.06, "peak": 16.15, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 16.22, "peak": 17.71, "min": 13.39}}, "power_watts_avg": 28.7, "energy_joules_est": 62.06, "sample_count": 21, "duration_seconds": 2.163}, "timestamp": "2026-01-19T14:54:42.041982"}
{"image_index": 242, "image_name": "000000025139.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025139.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2051.925, "latencies_ms": [2051.925], "images_per_second": 0.487, "prompt_tokens": 1099, "response_tokens_est": 38, "n_tiles": 1, "output_text": " The image features a close-up of a zebra's face, showcasing its distinctive black and white stripes, with another zebra in the background, partially visible, and a metal fence separating them.", "error": null, "sys_before": {"cpu_percent": 14.3, "ram_used_mb": 27555.0, "ram_available_mb": 98217.2, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 27555.2, "ram_available_mb": 98217.0, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.05, "peak": 40.56, "min": 16.95}, "VIN": {"avg": 66.82, "peak": 116.33, "min": 29.49}, "VIN_SYS_5V0": {"avg": 15.11, "peak": 16.15, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 16.16, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 29.05, "energy_joules_est": 59.62, "sample_count": 20, "duration_seconds": 2.052}, "timestamp": "2026-01-19T14:54:44.136720"}
{"image_index": 242, "image_name": "000000025139.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025139.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1197.749, "latencies_ms": [1197.749], "images_per_second": 0.835, "prompt_tokens": 1113, "response_tokens_est": 4, "n_tiles": 1, "output_text": " zebra: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27555.2, "ram_available_mb": 98217.0, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 27548.3, "ram_available_mb": 98223.9, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.37, "peak": 40.18, "min": 17.35}, "VIN": {"avg": 80.51, "peak": 124.37, "min": 30.77}, "VIN_SYS_5V0": {"avg": 14.98, "peak": 16.15, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 16.21, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 32.37, "energy_joules_est": 38.79, "sample_count": 12, "duration_seconds": 1.198}, "timestamp": "2026-01-19T14:54:45.388615"}
{"image_index": 242, "image_name": "000000025139.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025139.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2137.17, "latencies_ms": [2137.17], "images_per_second": 0.468, "prompt_tokens": 1117, "response_tokens_est": 40, "n_tiles": 1, "output_text": " The zebra in the foreground is close to the camera, while the other zebra is farther away. The zebra in the foreground is eating from a metal feeder, while the other zebra is standing behind it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27548.3, "ram_available_mb": 98223.9, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 27550.5, "ram_available_mb": 98221.7, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.62, "peak": 41.34, "min": 19.7}, "VIN": {"avg": 69.36, "peak": 96.96, "min": 29.59}, "VIN_SYS_5V0": {"avg": 14.99, "peak": 16.15, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 16.12, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 30.62, "energy_joules_est": 65.45, "sample_count": 21, "duration_seconds": 2.137}, "timestamp": "2026-01-19T14:54:47.574490"}
{"image_index": 242, "image_name": "000000025139.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025139.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1535.651, "latencies_ms": [1535.651], "images_per_second": 0.651, "prompt_tokens": 1111, "response_tokens_est": 17, "n_tiles": 1, "output_text": " A group of zebras are standing in a pen, eating grass and looking around.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27550.5, "ram_available_mb": 98221.7, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 27555.9, "ram_available_mb": 98216.3, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.44, "peak": 40.57, "min": 16.95}, "VIN": {"avg": 75.81, "peak": 126.43, "min": 28.88}, "VIN_SYS_5V0": {"avg": 14.99, "peak": 16.05, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 16.17, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 31.44, "energy_joules_est": 48.29, "sample_count": 15, "duration_seconds": 1.536}, "timestamp": "2026-01-19T14:54:49.138200"}
{"image_index": 242, "image_name": "000000025139.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025139.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2537.766, "latencies_ms": [2537.766], "images_per_second": 0.394, "prompt_tokens": 1109, "response_tokens_est": 57, "n_tiles": 1, "output_text": " The image features a close-up of a zebra's face, with its distinctive black and white stripes, and a background that includes a fence and some greenery. The lighting in the image is natural, likely from sunlight, and the zebra appears to be in a zoo or wildlife park.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27555.9, "ram_available_mb": 98216.3, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 27563.7, "ram_available_mb": 98208.5, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.02, "peak": 40.95, "min": 17.73}, "VIN": {"avg": 68.43, "peak": 124.44, "min": 30.01}, "VIN_SYS_5V0": {"avg": 15.13, "peak": 16.15, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 16.11, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 28.02, "energy_joules_est": 71.12, "sample_count": 25, "duration_seconds": 2.538}, "timestamp": "2026-01-19T14:54:51.765321"}
{"image_index": 243, "image_name": "000000025181.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025181.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2195.484, "latencies_ms": [2195.484], "images_per_second": 0.455, "prompt_tokens": 1099, "response_tokens_est": 43, "n_tiles": 1, "output_text": " The black and white photo captures a train station with a sign that reads \"La Spezia Centrale\" and a train on the tracks, with a bench on the platform and a mountain in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27563.7, "ram_available_mb": 98208.5, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 27565.4, "ram_available_mb": 98206.7, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.87, "peak": 40.18, "min": 15.38}, "VIN": {"avg": 68.3, "peak": 114.23, "min": 30.37}, "VIN_SYS_5V0": {"avg": 14.76, "peak": 16.05, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.87, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 27.87, "energy_joules_est": 61.2, "sample_count": 22, "duration_seconds": 2.196}, "timestamp": "2026-01-19T14:54:54.068451"}
{"image_index": 243, "image_name": "000000025181.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025181.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2583.621, "latencies_ms": [2583.621], "images_per_second": 0.387, "prompt_tokens": 1113, "response_tokens_est": 58, "n_tiles": 1, "output_text": " 1. sign: 1\n2. train: 1\n3. bench: 1\n4. platform: 1\n5. train tracks: 2\n6. train station: 1\n7. mountain: 1\n8. sky: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27565.4, "ram_available_mb": 98206.7, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 27567.8, "ram_available_mb": 98204.4, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.12, "peak": 40.18, "min": 15.38}, "VIN": {"avg": 68.83, "peak": 126.25, "min": 29.55}, "VIN_SYS_5V0": {"avg": 14.84, "peak": 16.05, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.95, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 27.12, "energy_joules_est": 70.07, "sample_count": 25, "duration_seconds": 2.584}, "timestamp": "2026-01-19T14:54:56.665683"}
{"image_index": 243, "image_name": "000000025181.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025181.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2028.314, "latencies_ms": [2028.314], "images_per_second": 0.493, "prompt_tokens": 1117, "response_tokens_est": 37, "n_tiles": 1, "output_text": " The train station platform is located in the foreground, with the train tracks extending into the background. The sign is positioned above the platform, indicating the direction to the center of the station.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27567.8, "ram_available_mb": 98204.4, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 27567.6, "ram_available_mb": 98204.5, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.17, "peak": 40.57, "min": 16.95}, "VIN": {"avg": 74.83, "peak": 125.37, "min": 31.73}, "VIN_SYS_5V0": {"avg": 15.17, "peak": 16.15, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 16.26, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 29.17, "energy_joules_est": 59.17, "sample_count": 20, "duration_seconds": 2.029}, "timestamp": "2026-01-19T14:54:58.754685"}
{"image_index": 243, "image_name": "000000025181.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025181.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1676.479, "latencies_ms": [1676.479], "images_per_second": 0.596, "prompt_tokens": 1111, "response_tokens_est": 22, "n_tiles": 1, "output_text": " A black and white photo of a train station with a sign that says La Spezia Centrale.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27567.6, "ram_available_mb": 98204.5, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 27577.5, "ram_available_mb": 98194.7, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.26, "peak": 40.57, "min": 16.95}, "VIN": {"avg": 74.2, "peak": 124.71, "min": 28.81}, "VIN_SYS_5V0": {"avg": 14.86, "peak": 16.05, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.89, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 30.26, "energy_joules_est": 50.75, "sample_count": 17, "duration_seconds": 1.677}, "timestamp": "2026-01-19T14:55:00.537629"}
{"image_index": 243, "image_name": "000000025181.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025181.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2205.237, "latencies_ms": [2205.237], "images_per_second": 0.453, "prompt_tokens": 1109, "response_tokens_est": 43, "n_tiles": 1, "output_text": " The image is in black and white, with the train station and tracks being the main focus. The sky is overcast, and the station is well-lit, suggesting it is either early morning or late afternoon.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27577.5, "ram_available_mb": 98194.7, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 27577.2, "ram_available_mb": 98194.9, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.22, "peak": 40.57, "min": 17.35}, "VIN": {"avg": 68.94, "peak": 125.31, "min": 27.95}, "VIN_SYS_5V0": {"avg": 14.74, "peak": 15.95, "min": 11.77}, "VDD_CPU_SOC_MSS": {"avg": 15.82, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 28.22, "energy_joules_est": 62.24, "sample_count": 22, "duration_seconds": 2.206}, "timestamp": "2026-01-19T14:55:02.834880"}
{"image_index": 244, "image_name": "000000025228.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025228.jpg", "image_width": 640, "image_height": 436, "image_resolution": "640x436", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1628.932, "latencies_ms": [1628.932], "images_per_second": 0.614, "prompt_tokens": 1099, "response_tokens_est": 21, "n_tiles": 1, "output_text": " A person is sitting on a red surfboard in the ocean, with a cloudy sky in the background.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 27577.2, "ram_available_mb": 98194.9, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 27586.9, "ram_available_mb": 98185.3, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.48, "peak": 40.16, "min": 15.38}, "VIN": {"avg": 74.36, "peak": 126.56, "min": 28.79}, "VIN_SYS_5V0": {"avg": 14.81, "peak": 16.05, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.92, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 30.48, "energy_joules_est": 49.66, "sample_count": 16, "duration_seconds": 1.629}, "timestamp": "2026-01-19T14:55:04.512008"}
{"image_index": 244, "image_name": "000000025228.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025228.jpg", "image_width": 640, "image_height": 436, "image_resolution": "640x436", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2624.209, "latencies_ms": [2624.209], "images_per_second": 0.381, "prompt_tokens": 1113, "response_tokens_est": 59, "n_tiles": 1, "output_text": " 1. person: 1\n2. surfboard: 1\n3. ocean: 1\n4. sky: 1\n5. clouds: 1\n6. horizon: 1\n7. water: 1\n8. surfboard logo: 1", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 27586.9, "ram_available_mb": 98185.3, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 27555.3, "ram_available_mb": 98216.9, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.4, "peak": 40.18, "min": 17.73}, "VIN": {"avg": 71.18, "peak": 126.92, "min": 29.26}, "VIN_SYS_5V0": {"avg": 15.0, "peak": 16.15, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 16.22, "peak": 18.11, "min": 13.79}}, "power_watts_avg": 27.4, "energy_joules_est": 71.91, "sample_count": 26, "duration_seconds": 2.625}, "timestamp": "2026-01-19T14:55:07.216647"}
{"image_index": 244, "image_name": "000000025228.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025228.jpg", "image_width": 640, "image_height": 436, "image_resolution": "640x436", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2032.894, "latencies_ms": [2032.894], "images_per_second": 0.492, "prompt_tokens": 1117, "response_tokens_est": 36, "n_tiles": 1, "output_text": " The person is sitting on the surfboard, which is in the foreground of the image. The ocean is in the background, and the sky is above the person and the ocean.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27555.3, "ram_available_mb": 98216.9, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 27561.8, "ram_available_mb": 98210.3, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.68, "peak": 40.16, "min": 15.38}, "VIN": {"avg": 71.74, "peak": 126.76, "min": 29.65}, "VIN_SYS_5V0": {"avg": 14.83, "peak": 16.05, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.91, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 28.68, "energy_joules_est": 58.32, "sample_count": 20, "duration_seconds": 2.033}, "timestamp": "2026-01-19T14:55:09.305916"}
{"image_index": 244, "image_name": "000000025228.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025228.jpg", "image_width": 640, "image_height": 436, "image_resolution": "640x436", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1457.051, "latencies_ms": [1457.051], "images_per_second": 0.686, "prompt_tokens": 1111, "response_tokens_est": 14, "n_tiles": 1, "output_text": " A person is sitting on a surfboard in the ocean at sunset.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27561.8, "ram_available_mb": 98210.3, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 27569.0, "ram_available_mb": 98203.2, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.26, "peak": 40.18, "min": 16.95}, "VIN": {"avg": 78.97, "peak": 126.41, "min": 30.35}, "VIN_SYS_5V0": {"avg": 14.86, "peak": 16.05, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.96, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 31.26, "energy_joules_est": 45.56, "sample_count": 15, "duration_seconds": 1.458}, "timestamp": "2026-01-19T14:55:10.868830"}
{"image_index": 244, "image_name": "000000025228.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025228.jpg", "image_width": 640, "image_height": 436, "image_resolution": "640x436", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2048.06, "latencies_ms": [2048.06], "images_per_second": 0.488, "prompt_tokens": 1109, "response_tokens_est": 37, "n_tiles": 1, "output_text": " The image features a person sitting on a red surfboard in the ocean, with a cloudy sky in the background. The lighting is dim, and the water appears to be choppy.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27569.0, "ram_available_mb": 98203.2, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 27574.9, "ram_available_mb": 98197.3, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.74, "peak": 40.57, "min": 19.71}, "VIN": {"avg": 70.67, "peak": 120.63, "min": 31.0}, "VIN_SYS_5V0": {"avg": 14.83, "peak": 16.05, "min": 11.87}, "VDD_CPU_SOC_MSS": {"avg": 15.87, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 29.74, "energy_joules_est": 60.92, "sample_count": 20, "duration_seconds": 2.048}, "timestamp": "2026-01-19T14:55:12.950025"}
{"image_index": 245, "image_name": "000000025386.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025386.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1580.062, "latencies_ms": [1580.062], "images_per_second": 0.633, "prompt_tokens": 1100, "response_tokens_est": 20, "n_tiles": 1, "output_text": " A man and a woman are sitting at a table on a train, eating sushi and other food.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 27574.9, "ram_available_mb": 98197.3, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 27581.1, "ram_available_mb": 98191.1, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.95, "peak": 40.57, "min": 17.35}, "VIN": {"avg": 67.16, "peak": 106.06, "min": 28.16}, "VIN_SYS_5V0": {"avg": 15.02, "peak": 16.05, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 16.22, "peak": 17.72, "min": 13.79}}, "power_watts_avg": 30.95, "energy_joules_est": 48.91, "sample_count": 16, "duration_seconds": 1.58}, "timestamp": "2026-01-19T14:55:14.620719"}
{"image_index": 245, "image_name": "000000025386.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025386.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2573.538, "latencies_ms": [2573.538], "images_per_second": 0.389, "prompt_tokens": 1114, "response_tokens_est": 57, "n_tiles": 1, "output_text": " 1. man: 1\n2. woman: 1\n3. chopsticks: 2\n4. tray: 1\n5. food: 1\n6. plate: 1\n7. bag: 1\n8. seat: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27581.1, "ram_available_mb": 98191.1, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 27589.3, "ram_available_mb": 98182.9, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.53, "peak": 40.18, "min": 18.52}, "VIN": {"avg": 67.25, "peak": 107.38, "min": 29.72}, "VIN_SYS_5V0": {"avg": 14.9, "peak": 16.05, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 15.92, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 27.53, "energy_joules_est": 70.86, "sample_count": 25, "duration_seconds": 2.574}, "timestamp": "2026-01-19T14:55:17.223582"}
{"image_index": 245, "image_name": "000000025386.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025386.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2395.292, "latencies_ms": [2395.292], "images_per_second": 0.417, "prompt_tokens": 1118, "response_tokens_est": 51, "n_tiles": 1, "output_text": " The man is sitting on the left side of the image, while the woman is on the right side. The man is closer to the camera than the woman. The food tray is in the middle of the image, and the window is in the background.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 27589.3, "ram_available_mb": 98182.9, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 27589.7, "ram_available_mb": 98182.4, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.88, "peak": 40.16, "min": 16.56}, "VIN": {"avg": 71.44, "peak": 106.11, "min": 38.27}, "VIN_SYS_5V0": {"avg": 15.12, "peak": 16.26, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 16.28, "peak": 17.71, "min": 14.18}}, "power_watts_avg": 27.88, "energy_joules_est": 66.79, "sample_count": 23, "duration_seconds": 2.396}, "timestamp": "2026-01-19T14:55:19.622859"}
{"image_index": 245, "image_name": "000000025386.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025386.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1572.214, "latencies_ms": [1572.214], "images_per_second": 0.636, "prompt_tokens": 1112, "response_tokens_est": 20, "n_tiles": 1, "output_text": " A couple is enjoying a meal on a train, with a view of the tracks outside the window.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27589.7, "ram_available_mb": 98182.4, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 27590.2, "ram_available_mb": 98182.0, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.78, "peak": 39.39, "min": 17.34}, "VIN": {"avg": 75.79, "peak": 120.57, "min": 28.88}, "VIN_SYS_5V0": {"avg": 15.23, "peak": 16.36, "min": 13.29}, "VDD_CPU_SOC_MSS": {"avg": 16.46, "peak": 17.71, "min": 14.57}}, "power_watts_avg": 30.78, "energy_joules_est": 48.41, "sample_count": 16, "duration_seconds": 1.573}, "timestamp": "2026-01-19T14:55:21.291584"}
{"image_index": 245, "image_name": "000000025386.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025386.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2025.0, "latencies_ms": [2025.0], "images_per_second": 0.494, "prompt_tokens": 1110, "response_tokens_est": 35, "n_tiles": 1, "output_text": " The image is taken in a well-lit train carriage with natural light coming through the windows. The colors in the image are vibrant and the materials are mostly plastic and metal.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 27590.2, "ram_available_mb": 98182.0, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 27591.0, "ram_available_mb": 98181.2, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.15, "peak": 40.16, "min": 18.14}, "VIN": {"avg": 72.39, "peak": 125.57, "min": 30.1}, "VIN_SYS_5V0": {"avg": 14.81, "peak": 16.05, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.91, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 29.15, "energy_joules_est": 59.04, "sample_count": 20, "duration_seconds": 2.025}, "timestamp": "2026-01-19T14:55:23.376474"}
{"image_index": 246, "image_name": "000000025393.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025393.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1668.355, "latencies_ms": [1668.355], "images_per_second": 0.599, "prompt_tokens": 1099, "response_tokens_est": 23, "n_tiles": 1, "output_text": " Two men are walking down the street at night, one in a white shirt and the other in a pink shirt.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 27591.0, "ram_available_mb": 98181.2, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 27606.3, "ram_available_mb": 98165.9, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.26, "peak": 40.57, "min": 16.56}, "VIN": {"avg": 71.38, "peak": 106.13, "min": 28.21}, "VIN_SYS_5V0": {"avg": 14.87, "peak": 16.05, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.95, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 30.26, "energy_joules_est": 50.51, "sample_count": 17, "duration_seconds": 1.669}, "timestamp": "2026-01-19T14:55:25.154880"}
{"image_index": 246, "image_name": "000000025393.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025393.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2528.003, "latencies_ms": [2528.003], "images_per_second": 0.396, "prompt_tokens": 1113, "response_tokens_est": 56, "n_tiles": 1, "output_text": " 1. man: 2\n2. tie: 1\n3. shirt: 1\n4. tie: 1\n5. pants: 1\n6. shoes: 1\n7. building: 1\n8. sign: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 27606.3, "ram_available_mb": 98165.9, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 27608.6, "ram_available_mb": 98163.6, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.42, "peak": 40.57, "min": 17.35}, "VIN": {"avg": 66.16, "peak": 125.5, "min": 29.2}, "VIN_SYS_5V0": {"avg": 14.78, "peak": 16.05, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.87, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 27.42, "energy_joules_est": 69.33, "sample_count": 25, "duration_seconds": 2.529}, "timestamp": "2026-01-19T14:55:27.751719"}
{"image_index": 246, "image_name": "000000025393.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025393.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2545.822, "latencies_ms": [2545.822], "images_per_second": 0.393, "prompt_tokens": 1117, "response_tokens_est": 57, "n_tiles": 1, "output_text": " The man on the left is standing closer to the camera than the man on the right. The man on the left is standing in front of the man on the right. The man on the right is standing on the sidewalk, while the man on the left is standing on the street.", "error": null, "sys_before": {"cpu_percent": 50.0, "ram_used_mb": 27609.3, "ram_available_mb": 98162.9, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 27610.0, "ram_available_mb": 98162.2, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.2, "peak": 40.56, "min": 15.77}, "VIN": {"avg": 69.73, "peak": 123.63, "min": 28.92}, "VIN_SYS_5V0": {"avg": 14.97, "peak": 16.15, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 16.05, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 27.2, "energy_joules_est": 69.26, "sample_count": 25, "duration_seconds": 2.546}, "timestamp": "2026-01-19T14:55:30.367480"}
{"image_index": 246, "image_name": "000000025393.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025393.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1577.1, "latencies_ms": [1577.1], "images_per_second": 0.634, "prompt_tokens": 1111, "response_tokens_est": 19, "n_tiles": 1, "output_text": " Two men are walking down the street at night, one of them is wearing a pink shirt.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27610.0, "ram_available_mb": 98162.2, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 6.7, "ram_used_mb": 27575.3, "ram_available_mb": 98196.9, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.46, "peak": 40.18, "min": 15.77}, "VIN": {"avg": 70.96, "peak": 86.95, "min": 28.82}, "VIN_SYS_5V0": {"avg": 14.91, "peak": 16.05, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 16.17, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 30.46, "energy_joules_est": 48.05, "sample_count": 16, "duration_seconds": 1.577}, "timestamp": "2026-01-19T14:55:32.045017"}
{"image_index": 246, "image_name": "000000025393.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025393.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1573.266, "latencies_ms": [1573.266], "images_per_second": 0.636, "prompt_tokens": 1109, "response_tokens_est": 19, "n_tiles": 1, "output_text": " The two men are walking on the sidewalk at night, with the streetlights illuminating the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27575.3, "ram_available_mb": 98196.9, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 27575.9, "ram_available_mb": 98196.3, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.14, "peak": 40.56, "min": 18.14}, "VIN": {"avg": 76.69, "peak": 124.23, "min": 28.77}, "VIN_SYS_5V0": {"avg": 14.76, "peak": 16.05, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.97, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 31.14, "energy_joules_est": 49.0, "sample_count": 16, "duration_seconds": 1.574}, "timestamp": "2026-01-19T14:55:33.722743"}
{"image_index": 247, "image_name": "000000025394.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025394.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1713.706, "latencies_ms": [1713.706], "images_per_second": 0.584, "prompt_tokens": 1100, "response_tokens_est": 25, "n_tiles": 1, "output_text": " A man wearing glasses and a gray shirt is pouring wine into a glass at a bar while another person holds a wine glass.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27575.9, "ram_available_mb": 98196.3, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 27579.3, "ram_available_mb": 98192.9, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.66, "peak": 40.57, "min": 18.14}, "VIN": {"avg": 74.45, "peak": 131.22, "min": 28.9}, "VIN_SYS_5V0": {"avg": 14.8, "peak": 16.15, "min": 11.77}, "VDD_CPU_SOC_MSS": {"avg": 15.89, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 30.66, "energy_joules_est": 52.57, "sample_count": 17, "duration_seconds": 1.715}, "timestamp": "2026-01-19T14:55:35.499836"}
{"image_index": 247, "image_name": "000000025394.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025394.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2720.436, "latencies_ms": [2720.436], "images_per_second": 0.368, "prompt_tokens": 1114, "response_tokens_est": 64, "n_tiles": 1, "output_text": " 1. man: 1\n2. glasses: 2\n3. wine bottles: 10\n4. wine rack: 1\n5. wooden counter: 1\n6. wine glass: 1\n7. wine bottle: 1\n8. wine glass holder: 1", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 27579.3, "ram_available_mb": 98192.9, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 27572.3, "ram_available_mb": 98199.9, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.91, "peak": 40.56, "min": 17.74}, "VIN": {"avg": 66.35, "peak": 105.04, "min": 28.86}, "VIN_SYS_5V0": {"avg": 14.98, "peak": 16.15, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.89, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 26.91, "energy_joules_est": 73.21, "sample_count": 27, "duration_seconds": 2.721}, "timestamp": "2026-01-19T14:55:38.310208"}
{"image_index": 247, "image_name": "000000025394.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025394.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2733.518, "latencies_ms": [2733.518], "images_per_second": 0.366, "prompt_tokens": 1118, "response_tokens_est": 59, "n_tiles": 1, "output_text": " The man is standing to the left of the bar counter, with the wine bottle and glasses in front of him. The wine bottle is placed on the counter, while the glasses are held by the man. The background shows shelves with wine bottles, indicating that the bar is stocked with various wines.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27572.3, "ram_available_mb": 98199.9, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 27558.2, "ram_available_mb": 98213.9, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 25.92, "peak": 40.18, "min": 15.38}, "VIN": {"avg": 71.24, "peak": 125.0, "min": 29.3}, "VIN_SYS_5V0": {"avg": 14.65, "peak": 16.05, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.69, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 25.92, "energy_joules_est": 70.87, "sample_count": 27, "duration_seconds": 2.734}, "timestamp": "2026-01-19T14:55:41.123085"}
{"image_index": 247, "image_name": "000000025394.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025394.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1709.08, "latencies_ms": [1709.08], "images_per_second": 0.585, "prompt_tokens": 1112, "response_tokens_est": 24, "n_tiles": 1, "output_text": " A man is standing behind a bar, pouring wine into a glass. There are wine bottles on the shelves behind him.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 27558.2, "ram_available_mb": 98213.9, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 27567.8, "ram_available_mb": 98204.4, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.59, "peak": 39.39, "min": 14.98}, "VIN": {"avg": 68.74, "peak": 94.9, "min": 30.03}, "VIN_SYS_5V0": {"avg": 14.79, "peak": 16.05, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.82, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 29.59, "energy_joules_est": 50.59, "sample_count": 17, "duration_seconds": 1.71}, "timestamp": "2026-01-19T14:55:42.893389"}
{"image_index": 247, "image_name": "000000025394.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025394.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1737.098, "latencies_ms": [1737.098], "images_per_second": 0.576, "prompt_tokens": 1110, "response_tokens_est": 26, "n_tiles": 1, "output_text": " The image is taken in a dimly lit room with warm lighting, and the wooden wine rack is filled with bottles of wine.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27567.8, "ram_available_mb": 98204.4, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 27570.2, "ram_available_mb": 98201.9, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.69, "peak": 40.57, "min": 18.14}, "VIN": {"avg": 69.62, "peak": 107.02, "min": 29.3}, "VIN_SYS_5V0": {"avg": 15.0, "peak": 16.15, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 16.09, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 30.69, "energy_joules_est": 53.32, "sample_count": 17, "duration_seconds": 1.737}, "timestamp": "2026-01-19T14:55:44.673936"}
{"image_index": 248, "image_name": "000000025424.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025424.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1921.18, "latencies_ms": [1921.18], "images_per_second": 0.521, "prompt_tokens": 1100, "response_tokens_est": 33, "n_tiles": 1, "output_text": " A tennis player is in the middle of a powerful swing with a blue and white tennis racket, attempting to hit a yellow tennis ball that is in the air.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27570.2, "ram_available_mb": 98201.9, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 27578.8, "ram_available_mb": 98193.4, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.75, "peak": 40.16, "min": 18.53}, "VIN": {"avg": 74.09, "peak": 125.26, "min": 29.24}, "VIN_SYS_5V0": {"avg": 15.08, "peak": 16.26, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 16.14, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 29.75, "energy_joules_est": 57.18, "sample_count": 19, "duration_seconds": 1.922}, "timestamp": "2026-01-19T14:55:46.658539"}
{"image_index": 248, "image_name": "000000025424.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025424.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2793.246, "latencies_ms": [2793.246], "images_per_second": 0.358, "prompt_tokens": 1114, "response_tokens_est": 66, "n_tiles": 1, "output_text": " 1. Tennis racket: 1\n2. Tennis ball: 1\n3. Grass: 1\n4. Player: 1\n5. Wristband: 1\n6. Shirt: 1\n7. Shorts: 1\n8. Footwear: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27578.8, "ram_available_mb": 98193.4, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 27582.1, "ram_available_mb": 98190.1, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.66, "peak": 40.18, "min": 17.35}, "VIN": {"avg": 70.47, "peak": 126.65, "min": 29.5}, "VIN_SYS_5V0": {"avg": 14.97, "peak": 16.15, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 16.01, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 26.66, "energy_joules_est": 74.48, "sample_count": 27, "duration_seconds": 2.794}, "timestamp": "2026-01-19T14:55:49.468467"}
{"image_index": 248, "image_name": "000000025424.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025424.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2453.151, "latencies_ms": [2453.151], "images_per_second": 0.408, "prompt_tokens": 1118, "response_tokens_est": 54, "n_tiles": 1, "output_text": " The tennis player is in the foreground, with the tennis ball in the middle ground, and the green grass of the tennis court in the background. The player is positioned to the left of the tennis ball, and the tennis court extends to the right side of the image.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 27582.1, "ram_available_mb": 98190.1, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 27595.9, "ram_available_mb": 98176.2, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.54, "peak": 40.16, "min": 16.56}, "VIN": {"avg": 68.82, "peak": 109.57, "min": 30.32}, "VIN_SYS_5V0": {"avg": 15.14, "peak": 16.15, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 16.22, "peak": 17.71, "min": 14.18}}, "power_watts_avg": 27.54, "energy_joules_est": 67.57, "sample_count": 24, "duration_seconds": 2.453}, "timestamp": "2026-01-19T14:55:51.973334"}
{"image_index": 248, "image_name": "000000025424.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025424.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1571.944, "latencies_ms": [1571.944], "images_per_second": 0.636, "prompt_tokens": 1112, "response_tokens_est": 19, "n_tiles": 1, "output_text": " A tennis player is playing on a grass court, swinging his racket to hit a ball.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27595.9, "ram_available_mb": 98176.2, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 27597.2, "ram_available_mb": 98174.9, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.23, "peak": 39.38, "min": 16.16}, "VIN": {"avg": 76.9, "peak": 119.22, "min": 29.54}, "VIN_SYS_5V0": {"avg": 14.92, "peak": 16.26, "min": 12.46}, "VDD_CPU_SOC_MSS": {"avg": 16.02, "peak": 17.71, "min": 13.39}}, "power_watts_avg": 30.23, "energy_joules_est": 47.53, "sample_count": 16, "duration_seconds": 1.572}, "timestamp": "2026-01-19T14:55:53.638427"}
{"image_index": 248, "image_name": "000000025424.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025424.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2632.01, "latencies_ms": [2632.01], "images_per_second": 0.38, "prompt_tokens": 1110, "response_tokens_est": 60, "n_tiles": 1, "output_text": " The image captures a dynamic moment on a grass court, where a tennis player is in the midst of a powerful swing with a blue and white racket. The grass is a vibrant green, contrasting with the player's white attire. The lighting is natural and bright, suggesting it's a sunny day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27597.2, "ram_available_mb": 98174.9, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 27618.5, "ram_available_mb": 98153.7, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.11, "peak": 40.18, "min": 18.13}, "VIN": {"avg": 67.3, "peak": 120.16, "min": 29.72}, "VIN_SYS_5V0": {"avg": 14.86, "peak": 16.05, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.94, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 27.11, "energy_joules_est": 71.37, "sample_count": 26, "duration_seconds": 2.633}, "timestamp": "2026-01-19T14:55:56.348024"}
{"image_index": 249, "image_name": "000000025560.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025560.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1670.49, "latencies_ms": [1670.49], "images_per_second": 0.599, "prompt_tokens": 1099, "response_tokens_est": 23, "n_tiles": 1, "output_text": " A cat is standing on a wooden shelf in front of a television, which is displaying a man in a suit.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 27616.8, "ram_available_mb": 98155.4, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 27606.7, "ram_available_mb": 98165.5, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.99, "peak": 40.16, "min": 15.38}, "VIN": {"avg": 73.31, "peak": 107.27, "min": 30.13}, "VIN_SYS_5V0": {"avg": 14.81, "peak": 16.05, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 15.91, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 29.99, "energy_joules_est": 50.11, "sample_count": 17, "duration_seconds": 1.671}, "timestamp": "2026-01-19T14:55:58.124575"}
{"image_index": 249, "image_name": "000000025560.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025560.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2621.984, "latencies_ms": [2621.984], "images_per_second": 0.381, "prompt_tokens": 1113, "response_tokens_est": 58, "n_tiles": 1, "output_text": " 1. cat: 1\n2. television: 1\n3. shelf: 1\n4. remote control: 1\n5. cup: 1\n6. bookshelf: 1\n7. wall: 1\n8. curtain: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27606.7, "ram_available_mb": 98165.5, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 27612.6, "ram_available_mb": 98159.5, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.11, "peak": 40.57, "min": 17.73}, "VIN": {"avg": 70.52, "peak": 127.4, "min": 29.72}, "VIN_SYS_5V0": {"avg": 14.82, "peak": 16.05, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.76, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 27.11, "energy_joules_est": 71.09, "sample_count": 26, "duration_seconds": 2.622}, "timestamp": "2026-01-19T14:56:00.824616"}
{"image_index": 249, "image_name": "000000025560.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025560.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2068.787, "latencies_ms": [2068.787], "images_per_second": 0.483, "prompt_tokens": 1117, "response_tokens_est": 38, "n_tiles": 1, "output_text": " The cat is positioned to the left of the television, which is located in the background. The television is situated on the right side of the wooden shelf, which is situated in the foreground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27612.6, "ram_available_mb": 98159.5, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 27613.2, "ram_available_mb": 98159.0, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.84, "peak": 40.16, "min": 15.38}, "VIN": {"avg": 70.69, "peak": 129.3, "min": 29.93}, "VIN_SYS_5V0": {"avg": 14.89, "peak": 16.05, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.91, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 28.84, "energy_joules_est": 59.68, "sample_count": 20, "duration_seconds": 2.069}, "timestamp": "2026-01-19T14:56:02.902851"}
{"image_index": 249, "image_name": "000000025560.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025560.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1444.138, "latencies_ms": [1444.138], "images_per_second": 0.692, "prompt_tokens": 1111, "response_tokens_est": 15, "n_tiles": 1, "output_text": " A cat is on top of a TV stand, looking at the TV.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 27613.2, "ram_available_mb": 98159.0, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 27626.4, "ram_available_mb": 98145.8, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.53, "peak": 40.18, "min": 18.52}, "VIN": {"avg": 74.41, "peak": 105.34, "min": 29.35}, "VIN_SYS_5V0": {"avg": 15.32, "peak": 16.15, "min": 13.29}, "VDD_CPU_SOC_MSS": {"avg": 16.39, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 32.53, "energy_joules_est": 47.0, "sample_count": 14, "duration_seconds": 1.445}, "timestamp": "2026-01-19T14:56:04.366883"}
{"image_index": 249, "image_name": "000000025560.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025560.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1401.913, "latencies_ms": [1401.913], "images_per_second": 0.713, "prompt_tokens": 1109, "response_tokens_est": 13, "n_tiles": 1, "output_text": " The cat is white and brown, and the television is black.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27626.4, "ram_available_mb": 98145.8, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 6.8, "ram_used_mb": 27563.8, "ram_available_mb": 98208.4, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 33.82, "peak": 40.95, "min": 22.46}, "VIN": {"avg": 87.82, "peak": 129.83, "min": 30.28}, "VIN_SYS_5V0": {"avg": 15.31, "peak": 16.26, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 16.68, "peak": 18.11, "min": 14.57}}, "power_watts_avg": 33.82, "energy_joules_est": 47.43, "sample_count": 14, "duration_seconds": 1.403}, "timestamp": "2026-01-19T14:56:05.822239"}
{"image_index": 250, "image_name": "000000025593.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025593.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1967.58, "latencies_ms": [1967.58], "images_per_second": 0.508, "prompt_tokens": 1099, "response_tokens_est": 35, "n_tiles": 1, "output_text": " The image features a blue circular sign with a white silhouette of a person and a child riding a bicycle, and a white rectangular sign with Chinese characters, indicating a pedestrian crossing.", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 27563.8, "ram_available_mb": 98208.4, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 27553.8, "ram_available_mb": 98218.4, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.61, "peak": 40.97, "min": 21.28}, "VIN": {"avg": 65.78, "peak": 123.98, "min": 29.36}, "VIN_SYS_5V0": {"avg": 15.06, "peak": 16.05, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 16.24, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 30.61, "energy_joules_est": 60.25, "sample_count": 19, "duration_seconds": 1.968}, "timestamp": "2026-01-19T14:56:07.819036"}
{"image_index": 250, "image_name": "000000025593.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025593.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2532.349, "latencies_ms": [2532.349], "images_per_second": 0.395, "prompt_tokens": 1113, "response_tokens_est": 57, "n_tiles": 1, "output_text": " 1. sign: 3\n2. tree: 1\n3. bicycle: 1\n4. road sign: 1\n5. sky: 1\n6. sun: 1\n7. light: 1\n8. pole: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27553.8, "ram_available_mb": 98218.4, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 27554.9, "ram_available_mb": 98217.3, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.61, "peak": 40.56, "min": 18.13}, "VIN": {"avg": 68.84, "peak": 123.36, "min": 32.24}, "VIN_SYS_5V0": {"avg": 15.13, "peak": 16.26, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 16.28, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 27.61, "energy_joules_est": 69.93, "sample_count": 25, "duration_seconds": 2.533}, "timestamp": "2026-01-19T14:56:10.421682"}
{"image_index": 250, "image_name": "000000025593.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025593.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2180.223, "latencies_ms": [2180.223], "images_per_second": 0.459, "prompt_tokens": 1117, "response_tokens_est": 43, "n_tiles": 1, "output_text": " The sign is located in the foreground of the image, with the blue circle sign positioned above the white sign. The blue circle sign is to the left of the white sign, and the trees are in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27554.9, "ram_available_mb": 98217.3, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 27561.7, "ram_available_mb": 98210.5, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.55, "peak": 40.57, "min": 15.77}, "VIN": {"avg": 70.2, "peak": 122.0, "min": 34.09}, "VIN_SYS_5V0": {"avg": 15.07, "peak": 16.15, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 16.22, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 28.55, "energy_joules_est": 62.25, "sample_count": 21, "duration_seconds": 2.18}, "timestamp": "2026-01-19T14:56:12.610440"}
{"image_index": 250, "image_name": "000000025593.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025593.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2373.097, "latencies_ms": [2373.097], "images_per_second": 0.421, "prompt_tokens": 1111, "response_tokens_est": 51, "n_tiles": 1, "output_text": " The image captures a street scene with a blue and white sign featuring a family and a bicycle, indicating a pedestrian crossing. The sign is mounted on a metal pole, and the background reveals a clear blue sky and lush green trees, suggesting a pleasant day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27561.7, "ram_available_mb": 98210.5, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 27566.1, "ram_available_mb": 98206.0, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.27, "peak": 40.18, "min": 18.14}, "VIN": {"avg": 71.71, "peak": 103.17, "min": 29.62}, "VIN_SYS_5V0": {"avg": 15.26, "peak": 16.26, "min": 13.39}, "VDD_CPU_SOC_MSS": {"avg": 16.42, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 28.27, "energy_joules_est": 67.1, "sample_count": 23, "duration_seconds": 2.374}, "timestamp": "2026-01-19T14:56:15.013747"}
{"image_index": 250, "image_name": "000000025593.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025593.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2193.189, "latencies_ms": [2193.189], "images_per_second": 0.456, "prompt_tokens": 1109, "response_tokens_est": 44, "n_tiles": 1, "output_text": " The image features a blue and white sign with a symbol of a person and a child on it, and a white sign with Chinese characters on it. The sky is clear and blue, and the sun is shining brightly.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27566.1, "ram_available_mb": 98206.0, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 27568.9, "ram_available_mb": 98203.3, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.38, "peak": 40.57, "min": 16.95}, "VIN": {"avg": 71.58, "peak": 130.89, "min": 28.89}, "VIN_SYS_5V0": {"avg": 15.08, "peak": 16.15, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 16.11, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 28.38, "energy_joules_est": 62.25, "sample_count": 22, "duration_seconds": 2.194}, "timestamp": "2026-01-19T14:56:17.304305"}
{"image_index": 251, "image_name": "000000025603.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025603.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1620.492, "latencies_ms": [1620.492], "images_per_second": 0.617, "prompt_tokens": 1099, "response_tokens_est": 21, "n_tiles": 1, "output_text": " A young girl with black hair is sitting at a table in a restaurant, eating a slice of pizza.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 27568.9, "ram_available_mb": 98203.3, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 27568.0, "ram_available_mb": 98204.2, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.46, "peak": 40.57, "min": 15.38}, "VIN": {"avg": 75.55, "peak": 106.13, "min": 29.93}, "VIN_SYS_5V0": {"avg": 14.8, "peak": 16.05, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.87, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 30.46, "energy_joules_est": 49.38, "sample_count": 16, "duration_seconds": 1.621}, "timestamp": "2026-01-19T14:56:18.990459"}
{"image_index": 251, "image_name": "000000025603.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025603.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1824.127, "latencies_ms": [1824.127], "images_per_second": 0.548, "prompt_tokens": 1113, "response_tokens_est": 29, "n_tiles": 1, "output_text": " table: 1, chair: 1, pizza: 1, glass: 1, book: 1, person: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27568.0, "ram_available_mb": 98204.2, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 27568.6, "ram_available_mb": 98203.6, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.51, "peak": 40.57, "min": 18.91}, "VIN": {"avg": 71.45, "peak": 95.78, "min": 29.56}, "VIN_SYS_5V0": {"avg": 15.01, "peak": 16.15, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 16.08, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 30.51, "energy_joules_est": 55.67, "sample_count": 18, "duration_seconds": 1.825}, "timestamp": "2026-01-19T14:56:20.867288"}
{"image_index": 251, "image_name": "000000025603.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025603.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2862.559, "latencies_ms": [2862.559], "images_per_second": 0.349, "prompt_tokens": 1117, "response_tokens_est": 69, "n_tiles": 1, "output_text": " The girl is sitting at a table in the foreground, with a glass of water and a pizza on the table in front of her. The pizza is on the left side of the table, while the glass of water is on the right side. The background shows other tables and chairs, indicating that the girl is in a restaurant or cafe.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 27568.6, "ram_available_mb": 98203.6, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 27581.4, "ram_available_mb": 98190.8, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.65, "peak": 40.56, "min": 18.13}, "VIN": {"avg": 65.13, "peak": 92.09, "min": 28.9}, "VIN_SYS_5V0": {"avg": 14.92, "peak": 16.05, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 16.02, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 26.65, "energy_joules_est": 76.3, "sample_count": 28, "duration_seconds": 2.863}, "timestamp": "2026-01-19T14:56:23.790522"}
{"image_index": 251, "image_name": "000000025603.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025603.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1535.619, "latencies_ms": [1535.619], "images_per_second": 0.651, "prompt_tokens": 1111, "response_tokens_est": 18, "n_tiles": 1, "output_text": " A young girl is sitting at a table in a restaurant, eating a slice of pizza.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27581.4, "ram_available_mb": 98190.8, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 27589.8, "ram_available_mb": 98182.3, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.07, "peak": 40.16, "min": 15.77}, "VIN": {"avg": 75.85, "peak": 117.84, "min": 29.36}, "VIN_SYS_5V0": {"avg": 15.0, "peak": 16.05, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 16.22, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 31.07, "energy_joules_est": 47.72, "sample_count": 15, "duration_seconds": 1.536}, "timestamp": "2026-01-19T14:56:25.354972"}
{"image_index": 251, "image_name": "000000025603.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025603.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2046.356, "latencies_ms": [2046.356], "images_per_second": 0.489, "prompt_tokens": 1109, "response_tokens_est": 38, "n_tiles": 1, "output_text": " The image is taken in a dimly lit restaurant with warm lighting. The colors in the image are mostly muted with the exception of the girl's hair, which is a vibrant yellow.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27589.8, "ram_available_mb": 98182.3, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 27595.5, "ram_available_mb": 98176.7, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.02, "peak": 40.95, "min": 20.1}, "VIN": {"avg": 78.08, "peak": 128.07, "min": 29.22}, "VIN_SYS_5V0": {"avg": 15.14, "peak": 16.15, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 16.26, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 30.02, "energy_joules_est": 61.44, "sample_count": 20, "duration_seconds": 2.047}, "timestamp": "2026-01-19T14:56:27.442898"}
{"image_index": 252, "image_name": "000000025986.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025986.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2539.726, "latencies_ms": [2539.726], "images_per_second": 0.394, "prompt_tokens": 1099, "response_tokens_est": 57, "n_tiles": 1, "output_text": " The image captures a vibrant and appetizing spread of food on a kitchen counter, with a variety of dishes including a bowl of green vegetables, a plate of rice and vegetables, and a plate of meat and vegetables, all set against the backdrop of a wooden cabinet and a yellow box.", "error": null, "sys_before": {"cpu_percent": 18.2, "ram_used_mb": 27595.5, "ram_available_mb": 98176.7, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 6.3, "ram_used_mb": 27557.8, "ram_available_mb": 98214.4, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.34, "peak": 40.57, "min": 16.95}, "VIN": {"avg": 69.78, "peak": 107.13, "min": 28.75}, "VIN_SYS_5V0": {"avg": 15.04, "peak": 16.15, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 16.21, "peak": 18.11, "min": 13.79}}, "power_watts_avg": 27.34, "energy_joules_est": 69.45, "sample_count": 25, "duration_seconds": 2.54}, "timestamp": "2026-01-19T14:56:30.045092"}
{"image_index": 252, "image_name": "000000025986.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025986.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1955.263, "latencies_ms": [1955.263], "images_per_second": 0.511, "prompt_tokens": 1113, "response_tokens_est": 34, "n_tiles": 1, "output_text": " bowl: 1, plate: 2, glass: 1, food: 1, broccoli: 1, cauliflower: 1, cauliflower: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27557.8, "ram_available_mb": 98214.4, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 27553.5, "ram_available_mb": 98218.6, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.34, "peak": 40.18, "min": 15.77}, "VIN": {"avg": 70.38, "peak": 108.32, "min": 29.22}, "VIN_SYS_5V0": {"avg": 15.0, "peak": 16.05, "min": 12.36}, "VDD_CPU_SOC_MSS": {"avg": 16.04, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 29.34, "energy_joules_est": 57.38, "sample_count": 19, "duration_seconds": 1.956}, "timestamp": "2026-01-19T14:56:32.021612"}
{"image_index": 252, "image_name": "000000025986.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025986.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2611.513, "latencies_ms": [2611.513], "images_per_second": 0.383, "prompt_tokens": 1117, "response_tokens_est": 60, "n_tiles": 1, "output_text": " The main objects are arranged in a way that the plates of food are placed in the foreground, with the bowls of vegetables and the box of bread in the background. The plates of food are positioned to the left of the bowls, and the box of bread is located to the right of the bowls.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27553.5, "ram_available_mb": 98218.6, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 27555.1, "ram_available_mb": 98217.1, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.41, "peak": 40.57, "min": 17.73}, "VIN": {"avg": 62.73, "peak": 125.75, "min": 29.23}, "VIN_SYS_5V0": {"avg": 15.11, "peak": 16.15, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 16.08, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 27.41, "energy_joules_est": 71.59, "sample_count": 26, "duration_seconds": 2.612}, "timestamp": "2026-01-19T14:56:34.716805"}
{"image_index": 252, "image_name": "000000025986.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025986.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1556.834, "latencies_ms": [1556.834], "images_per_second": 0.642, "prompt_tokens": 1111, "response_tokens_est": 18, "n_tiles": 1, "output_text": " A group of people are gathered around a table in a kitchen, enjoying a meal together.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27555.1, "ram_available_mb": 98217.1, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 27553.8, "ram_available_mb": 98218.4, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.89, "peak": 40.18, "min": 15.38}, "VIN": {"avg": 77.82, "peak": 123.66, "min": 33.02}, "VIN_SYS_5V0": {"avg": 14.85, "peak": 16.05, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.91, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 30.89, "energy_joules_est": 48.1, "sample_count": 15, "duration_seconds": 1.557}, "timestamp": "2026-01-19T14:56:36.281560"}
{"image_index": 252, "image_name": "000000025986.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025986.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2154.318, "latencies_ms": [2154.318], "images_per_second": 0.464, "prompt_tokens": 1109, "response_tokens_est": 41, "n_tiles": 1, "output_text": " The image is taken in a kitchen with a wooden wall in the background. The lighting is natural, coming from the window. The colors in the image are vibrant, with the food being the main focus.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27553.8, "ram_available_mb": 98218.4, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 27573.3, "ram_available_mb": 98198.9, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.79, "peak": 40.57, "min": 19.7}, "VIN": {"avg": 66.8, "peak": 85.2, "min": 29.21}, "VIN_SYS_5V0": {"avg": 15.24, "peak": 16.26, "min": 13.49}, "VDD_CPU_SOC_MSS": {"avg": 16.2, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 29.79, "energy_joules_est": 64.18, "sample_count": 21, "duration_seconds": 2.155}, "timestamp": "2026-01-19T14:56:38.469475"}
{"image_index": 253, "image_name": "000000026204.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026204.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2054.538, "latencies_ms": [2054.538], "images_per_second": 0.487, "prompt_tokens": 1099, "response_tokens_est": 38, "n_tiles": 1, "output_text": " The image captures a bustling city street with a green bus, a white truck, and a red car driving down the road, while a black car is parked on the side of the street.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 27573.3, "ram_available_mb": 98198.9, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 27572.3, "ram_available_mb": 98199.8, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.09, "peak": 40.56, "min": 16.56}, "VIN": {"avg": 71.35, "peak": 105.86, "min": 29.56}, "VIN_SYS_5V0": {"avg": 15.06, "peak": 16.05, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 16.1, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 29.09, "energy_joules_est": 59.79, "sample_count": 20, "duration_seconds": 2.055}, "timestamp": "2026-01-19T14:56:40.564578"}
{"image_index": 253, "image_name": "000000026204.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026204.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2654.628, "latencies_ms": [2654.628], "images_per_second": 0.377, "prompt_tokens": 1113, "response_tokens_est": 61, "n_tiles": 1, "output_text": " 1. Bus: 1\n2. Car: 4\n3. Truck: 1\n4. Van: 1\n5. Bus stop: 1\n6. Bus stop sign: 1\n7. Street sign: 1\n8. Building: 3", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 27572.3, "ram_available_mb": 98199.8, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 27580.4, "ram_available_mb": 98191.8, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.14, "peak": 40.56, "min": 17.35}, "VIN": {"avg": 70.55, "peak": 110.91, "min": 28.98}, "VIN_SYS_5V0": {"avg": 15.05, "peak": 16.15, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 16.11, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 27.14, "energy_joules_est": 72.06, "sample_count": 26, "duration_seconds": 2.655}, "timestamp": "2026-01-19T14:56:43.275132"}
{"image_index": 253, "image_name": "000000026204.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026204.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2181.911, "latencies_ms": [2181.911], "images_per_second": 0.458, "prompt_tokens": 1117, "response_tokens_est": 42, "n_tiles": 1, "output_text": " The bus is positioned in the middle of the street, with the cars on the left and the pedestrians on the right. The bus is closer to the camera than the cars, and the pedestrians are farther away.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 27580.4, "ram_available_mb": 98191.8, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 27577.3, "ram_available_mb": 98194.9, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.59, "peak": 40.18, "min": 16.16}, "VIN": {"avg": 77.77, "peak": 126.62, "min": 34.76}, "VIN_SYS_5V0": {"avg": 14.97, "peak": 16.05, "min": 12.36}, "VDD_CPU_SOC_MSS": {"avg": 16.05, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 28.59, "energy_joules_est": 62.4, "sample_count": 21, "duration_seconds": 2.183}, "timestamp": "2026-01-19T14:56:45.463488"}
{"image_index": 253, "image_name": "000000026204.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026204.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2719.499, "latencies_ms": [2719.499], "images_per_second": 0.368, "prompt_tokens": 1111, "response_tokens_est": 64, "n_tiles": 1, "output_text": " The image captures a bustling city street with a variety of vehicles, including cars, buses, and a truck, all navigating through the urban landscape. The street is lined with tall buildings, creating a dense urban environment. The presence of a bus stop sign indicates that public transportation is an integral part of the city's infrastructure.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27577.3, "ram_available_mb": 98194.9, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 27586.7, "ram_available_mb": 98185.5, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.88, "peak": 40.57, "min": 17.34}, "VIN": {"avg": 70.27, "peak": 126.29, "min": 27.31}, "VIN_SYS_5V0": {"avg": 15.09, "peak": 16.15, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 16.14, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 26.88, "energy_joules_est": 73.11, "sample_count": 27, "duration_seconds": 2.72}, "timestamp": "2026-01-19T14:56:48.278515"}
{"image_index": 253, "image_name": "000000026204.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026204.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2181.792, "latencies_ms": [2181.792], "images_per_second": 0.458, "prompt_tokens": 1109, "response_tokens_est": 42, "n_tiles": 1, "output_text": " The image captures a vibrant city street bathed in natural light, with the sky peeking through the towering buildings. The colors are a mix of urban gray and the lush green of the trees lining the sidewalk.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27586.7, "ram_available_mb": 98185.5, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 27586.6, "ram_available_mb": 98185.6, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.23, "peak": 39.77, "min": 14.98}, "VIN": {"avg": 69.44, "peak": 118.65, "min": 30.43}, "VIN_SYS_5V0": {"avg": 14.85, "peak": 16.05, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.88, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 28.23, "energy_joules_est": 61.61, "sample_count": 21, "duration_seconds": 2.182}, "timestamp": "2026-01-19T14:56:50.468618"}
{"image_index": 254, "image_name": "000000026465.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026465.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1613.901, "latencies_ms": [1613.901], "images_per_second": 0.62, "prompt_tokens": 1099, "response_tokens_est": 22, "n_tiles": 1, "output_text": " A black laptop computer with a blue screen sits on a table with a cell phone and a remote control nearby.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27586.6, "ram_available_mb": 98185.6, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 27594.8, "ram_available_mb": 98177.4, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.29, "peak": 39.77, "min": 18.52}, "VIN": {"avg": 73.47, "peak": 100.07, "min": 27.43}, "VIN_SYS_5V0": {"avg": 15.28, "peak": 16.26, "min": 13.29}, "VDD_CPU_SOC_MSS": {"avg": 16.44, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 31.29, "energy_joules_est": 50.52, "sample_count": 16, "duration_seconds": 1.615}, "timestamp": "2026-01-19T14:56:52.139949"}
{"image_index": 254, "image_name": "000000026465.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026465.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2037.638, "latencies_ms": [2037.638], "images_per_second": 0.491, "prompt_tokens": 1113, "response_tokens_est": 37, "n_tiles": 1, "output_text": " laptop: 1, phone: 2, remote: 1, cell phone: 1, tv: 1, tv remote: 1, tv stand: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27594.8, "ram_available_mb": 98177.4, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 6.7, "ram_used_mb": 27521.2, "ram_available_mb": 98251.0, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.62, "peak": 40.16, "min": 18.91}, "VIN": {"avg": 62.05, "peak": 105.62, "min": 29.33}, "VIN_SYS_5V0": {"avg": 15.03, "peak": 16.15, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 16.3, "peak": 18.11, "min": 13.79}}, "power_watts_avg": 29.62, "energy_joules_est": 60.36, "sample_count": 20, "duration_seconds": 2.038}, "timestamp": "2026-01-19T14:56:54.216349"}
{"image_index": 254, "image_name": "000000026465.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026465.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1840.703, "latencies_ms": [1840.703], "images_per_second": 0.543, "prompt_tokens": 1117, "response_tokens_est": 30, "n_tiles": 1, "output_text": " The laptop is on the left side of the table, the cell phone is on the right side, and the remote is in front of the laptop.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 27521.2, "ram_available_mb": 98251.0, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 27529.1, "ram_available_mb": 98243.1, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.2, "peak": 40.56, "min": 17.35}, "VIN": {"avg": 71.76, "peak": 126.69, "min": 28.6}, "VIN_SYS_5V0": {"avg": 15.13, "peak": 16.15, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 16.16, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 30.2, "energy_joules_est": 55.6, "sample_count": 18, "duration_seconds": 1.841}, "timestamp": "2026-01-19T14:56:56.092949"}
{"image_index": 254, "image_name": "000000026465.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026465.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1579.087, "latencies_ms": [1579.087], "images_per_second": 0.633, "prompt_tokens": 1111, "response_tokens_est": 20, "n_tiles": 1, "output_text": " A person is sitting at a table with a laptop, a cell phone, and a remote control.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27529.1, "ram_available_mb": 98243.1, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 27531.2, "ram_available_mb": 98240.9, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.37, "peak": 40.56, "min": 18.14}, "VIN": {"avg": 79.6, "peak": 124.01, "min": 27.15}, "VIN_SYS_5V0": {"avg": 15.09, "peak": 16.15, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 16.19, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 31.37, "energy_joules_est": 49.54, "sample_count": 16, "duration_seconds": 1.579}, "timestamp": "2026-01-19T14:56:57.758569"}
{"image_index": 254, "image_name": "000000026465.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026465.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1501.122, "latencies_ms": [1501.122], "images_per_second": 0.666, "prompt_tokens": 1109, "response_tokens_est": 16, "n_tiles": 1, "output_text": " The laptop is black, the phone is black, and the table is white.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27531.2, "ram_available_mb": 98240.9, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 27540.8, "ram_available_mb": 98231.4, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.73, "peak": 40.57, "min": 18.14}, "VIN": {"avg": 72.15, "peak": 131.54, "min": 28.65}, "VIN_SYS_5V0": {"avg": 14.77, "peak": 16.05, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.88, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 31.73, "energy_joules_est": 47.65, "sample_count": 15, "duration_seconds": 1.502}, "timestamp": "2026-01-19T14:56:59.331206"}
{"image_index": 255, "image_name": "000000026564.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026564.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2116.616, "latencies_ms": [2116.616], "images_per_second": 0.472, "prompt_tokens": 1099, "response_tokens_est": 38, "n_tiles": 1, "output_text": " The image depicts a cluttered office desk with a computer monitor, keyboard, and mouse, as well as a laptop, books, and a water bottle, with a window in the background.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 27540.8, "ram_available_mb": 98231.4, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 27561.1, "ram_available_mb": 98211.0, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.21, "peak": 40.57, "min": 18.91}, "VIN": {"avg": 71.6, "peak": 119.36, "min": 28.74}, "VIN_SYS_5V0": {"avg": 14.82, "peak": 15.95, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.84, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 29.21, "energy_joules_est": 61.85, "sample_count": 21, "duration_seconds": 2.117}, "timestamp": "2026-01-19T14:57:01.520987"}
{"image_index": 255, "image_name": "000000026564.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026564.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2589.803, "latencies_ms": [2589.803], "images_per_second": 0.386, "prompt_tokens": 1113, "response_tokens_est": 58, "n_tiles": 1, "output_text": " 1. monitor: 1\n2. keyboard: 1\n3. mouse: 1\n4. laptop: 1\n5. books: 10\n6. water bottle: 1\n7. pen: 1\n8. book: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27561.1, "ram_available_mb": 98211.0, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 27562.1, "ram_available_mb": 98210.1, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.26, "peak": 40.18, "min": 16.16}, "VIN": {"avg": 67.05, "peak": 125.29, "min": 30.01}, "VIN_SYS_5V0": {"avg": 14.89, "peak": 16.05, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 15.92, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 27.26, "energy_joules_est": 70.61, "sample_count": 25, "duration_seconds": 2.59}, "timestamp": "2026-01-19T14:57:04.122255"}
{"image_index": 255, "image_name": "000000026564.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026564.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2544.66, "latencies_ms": [2544.66], "images_per_second": 0.393, "prompt_tokens": 1117, "response_tokens_est": 57, "n_tiles": 1, "output_text": " The computer monitor is positioned to the left of the keyboard, with the laptop to its right. The books are stacked on the left side of the desk, while the water bottle is placed on the right side. The window is located behind the desk, providing natural light to the workspace.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27562.1, "ram_available_mb": 98210.1, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 27568.2, "ram_available_mb": 98204.0, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.36, "peak": 40.57, "min": 16.95}, "VIN": {"avg": 69.74, "peak": 122.22, "min": 28.8}, "VIN_SYS_5V0": {"avg": 15.1, "peak": 16.15, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 16.18, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 27.36, "energy_joules_est": 69.63, "sample_count": 25, "duration_seconds": 2.545}, "timestamp": "2026-01-19T14:57:06.728086"}
{"image_index": 255, "image_name": "000000026564.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026564.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1768.056, "latencies_ms": [1768.056], "images_per_second": 0.566, "prompt_tokens": 1111, "response_tokens_est": 26, "n_tiles": 1, "output_text": " A cluttered desk with a computer monitor, keyboard, and mouse is in front of a window with a view of a building.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 27568.2, "ram_available_mb": 98204.0, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 27570.1, "ram_available_mb": 98202.1, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.17, "peak": 40.18, "min": 15.77}, "VIN": {"avg": 76.37, "peak": 117.52, "min": 31.83}, "VIN_SYS_5V0": {"avg": 14.94, "peak": 16.05, "min": 12.25}, "VDD_CPU_SOC_MSS": {"avg": 16.0, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 30.17, "energy_joules_est": 53.36, "sample_count": 17, "duration_seconds": 1.769}, "timestamp": "2026-01-19T14:57:08.500272"}
{"image_index": 255, "image_name": "000000026564.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026564.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1712.929, "latencies_ms": [1712.929], "images_per_second": 0.584, "prompt_tokens": 1109, "response_tokens_est": 24, "n_tiles": 1, "output_text": " The room is well-lit with natural light coming through the window, and the walls are painted in a light color.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27570.1, "ram_available_mb": 98202.1, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 27591.7, "ram_available_mb": 98180.5, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.24, "peak": 40.97, "min": 19.71}, "VIN": {"avg": 78.47, "peak": 128.32, "min": 27.51}, "VIN_SYS_5V0": {"avg": 15.24, "peak": 16.26, "min": 13.39}, "VDD_CPU_SOC_MSS": {"avg": 16.35, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 31.24, "energy_joules_est": 53.53, "sample_count": 17, "duration_seconds": 1.713}, "timestamp": "2026-01-19T14:57:10.271409"}
{"image_index": 256, "image_name": "000000026690.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026690.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1773.597, "latencies_ms": [1773.597], "images_per_second": 0.564, "prompt_tokens": 1100, "response_tokens_est": 27, "n_tiles": 1, "output_text": " A man in a helmet and black shirt is jumping in the air with his skateboard while a crowd of people watch from the stands.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 27591.7, "ram_available_mb": 98180.5, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 27591.9, "ram_available_mb": 98180.3, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.96, "peak": 40.57, "min": 17.74}, "VIN": {"avg": 78.81, "peak": 125.8, "min": 28.14}, "VIN_SYS_5V0": {"avg": 14.9, "peak": 16.15, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.99, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 29.96, "energy_joules_est": 53.16, "sample_count": 18, "duration_seconds": 1.774}, "timestamp": "2026-01-19T14:57:12.149532"}
{"image_index": 256, "image_name": "000000026690.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026690.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2587.963, "latencies_ms": [2587.963], "images_per_second": 0.386, "prompt_tokens": 1114, "response_tokens_est": 58, "n_tiles": 1, "output_text": " 1. person: 1\n2. helmet: 1\n3. skateboard: 1\n4. crowd: 1\n5. camera: 1\n6. banner: 1\n7. railing: 1\n8. wall: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27591.9, "ram_available_mb": 98180.3, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 27595.6, "ram_available_mb": 98176.6, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.25, "peak": 40.16, "min": 16.95}, "VIN": {"avg": 62.94, "peak": 86.5, "min": 31.41}, "VIN_SYS_5V0": {"avg": 14.86, "peak": 16.05, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 16.0, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 27.25, "energy_joules_est": 70.53, "sample_count": 25, "duration_seconds": 2.588}, "timestamp": "2026-01-19T14:57:14.751287"}
{"image_index": 256, "image_name": "000000026690.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026690.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2283.31, "latencies_ms": [2283.31], "images_per_second": 0.438, "prompt_tokens": 1118, "response_tokens_est": 47, "n_tiles": 1, "output_text": " The skateboarder is in the foreground, performing a trick in the air. The crowd is in the background, watching the performance. The skateboarder is to the left of the camera, while the crowd is to the right.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27595.6, "ram_available_mb": 98176.6, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 7.1, "ram_used_mb": 27550.4, "ram_available_mb": 98221.7, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.33, "peak": 40.16, "min": 16.95}, "VIN": {"avg": 74.14, "peak": 120.39, "min": 36.05}, "VIN_SYS_5V0": {"avg": 15.21, "peak": 16.26, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 16.55, "peak": 17.72, "min": 14.18}}, "power_watts_avg": 28.33, "energy_joules_est": 64.71, "sample_count": 22, "duration_seconds": 2.284}, "timestamp": "2026-01-19T14:57:17.040229"}
{"image_index": 256, "image_name": "000000026690.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026690.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1460.954, "latencies_ms": [1460.954], "images_per_second": 0.684, "prompt_tokens": 1112, "response_tokens_est": 16, "n_tiles": 1, "output_text": " A skateboarder is performing a trick in the air while a crowd watches.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 27550.4, "ram_available_mb": 98221.7, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 27536.4, "ram_available_mb": 98235.7, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.22, "peak": 39.39, "min": 17.34}, "VIN": {"avg": 78.74, "peak": 124.08, "min": 28.54}, "VIN_SYS_5V0": {"avg": 15.25, "peak": 16.26, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 16.48, "peak": 17.72, "min": 14.18}}, "power_watts_avg": 31.22, "energy_joules_est": 45.62, "sample_count": 15, "duration_seconds": 1.461}, "timestamp": "2026-01-19T14:57:18.606708"}
{"image_index": 256, "image_name": "000000026690.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026690.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2111.663, "latencies_ms": [2111.663], "images_per_second": 0.474, "prompt_tokens": 1110, "response_tokens_est": 38, "n_tiles": 1, "output_text": " The image features a skateboarder performing a trick in an indoor arena with a crowd of spectators. The lighting is dim, and the skateboarder is wearing a helmet and protective gear.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27536.4, "ram_available_mb": 98235.7, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 27544.2, "ram_available_mb": 98228.0, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.76, "peak": 40.57, "min": 18.53}, "VIN": {"avg": 68.06, "peak": 107.7, "min": 30.03}, "VIN_SYS_5V0": {"avg": 14.76, "peak": 16.05, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.81, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 28.76, "energy_joules_est": 60.75, "sample_count": 21, "duration_seconds": 2.112}, "timestamp": "2026-01-19T14:57:20.791286"}
{"image_index": 257, "image_name": "000000026926.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026926.jpg", "image_width": 423, "image_height": 640, "image_resolution": "423x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1729.868, "latencies_ms": [1729.868], "images_per_second": 0.578, "prompt_tokens": 1100, "response_tokens_est": 25, "n_tiles": 1, "output_text": " A red fire hydrant with a smiley face and a smiley face on it is on the side of the road.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 27544.2, "ram_available_mb": 98228.0, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 27538.3, "ram_available_mb": 98233.9, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.89, "peak": 39.77, "min": 15.77}, "VIN": {"avg": 74.13, "peak": 123.48, "min": 28.92}, "VIN_SYS_5V0": {"avg": 14.84, "peak": 16.15, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 15.84, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 29.89, "energy_joules_est": 51.73, "sample_count": 17, "duration_seconds": 1.731}, "timestamp": "2026-01-19T14:57:22.566663"}
{"image_index": 257, "image_name": "000000026926.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026926.jpg", "image_width": 423, "image_height": 640, "image_resolution": "423x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2157.135, "latencies_ms": [2157.135], "images_per_second": 0.464, "prompt_tokens": 1114, "response_tokens_est": 42, "n_tiles": 1, "output_text": " hydrant: 1\nsmile: 1\ntree: 1\ncar: 1\nstreet: 1\nbuilding: 1\ntree trunk: 1\nsky: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27538.3, "ram_available_mb": 98233.9, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 27528.1, "ram_available_mb": 98244.1, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.1, "peak": 40.57, "min": 18.53}, "VIN": {"avg": 70.38, "peak": 111.57, "min": 29.78}, "VIN_SYS_5V0": {"avg": 15.1, "peak": 16.15, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 16.18, "peak": 17.71, "min": 13.79}}, "power_watts_avg": 29.1, "energy_joules_est": 62.79, "sample_count": 21, "duration_seconds": 2.158}, "timestamp": "2026-01-19T14:57:24.751560"}
{"image_index": 257, "image_name": "000000026926.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026926.jpg", "image_width": 423, "image_height": 640, "image_resolution": "423x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1785.978, "latencies_ms": [1785.978], "images_per_second": 0.56, "prompt_tokens": 1118, "response_tokens_est": 28, "n_tiles": 1, "output_text": " The fire hydrant is located on the right side of the image, in the foreground, and is positioned near a tree and a street.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27528.1, "ram_available_mb": 98244.1, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 27530.8, "ram_available_mb": 98241.4, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.96, "peak": 40.56, "min": 17.35}, "VIN": {"avg": 66.09, "peak": 106.59, "min": 30.19}, "VIN_SYS_5V0": {"avg": 15.11, "peak": 16.26, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 16.21, "peak": 17.71, "min": 14.18}}, "power_watts_avg": 29.96, "energy_joules_est": 53.52, "sample_count": 18, "duration_seconds": 1.786}, "timestamp": "2026-01-19T14:57:26.627853"}
{"image_index": 257, "image_name": "000000026926.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026926.jpg", "image_width": 423, "image_height": 640, "image_resolution": "423x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1810.636, "latencies_ms": [1810.636], "images_per_second": 0.552, "prompt_tokens": 1112, "response_tokens_est": 28, "n_tiles": 1, "output_text": " A red fire hydrant with a smiley face is on the side of the road, with a tree and a building in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27530.8, "ram_available_mb": 98241.4, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 27535.0, "ram_available_mb": 98237.2, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.61, "peak": 39.78, "min": 16.95}, "VIN": {"avg": 68.03, "peak": 104.2, "min": 28.68}, "VIN_SYS_5V0": {"avg": 14.79, "peak": 16.15, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.97, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 29.61, "energy_joules_est": 53.63, "sample_count": 18, "duration_seconds": 1.811}, "timestamp": "2026-01-19T14:57:28.493471"}
{"image_index": 257, "image_name": "000000026926.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026926.jpg", "image_width": 423, "image_height": 640, "image_resolution": "423x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1719.766, "latencies_ms": [1719.766], "images_per_second": 0.581, "prompt_tokens": 1110, "response_tokens_est": 24, "n_tiles": 1, "output_text": " The fire hydrant is bright red with black markings, and it is located on a sunny day with a clear sky.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27535.0, "ram_available_mb": 98237.2, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 27540.0, "ram_available_mb": 98232.2, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.47, "peak": 40.56, "min": 17.74}, "VIN": {"avg": 73.42, "peak": 121.92, "min": 28.49}, "VIN_SYS_5V0": {"avg": 14.94, "peak": 16.15, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 16.09, "peak": 17.71, "min": 13.39}}, "power_watts_avg": 30.47, "energy_joules_est": 52.41, "sample_count": 17, "duration_seconds": 1.72}, "timestamp": "2026-01-19T14:57:30.266266"}
{"image_index": 258, "image_name": "000000026941.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026941.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1568.608, "latencies_ms": [1568.608], "images_per_second": 0.638, "prompt_tokens": 1099, "response_tokens_est": 18, "n_tiles": 1, "output_text": " A green cart with a bunch of old suitcases on it is sitting outside a building.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 27540.0, "ram_available_mb": 98232.2, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 27565.6, "ram_available_mb": 98206.6, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.0, "peak": 40.57, "min": 17.74}, "VIN": {"avg": 75.56, "peak": 124.68, "min": 27.98}, "VIN_SYS_5V0": {"avg": 14.74, "peak": 15.95, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.82, "peak": 16.92, "min": 13.39}}, "power_watts_avg": 31.0, "energy_joules_est": 48.65, "sample_count": 16, "duration_seconds": 1.569}, "timestamp": "2026-01-19T14:57:31.942577"}
{"image_index": 258, "image_name": "000000026941.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026941.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2666.726, "latencies_ms": [2666.726], "images_per_second": 0.375, "prompt_tokens": 1113, "response_tokens_est": 61, "n_tiles": 1, "output_text": " 1. suitcase: 6\n2. cart: 1\n3. door: 1\n4. wheel: 2\n5. handle: 1\n6. handlebar: 1\n7. handlebar wheel: 1\n8. cart wheel: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27565.6, "ram_available_mb": 98206.6, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 27579.6, "ram_available_mb": 98192.6, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.29, "peak": 40.16, "min": 18.14}, "VIN": {"avg": 66.55, "peak": 104.68, "min": 28.61}, "VIN_SYS_5V0": {"avg": 14.8, "peak": 15.95, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.79, "peak": 17.32, "min": 12.61}}, "power_watts_avg": 27.29, "energy_joules_est": 72.79, "sample_count": 26, "duration_seconds": 2.667}, "timestamp": "2026-01-19T14:57:34.651871"}
{"image_index": 258, "image_name": "000000026941.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026941.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2913.617, "latencies_ms": [2913.617], "images_per_second": 0.343, "prompt_tokens": 1117, "response_tokens_est": 71, "n_tiles": 1, "output_text": " The green cart is positioned in the foreground, with the suitcases stacked on top of it. The suitcases are arranged in a haphazard manner, with some leaning against each other and others stacked on top of one another. The cart is located near a building with green doors and windows, and there is a poster on the wall behind the cart.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27579.6, "ram_available_mb": 98192.6, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 27588.0, "ram_available_mb": 98184.1, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.51, "peak": 40.57, "min": 16.16}, "VIN": {"avg": 71.81, "peak": 116.64, "min": 32.97}, "VIN_SYS_5V0": {"avg": 15.03, "peak": 16.15, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 16.02, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 26.51, "energy_joules_est": 77.25, "sample_count": 28, "duration_seconds": 2.914}, "timestamp": "2026-01-19T14:57:37.571060"}
{"image_index": 258, "image_name": "000000026941.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026941.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1539.269, "latencies_ms": [1539.269], "images_per_second": 0.65, "prompt_tokens": 1111, "response_tokens_est": 18, "n_tiles": 1, "output_text": " A green cart is filled with old suitcases and a cart is parked outside a building.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 27588.0, "ram_available_mb": 98184.1, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 6.4, "ram_used_mb": 27542.1, "ram_available_mb": 98230.1, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.49, "peak": 40.57, "min": 16.95}, "VIN": {"avg": 80.65, "peak": 121.58, "min": 30.09}, "VIN_SYS_5V0": {"avg": 15.3, "peak": 16.26, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 16.61, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 31.49, "energy_joules_est": 48.49, "sample_count": 15, "duration_seconds": 1.54}, "timestamp": "2026-01-19T14:57:39.141651"}
{"image_index": 258, "image_name": "000000026941.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026941.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2217.533, "latencies_ms": [2217.533], "images_per_second": 0.451, "prompt_tokens": 1109, "response_tokens_est": 42, "n_tiles": 1, "output_text": " The image features a green trolley with a stack of old suitcases on it, with the suitcases being brown, blue, and green. The lighting is natural, and the weather appears to be clear.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27542.1, "ram_available_mb": 98230.1, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 27542.9, "ram_available_mb": 98229.3, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.99, "peak": 40.57, "min": 18.52}, "VIN": {"avg": 70.46, "peak": 117.97, "min": 29.54}, "VIN_SYS_5V0": {"avg": 14.97, "peak": 16.05, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 16.07, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 28.99, "energy_joules_est": 64.3, "sample_count": 22, "duration_seconds": 2.218}, "timestamp": "2026-01-19T14:57:41.435534"}
{"image_index": 259, "image_name": "000000027186.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027186.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1560.364, "latencies_ms": [1560.364], "images_per_second": 0.641, "prompt_tokens": 1099, "response_tokens_est": 18, "n_tiles": 1, "output_text": " A young girl in a pink dress is playing a video game on a Wii console.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27542.9, "ram_available_mb": 98229.3, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 27551.0, "ram_available_mb": 98221.2, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.81, "peak": 40.16, "min": 15.77}, "VIN": {"avg": 74.2, "peak": 124.02, "min": 29.87}, "VIN_SYS_5V0": {"avg": 14.78, "peak": 16.05, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.91, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 30.81, "energy_joules_est": 48.09, "sample_count": 15, "duration_seconds": 1.561}, "timestamp": "2026-01-19T14:57:43.012732"}
{"image_index": 259, "image_name": "000000027186.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027186.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2823.255, "latencies_ms": [2823.255], "images_per_second": 0.354, "prompt_tokens": 1113, "response_tokens_est": 68, "n_tiles": 1, "output_text": " 1. girl: 1\n2. sofa: 1\n3. window blinds: 1\n4. white object: 1\n5. girl's hand: 1\n6. girl's arm: 1\n7. girl's leg: 1\n8. girl's foot: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27551.0, "ram_available_mb": 98221.2, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 27549.2, "ram_available_mb": 98223.0, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.14, "peak": 40.95, "min": 17.34}, "VIN": {"avg": 67.72, "peak": 117.69, "min": 28.35}, "VIN_SYS_5V0": {"avg": 15.09, "peak": 16.15, "min": 13.17}, "VDD_CPU_SOC_MSS": {"avg": 16.14, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 27.14, "energy_joules_est": 76.64, "sample_count": 28, "duration_seconds": 2.824}, "timestamp": "2026-01-19T14:57:45.935633"}
{"image_index": 259, "image_name": "000000027186.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027186.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2130.413, "latencies_ms": [2130.413], "images_per_second": 0.469, "prompt_tokens": 1117, "response_tokens_est": 40, "n_tiles": 1, "output_text": " The girl is standing in the foreground of the image, holding a Wii remote in her right hand. The couch is located in the background, and the window blinds are positioned behind the couch.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 27549.2, "ram_available_mb": 98223.0, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 27564.5, "ram_available_mb": 98207.7, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.25, "peak": 39.77, "min": 14.98}, "VIN": {"avg": 70.12, "peak": 94.21, "min": 29.92}, "VIN_SYS_5V0": {"avg": 14.8, "peak": 15.95, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.82, "peak": 17.32, "min": 12.61}}, "power_watts_avg": 28.25, "energy_joules_est": 60.19, "sample_count": 21, "duration_seconds": 2.131}, "timestamp": "2026-01-19T14:57:48.111214"}
{"image_index": 259, "image_name": "000000027186.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027186.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1565.525, "latencies_ms": [1565.525], "images_per_second": 0.639, "prompt_tokens": 1111, "response_tokens_est": 18, "n_tiles": 1, "output_text": " A young girl is playing a video game on a Wii console in a living room.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27564.5, "ram_available_mb": 98207.7, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 27565.3, "ram_available_mb": 98206.8, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.7, "peak": 40.18, "min": 16.95}, "VIN": {"avg": 73.58, "peak": 110.59, "min": 28.27}, "VIN_SYS_5V0": {"avg": 14.91, "peak": 16.15, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 16.02, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 30.7, "energy_joules_est": 48.09, "sample_count": 16, "duration_seconds": 1.566}, "timestamp": "2026-01-19T14:57:49.779006"}
{"image_index": 259, "image_name": "000000027186.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027186.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2294.433, "latencies_ms": [2294.433], "images_per_second": 0.436, "prompt_tokens": 1109, "response_tokens_est": 46, "n_tiles": 1, "output_text": " The image is a photograph with a warm color tone, taken in a well-lit room with natural light coming from the window. The girl is wearing a pink dress with a floral pattern, and the couch is a neutral color.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 27565.3, "ram_available_mb": 98206.8, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 27570.2, "ram_available_mb": 98201.9, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.04, "peak": 40.16, "min": 17.74}, "VIN": {"avg": 68.92, "peak": 106.21, "min": 27.49}, "VIN_SYS_5V0": {"avg": 14.78, "peak": 16.05, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.82, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 28.04, "energy_joules_est": 64.34, "sample_count": 23, "duration_seconds": 2.295}, "timestamp": "2026-01-19T14:57:52.173625"}
{"image_index": 260, "image_name": "000000027620.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027620.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1731.208, "latencies_ms": [1731.208], "images_per_second": 0.578, "prompt_tokens": 1099, "response_tokens_est": 25, "n_tiles": 1, "output_text": " A cluttered desk with a laptop, keyboard, and headphones sits in front of a window with a trash can beside it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27570.2, "ram_available_mb": 98201.9, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 27575.6, "ram_available_mb": 98196.6, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.99, "peak": 40.18, "min": 15.38}, "VIN": {"avg": 72.11, "peak": 120.01, "min": 31.03}, "VIN_SYS_5V0": {"avg": 14.77, "peak": 15.95, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.84, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 29.99, "energy_joules_est": 51.93, "sample_count": 17, "duration_seconds": 1.732}, "timestamp": "2026-01-19T14:57:53.948600"}
{"image_index": 260, "image_name": "000000027620.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027620.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2217.18, "latencies_ms": [2217.18], "images_per_second": 0.451, "prompt_tokens": 1113, "response_tokens_est": 44, "n_tiles": 1, "output_text": " 1. black laptop\n2. black headphones\n3. black keyboard\n4. black mouse\n5. black computer monitor\n6. white trash can\n7. white computer tower\n8. black office chair", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27575.6, "ram_available_mb": 98196.6, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 27559.0, "ram_available_mb": 98213.2, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.72, "peak": 40.57, "min": 18.52}, "VIN": {"avg": 71.01, "peak": 107.22, "min": 29.1}, "VIN_SYS_5V0": {"avg": 15.03, "peak": 16.15, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 16.12, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 28.72, "energy_joules_est": 63.69, "sample_count": 22, "duration_seconds": 2.218}, "timestamp": "2026-01-19T14:57:56.243313"}
{"image_index": 260, "image_name": "000000027620.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027620.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1888.432, "latencies_ms": [1888.432], "images_per_second": 0.53, "prompt_tokens": 1117, "response_tokens_est": 31, "n_tiles": 1, "output_text": " The laptop is on the left side of the desk, the chair is on the right side, and the trash can is in the middle of the desk.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27559.0, "ram_available_mb": 98213.2, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 27563.9, "ram_available_mb": 98208.3, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.28, "peak": 40.16, "min": 16.16}, "VIN": {"avg": 69.35, "peak": 121.9, "min": 28.19}, "VIN_SYS_5V0": {"avg": 14.82, "peak": 16.05, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 15.93, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 29.28, "energy_joules_est": 55.31, "sample_count": 19, "duration_seconds": 1.889}, "timestamp": "2026-01-19T14:57:58.224970"}
{"image_index": 260, "image_name": "000000027620.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027620.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1589.081, "latencies_ms": [1589.081], "images_per_second": 0.629, "prompt_tokens": 1111, "response_tokens_est": 19, "n_tiles": 1, "output_text": " A cluttered office with a computer on a desk, a chair, and a trash can.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27563.9, "ram_available_mb": 98208.3, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 27579.9, "ram_available_mb": 98192.3, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.78, "peak": 40.18, "min": 16.56}, "VIN": {"avg": 74.01, "peak": 126.13, "min": 29.24}, "VIN_SYS_5V0": {"avg": 14.74, "peak": 16.05, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.9, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 30.78, "energy_joules_est": 48.92, "sample_count": 16, "duration_seconds": 1.589}, "timestamp": "2026-01-19T14:57:59.883725"}
{"image_index": 260, "image_name": "000000027620.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027620.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2236.679, "latencies_ms": [2236.679], "images_per_second": 0.447, "prompt_tokens": 1109, "response_tokens_est": 42, "n_tiles": 1, "output_text": " The room is well-lit with natural light coming through the windows, and the walls are painted white. The desk is made of glass and metal, and the chair is upholstered in gray fabric.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27579.9, "ram_available_mb": 98192.3, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 27540.4, "ram_available_mb": 98231.8, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.6, "peak": 40.18, "min": 18.91}, "VIN": {"avg": 69.26, "peak": 124.36, "min": 29.0}, "VIN_SYS_5V0": {"avg": 14.78, "peak": 15.95, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 16.0, "peak": 17.71, "min": 13.39}}, "power_watts_avg": 28.6, "energy_joules_est": 63.98, "sample_count": 22, "duration_seconds": 2.237}, "timestamp": "2026-01-19T14:58:02.173836"}
{"image_index": 261, "image_name": "000000027696.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027696.jpg", "image_width": 640, "image_height": 410, "image_resolution": "640x410", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1500.273, "latencies_ms": [1500.273], "images_per_second": 0.667, "prompt_tokens": 1099, "response_tokens_est": 16, "n_tiles": 1, "output_text": " A person is eating a pizza with mushrooms, peppers, and cheese on it.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 27540.4, "ram_available_mb": 98231.8, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 27537.7, "ram_available_mb": 98234.4, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.02, "peak": 40.16, "min": 16.16}, "VIN": {"avg": 77.82, "peak": 120.13, "min": 30.48}, "VIN_SYS_5V0": {"avg": 14.88, "peak": 16.05, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.93, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 31.02, "energy_joules_est": 46.55, "sample_count": 15, "duration_seconds": 1.501}, "timestamp": "2026-01-19T14:58:03.749129"}
{"image_index": 261, "image_name": "000000027696.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027696.jpg", "image_width": 640, "image_height": 410, "image_resolution": "640x410", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2562.807, "latencies_ms": [2562.807], "images_per_second": 0.39, "prompt_tokens": 1113, "response_tokens_est": 57, "n_tiles": 1, "output_text": " 1. plate: 1\n2. pizza: 1\n3. fork: 1\n4. hand: 1\n5. cheese: 1\n6. pepperoni: 1\n7. mushrooms: 1\n8. peppers: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27537.7, "ram_available_mb": 98234.4, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 27547.0, "ram_available_mb": 98225.2, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.97, "peak": 40.56, "min": 18.91}, "VIN": {"avg": 62.64, "peak": 118.53, "min": 29.63}, "VIN_SYS_5V0": {"avg": 14.94, "peak": 16.15, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.97, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 27.97, "energy_joules_est": 71.69, "sample_count": 25, "duration_seconds": 2.563}, "timestamp": "2026-01-19T14:58:06.341983"}
{"image_index": 261, "image_name": "000000027696.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027696.jpg", "image_width": 640, "image_height": 410, "image_resolution": "640x410", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1804.421, "latencies_ms": [1804.421], "images_per_second": 0.554, "prompt_tokens": 1117, "response_tokens_est": 28, "n_tiles": 1, "output_text": " The pizza is in the foreground, on a white plate, and the fork is in the person's hand, which is in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27547.0, "ram_available_mb": 98225.2, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 27560.2, "ram_available_mb": 98211.9, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.81, "peak": 40.18, "min": 16.56}, "VIN": {"avg": 72.07, "peak": 104.38, "min": 28.55}, "VIN_SYS_5V0": {"avg": 15.08, "peak": 16.15, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 16.14, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 29.81, "energy_joules_est": 53.8, "sample_count": 18, "duration_seconds": 1.805}, "timestamp": "2026-01-19T14:58:08.224753"}
{"image_index": 261, "image_name": "000000027696.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027696.jpg", "image_width": 640, "image_height": 410, "image_resolution": "640x410", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1517.572, "latencies_ms": [1517.572], "images_per_second": 0.659, "prompt_tokens": 1111, "response_tokens_est": 16, "n_tiles": 1, "output_text": " A person is eating a pizza with mushrooms, peppers, and cheese on it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27560.2, "ram_available_mb": 98211.9, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 27583.3, "ram_available_mb": 98188.9, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.23, "peak": 40.18, "min": 17.35}, "VIN": {"avg": 71.77, "peak": 118.79, "min": 29.05}, "VIN_SYS_5V0": {"avg": 14.77, "peak": 16.05, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.88, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 31.23, "energy_joules_est": 47.41, "sample_count": 15, "duration_seconds": 1.518}, "timestamp": "2026-01-19T14:58:09.797751"}
{"image_index": 261, "image_name": "000000027696.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027696.jpg", "image_width": 640, "image_height": 410, "image_resolution": "640x410", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1423.25, "latencies_ms": [1423.25], "images_per_second": 0.703, "prompt_tokens": 1109, "response_tokens_est": 13, "n_tiles": 1, "output_text": " The pizza is red and white, and the plate is white.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27583.3, "ram_available_mb": 98188.9, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 27585.1, "ram_available_mb": 98187.1, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.95, "peak": 40.57, "min": 20.5}, "VIN": {"avg": 70.78, "peak": 103.45, "min": 28.69}, "VIN_SYS_5V0": {"avg": 15.0, "peak": 16.15, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 16.2, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 32.95, "energy_joules_est": 46.91, "sample_count": 14, "duration_seconds": 1.424}, "timestamp": "2026-01-19T14:58:11.259936"}
{"image_index": 262, "image_name": "000000027768.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027768.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2065.957, "latencies_ms": [2065.957], "images_per_second": 0.484, "prompt_tokens": 1432, "response_tokens_est": 23, "n_tiles": 1, "output_text": " A white and red bus with the words \"Metropolitan Transit System\" on the side is parked on the street.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 27585.1, "ram_available_mb": 98187.1, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 27580.1, "ram_available_mb": 98192.1, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.2, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.93, "peak": 40.56, "min": 22.07}, "VIN": {"avg": 76.22, "peak": 117.5, "min": 30.54}, "VIN_SYS_5V0": {"avg": 15.54, "peak": 16.76, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 16.69, "peak": 18.11, "min": 13.79}}, "power_watts_avg": 32.93, "energy_joules_est": 68.06, "sample_count": 20, "duration_seconds": 2.067}, "timestamp": "2026-01-19T14:58:13.344490"}
{"image_index": 262, "image_name": "000000027768.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027768.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2953.216, "latencies_ms": [2953.216], "images_per_second": 0.339, "prompt_tokens": 1446, "response_tokens_est": 57, "n_tiles": 1, "output_text": " 1. bus: 1\n2. windows: 10\n3. seats: 2\n4. people: 2\n5. door: 1\n6. wheels: 2\n7. sign: 1\n8. building: 1", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 27580.1, "ram_available_mb": 98192.1, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 27584.5, "ram_available_mb": 98187.7, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.11, "peak": 40.56, "min": 18.91}, "VIN": {"avg": 69.4, "peak": 119.06, "min": 28.77}, "VIN_SYS_5V0": {"avg": 15.36, "peak": 16.76, "min": 13.29}, "VDD_CPU_SOC_MSS": {"avg": 16.37, "peak": 17.71, "min": 14.57}}, "power_watts_avg": 29.11, "energy_joules_est": 85.98, "sample_count": 29, "duration_seconds": 2.954}, "timestamp": "2026-01-19T14:58:16.362571"}
{"image_index": 262, "image_name": "000000027768.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027768.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2416.809, "latencies_ms": [2416.809], "images_per_second": 0.414, "prompt_tokens": 1450, "response_tokens_est": 36, "n_tiles": 1, "output_text": " The bus is parked on the left side of the street, with the sidewalk and trees on the right side. The bus is in the foreground, with the building in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27584.5, "ram_available_mb": 98187.7, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 27598.7, "ram_available_mb": 98173.5, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.06, "peak": 40.18, "min": 16.16}, "VIN": {"avg": 72.85, "peak": 127.78, "min": 28.34}, "VIN_SYS_5V0": {"avg": 15.17, "peak": 16.56, "min": 12.25}, "VDD_CPU_SOC_MSS": {"avg": 16.22, "peak": 17.71, "min": 13.39}}, "power_watts_avg": 30.06, "energy_joules_est": 72.67, "sample_count": 24, "duration_seconds": 2.417}, "timestamp": "2026-01-19T14:58:18.866596"}
{"image_index": 262, "image_name": "000000027768.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027768.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2067.363, "latencies_ms": [2067.363], "images_per_second": 0.484, "prompt_tokens": 1444, "response_tokens_est": 23, "n_tiles": 1, "output_text": " A white and red bus is parked on the side of the road, with a building and trees in the background.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 27598.7, "ram_available_mb": 98173.5, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 27600.1, "ram_available_mb": 98172.1, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.77, "peak": 40.18, "min": 16.56}, "VIN": {"avg": 74.2, "peak": 120.58, "min": 30.37}, "VIN_SYS_5V0": {"avg": 15.17, "peak": 16.56, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 16.34, "peak": 17.71, "min": 13.0}}, "power_watts_avg": 31.77, "energy_joules_est": 65.7, "sample_count": 20, "duration_seconds": 2.068}, "timestamp": "2026-01-19T14:58:20.961655"}
{"image_index": 262, "image_name": "000000027768.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027768.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1886.9, "latencies_ms": [1886.9], "images_per_second": 0.53, "prompt_tokens": 1442, "response_tokens_est": 17, "n_tiles": 1, "output_text": " The bus is white and red, and it is parked in a sunny parking lot.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27600.1, "ram_available_mb": 98172.1, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 27602.8, "ram_available_mb": 98169.4, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 33.05, "peak": 40.56, "min": 19.71}, "VIN": {"avg": 78.77, "peak": 127.59, "min": 28.63}, "VIN_SYS_5V0": {"avg": 15.42, "peak": 16.76, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 16.6, "peak": 18.11, "min": 14.18}}, "power_watts_avg": 33.05, "energy_joules_est": 62.37, "sample_count": 19, "duration_seconds": 1.887}, "timestamp": "2026-01-19T14:58:22.945166"}
{"image_index": 263, "image_name": "000000027932.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027932.jpg", "image_width": 288, "image_height": 307, "image_resolution": "288x307", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2051.002, "latencies_ms": [2051.002], "images_per_second": 0.488, "prompt_tokens": 1432, "response_tokens_est": 21, "n_tiles": 1, "output_text": " A baseball glove and a baseball cap are placed on the ground, with the cap resting on the glove.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27602.8, "ram_available_mb": 98169.4, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 27566.2, "ram_available_mb": 98205.9, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.2, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.03, "peak": 40.18, "min": 18.92}, "VIN": {"avg": 77.04, "peak": 127.65, "min": 29.0}, "VIN_SYS_5V0": {"avg": 15.23, "peak": 16.76, "min": 11.87}, "VDD_CPU_SOC_MSS": {"avg": 16.61, "peak": 18.5, "min": 13.0}}, "power_watts_avg": 32.03, "energy_joules_est": 65.72, "sample_count": 20, "duration_seconds": 2.052}, "timestamp": "2026-01-19T14:58:25.024363"}
{"image_index": 263, "image_name": "000000027932.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027932.jpg", "image_width": 288, "image_height": 307, "image_resolution": "288x307", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2505.695, "latencies_ms": [2505.695], "images_per_second": 0.399, "prompt_tokens": 1446, "response_tokens_est": 40, "n_tiles": 1, "output_text": " baseball cap: 1, baseball glove: 1, baseball: 1, baseball bat: 0, baseball field: 0, baseball bat: 0, baseball field: 0", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27566.2, "ram_available_mb": 98205.9, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 27565.1, "ram_available_mb": 98207.0, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.54, "peak": 40.57, "min": 18.92}, "VIN": {"avg": 73.29, "peak": 124.09, "min": 30.29}, "VIN_SYS_5V0": {"avg": 15.34, "peak": 16.66, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 16.42, "peak": 17.71, "min": 14.18}}, "power_watts_avg": 30.54, "energy_joules_est": 76.54, "sample_count": 25, "duration_seconds": 2.506}, "timestamp": "2026-01-19T14:58:27.628338"}
{"image_index": 263, "image_name": "000000027932.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027932.jpg", "image_width": 288, "image_height": 307, "image_resolution": "288x307", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2717.133, "latencies_ms": [2717.133], "images_per_second": 0.368, "prompt_tokens": 1450, "response_tokens_est": 41, "n_tiles": 1, "output_text": " The baseball cap is positioned to the left of the baseball glove, which is located in the foreground of the image. The glove is resting on the ground, while the cap is placed on top of it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27565.1, "ram_available_mb": 98207.0, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 27574.7, "ram_available_mb": 98197.5, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.54, "peak": 39.78, "min": 16.16}, "VIN": {"avg": 68.52, "peak": 123.95, "min": 27.81}, "VIN_SYS_5V0": {"avg": 14.79, "peak": 16.46, "min": 11.77}, "VDD_CPU_SOC_MSS": {"avg": 15.82, "peak": 17.71, "min": 13.0}}, "power_watts_avg": 28.54, "energy_joules_est": 77.56, "sample_count": 27, "duration_seconds": 2.718}, "timestamp": "2026-01-19T14:58:30.432288"}
{"image_index": 263, "image_name": "000000027932.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027932.jpg", "image_width": 288, "image_height": 307, "image_resolution": "288x307", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1819.353, "latencies_ms": [1819.353], "images_per_second": 0.55, "prompt_tokens": 1444, "response_tokens_est": 13, "n_tiles": 1, "output_text": " A baseball glove and a baseball cap are placed on the ground.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 27574.7, "ram_available_mb": 98197.5, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 27570.9, "ram_available_mb": 98201.2, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.11, "peak": 40.18, "min": 15.38}, "VIN": {"avg": 79.52, "peak": 126.55, "min": 29.33}, "VIN_SYS_5V0": {"avg": 15.02, "peak": 16.56, "min": 11.66}, "VDD_CPU_SOC_MSS": {"avg": 16.21, "peak": 17.71, "min": 12.61}}, "power_watts_avg": 32.11, "energy_joules_est": 58.43, "sample_count": 18, "duration_seconds": 1.82}, "timestamp": "2026-01-19T14:58:32.316387"}
{"image_index": 263, "image_name": "000000027932.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027932.jpg", "image_width": 288, "image_height": 307, "image_resolution": "288x307", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2280.74, "latencies_ms": [2280.74], "images_per_second": 0.438, "prompt_tokens": 1442, "response_tokens_est": 31, "n_tiles": 1, "output_text": " The image has a blue baseball cap with a white logo on it, and the baseball glove is brown. The cap is placed on top of the glove.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 27570.9, "ram_available_mb": 98201.2, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 27569.8, "ram_available_mb": 98202.4, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.82, "peak": 40.57, "min": 20.5}, "VIN": {"avg": 75.0, "peak": 119.29, "min": 31.17}, "VIN_SYS_5V0": {"avg": 15.25, "peak": 16.66, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 16.46, "peak": 17.71, "min": 13.39}}, "power_watts_avg": 31.82, "energy_joules_est": 72.58, "sample_count": 22, "duration_seconds": 2.281}, "timestamp": "2026-01-19T14:58:34.608178"}
{"image_index": 264, "image_name": "000000027972.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027972.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1536.606, "latencies_ms": [1536.606], "images_per_second": 0.651, "prompt_tokens": 1099, "response_tokens_est": 18, "n_tiles": 1, "output_text": " A man wearing a red shirt is riding a surfboard on a wave in the ocean.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 27569.8, "ram_available_mb": 98202.4, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 27572.0, "ram_available_mb": 98200.2, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.99, "peak": 40.56, "min": 18.91}, "VIN": {"avg": 68.74, "peak": 105.09, "min": 30.55}, "VIN_SYS_5V0": {"avg": 15.2, "peak": 16.15, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 16.38, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 31.99, "energy_joules_est": 49.18, "sample_count": 15, "duration_seconds": 1.537}, "timestamp": "2026-01-19T14:58:36.182355"}
{"image_index": 264, "image_name": "000000027972.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027972.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2756.583, "latencies_ms": [2756.583], "images_per_second": 0.363, "prompt_tokens": 1113, "response_tokens_est": 62, "n_tiles": 1, "output_text": " 1. Surfer: 1\n2. Surfboard: 1\n3. Ocean: 1\n4. Wave: 1\n5. Water: 1\n6. Sky: 1\n7. Clouds: 1\n8. Surfboard logo: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27572.0, "ram_available_mb": 98200.2, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 27581.4, "ram_available_mb": 98190.8, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.3, "peak": 40.56, "min": 18.52}, "VIN": {"avg": 65.46, "peak": 107.24, "min": 29.62}, "VIN_SYS_5V0": {"avg": 14.99, "peak": 16.15, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 15.97, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 27.3, "energy_joules_est": 75.27, "sample_count": 27, "duration_seconds": 2.757}, "timestamp": "2026-01-19T14:58:38.976306"}
{"image_index": 264, "image_name": "000000027972.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027972.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2174.709, "latencies_ms": [2174.709], "images_per_second": 0.46, "prompt_tokens": 1117, "response_tokens_est": 42, "n_tiles": 1, "output_text": " The surfer is positioned in the foreground of the image, riding a wave that dominates the majority of the frame. The wave is in the background, creating a dynamic contrast between the surfer and the water.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 27581.4, "ram_available_mb": 98190.8, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 27593.5, "ram_available_mb": 98178.7, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.53, "peak": 40.16, "min": 15.77}, "VIN": {"avg": 67.39, "peak": 107.63, "min": 32.66}, "VIN_SYS_5V0": {"avg": 15.06, "peak": 16.15, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 16.11, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 28.53, "energy_joules_est": 62.06, "sample_count": 21, "duration_seconds": 2.175}, "timestamp": "2026-01-19T14:58:41.160346"}
{"image_index": 264, "image_name": "000000027972.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027972.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1365.582, "latencies_ms": [1365.582], "images_per_second": 0.732, "prompt_tokens": 1111, "response_tokens_est": 11, "n_tiles": 1, "output_text": " A man is surfing on a wave in the ocean.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 27593.5, "ram_available_mb": 98178.7, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 27586.6, "ram_available_mb": 98185.6, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.91, "peak": 40.56, "min": 17.34}, "VIN": {"avg": 72.84, "peak": 94.33, "min": 27.61}, "VIN_SYS_5V0": {"avg": 15.17, "peak": 16.15, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 16.37, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 31.91, "energy_joules_est": 43.59, "sample_count": 14, "duration_seconds": 1.366}, "timestamp": "2026-01-19T14:58:42.621239"}
{"image_index": 264, "image_name": "000000027972.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027972.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2415.664, "latencies_ms": [2415.664], "images_per_second": 0.414, "prompt_tokens": 1109, "response_tokens_est": 51, "n_tiles": 1, "output_text": " The image features a surfer riding a wave in a vibrant blue ocean, with the surfer wearing a red shirt and a yellow surfboard. The lighting is natural and bright, suggesting a sunny day, and the water appears to be clear and clean.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27586.6, "ram_available_mb": 98185.6, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 27589.8, "ram_available_mb": 98182.3, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.37, "peak": 40.95, "min": 18.14}, "VIN": {"avg": 75.28, "peak": 127.72, "min": 29.38}, "VIN_SYS_5V0": {"avg": 14.85, "peak": 16.05, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 16.03, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 28.37, "energy_joules_est": 68.54, "sample_count": 24, "duration_seconds": 2.416}, "timestamp": "2026-01-19T14:58:45.114125"}
{"image_index": 265, "image_name": "000000027982.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027982.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1650.681, "latencies_ms": [1650.681], "images_per_second": 0.606, "prompt_tokens": 1099, "response_tokens_est": 21, "n_tiles": 1, "output_text": " A black and white photo of a bathroom with a toilet, sink, and a mirror above the sink.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27589.8, "ram_available_mb": 98182.3, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 6.8, "ram_used_mb": 27562.8, "ram_available_mb": 98209.4, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.55, "peak": 40.16, "min": 15.77}, "VIN": {"avg": 77.78, "peak": 119.46, "min": 30.84}, "VIN_SYS_5V0": {"avg": 14.89, "peak": 16.05, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 16.17, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 30.55, "energy_joules_est": 50.44, "sample_count": 16, "duration_seconds": 1.651}, "timestamp": "2026-01-19T14:58:46.785317"}
{"image_index": 265, "image_name": "000000027982.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027982.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1976.608, "latencies_ms": [1976.608], "images_per_second": 0.506, "prompt_tokens": 1113, "response_tokens_est": 35, "n_tiles": 1, "output_text": " toilet: 1, sink: 1, mirror: 1, cup: 1, bottle: 1, cupboard: 1, wall: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27562.8, "ram_available_mb": 98209.4, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 27557.0, "ram_available_mb": 98215.1, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.46, "peak": 40.97, "min": 20.1}, "VIN": {"avg": 76.11, "peak": 125.27, "min": 33.35}, "VIN_SYS_5V0": {"avg": 15.29, "peak": 16.26, "min": 13.29}, "VDD_CPU_SOC_MSS": {"avg": 16.6, "peak": 17.71, "min": 14.96}}, "power_watts_avg": 30.46, "energy_joules_est": 60.22, "sample_count": 19, "duration_seconds": 1.977}, "timestamp": "2026-01-19T14:58:48.765914"}
{"image_index": 265, "image_name": "000000027982.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027982.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1929.277, "latencies_ms": [1929.277], "images_per_second": 0.518, "prompt_tokens": 1117, "response_tokens_est": 33, "n_tiles": 1, "output_text": " The toilet is located on the left side of the image, while the sink is on the right side. The sink is positioned closer to the camera than the toilet.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27557.0, "ram_available_mb": 98215.1, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 27566.9, "ram_available_mb": 98205.3, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.04, "peak": 40.56, "min": 18.52}, "VIN": {"avg": 82.18, "peak": 130.47, "min": 29.2}, "VIN_SYS_5V0": {"avg": 15.22, "peak": 16.26, "min": 13.29}, "VDD_CPU_SOC_MSS": {"avg": 16.39, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 30.04, "energy_joules_est": 57.97, "sample_count": 19, "duration_seconds": 1.93}, "timestamp": "2026-01-19T14:58:50.741319"}
{"image_index": 265, "image_name": "000000027982.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027982.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1374.188, "latencies_ms": [1374.188], "images_per_second": 0.728, "prompt_tokens": 1111, "response_tokens_est": 11, "n_tiles": 1, "output_text": " A bathroom with a toilet, sink, and mirror.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27566.9, "ram_available_mb": 98205.3, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 27554.1, "ram_available_mb": 98218.0, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.99, "peak": 40.56, "min": 17.73}, "VIN": {"avg": 72.56, "peak": 103.55, "min": 27.33}, "VIN_SYS_5V0": {"avg": 14.93, "peak": 16.15, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 16.14, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 31.99, "energy_joules_est": 43.97, "sample_count": 14, "duration_seconds": 1.375}, "timestamp": "2026-01-19T14:58:52.208466"}
{"image_index": 265, "image_name": "000000027982.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027982.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1554.702, "latencies_ms": [1554.702], "images_per_second": 0.643, "prompt_tokens": 1109, "response_tokens_est": 17, "n_tiles": 1, "output_text": " The bathroom is black and white, with a granite countertop and a white toilet.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 27554.1, "ram_available_mb": 98218.0, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 27561.8, "ram_available_mb": 98210.4, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.3, "peak": 40.56, "min": 20.89}, "VIN": {"avg": 73.75, "peak": 112.3, "min": 31.09}, "VIN_SYS_5V0": {"avg": 14.77, "peak": 16.05, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.93, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 32.3, "energy_joules_est": 50.23, "sample_count": 15, "duration_seconds": 1.555}, "timestamp": "2026-01-19T14:58:53.780994"}
{"image_index": 266, "image_name": "000000028285.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000028285.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2101.717, "latencies_ms": [2101.717], "images_per_second": 0.476, "prompt_tokens": 1099, "response_tokens_est": 40, "n_tiles": 1, "output_text": " The image captures a picturesque scene of a white clock tower with a blue dome, standing tall against the backdrop of a clear blue sky, with a white ornate structure adorned with intricate designs in the foreground.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 27561.8, "ram_available_mb": 98210.4, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 27565.7, "ram_available_mb": 98206.5, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.62, "peak": 40.95, "min": 18.53}, "VIN": {"avg": 71.32, "peak": 122.91, "min": 28.39}, "VIN_SYS_5V0": {"avg": 15.08, "peak": 16.15, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 16.27, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 29.62, "energy_joules_est": 62.28, "sample_count": 21, "duration_seconds": 2.103}, "timestamp": "2026-01-19T14:58:55.971434"}
{"image_index": 266, "image_name": "000000028285.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000028285.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2678.111, "latencies_ms": [2678.111], "images_per_second": 0.373, "prompt_tokens": 1113, "response_tokens_est": 61, "n_tiles": 1, "output_text": " 1. clock tower: 1\n2. roof tiles: 1\n3. gazebo: 1\n4. clock face: 1\n5. antenna: 1\n6. roof: 1\n7. tree: 1\n8. sky: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 27565.7, "ram_available_mb": 98206.5, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 27576.7, "ram_available_mb": 98195.4, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.9, "peak": 40.18, "min": 16.16}, "VIN": {"avg": 69.97, "peak": 115.83, "min": 29.36}, "VIN_SYS_5V0": {"avg": 14.93, "peak": 16.05, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.99, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 26.9, "energy_joules_est": 72.05, "sample_count": 26, "duration_seconds": 2.678}, "timestamp": "2026-01-19T14:58:58.673850"}
{"image_index": 266, "image_name": "000000028285.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000028285.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2648.891, "latencies_ms": [2648.891], "images_per_second": 0.378, "prompt_tokens": 1117, "response_tokens_est": 58, "n_tiles": 1, "output_text": " The clock tower is positioned in the background, far away from the camera, and is surrounded by a clear blue sky. The ornate white and blue clock tower is situated on the right side of the image, with the ornate white and blue structure of the gazebo on the left side.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27576.7, "ram_available_mb": 98195.4, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 27543.0, "ram_available_mb": 98229.2, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.87, "peak": 40.57, "min": 16.56}, "VIN": {"avg": 65.91, "peak": 99.85, "min": 28.81}, "VIN_SYS_5V0": {"avg": 15.02, "peak": 16.15, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 16.08, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 26.87, "energy_joules_est": 71.19, "sample_count": 26, "duration_seconds": 2.649}, "timestamp": "2026-01-19T14:59:01.365655"}
{"image_index": 266, "image_name": "000000028285.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000028285.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2987.126, "latencies_ms": [2987.126], "images_per_second": 0.335, "prompt_tokens": 1111, "response_tokens_est": 73, "n_tiles": 1, "output_text": " The image captures a picturesque scene of a clock tower standing majestically against the backdrop of a clear blue sky. The tower, painted in pristine white, is adorned with a vibrant green roof and a clock face that stands out against its white facade. The tower is nestled amidst a lush landscape of trees and hills, adding a touch of serenity to the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27543.0, "ram_available_mb": 98229.2, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 27557.4, "ram_available_mb": 98214.7, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.03, "peak": 40.16, "min": 15.77}, "VIN": {"avg": 65.0, "peak": 132.01, "min": 30.14}, "VIN_SYS_5V0": {"avg": 14.92, "peak": 16.05, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.93, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 26.03, "energy_joules_est": 77.77, "sample_count": 29, "duration_seconds": 2.988}, "timestamp": "2026-01-19T14:59:04.378449"}
{"image_index": 266, "image_name": "000000028285.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000028285.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2277.992, "latencies_ms": [2277.992], "images_per_second": 0.439, "prompt_tokens": 1109, "response_tokens_est": 46, "n_tiles": 1, "output_text": " The image features a white clock tower with a blue dome and a red roof, set against a clear blue sky. The tower is adorned with intricate white and gold detailing, and the surrounding area is covered in terracotta tiles.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27557.4, "ram_available_mb": 98214.7, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 27565.8, "ram_available_mb": 98206.4, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.17, "peak": 40.18, "min": 16.16}, "VIN": {"avg": 69.0, "peak": 104.41, "min": 29.35}, "VIN_SYS_5V0": {"avg": 15.11, "peak": 16.15, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 16.12, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 28.17, "energy_joules_est": 64.18, "sample_count": 22, "duration_seconds": 2.278}, "timestamp": "2026-01-19T14:59:06.675480"}
{"image_index": 267, "image_name": "000000028449.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000028449.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1874.103, "latencies_ms": [1874.103], "images_per_second": 0.534, "prompt_tokens": 1099, "response_tokens_est": 31, "n_tiles": 1, "output_text": " In the image, a group of elephants is seen walking on a dirt path in a forest, with one elephant in the foreground and others in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27565.8, "ram_available_mb": 98206.4, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 27575.6, "ram_available_mb": 98196.6, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.07, "peak": 40.56, "min": 16.95}, "VIN": {"avg": 74.66, "peak": 106.23, "min": 39.61}, "VIN_SYS_5V0": {"avg": 15.22, "peak": 16.15, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 16.27, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 30.07, "energy_joules_est": 56.37, "sample_count": 18, "duration_seconds": 1.875}, "timestamp": "2026-01-19T14:59:08.556402"}
{"image_index": 267, "image_name": "000000028449.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000028449.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2074.73, "latencies_ms": [2074.73], "images_per_second": 0.482, "prompt_tokens": 1113, "response_tokens_est": 39, "n_tiles": 1, "output_text": " elephant: 1, elephant: 1, elephant: 1, elephant: 1, elephant: 1, elephant: 1, elephant: 1, elephant: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27575.6, "ram_available_mb": 98196.6, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 27588.1, "ram_available_mb": 98184.1, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.67, "peak": 40.97, "min": 18.91}, "VIN": {"avg": 64.11, "peak": 123.29, "min": 37.81}, "VIN_SYS_5V0": {"avg": 15.31, "peak": 16.26, "min": 13.39}, "VDD_CPU_SOC_MSS": {"avg": 16.3, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 29.67, "energy_joules_est": 61.57, "sample_count": 20, "duration_seconds": 2.075}, "timestamp": "2026-01-19T14:59:10.641258"}
{"image_index": 267, "image_name": "000000028449.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000028449.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1931.051, "latencies_ms": [1931.051], "images_per_second": 0.518, "prompt_tokens": 1117, "response_tokens_est": 33, "n_tiles": 1, "output_text": " The main elephant is in the foreground, with the other elephants in the background. The elephants are walking on a dirt path, with trees and bushes on either side.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27588.1, "ram_available_mb": 98184.1, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 27603.3, "ram_available_mb": 98168.8, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.86, "peak": 40.57, "min": 17.73}, "VIN": {"avg": 76.34, "peak": 130.77, "min": 29.38}, "VIN_SYS_5V0": {"avg": 15.22, "peak": 16.26, "min": 13.29}, "VDD_CPU_SOC_MSS": {"avg": 16.31, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 29.86, "energy_joules_est": 57.68, "sample_count": 19, "duration_seconds": 1.932}, "timestamp": "2026-01-19T14:59:12.625507"}
{"image_index": 267, "image_name": "000000028449.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000028449.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1760.923, "latencies_ms": [1760.923], "images_per_second": 0.568, "prompt_tokens": 1111, "response_tokens_est": 26, "n_tiles": 1, "output_text": " A herd of elephants is walking through a forest, with one elephant in the foreground and the rest of the herd in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27603.3, "ram_available_mb": 98168.8, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 27606.2, "ram_available_mb": 98165.9, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.66, "peak": 40.57, "min": 17.35}, "VIN": {"avg": 76.73, "peak": 120.37, "min": 30.4}, "VIN_SYS_5V0": {"avg": 14.99, "peak": 16.05, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 16.14, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 30.66, "energy_joules_est": 54.0, "sample_count": 17, "duration_seconds": 1.761}, "timestamp": "2026-01-19T14:59:14.406041"}
{"image_index": 267, "image_name": "000000028449.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000028449.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1492.225, "latencies_ms": [1492.225], "images_per_second": 0.67, "prompt_tokens": 1109, "response_tokens_est": 16, "n_tiles": 1, "output_text": " The elephants are gray, the trees are green, and the ground is brown.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27606.2, "ram_available_mb": 98165.9, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 27608.4, "ram_available_mb": 98163.7, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.04, "peak": 40.56, "min": 19.32}, "VIN": {"avg": 72.5, "peak": 116.86, "min": 27.98}, "VIN_SYS_5V0": {"avg": 15.12, "peak": 16.15, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 16.32, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 32.04, "energy_joules_est": 47.83, "sample_count": 15, "duration_seconds": 1.493}, "timestamp": "2026-01-19T14:59:15.980537"}
{"image_index": 268, "image_name": "000000028452.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000028452.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1828.402, "latencies_ms": [1828.402], "images_per_second": 0.547, "prompt_tokens": 1100, "response_tokens_est": 29, "n_tiles": 1, "output_text": " An open refrigerator with no food inside, a bottle of yellow liquid on the top shelf, and a carton of eggs on the middle shelf.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 27608.4, "ram_available_mb": 98163.7, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 6.9, "ram_used_mb": 27535.9, "ram_available_mb": 98236.3, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.44, "peak": 40.16, "min": 19.32}, "VIN": {"avg": 75.65, "peak": 131.35, "min": 30.77}, "VIN_SYS_5V0": {"avg": 14.89, "peak": 16.15, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 16.21, "peak": 18.11, "min": 13.39}}, "power_watts_avg": 30.44, "energy_joules_est": 55.68, "sample_count": 18, "duration_seconds": 1.829}, "timestamp": "2026-01-19T14:59:17.862056"}
{"image_index": 268, "image_name": "000000028452.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000028452.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2696.892, "latencies_ms": [2696.892], "images_per_second": 0.371, "prompt_tokens": 1114, "response_tokens_est": 63, "n_tiles": 1, "output_text": " 1. Refrigerator: 1\n2. Egg carton: 1\n3. Bottle: 1\n4. Shelves: 4\n5. Drawers: 4\n6. Floor: 1\n7. Tile: 1\n8. Light: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27535.9, "ram_available_mb": 98236.3, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 27536.2, "ram_available_mb": 98235.9, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.2, "peak": 40.18, "min": 18.13}, "VIN": {"avg": 73.03, "peak": 119.52, "min": 29.97}, "VIN_SYS_5V0": {"avg": 15.06, "peak": 16.15, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 16.22, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 27.2, "energy_joules_est": 73.36, "sample_count": 26, "duration_seconds": 2.697}, "timestamp": "2026-01-19T14:59:20.572308"}
{"image_index": 268, "image_name": "000000028452.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000028452.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2324.611, "latencies_ms": [2324.611], "images_per_second": 0.43, "prompt_tokens": 1118, "response_tokens_est": 49, "n_tiles": 1, "output_text": " The refrigerator is located in the kitchen, with the left side of the refrigerator being closer to the camera than the right side. The bottle of mustard is placed on the top shelf of the refrigerator, which is positioned in the middle of the refrigerator.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27536.2, "ram_available_mb": 98235.9, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 27542.2, "ram_available_mb": 98229.9, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.9, "peak": 39.78, "min": 16.95}, "VIN": {"avg": 72.02, "peak": 126.24, "min": 28.77}, "VIN_SYS_5V0": {"avg": 15.18, "peak": 16.26, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 16.31, "peak": 17.71, "min": 14.18}}, "power_watts_avg": 27.9, "energy_joules_est": 64.87, "sample_count": 23, "duration_seconds": 2.325}, "timestamp": "2026-01-19T14:59:22.964884"}
{"image_index": 268, "image_name": "000000028452.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000028452.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2347.536, "latencies_ms": [2347.536], "images_per_second": 0.426, "prompt_tokens": 1112, "response_tokens_est": 49, "n_tiles": 1, "output_text": " The image captures a scene of a kitchen, where an open refrigerator stands, its interior empty and devoid of any food items. The refrigerator, with its white exterior, is positioned against a wall, and its door is open, revealing the interior.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27542.2, "ram_available_mb": 98229.9, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 27539.3, "ram_available_mb": 98232.9, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.53, "peak": 39.77, "min": 15.77}, "VIN": {"avg": 71.62, "peak": 132.07, "min": 29.53}, "VIN_SYS_5V0": {"avg": 15.0, "peak": 16.26, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 16.06, "peak": 17.71, "min": 13.39}}, "power_watts_avg": 27.53, "energy_joules_est": 64.64, "sample_count": 23, "duration_seconds": 2.348}, "timestamp": "2026-01-19T14:59:25.349857"}
{"image_index": 268, "image_name": "000000028452.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000028452.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1342.703, "latencies_ms": [1342.703], "images_per_second": 0.745, "prompt_tokens": 1110, "response_tokens_est": 11, "n_tiles": 1, "output_text": " The refrigerator is white and has a light on inside.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 27539.3, "ram_available_mb": 98232.9, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 27544.4, "ram_available_mb": 98227.8, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.09, "peak": 39.78, "min": 16.56}, "VIN": {"avg": 77.79, "peak": 121.55, "min": 31.2}, "VIN_SYS_5V0": {"avg": 15.15, "peak": 16.15, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 16.38, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 32.09, "energy_joules_est": 43.1, "sample_count": 13, "duration_seconds": 1.343}, "timestamp": "2026-01-19T14:59:26.709779"}
{"image_index": 269, "image_name": "000000028809.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000028809.jpg", "image_width": 498, "image_height": 500, "image_resolution": "498x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1807.642, "latencies_ms": [1807.642], "images_per_second": 0.553, "prompt_tokens": 1432, "response_tokens_est": 13, "n_tiles": 1, "output_text": " A bunch of bananas are on a table with a purple background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27544.4, "ram_available_mb": 98227.8, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 27547.8, "ram_available_mb": 98224.4, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.2, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 34.18, "peak": 40.97, "min": 23.64}, "VIN": {"avg": 78.3, "peak": 129.88, "min": 29.46}, "VIN_SYS_5V0": {"avg": 15.58, "peak": 16.76, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 16.91, "peak": 18.11, "min": 14.57}}, "power_watts_avg": 34.18, "energy_joules_est": 61.8, "sample_count": 18, "duration_seconds": 1.808}, "timestamp": "2026-01-19T14:59:28.592969"}
{"image_index": 269, "image_name": "000000028809.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000028809.jpg", "image_width": 498, "image_height": 500, "image_resolution": "498x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1571.819, "latencies_ms": [1571.819], "images_per_second": 0.636, "prompt_tokens": 1446, "response_tokens_est": 4, "n_tiles": 1, "output_text": " banana: 6", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27547.8, "ram_available_mb": 98224.4, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 2.3, "ram_used_mb": 27569.9, "ram_available_mb": 98202.3, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 34.08, "peak": 40.18, "min": 20.5}, "VIN": {"avg": 77.88, "peak": 127.02, "min": 30.17}, "VIN_SYS_5V0": {"avg": 15.15, "peak": 16.56, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 16.39, "peak": 17.71, "min": 13.39}}, "power_watts_avg": 34.08, "energy_joules_est": 53.58, "sample_count": 16, "duration_seconds": 1.572}, "timestamp": "2026-01-19T14:59:30.262883"}
{"image_index": 269, "image_name": "000000028809.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000028809.jpg", "image_width": 498, "image_height": 500, "image_resolution": "498x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2756.17, "latencies_ms": [2756.17], "images_per_second": 0.363, "prompt_tokens": 1450, "response_tokens_est": 48, "n_tiles": 1, "output_text": " The bananas are located in the foreground of the image, with a purple object in the background. The bananas are arranged in a row, with the leftmost banana being the closest to the viewer and the rightmost banana being the farthest away.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27569.9, "ram_available_mb": 98202.3, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 27578.5, "ram_available_mb": 98193.7, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.35, "peak": 40.57, "min": 19.7}, "VIN": {"avg": 72.8, "peak": 128.3, "min": 28.8}, "VIN_SYS_5V0": {"avg": 15.13, "peak": 16.56, "min": 11.87}, "VDD_CPU_SOC_MSS": {"avg": 16.23, "peak": 17.71, "min": 13.39}}, "power_watts_avg": 30.35, "energy_joules_est": 83.67, "sample_count": 27, "duration_seconds": 2.757}, "timestamp": "2026-01-19T14:59:33.071560"}
{"image_index": 269, "image_name": "000000028809.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000028809.jpg", "image_width": 498, "image_height": 500, "image_resolution": "498x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1807.988, "latencies_ms": [1807.988], "images_per_second": 0.553, "prompt_tokens": 1444, "response_tokens_est": 13, "n_tiles": 1, "output_text": " A bunch of bananas are on a table with a purple background.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 27578.5, "ram_available_mb": 98193.7, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 27579.2, "ram_available_mb": 98193.0, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.45, "peak": 39.78, "min": 16.95}, "VIN": {"avg": 80.8, "peak": 119.59, "min": 30.54}, "VIN_SYS_5V0": {"avg": 15.22, "peak": 16.56, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 16.36, "peak": 17.71, "min": 13.39}}, "power_watts_avg": 32.45, "energy_joules_est": 58.68, "sample_count": 18, "duration_seconds": 1.808}, "timestamp": "2026-01-19T14:59:34.949328"}
{"image_index": 269, "image_name": "000000028809.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000028809.jpg", "image_width": 498, "image_height": 500, "image_resolution": "498x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1811.267, "latencies_ms": [1811.267], "images_per_second": 0.552, "prompt_tokens": 1442, "response_tokens_est": 13, "n_tiles": 1, "output_text": " The bananas are yellow and ripe, and the background is purple.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27579.2, "ram_available_mb": 98193.0, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 27584.6, "ram_available_mb": 98187.6, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 33.66, "peak": 40.57, "min": 21.28}, "VIN": {"avg": 79.13, "peak": 128.32, "min": 29.97}, "VIN_SYS_5V0": {"avg": 15.22, "peak": 16.66, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 16.56, "peak": 17.71, "min": 13.79}}, "power_watts_avg": 33.66, "energy_joules_est": 60.98, "sample_count": 18, "duration_seconds": 1.812}, "timestamp": "2026-01-19T14:59:36.829001"}
{"image_index": 270, "image_name": "000000028993.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000028993.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2368.436, "latencies_ms": [2368.436], "images_per_second": 0.422, "prompt_tokens": 1432, "response_tokens_est": 34, "n_tiles": 1, "output_text": " In the image, there are four orange cylindrical bollards with yellow caps, standing in a row on a sidewalk, with a city street and buildings in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27584.6, "ram_available_mb": 98187.6, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 27592.3, "ram_available_mb": 98179.8, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.2, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.26, "peak": 40.56, "min": 20.89}, "VIN": {"avg": 73.76, "peak": 119.09, "min": 29.23}, "VIN_SYS_5V0": {"avg": 15.38, "peak": 16.76, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 16.57, "peak": 18.11, "min": 13.39}}, "power_watts_avg": 31.26, "energy_joules_est": 74.06, "sample_count": 23, "duration_seconds": 2.369}, "timestamp": "2026-01-19T14:59:39.228098"}
{"image_index": 270, "image_name": "000000028993.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000028993.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3083.09, "latencies_ms": [3083.09], "images_per_second": 0.324, "prompt_tokens": 1446, "response_tokens_est": 61, "n_tiles": 1, "output_text": " 1. Bollards: 4\n2. Trees: 2\n3. Buildings: 5\n4. Streetlights: 2\n5. Banners: 2\n6. Poles: 2\n7. Windows: 10\n8. Cars: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27592.3, "ram_available_mb": 98179.8, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 27540.9, "ram_available_mb": 98231.2, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.47, "peak": 40.16, "min": 18.14}, "VIN": {"avg": 71.57, "peak": 129.25, "min": 30.01}, "VIN_SYS_5V0": {"avg": 15.29, "peak": 16.76, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 16.46, "peak": 18.11, "min": 14.18}}, "power_watts_avg": 28.47, "energy_joules_est": 87.79, "sample_count": 30, "duration_seconds": 3.084}, "timestamp": "2026-01-19T14:59:42.358506"}
{"image_index": 270, "image_name": "000000028993.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000028993.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2589.065, "latencies_ms": [2589.065], "images_per_second": 0.386, "prompt_tokens": 1450, "response_tokens_est": 43, "n_tiles": 1, "output_text": " The three orange bollards are positioned in the foreground, with the city buildings in the background. The bollards are located on the left side of the image, while the buildings extend across the right side.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27540.6, "ram_available_mb": 98231.6, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 27540.3, "ram_available_mb": 98231.8, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.04, "peak": 40.18, "min": 16.56}, "VIN": {"avg": 74.81, "peak": 116.47, "min": 31.66}, "VIN_SYS_5V0": {"avg": 15.33, "peak": 16.66, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 16.44, "peak": 17.71, "min": 13.78}}, "power_watts_avg": 30.04, "energy_joules_est": 77.79, "sample_count": 25, "duration_seconds": 2.589}, "timestamp": "2026-01-19T14:59:44.958980"}
{"image_index": 270, "image_name": "000000028993.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000028993.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2356.294, "latencies_ms": [2356.294], "images_per_second": 0.424, "prompt_tokens": 1444, "response_tokens_est": 35, "n_tiles": 1, "output_text": " The image captures a bustling city square with a row of orange bollards standing in the foreground, while the background is filled with tall buildings and a few pedestrians walking around.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27540.3, "ram_available_mb": 98231.8, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 27541.0, "ram_available_mb": 98231.2, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.04, "peak": 40.57, "min": 17.73}, "VIN": {"avg": 71.63, "peak": 112.09, "min": 29.85}, "VIN_SYS_5V0": {"avg": 15.5, "peak": 16.76, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 16.62, "peak": 18.11, "min": 14.57}}, "power_watts_avg": 31.04, "energy_joules_est": 73.15, "sample_count": 23, "duration_seconds": 2.357}, "timestamp": "2026-01-19T14:59:47.359757"}
{"image_index": 270, "image_name": "000000028993.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000028993.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2155.903, "latencies_ms": [2155.903], "images_per_second": 0.464, "prompt_tokens": 1442, "response_tokens_est": 27, "n_tiles": 1, "output_text": " The image features a city square with a row of orange bollards, a snow-covered sidewalk, and a clear blue sky.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27541.0, "ram_available_mb": 98231.2, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 27552.8, "ram_available_mb": 98219.3, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.76, "peak": 40.16, "min": 17.74}, "VIN": {"avg": 75.01, "peak": 121.92, "min": 30.24}, "VIN_SYS_5V0": {"avg": 15.36, "peak": 16.66, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 16.52, "peak": 17.71, "min": 13.79}}, "power_watts_avg": 31.76, "energy_joules_est": 68.49, "sample_count": 21, "duration_seconds": 2.156}, "timestamp": "2026-01-19T14:59:49.549088"}
{"image_index": 271, "image_name": "000000029187.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029187.jpg", "image_width": 640, "image_height": 491, "image_resolution": "640x491", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2259.963, "latencies_ms": [2259.963], "images_per_second": 0.442, "prompt_tokens": 1432, "response_tokens_est": 30, "n_tiles": 1, "output_text": " A person wearing a helmet and a yellow shirt is riding a horse in a race, with a sign that says \"Magnum\" in the background.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 27552.8, "ram_available_mb": 98219.3, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 27554.9, "ram_available_mb": 98217.3, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.2, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.3, "peak": 40.56, "min": 18.92}, "VIN": {"avg": 74.26, "peak": 121.54, "min": 29.48}, "VIN_SYS_5V0": {"avg": 15.45, "peak": 16.76, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 16.61, "peak": 18.11, "min": 13.79}}, "power_watts_avg": 31.3, "energy_joules_est": 70.76, "sample_count": 22, "duration_seconds": 2.261}, "timestamp": "2026-01-19T14:59:51.841269"}
{"image_index": 271, "image_name": "000000029187.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029187.jpg", "image_width": 640, "image_height": 491, "image_resolution": "640x491", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2963.339, "latencies_ms": [2963.339], "images_per_second": 0.337, "prompt_tokens": 1446, "response_tokens_est": 57, "n_tiles": 1, "output_text": " 1. horse: 1\n2. rider: 1\n3. cart: 1\n4. number plate: 1\n5. helmet: 1\n6. grass: 1\n7. dirt: 1\n8. track: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27554.9, "ram_available_mb": 98217.3, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 27562.4, "ram_available_mb": 98209.8, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.94, "peak": 40.56, "min": 18.52}, "VIN": {"avg": 72.26, "peak": 115.32, "min": 29.2}, "VIN_SYS_5V0": {"avg": 15.27, "peak": 16.66, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 16.39, "peak": 18.11, "min": 14.18}}, "power_watts_avg": 28.94, "energy_joules_est": 85.77, "sample_count": 29, "duration_seconds": 2.964}, "timestamp": "2026-01-19T14:59:54.856644"}
{"image_index": 271, "image_name": "000000029187.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029187.jpg", "image_width": 640, "image_height": 491, "image_resolution": "640x491", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2433.93, "latencies_ms": [2433.93], "images_per_second": 0.411, "prompt_tokens": 1450, "response_tokens_est": 37, "n_tiles": 1, "output_text": " The horse is in the foreground, pulling the cart, while the rider is in the background. The cart is in the middle of the track, and the rider is wearing a helmet.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27562.4, "ram_available_mb": 98209.8, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 27562.0, "ram_available_mb": 98210.2, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.22, "peak": 40.16, "min": 16.56}, "VIN": {"avg": 73.25, "peak": 116.79, "min": 30.06}, "VIN_SYS_5V0": {"avg": 15.24, "peak": 16.66, "min": 12.36}, "VDD_CPU_SOC_MSS": {"avg": 16.34, "peak": 17.71, "min": 13.39}}, "power_watts_avg": 30.22, "energy_joules_est": 73.57, "sample_count": 24, "duration_seconds": 2.434}, "timestamp": "2026-01-19T14:59:57.360404"}
{"image_index": 271, "image_name": "000000029187.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029187.jpg", "image_width": 640, "image_height": 491, "image_resolution": "640x491", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2333.333, "latencies_ms": [2333.333], "images_per_second": 0.429, "prompt_tokens": 1444, "response_tokens_est": 33, "n_tiles": 1, "output_text": " A young man wearing a helmet and a yellow shirt is riding a horse in a dirt track. The horse is pulling a cart with a number 8 on it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27562.0, "ram_available_mb": 98210.2, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 27583.6, "ram_available_mb": 98188.5, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.66, "peak": 40.57, "min": 16.95}, "VIN": {"avg": 72.89, "peak": 120.87, "min": 29.03}, "VIN_SYS_5V0": {"avg": 15.19, "peak": 16.56, "min": 12.15}, "VDD_CPU_SOC_MSS": {"avg": 16.28, "peak": 17.71, "min": 13.39}}, "power_watts_avg": 30.66, "energy_joules_est": 71.55, "sample_count": 23, "duration_seconds": 2.334}, "timestamp": "2026-01-19T14:59:59.753022"}
{"image_index": 271, "image_name": "000000029187.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029187.jpg", "image_width": 640, "image_height": 491, "image_resolution": "640x491", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3201.878, "latencies_ms": [3201.878], "images_per_second": 0.312, "prompt_tokens": 1442, "response_tokens_est": 62, "n_tiles": 1, "output_text": " The image features a brown horse pulling a cart with a person wearing a helmet. The horse is running on a dirt track, and the background includes a grassy field and a fence. The lighting is natural, and the colors are vibrant, with the brown of the horse and the green of the field standing out.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27583.6, "ram_available_mb": 98188.5, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 27584.5, "ram_available_mb": 98187.7, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.92, "peak": 40.16, "min": 17.35}, "VIN": {"avg": 69.89, "peak": 124.32, "min": 30.12}, "VIN_SYS_5V0": {"avg": 15.08, "peak": 16.56, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 16.07, "peak": 17.71, "min": 13.39}}, "power_watts_avg": 27.92, "energy_joules_est": 89.4, "sample_count": 31, "duration_seconds": 3.202}, "timestamp": "2026-01-19T15:00:02.975353"}
{"image_index": 272, "image_name": "000000029393.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029393.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1594.104, "latencies_ms": [1594.104], "images_per_second": 0.627, "prompt_tokens": 1099, "response_tokens_est": 21, "n_tiles": 1, "output_text": " A brown dog is standing on a wooden platform in a backyard with a tree full of oranges behind it.", "error": null, "sys_before": {"cpu_percent": 16.7, "ram_used_mb": 27584.5, "ram_available_mb": 98187.7, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 27588.4, "ram_available_mb": 98183.8, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.07, "peak": 39.78, "min": 17.35}, "VIN": {"avg": 75.12, "peak": 108.13, "min": 28.58}, "VIN_SYS_5V0": {"avg": 15.15, "peak": 16.15, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 16.24, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 31.07, "energy_joules_est": 49.55, "sample_count": 16, "duration_seconds": 1.595}, "timestamp": "2026-01-19T15:00:04.650019"}
{"image_index": 272, "image_name": "000000029393.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029393.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2530.553, "latencies_ms": [2530.553], "images_per_second": 0.395, "prompt_tokens": 1113, "response_tokens_est": 56, "n_tiles": 1, "output_text": " 1. dog: 1\n2. fence: 1\n3. tree: 1\n4. grass: 1\n5. house: 1\n6. bench: 1\n7. wall: 1\n8. ground: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27588.4, "ram_available_mb": 98183.8, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 6.5, "ram_used_mb": 27538.5, "ram_available_mb": 98233.6, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.69, "peak": 40.56, "min": 18.13}, "VIN": {"avg": 67.54, "peak": 102.68, "min": 29.8}, "VIN_SYS_5V0": {"avg": 14.9, "peak": 16.15, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 16.24, "peak": 18.11, "min": 13.39}}, "power_watts_avg": 27.69, "energy_joules_est": 70.08, "sample_count": 25, "duration_seconds": 2.531}, "timestamp": "2026-01-19T15:00:07.251501"}
{"image_index": 272, "image_name": "000000029393.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029393.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2036.037, "latencies_ms": [2036.037], "images_per_second": 0.491, "prompt_tokens": 1117, "response_tokens_est": 37, "n_tiles": 1, "output_text": " The dog is standing on the left side of the image, with the tree and fence in the background. The dog is in the foreground, with the house and garden in the background.", "error": null, "sys_before": {"cpu_percent": 50.0, "ram_used_mb": 27538.5, "ram_available_mb": 98233.6, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 27537.5, "ram_available_mb": 98234.6, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.94, "peak": 40.57, "min": 15.77}, "VIN": {"avg": 70.81, "peak": 99.68, "min": 28.77}, "VIN_SYS_5V0": {"avg": 14.89, "peak": 16.05, "min": 12.15}, "VDD_CPU_SOC_MSS": {"avg": 16.08, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 28.94, "energy_joules_est": 58.94, "sample_count": 20, "duration_seconds": 2.036}, "timestamp": "2026-01-19T15:00:09.333930"}
{"image_index": 272, "image_name": "000000029393.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029393.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1564.706, "latencies_ms": [1564.706], "images_per_second": 0.639, "prompt_tokens": 1111, "response_tokens_est": 19, "n_tiles": 1, "output_text": " A brown dog is standing on a wooden platform in a backyard with a tree full of oranges.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27537.5, "ram_available_mb": 98234.6, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 27544.3, "ram_available_mb": 98227.9, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.0, "peak": 40.56, "min": 17.35}, "VIN": {"avg": 74.16, "peak": 109.97, "min": 29.29}, "VIN_SYS_5V0": {"avg": 15.01, "peak": 16.15, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 16.19, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 31.0, "energy_joules_est": 48.52, "sample_count": 16, "duration_seconds": 1.565}, "timestamp": "2026-01-19T15:00:11.003881"}
{"image_index": 272, "image_name": "000000029393.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029393.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1632.275, "latencies_ms": [1632.275], "images_per_second": 0.613, "prompt_tokens": 1109, "response_tokens_est": 21, "n_tiles": 1, "output_text": " The dog is brown, the fence is brown, the tree is green, and the sky is blue.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27544.3, "ram_available_mb": 98227.9, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 27533.4, "ram_available_mb": 98238.7, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.42, "peak": 40.56, "min": 18.14}, "VIN": {"avg": 76.99, "peak": 125.42, "min": 28.64}, "VIN_SYS_5V0": {"avg": 14.81, "peak": 16.05, "min": 11.87}, "VDD_CPU_SOC_MSS": {"avg": 15.94, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 31.42, "energy_joules_est": 51.3, "sample_count": 16, "duration_seconds": 1.633}, "timestamp": "2026-01-19T15:00:12.665035"}
{"image_index": 273, "image_name": "000000029397.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029397.jpg", "image_width": 640, "image_height": 449, "image_resolution": "640x449", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2509.07, "latencies_ms": [2509.07], "images_per_second": 0.399, "prompt_tokens": 1099, "response_tokens_est": 56, "n_tiles": 1, "output_text": " In the image, a person is standing on a wooden bench with their feet on a sign that reads \"WE'RE HERE TO BE STRONG\". The bench is placed on a brick floor, and the background features a white wall with a blue and red graffiti-like design.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 27533.4, "ram_available_mb": 98238.7, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 27543.1, "ram_available_mb": 98229.1, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.81, "peak": 40.97, "min": 17.73}, "VIN": {"avg": 64.99, "peak": 84.7, "min": 28.77}, "VIN_SYS_5V0": {"avg": 15.02, "peak": 16.15, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 16.17, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 27.81, "energy_joules_est": 69.79, "sample_count": 25, "duration_seconds": 2.51}, "timestamp": "2026-01-19T15:00:15.278611"}
{"image_index": 273, "image_name": "000000029397.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029397.jpg", "image_width": 640, "image_height": 449, "image_resolution": "640x449", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1720.419, "latencies_ms": [1720.419], "images_per_second": 0.581, "prompt_tokens": 1113, "response_tokens_est": 24, "n_tiles": 1, "output_text": " bench: 1, person: 1, paper: 1, shoe: 1, brick: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27543.1, "ram_available_mb": 98229.1, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 27543.3, "ram_available_mb": 98228.8, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.92, "peak": 40.18, "min": 15.38}, "VIN": {"avg": 73.38, "peak": 116.94, "min": 30.61}, "VIN_SYS_5V0": {"avg": 14.74, "peak": 16.05, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.93, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 29.92, "energy_joules_est": 51.49, "sample_count": 17, "duration_seconds": 1.721}, "timestamp": "2026-01-19T15:00:17.053125"}
{"image_index": 273, "image_name": "000000029397.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029397.jpg", "image_width": 640, "image_height": 449, "image_resolution": "640x449", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2822.201, "latencies_ms": [2822.201], "images_per_second": 0.354, "prompt_tokens": 1117, "response_tokens_est": 67, "n_tiles": 1, "output_text": " The bench is positioned in the foreground, with the person's legs and feet prominently displayed. The sign is placed to the left of the bench, suggesting it is a temporary or makeshift setup. The person's legs are positioned on the bench, with their feet resting on the edge, indicating they are either sitting or standing on the bench.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27543.3, "ram_available_mb": 98228.8, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 27550.5, "ram_available_mb": 98221.7, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.82, "peak": 40.56, "min": 17.73}, "VIN": {"avg": 60.58, "peak": 120.85, "min": 28.27}, "VIN_SYS_5V0": {"avg": 14.96, "peak": 16.05, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.93, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 26.82, "energy_joules_est": 75.7, "sample_count": 28, "duration_seconds": 2.823}, "timestamp": "2026-01-19T15:00:19.964184"}
{"image_index": 273, "image_name": "000000029397.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029397.jpg", "image_width": 640, "image_height": 449, "image_resolution": "640x449", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1795.376, "latencies_ms": [1795.376], "images_per_second": 0.557, "prompt_tokens": 1111, "response_tokens_est": 27, "n_tiles": 1, "output_text": " A person wearing bright orange pants and blue and yellow shoes is standing on a wooden bench. The bench is placed on a brick sidewalk.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 27550.5, "ram_available_mb": 98221.7, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 27553.2, "ram_available_mb": 98218.9, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.35, "peak": 40.18, "min": 14.98}, "VIN": {"avg": 71.03, "peak": 117.93, "min": 27.78}, "VIN_SYS_5V0": {"avg": 14.79, "peak": 16.05, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.92, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 29.35, "energy_joules_est": 52.7, "sample_count": 18, "duration_seconds": 1.796}, "timestamp": "2026-01-19T15:00:21.843353"}
{"image_index": 273, "image_name": "000000029397.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029397.jpg", "image_width": 640, "image_height": 449, "image_resolution": "640x449", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2902.439, "latencies_ms": [2902.439], "images_per_second": 0.345, "prompt_tokens": 1109, "response_tokens_est": 70, "n_tiles": 1, "output_text": " The image features a wooden bench with a person's legs and feet on it, wearing bright orange pants and blue and yellow shoes. The bench is placed on a brick floor, and there is a sign on the left side of the bench. The lighting in the image is natural, coming from the right side, and the colors are vibrant and bright.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 27553.2, "ram_available_mb": 98218.9, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 27576.4, "ram_available_mb": 98195.8, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.29, "peak": 40.18, "min": 17.34}, "VIN": {"avg": 68.39, "peak": 98.11, "min": 27.78}, "VIN_SYS_5V0": {"avg": 14.86, "peak": 16.05, "min": 12.15}, "VDD_CPU_SOC_MSS": {"avg": 15.88, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 26.29, "energy_joules_est": 76.33, "sample_count": 29, "duration_seconds": 2.903}, "timestamp": "2026-01-19T15:00:24.839129"}
{"image_index": 274, "image_name": "000000029596.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029596.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2122.966, "latencies_ms": [2122.966], "images_per_second": 0.471, "prompt_tokens": 1099, "response_tokens_est": 40, "n_tiles": 1, "output_text": " The image depicts a cozy living room with a red sofa, a dining table set with a white tablecloth and silverware, a television mounted on the wall, and a window with brown curtains.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 27576.4, "ram_available_mb": 98195.8, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 27576.9, "ram_available_mb": 98195.3, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.18, "peak": 40.18, "min": 14.98}, "VIN": {"avg": 70.83, "peak": 121.78, "min": 30.21}, "VIN_SYS_5V0": {"avg": 14.76, "peak": 16.05, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.92, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 28.18, "energy_joules_est": 59.85, "sample_count": 21, "duration_seconds": 2.124}, "timestamp": "2026-01-19T15:00:27.029043"}
{"image_index": 274, "image_name": "000000029596.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029596.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2583.292, "latencies_ms": [2583.292], "images_per_second": 0.387, "prompt_tokens": 1113, "response_tokens_est": 58, "n_tiles": 1, "output_text": " 1. red sofa: 1\n2. table: 1\n3. chairs: 2\n4. lamp: 2\n5. vase: 1\n6. curtains: 2\n7. television: 1\n8. picture: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27576.9, "ram_available_mb": 98195.3, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 27584.7, "ram_available_mb": 98187.5, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.22, "peak": 40.18, "min": 16.56}, "VIN": {"avg": 72.8, "peak": 127.59, "min": 30.38}, "VIN_SYS_5V0": {"avg": 14.94, "peak": 16.05, "min": 12.36}, "VDD_CPU_SOC_MSS": {"avg": 15.98, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 27.22, "energy_joules_est": 70.33, "sample_count": 25, "duration_seconds": 2.584}, "timestamp": "2026-01-19T15:00:29.634131"}
{"image_index": 274, "image_name": "000000029596.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029596.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2240.996, "latencies_ms": [2240.996], "images_per_second": 0.446, "prompt_tokens": 1117, "response_tokens_est": 45, "n_tiles": 1, "output_text": " The red sofa is located on the left side of the room, with the dining table and chairs placed in the center. The television is positioned on the right side of the room, with the window and curtains behind it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27584.7, "ram_available_mb": 98187.5, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 6.6, "ram_used_mb": 27540.1, "ram_available_mb": 98232.1, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.33, "peak": 40.18, "min": 16.55}, "VIN": {"avg": 68.21, "peak": 85.46, "min": 29.9}, "VIN_SYS_5V0": {"avg": 15.11, "peak": 16.15, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 16.39, "peak": 17.71, "min": 14.18}}, "power_watts_avg": 28.33, "energy_joules_est": 63.5, "sample_count": 22, "duration_seconds": 2.241}, "timestamp": "2026-01-19T15:00:31.928891"}
{"image_index": 274, "image_name": "000000029596.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029596.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2323.536, "latencies_ms": [2323.536], "images_per_second": 0.43, "prompt_tokens": 1111, "response_tokens_est": 48, "n_tiles": 1, "output_text": " The image depicts a cozy living room with a red sofa, a dining table set with a white tablecloth, and a television mounted on the wall. The room is well-lit with natural light coming from a window with brown curtains.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27540.1, "ram_available_mb": 98232.1, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 27544.3, "ram_available_mb": 98227.9, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.88, "peak": 40.16, "min": 16.56}, "VIN": {"avg": 67.22, "peak": 124.22, "min": 30.02}, "VIN_SYS_5V0": {"avg": 14.99, "peak": 16.05, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.94, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 27.88, "energy_joules_est": 64.79, "sample_count": 23, "duration_seconds": 2.324}, "timestamp": "2026-01-19T15:00:34.324970"}
{"image_index": 274, "image_name": "000000029596.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029596.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2208.155, "latencies_ms": [2208.155], "images_per_second": 0.453, "prompt_tokens": 1109, "response_tokens_est": 43, "n_tiles": 1, "output_text": " The room is bathed in warm sunlight, casting a soft glow on the furnishings. The walls are painted a soothing light yellow, and the furniture is made of wood, giving the room a warm and inviting atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27544.3, "ram_available_mb": 98227.9, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 27545.7, "ram_available_mb": 98226.5, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.08, "peak": 40.18, "min": 15.77}, "VIN": {"avg": 70.17, "peak": 117.03, "min": 28.71}, "VIN_SYS_5V0": {"avg": 14.89, "peak": 16.05, "min": 12.15}, "VDD_CPU_SOC_MSS": {"avg": 15.93, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 28.08, "energy_joules_est": 62.01, "sample_count": 22, "duration_seconds": 2.208}, "timestamp": "2026-01-19T15:00:36.615245"}
{"image_index": 275, "image_name": "000000029640.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029640.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1679.105, "latencies_ms": [1679.105], "images_per_second": 0.596, "prompt_tokens": 1099, "response_tokens_est": 23, "n_tiles": 1, "output_text": " A black frying pan filled with a mixture of broccoli, carrots, and meat is being stirred with a metal spoon.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 27545.7, "ram_available_mb": 98226.5, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 27556.3, "ram_available_mb": 98215.8, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.08, "peak": 40.18, "min": 15.77}, "VIN": {"avg": 70.24, "peak": 122.14, "min": 30.18}, "VIN_SYS_5V0": {"avg": 14.79, "peak": 16.05, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.82, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 30.08, "energy_joules_est": 50.53, "sample_count": 17, "duration_seconds": 1.68}, "timestamp": "2026-01-19T15:00:38.388194"}
{"image_index": 275, "image_name": "000000029640.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029640.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2759.835, "latencies_ms": [2759.835], "images_per_second": 0.362, "prompt_tokens": 1113, "response_tokens_est": 64, "n_tiles": 1, "output_text": " 1. broccoli: 12\n2. carrots: 10\n3. meat: 12\n4. potatoes: 10\n5. seasoning: 10\n6. broccoli: 12\n7. carrots: 10\n8. meat: 12", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 27556.3, "ram_available_mb": 98215.8, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 27559.0, "ram_available_mb": 98213.1, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.83, "peak": 40.57, "min": 17.74}, "VIN": {"avg": 70.45, "peak": 121.24, "min": 29.79}, "VIN_SYS_5V0": {"avg": 14.91, "peak": 16.05, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.92, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 26.83, "energy_joules_est": 74.06, "sample_count": 27, "duration_seconds": 2.76}, "timestamp": "2026-01-19T15:00:41.195975"}
{"image_index": 275, "image_name": "000000029640.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029640.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2248.7, "latencies_ms": [2248.7], "images_per_second": 0.445, "prompt_tokens": 1117, "response_tokens_est": 45, "n_tiles": 1, "output_text": " The broccoli is in the middle of the pan, the carrots are on the right side, and the meat is on the left side. The pan is on the stove, and the broccoli is in the middle of the pan.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 27559.0, "ram_available_mb": 98213.1, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 27573.2, "ram_available_mb": 98199.0, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.13, "peak": 40.18, "min": 15.77}, "VIN": {"avg": 69.74, "peak": 107.35, "min": 29.59}, "VIN_SYS_5V0": {"avg": 15.0, "peak": 16.15, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 16.12, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 28.13, "energy_joules_est": 63.27, "sample_count": 22, "duration_seconds": 2.249}, "timestamp": "2026-01-19T15:00:43.487422"}
{"image_index": 275, "image_name": "000000029640.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029640.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1391.697, "latencies_ms": [1391.697], "images_per_second": 0.719, "prompt_tokens": 1111, "response_tokens_est": 12, "n_tiles": 1, "output_text": " A black pan with food in it on a stove top.", "error": null, "sys_before": {"cpu_percent": 40.0, "ram_used_mb": 27573.2, "ram_available_mb": 98199.0, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 27575.1, "ram_available_mb": 98197.0, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.71, "peak": 40.56, "min": 16.56}, "VIN": {"avg": 74.55, "peak": 105.17, "min": 27.31}, "VIN_SYS_5V0": {"avg": 15.0, "peak": 16.15, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 16.11, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 31.71, "energy_joules_est": 44.15, "sample_count": 14, "duration_seconds": 1.392}, "timestamp": "2026-01-19T15:00:44.944392"}
{"image_index": 275, "image_name": "000000029640.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029640.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3114.885, "latencies_ms": [3114.885], "images_per_second": 0.321, "prompt_tokens": 1109, "response_tokens_est": 78, "n_tiles": 1, "output_text": " The image features a black frying pan with a variety of vegetables, including broccoli, carrots, and onions, being cooked in a pan on a stove. The vegetables are brightly colored, with the broccoli being a vibrant green, the carrots being orange, and the onions being white. The lighting in the image is bright and even, suggesting that the photo was taken in a well-lit kitchen.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 27575.1, "ram_available_mb": 98197.0, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 27576.9, "ram_available_mb": 98195.2, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.43, "peak": 40.95, "min": 16.95}, "VIN": {"avg": 69.91, "peak": 116.87, "min": 27.47}, "VIN_SYS_5V0": {"avg": 14.95, "peak": 16.15, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.98, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 26.43, "energy_joules_est": 82.34, "sample_count": 31, "duration_seconds": 3.115}, "timestamp": "2026-01-19T15:00:48.159868"}
{"image_index": 276, "image_name": "000000029675.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029675.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1455.172, "latencies_ms": [1455.172], "images_per_second": 0.687, "prompt_tokens": 1100, "response_tokens_est": 15, "n_tiles": 1, "output_text": " Three hot dogs with mustard on a plate are on a dark countertop.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 27576.9, "ram_available_mb": 98195.2, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 7.1, "ram_used_mb": 27532.5, "ram_available_mb": 98239.7, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.98, "peak": 39.77, "min": 14.59}, "VIN": {"avg": 79.73, "peak": 126.62, "min": 33.26}, "VIN_SYS_5V0": {"avg": 14.84, "peak": 16.15, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 16.2, "peak": 18.11, "min": 13.0}}, "power_watts_avg": 30.98, "energy_joules_est": 45.09, "sample_count": 14, "duration_seconds": 1.455}, "timestamp": "2026-01-19T15:00:49.626350"}
{"image_index": 276, "image_name": "000000029675.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029675.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1703.747, "latencies_ms": [1703.747], "images_per_second": 0.587, "prompt_tokens": 1114, "response_tokens_est": 25, "n_tiles": 1, "output_text": " hot dog: 3, mustard: 3, bun: 3, plate: 1, book: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27532.5, "ram_available_mb": 98239.7, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 27530.2, "ram_available_mb": 98241.9, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.91, "peak": 40.57, "min": 21.28}, "VIN": {"avg": 75.05, "peak": 123.96, "min": 30.74}, "VIN_SYS_5V0": {"avg": 15.33, "peak": 16.26, "min": 13.49}, "VDD_CPU_SOC_MSS": {"avg": 16.49, "peak": 17.71, "min": 14.96}}, "power_watts_avg": 31.91, "energy_joules_est": 54.38, "sample_count": 17, "duration_seconds": 1.704}, "timestamp": "2026-01-19T15:00:51.400120"}
{"image_index": 276, "image_name": "000000029675.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029675.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2198.88, "latencies_ms": [2198.88], "images_per_second": 0.455, "prompt_tokens": 1118, "response_tokens_est": 43, "n_tiles": 1, "output_text": " The hot dogs are placed on the plate, which is located in the foreground of the image. The hot dogs are positioned to the left of the plate, and the book is located to the right of the plate.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27530.2, "ram_available_mb": 98241.9, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 27538.2, "ram_available_mb": 98234.0, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.37, "peak": 40.16, "min": 18.14}, "VIN": {"avg": 68.26, "peak": 127.31, "min": 27.04}, "VIN_SYS_5V0": {"avg": 14.87, "peak": 16.15, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.96, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 28.37, "energy_joules_est": 62.39, "sample_count": 22, "duration_seconds": 2.199}, "timestamp": "2026-01-19T15:00:53.696059"}
{"image_index": 276, "image_name": "000000029675.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029675.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1482.084, "latencies_ms": [1482.084], "images_per_second": 0.675, "prompt_tokens": 1112, "response_tokens_est": 15, "n_tiles": 1, "output_text": " Three hot dogs with mustard on a plate are on a dark countertop.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27538.2, "ram_available_mb": 98234.0, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 27538.7, "ram_available_mb": 98233.5, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.39, "peak": 39.39, "min": 15.38}, "VIN": {"avg": 69.87, "peak": 106.51, "min": 29.83}, "VIN_SYS_5V0": {"avg": 14.67, "peak": 16.15, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.85, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 30.39, "energy_joules_est": 45.05, "sample_count": 15, "duration_seconds": 1.483}, "timestamp": "2026-01-19T15:00:55.256479"}
{"image_index": 276, "image_name": "000000029675.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029675.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2996.692, "latencies_ms": [2996.692], "images_per_second": 0.334, "prompt_tokens": 1110, "response_tokens_est": 74, "n_tiles": 1, "output_text": " The image features a plate with three hot dogs, each topped with mustard, placed on a dark brown plate. The hot dogs are in buns, and the mustard is bright yellow, contrasting with the red of the hot dogs. The background is a dark green countertop, and there is a book with a blue cover visible in the top right corner of the image.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 27538.7, "ram_available_mb": 98233.5, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 27546.3, "ram_available_mb": 98225.9, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.61, "peak": 40.56, "min": 18.92}, "VIN": {"avg": 71.46, "peak": 123.79, "min": 30.13}, "VIN_SYS_5V0": {"avg": 14.92, "peak": 16.05, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.95, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 26.61, "energy_joules_est": 79.75, "sample_count": 29, "duration_seconds": 2.997}, "timestamp": "2026-01-19T15:00:58.280933"}
{"image_index": 277, "image_name": "000000029984.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029984.jpg", "image_width": 640, "image_height": 492, "image_resolution": "640x492", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2455.601, "latencies_ms": [2455.601], "images_per_second": 0.407, "prompt_tokens": 1432, "response_tokens_est": 36, "n_tiles": 1, "output_text": " In the image, there are four people swimming in the ocean, with a green umbrella and two chairs placed on the beach, and a bird standing on the sand near the water.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27546.3, "ram_available_mb": 98225.9, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 27532.3, "ram_available_mb": 98239.8, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.2, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.78, "peak": 39.78, "min": 16.16}, "VIN": {"avg": 74.82, "peak": 129.78, "min": 30.37}, "VIN_SYS_5V0": {"avg": 15.4, "peak": 16.76, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 16.45, "peak": 18.11, "min": 13.79}}, "power_watts_avg": 29.78, "energy_joules_est": 73.15, "sample_count": 24, "duration_seconds": 2.456}, "timestamp": "2026-01-19T15:01:00.783599"}
{"image_index": 277, "image_name": "000000029984.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029984.jpg", "image_width": 640, "image_height": 492, "image_resolution": "640x492", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1855.236, "latencies_ms": [1855.236], "images_per_second": 0.539, "prompt_tokens": 1446, "response_tokens_est": 15, "n_tiles": 1, "output_text": " beach umbrella: 1\nchairs: 2\npeople: 4", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27532.3, "ram_available_mb": 98239.8, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 27545.0, "ram_available_mb": 98227.2, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.78, "peak": 40.18, "min": 16.95}, "VIN": {"avg": 78.44, "peak": 118.86, "min": 29.76}, "VIN_SYS_5V0": {"avg": 15.35, "peak": 16.66, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 16.49, "peak": 17.71, "min": 13.39}}, "power_watts_avg": 32.78, "energy_joules_est": 60.83, "sample_count": 18, "duration_seconds": 1.856}, "timestamp": "2026-01-19T15:01:02.662891"}
{"image_index": 277, "image_name": "000000029984.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029984.jpg", "image_width": 640, "image_height": 492, "image_resolution": "640x492", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3033.051, "latencies_ms": [3033.051], "images_per_second": 0.33, "prompt_tokens": 1450, "response_tokens_est": 59, "n_tiles": 1, "output_text": " The green umbrella is positioned to the right of the beach chairs, which are situated in the foreground of the image. The people are swimming in the water, which is located in the middle ground of the image, while the beach chairs are placed on the sandy beach, which is in the foreground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27545.0, "ram_available_mb": 98227.2, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 27546.9, "ram_available_mb": 98225.3, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.11, "peak": 40.95, "min": 18.13}, "VIN": {"avg": 70.69, "peak": 123.48, "min": 29.61}, "VIN_SYS_5V0": {"avg": 15.27, "peak": 16.66, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 16.38, "peak": 18.11, "min": 14.18}}, "power_watts_avg": 29.11, "energy_joules_est": 88.31, "sample_count": 30, "duration_seconds": 3.034}, "timestamp": "2026-01-19T15:01:05.788377"}
{"image_index": 277, "image_name": "000000029984.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029984.jpg", "image_width": 640, "image_height": 492, "image_resolution": "640x492", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1970.783, "latencies_ms": [1970.783], "images_per_second": 0.507, "prompt_tokens": 1444, "response_tokens_est": 19, "n_tiles": 1, "output_text": " A group of people are swimming in the ocean near a beach with a green umbrella and chairs.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 27546.9, "ram_available_mb": 98225.3, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 27567.1, "ram_available_mb": 98205.0, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.84, "peak": 40.16, "min": 15.38}, "VIN": {"avg": 78.63, "peak": 117.84, "min": 30.01}, "VIN_SYS_5V0": {"avg": 15.14, "peak": 16.56, "min": 11.77}, "VDD_CPU_SOC_MSS": {"avg": 16.33, "peak": 17.71, "min": 13.0}}, "power_watts_avg": 31.84, "energy_joules_est": 62.76, "sample_count": 19, "duration_seconds": 1.971}, "timestamp": "2026-01-19T15:01:07.773463"}
{"image_index": 277, "image_name": "000000029984.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029984.jpg", "image_width": 640, "image_height": 492, "image_resolution": "640x492", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2440.627, "latencies_ms": [2440.627], "images_per_second": 0.41, "prompt_tokens": 1442, "response_tokens_est": 37, "n_tiles": 1, "output_text": " The image depicts a beach scene with a group of people swimming in the ocean, with a green umbrella providing shade. The sky is overcast, and the water is a deep blue.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27567.1, "ram_available_mb": 98205.0, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 27576.9, "ram_available_mb": 98195.2, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.07, "peak": 40.57, "min": 20.1}, "VIN": {"avg": 73.17, "peak": 128.85, "min": 30.85}, "VIN_SYS_5V0": {"avg": 15.37, "peak": 16.66, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 16.47, "peak": 17.71, "min": 14.57}}, "power_watts_avg": 31.07, "energy_joules_est": 75.84, "sample_count": 24, "duration_seconds": 2.441}, "timestamp": "2026-01-19T15:01:10.273098"}
{"image_index": 278, "image_name": "000000030213.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030213.jpg", "image_width": 640, "image_height": 449, "image_resolution": "640x449", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1992.504, "latencies_ms": [1992.504], "images_per_second": 0.502, "prompt_tokens": 1099, "response_tokens_est": 35, "n_tiles": 1, "output_text": " The image depicts a vintage kitchen with green wallpaper, a white sink, a white refrigerator, and a white table with a glass top, surrounded by various kitchen appliances and utensils.", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 27576.9, "ram_available_mb": 98195.2, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 27562.0, "ram_available_mb": 98210.2, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.98, "peak": 40.57, "min": 16.95}, "VIN": {"avg": 68.6, "peak": 126.15, "min": 29.09}, "VIN_SYS_5V0": {"avg": 14.87, "peak": 16.05, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 16.04, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 28.98, "energy_joules_est": 57.76, "sample_count": 20, "duration_seconds": 1.993}, "timestamp": "2026-01-19T15:01:12.365082"}
{"image_index": 278, "image_name": "000000030213.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030213.jpg", "image_width": 640, "image_height": 449, "image_resolution": "640x449", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2106.444, "latencies_ms": [2106.444], "images_per_second": 0.475, "prompt_tokens": 1113, "response_tokens_est": 39, "n_tiles": 1, "output_text": " table: 1, chair: 1, sink: 1, refrigerator: 1, stove: 1, oven: 1, cabinet: 1, counter: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27562.0, "ram_available_mb": 98210.2, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 27522.3, "ram_available_mb": 98249.9, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.44, "peak": 40.18, "min": 16.16}, "VIN": {"avg": 73.37, "peak": 125.99, "min": 28.48}, "VIN_SYS_5V0": {"avg": 14.84, "peak": 16.15, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.99, "peak": 17.71, "min": 13.0}}, "power_watts_avg": 28.44, "energy_joules_est": 59.92, "sample_count": 21, "duration_seconds": 2.107}, "timestamp": "2026-01-19T15:01:14.558929"}
{"image_index": 278, "image_name": "000000030213.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030213.jpg", "image_width": 640, "image_height": 449, "image_resolution": "640x449", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2839.732, "latencies_ms": [2839.732], "images_per_second": 0.352, "prompt_tokens": 1117, "response_tokens_est": 64, "n_tiles": 1, "output_text": " The sink is located to the right of the stove, and the refrigerator is positioned to the right of the sink. The table is situated in the center of the room, with the chair placed in front of it. The green wallpaper covers the entire wall, and the floral-patterned carpet is spread across the floor.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 27524.6, "ram_available_mb": 98247.6, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 27529.8, "ram_available_mb": 98242.4, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.13, "peak": 40.18, "min": 16.16}, "VIN": {"avg": 67.93, "peak": 122.57, "min": 28.61}, "VIN_SYS_5V0": {"avg": 14.79, "peak": 16.05, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.79, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 26.13, "energy_joules_est": 74.21, "sample_count": 28, "duration_seconds": 2.84}, "timestamp": "2026-01-19T15:01:17.477108"}
{"image_index": 278, "image_name": "000000030213.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030213.jpg", "image_width": 640, "image_height": 449, "image_resolution": "640x449", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2083.687, "latencies_ms": [2083.687], "images_per_second": 0.48, "prompt_tokens": 1111, "response_tokens_est": 38, "n_tiles": 1, "output_text": " The image depicts a vintage kitchen with green wallpaper and a variety of appliances. The kitchen is equipped with a sink, a stove, and a refrigerator, all of which are in good condition.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27529.8, "ram_available_mb": 98242.4, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 27543.2, "ram_available_mb": 98228.9, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.16, "peak": 40.18, "min": 14.98}, "VIN": {"avg": 73.66, "peak": 123.04, "min": 27.45}, "VIN_SYS_5V0": {"avg": 14.78, "peak": 16.05, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.96, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 28.16, "energy_joules_est": 58.69, "sample_count": 21, "duration_seconds": 2.084}, "timestamp": "2026-01-19T15:01:19.664549"}
{"image_index": 278, "image_name": "000000030213.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030213.jpg", "image_width": 640, "image_height": 449, "image_resolution": "640x449", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1904.99, "latencies_ms": [1904.99], "images_per_second": 0.525, "prompt_tokens": 1109, "response_tokens_est": 31, "n_tiles": 1, "output_text": " The room is well-lit with natural light, and the walls are covered in green wallpaper. The floor is covered in a green and yellow floral carpet.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 27543.2, "ram_available_mb": 98228.9, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 27557.3, "ram_available_mb": 98214.9, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.09, "peak": 39.77, "min": 15.77}, "VIN": {"avg": 68.85, "peak": 94.43, "min": 30.25}, "VIN_SYS_5V0": {"avg": 14.7, "peak": 15.95, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.94, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 29.09, "energy_joules_est": 55.43, "sample_count": 19, "duration_seconds": 1.905}, "timestamp": "2026-01-19T15:01:21.641287"}
{"image_index": 279, "image_name": "000000030494.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030494.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1737.516, "latencies_ms": [1737.516], "images_per_second": 0.576, "prompt_tokens": 1099, "response_tokens_est": 21, "n_tiles": 1, "output_text": " A black and white dog is walking on a dirt path with a brown and white toy in its mouth.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 27557.3, "ram_available_mb": 98214.9, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 27557.9, "ram_available_mb": 98214.3, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.24, "peak": 40.57, "min": 17.35}, "VIN": {"avg": 69.82, "peak": 105.63, "min": 29.94}, "VIN_SYS_5V0": {"avg": 14.72, "peak": 16.05, "min": 12.15}, "VDD_CPU_SOC_MSS": {"avg": 15.96, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 30.24, "energy_joules_est": 52.57, "sample_count": 17, "duration_seconds": 1.738}, "timestamp": "2026-01-19T15:01:23.419264"}
{"image_index": 279, "image_name": "000000030494.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030494.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1938.308, "latencies_ms": [1938.308], "images_per_second": 0.516, "prompt_tokens": 1113, "response_tokens_est": 34, "n_tiles": 1, "output_text": " dog: 1, tree: 1, leaves: 1, shadow: 1, ground: 1, bark: 1, grass: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27557.9, "ram_available_mb": 98214.3, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 27561.9, "ram_available_mb": 98210.2, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.88, "peak": 40.18, "min": 17.74}, "VIN": {"avg": 74.06, "peak": 126.81, "min": 29.64}, "VIN_SYS_5V0": {"avg": 14.94, "peak": 16.05, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 16.16, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 29.88, "energy_joules_est": 57.93, "sample_count": 19, "duration_seconds": 1.939}, "timestamp": "2026-01-19T15:01:25.405288"}
{"image_index": 279, "image_name": "000000030494.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030494.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2139.563, "latencies_ms": [2139.563], "images_per_second": 0.467, "prompt_tokens": 1117, "response_tokens_est": 41, "n_tiles": 1, "output_text": " The dog is in the foreground, walking towards the right side of the image. The tree is in the background, to the left of the dog. The dog is closer to the camera than the tree.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27561.9, "ram_available_mb": 98210.2, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 27573.4, "ram_available_mb": 98198.7, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.76, "peak": 40.56, "min": 17.35}, "VIN": {"avg": 67.25, "peak": 99.0, "min": 29.7}, "VIN_SYS_5V0": {"avg": 14.95, "peak": 16.15, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 16.12, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 28.76, "energy_joules_est": 61.55, "sample_count": 21, "duration_seconds": 2.14}, "timestamp": "2026-01-19T15:01:27.611280"}
{"image_index": 279, "image_name": "000000030494.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030494.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1655.295, "latencies_ms": [1655.295], "images_per_second": 0.604, "prompt_tokens": 1111, "response_tokens_est": 22, "n_tiles": 1, "output_text": " A black and white dog is walking on a dirt path in a wooded area, sniffing around a tree.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27573.4, "ram_available_mb": 98198.7, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 27589.9, "ram_available_mb": 98182.3, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.83, "peak": 40.18, "min": 16.56}, "VIN": {"avg": 69.46, "peak": 121.65, "min": 30.2}, "VIN_SYS_5V0": {"avg": 14.91, "peak": 16.05, "min": 12.15}, "VDD_CPU_SOC_MSS": {"avg": 15.97, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 30.83, "energy_joules_est": 51.05, "sample_count": 16, "duration_seconds": 1.656}, "timestamp": "2026-01-19T15:01:29.278563"}
{"image_index": 279, "image_name": "000000030494.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030494.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1520.544, "latencies_ms": [1520.544], "images_per_second": 0.658, "prompt_tokens": 1109, "response_tokens_est": 18, "n_tiles": 1, "output_text": " The dog is black and white, the leaves are brown, and the tree is green.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27589.9, "ram_available_mb": 98182.3, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 27587.3, "ram_available_mb": 98184.8, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.56, "peak": 40.57, "min": 20.49}, "VIN": {"avg": 72.73, "peak": 129.6, "min": 30.12}, "VIN_SYS_5V0": {"avg": 15.31, "peak": 16.26, "min": 13.29}, "VDD_CPU_SOC_MSS": {"avg": 16.35, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 32.56, "energy_joules_est": 49.52, "sample_count": 15, "duration_seconds": 1.521}, "timestamp": "2026-01-19T15:01:30.852459"}
{"image_index": 280, "image_name": "000000030504.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030504.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1656.67, "latencies_ms": [1656.67], "images_per_second": 0.604, "prompt_tokens": 1100, "response_tokens_est": 23, "n_tiles": 1, "output_text": " A person is skiing down a snowy hill, wearing a white helmet, a red backpack, and green skis.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 27587.3, "ram_available_mb": 98184.8, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 27585.5, "ram_available_mb": 98186.7, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.66, "peak": 40.56, "min": 20.1}, "VIN": {"avg": 66.62, "peak": 104.65, "min": 33.05}, "VIN_SYS_5V0": {"avg": 15.11, "peak": 16.26, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 16.24, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 31.66, "energy_joules_est": 52.48, "sample_count": 16, "duration_seconds": 1.658}, "timestamp": "2026-01-19T15:01:32.524071"}
{"image_index": 280, "image_name": "000000030504.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030504.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2574.248, "latencies_ms": [2574.248], "images_per_second": 0.388, "prompt_tokens": 1114, "response_tokens_est": 59, "n_tiles": 1, "output_text": " 1. skis: 2\n2. ski poles: 2\n3. backpack: 1\n4. helmet: 1\n5. skier: 1\n6. snow: 1\n7. trees: 1\n8. sky: 1", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 27585.5, "ram_available_mb": 98186.7, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 6.4, "ram_used_mb": 27560.0, "ram_available_mb": 98212.2, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.97, "peak": 40.16, "min": 19.31}, "VIN": {"avg": 70.87, "peak": 105.96, "min": 30.68}, "VIN_SYS_5V0": {"avg": 15.3, "peak": 16.26, "min": 13.39}, "VDD_CPU_SOC_MSS": {"avg": 16.41, "peak": 17.71, "min": 14.57}}, "power_watts_avg": 27.97, "energy_joules_est": 72.01, "sample_count": 25, "duration_seconds": 2.575}, "timestamp": "2026-01-19T15:01:35.119226"}
{"image_index": 280, "image_name": "000000030504.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030504.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2190.614, "latencies_ms": [2190.614], "images_per_second": 0.456, "prompt_tokens": 1118, "response_tokens_est": 44, "n_tiles": 1, "output_text": " The skier is positioned in the foreground of the image, with the ski trail and trees in the background. The skier is moving towards the right side of the image, with the ski trail extending into the distance.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27560.0, "ram_available_mb": 98212.2, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 27550.8, "ram_available_mb": 98221.4, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.28, "peak": 39.78, "min": 16.95}, "VIN": {"avg": 74.47, "peak": 124.22, "min": 28.38}, "VIN_SYS_5V0": {"avg": 15.2, "peak": 16.26, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 16.2, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 28.28, "energy_joules_est": 61.96, "sample_count": 22, "duration_seconds": 2.191}, "timestamp": "2026-01-19T15:01:37.406649"}
{"image_index": 280, "image_name": "000000030504.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030504.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1437.521, "latencies_ms": [1437.521], "images_per_second": 0.696, "prompt_tokens": 1112, "response_tokens_est": 14, "n_tiles": 1, "output_text": " A person is skiing down a snowy hill with trees in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27550.8, "ram_available_mb": 98221.4, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 27551.0, "ram_available_mb": 98221.2, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.07, "peak": 39.78, "min": 15.77}, "VIN": {"avg": 79.27, "peak": 127.96, "min": 30.24}, "VIN_SYS_5V0": {"avg": 14.85, "peak": 16.15, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.92, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 31.07, "energy_joules_est": 44.69, "sample_count": 14, "duration_seconds": 1.438}, "timestamp": "2026-01-19T15:01:38.871593"}
{"image_index": 280, "image_name": "000000030504.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030504.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1966.314, "latencies_ms": [1966.314], "images_per_second": 0.509, "prompt_tokens": 1110, "response_tokens_est": 35, "n_tiles": 1, "output_text": " The image features a person skiing down a snowy hill, wearing a white helmet and carrying a red backpack. The sky is clear and blue, and the snow is pristine white.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27551.0, "ram_available_mb": 98221.2, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 27551.3, "ram_available_mb": 98220.9, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.69, "peak": 40.57, "min": 21.28}, "VIN": {"avg": 69.71, "peak": 113.33, "min": 29.66}, "VIN_SYS_5V0": {"avg": 15.21, "peak": 16.26, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 16.31, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 30.69, "energy_joules_est": 60.36, "sample_count": 19, "duration_seconds": 1.967}, "timestamp": "2026-01-19T15:01:40.852301"}
{"image_index": 281, "image_name": "000000030675.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030675.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1784.402, "latencies_ms": [1784.402], "images_per_second": 0.56, "prompt_tokens": 1099, "response_tokens_est": 28, "n_tiles": 1, "output_text": " A large orange and black train with the number 6309 on the front is traveling down a track with trees in the background.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 27551.3, "ram_available_mb": 98220.9, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 27552.2, "ram_available_mb": 98220.0, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.29, "peak": 40.57, "min": 18.13}, "VIN": {"avg": 70.71, "peak": 122.13, "min": 28.74}, "VIN_SYS_5V0": {"avg": 15.12, "peak": 16.15, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 16.36, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 30.29, "energy_joules_est": 54.06, "sample_count": 18, "duration_seconds": 1.785}, "timestamp": "2026-01-19T15:01:42.731268"}
{"image_index": 281, "image_name": "000000030675.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030675.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2640.781, "latencies_ms": [2640.781], "images_per_second": 0.379, "prompt_tokens": 1113, "response_tokens_est": 60, "n_tiles": 1, "output_text": " 1. train: 1\n2. trees: 1\n3. sky: 1\n4. gravel: 1\n5. fence: 1\n6. train number: 1\n7. train engine: 1\n8. train caboose: 1", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 27552.2, "ram_available_mb": 98220.0, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 27558.6, "ram_available_mb": 98213.5, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.05, "peak": 40.57, "min": 16.95}, "VIN": {"avg": 60.58, "peak": 119.36, "min": 28.93}, "VIN_SYS_5V0": {"avg": 14.88, "peak": 16.05, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.91, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 27.05, "energy_joules_est": 71.45, "sample_count": 26, "duration_seconds": 2.641}, "timestamp": "2026-01-19T15:01:45.436406"}
{"image_index": 281, "image_name": "000000030675.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030675.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1943.343, "latencies_ms": [1943.343], "images_per_second": 0.515, "prompt_tokens": 1117, "response_tokens_est": 33, "n_tiles": 1, "output_text": " The main object, the train, is positioned in the foreground, moving from left to right. The trees are located in the background, far away from the camera.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27558.6, "ram_available_mb": 98213.5, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 27572.4, "ram_available_mb": 98199.8, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.38, "peak": 40.18, "min": 15.77}, "VIN": {"avg": 70.3, "peak": 119.74, "min": 29.02}, "VIN_SYS_5V0": {"avg": 14.98, "peak": 16.05, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 16.0, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 29.38, "energy_joules_est": 57.11, "sample_count": 19, "duration_seconds": 1.944}, "timestamp": "2026-01-19T15:01:47.405720"}
{"image_index": 281, "image_name": "000000030675.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030675.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1639.617, "latencies_ms": [1639.617], "images_per_second": 0.61, "prompt_tokens": 1111, "response_tokens_est": 21, "n_tiles": 1, "output_text": " A large orange and black train is traveling down a track in a rural area with trees in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27572.4, "ram_available_mb": 98199.8, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 27574.2, "ram_available_mb": 98198.0, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.15, "peak": 40.56, "min": 17.73}, "VIN": {"avg": 68.22, "peak": 97.95, "min": 29.93}, "VIN_SYS_5V0": {"avg": 15.13, "peak": 16.15, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 16.22, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 31.15, "energy_joules_est": 51.09, "sample_count": 16, "duration_seconds": 1.64}, "timestamp": "2026-01-19T15:01:49.075099"}
{"image_index": 281, "image_name": "000000030675.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030675.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2404.101, "latencies_ms": [2404.101], "images_per_second": 0.416, "prompt_tokens": 1109, "response_tokens_est": 51, "n_tiles": 1, "output_text": " The locomotive is painted in vibrant shades of orange and black, with the number 6309 prominently displayed on the front. The sky is a clear blue, and the trees in the background are bare, suggesting it might be late autumn or winter.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27574.2, "ram_available_mb": 98198.0, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 27576.3, "ram_available_mb": 98195.9, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.1, "peak": 40.56, "min": 17.74}, "VIN": {"avg": 70.95, "peak": 108.61, "min": 28.54}, "VIN_SYS_5V0": {"avg": 15.02, "peak": 16.15, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 16.16, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 28.1, "energy_joules_est": 67.57, "sample_count": 24, "duration_seconds": 2.405}, "timestamp": "2026-01-19T15:01:51.575182"}
{"image_index": 282, "image_name": "000000030785.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030785.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1931.128, "latencies_ms": [1931.128], "images_per_second": 0.518, "prompt_tokens": 1099, "response_tokens_est": 33, "n_tiles": 1, "output_text": " The image shows a wooden table with a plate containing a slice of bread topped with guacamole, a bowl of broccoli, and a serving of white dip.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 27576.3, "ram_available_mb": 98195.9, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 27587.5, "ram_available_mb": 98184.6, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.11, "peak": 40.18, "min": 14.99}, "VIN": {"avg": 77.42, "peak": 123.23, "min": 27.27}, "VIN_SYS_5V0": {"avg": 14.79, "peak": 15.95, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.91, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 29.11, "energy_joules_est": 56.24, "sample_count": 19, "duration_seconds": 1.932}, "timestamp": "2026-01-19T15:01:53.566626"}
{"image_index": 282, "image_name": "000000030785.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030785.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2532.127, "latencies_ms": [2532.127], "images_per_second": 0.395, "prompt_tokens": 1113, "response_tokens_est": 56, "n_tiles": 1, "output_text": " 1. plate: 1\n2. bread: 2\n3. broccoli: 1\n4. bowl: 1\n5. cream: 1\n6. avocado: 1\n7. table: 1\n8. text: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27587.5, "ram_available_mb": 98184.6, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 27588.1, "ram_available_mb": 98184.1, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.42, "peak": 40.56, "min": 17.35}, "VIN": {"avg": 70.42, "peak": 102.48, "min": 30.81}, "VIN_SYS_5V0": {"avg": 14.97, "peak": 16.05, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 16.11, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 27.42, "energy_joules_est": 69.45, "sample_count": 25, "duration_seconds": 2.533}, "timestamp": "2026-01-19T15:01:56.173922"}
{"image_index": 282, "image_name": "000000030785.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030785.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2069.984, "latencies_ms": [2069.984], "images_per_second": 0.483, "prompt_tokens": 1117, "response_tokens_est": 38, "n_tiles": 1, "output_text": " The plate is in the foreground, with the bread, avocado, and broccoli in the background. The broccoli is near the plate, while the bread is on the left side of the plate.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27588.1, "ram_available_mb": 98184.1, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 27592.0, "ram_available_mb": 98180.2, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.99, "peak": 40.16, "min": 15.77}, "VIN": {"avg": 77.54, "peak": 124.15, "min": 32.53}, "VIN_SYS_5V0": {"avg": 14.94, "peak": 16.05, "min": 12.15}, "VDD_CPU_SOC_MSS": {"avg": 16.04, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 28.99, "energy_joules_est": 60.03, "sample_count": 20, "duration_seconds": 2.071}, "timestamp": "2026-01-19T15:01:58.249175"}
{"image_index": 282, "image_name": "000000030785.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030785.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1311.58, "latencies_ms": [1311.58], "images_per_second": 0.762, "prompt_tokens": 1111, "response_tokens_est": 10, "n_tiles": 1, "output_text": " A plate of food is on a wooden table.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27592.0, "ram_available_mb": 98180.2, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 6.9, "ram_used_mb": 27563.2, "ram_available_mb": 98209.0, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.84, "peak": 39.77, "min": 18.91}, "VIN": {"avg": 72.29, "peak": 85.97, "min": 30.04}, "VIN_SYS_5V0": {"avg": 15.39, "peak": 16.26, "min": 13.49}, "VDD_CPU_SOC_MSS": {"avg": 16.77, "peak": 17.71, "min": 14.57}}, "power_watts_avg": 32.84, "energy_joules_est": 43.08, "sample_count": 13, "duration_seconds": 1.312}, "timestamp": "2026-01-19T15:01:59.609408"}
{"image_index": 282, "image_name": "000000030785.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030785.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2151.222, "latencies_ms": [2151.222], "images_per_second": 0.465, "prompt_tokens": 1109, "response_tokens_est": 40, "n_tiles": 1, "output_text": " The image features a wooden table with a plate of food on it, including a bowl of broccoli and a slice of bread with avocado on it. The lighting is natural, and the colors are vibrant.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27563.2, "ram_available_mb": 98209.0, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 27572.1, "ram_available_mb": 98200.1, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.98, "peak": 40.97, "min": 19.7}, "VIN": {"avg": 66.21, "peak": 93.76, "min": 28.89}, "VIN_SYS_5V0": {"avg": 15.01, "peak": 16.05, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 16.09, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 29.98, "energy_joules_est": 64.5, "sample_count": 21, "duration_seconds": 2.152}, "timestamp": "2026-01-19T15:02:01.795483"}
{"image_index": 283, "image_name": "000000030828.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030828.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1485.713, "latencies_ms": [1485.713], "images_per_second": 0.673, "prompt_tokens": 1099, "response_tokens_est": 16, "n_tiles": 1, "output_text": " A person is sleeping on a bench with a blanket and a bag on it.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 27572.1, "ram_available_mb": 98200.1, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 27553.9, "ram_available_mb": 98218.3, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.31, "peak": 40.56, "min": 16.95}, "VIN": {"avg": 72.56, "peak": 114.22, "min": 30.26}, "VIN_SYS_5V0": {"avg": 14.97, "peak": 16.05, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 16.09, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 31.31, "energy_joules_est": 46.54, "sample_count": 15, "duration_seconds": 1.487}, "timestamp": "2026-01-19T15:02:03.369564"}
{"image_index": 283, "image_name": "000000030828.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030828.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2102.824, "latencies_ms": [2102.824], "images_per_second": 0.476, "prompt_tokens": 1113, "response_tokens_est": 39, "n_tiles": 1, "output_text": " bench: 1\nbag: 1\npurse: 1\nparking meter: 2\nstreetlamp: 2\ngrass: 1\ntree: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27553.9, "ram_available_mb": 98218.3, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 27557.6, "ram_available_mb": 98214.6, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.27, "peak": 40.57, "min": 18.91}, "VIN": {"avg": 70.67, "peak": 112.74, "min": 28.68}, "VIN_SYS_5V0": {"avg": 14.88, "peak": 16.05, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 16.01, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 29.27, "energy_joules_est": 61.56, "sample_count": 21, "duration_seconds": 2.103}, "timestamp": "2026-01-19T15:02:05.552263"}
{"image_index": 283, "image_name": "000000030828.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030828.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2279.874, "latencies_ms": [2279.874], "images_per_second": 0.439, "prompt_tokens": 1117, "response_tokens_est": 44, "n_tiles": 1, "output_text": " The bench is located in the foreground of the image, with the parking meter and the person sleeping on it in the background. The person is lying on the bench, which is positioned to the left of the parking meter.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27557.6, "ram_available_mb": 98214.6, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 27560.5, "ram_available_mb": 98211.7, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.06, "peak": 40.16, "min": 16.16}, "VIN": {"avg": 72.61, "peak": 123.51, "min": 30.9}, "VIN_SYS_5V0": {"avg": 14.83, "peak": 16.05, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.95, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 28.06, "energy_joules_est": 63.99, "sample_count": 22, "duration_seconds": 2.28}, "timestamp": "2026-01-19T15:02:07.846025"}
{"image_index": 283, "image_name": "000000030828.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030828.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1364.719, "latencies_ms": [1364.719], "images_per_second": 0.733, "prompt_tokens": 1111, "response_tokens_est": 11, "n_tiles": 1, "output_text": " A person is sleeping on a bench in a park.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 27560.5, "ram_available_mb": 98211.7, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 27573.8, "ram_available_mb": 98198.4, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.77, "peak": 40.56, "min": 16.95}, "VIN": {"avg": 75.17, "peak": 107.74, "min": 27.59}, "VIN_SYS_5V0": {"avg": 15.07, "peak": 16.15, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 16.25, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 31.77, "energy_joules_est": 43.37, "sample_count": 14, "duration_seconds": 1.365}, "timestamp": "2026-01-19T15:02:09.306925"}
{"image_index": 283, "image_name": "000000030828.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030828.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1897.532, "latencies_ms": [1897.532], "images_per_second": 0.527, "prompt_tokens": 1109, "response_tokens_est": 31, "n_tiles": 1, "output_text": " The bench is made of wood and is covered with an orange blanket. The scene is bathed in natural light, and the grass is a vibrant green.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27573.8, "ram_available_mb": 98198.4, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 27573.6, "ram_available_mb": 98198.6, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.36, "peak": 40.57, "min": 20.1}, "VIN": {"avg": 70.95, "peak": 106.15, "min": 27.34}, "VIN_SYS_5V0": {"avg": 14.77, "peak": 16.05, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.93, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 30.36, "energy_joules_est": 57.62, "sample_count": 19, "duration_seconds": 1.898}, "timestamp": "2026-01-19T15:02:11.287276"}
{"image_index": 284, "image_name": "000000031050.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031050.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1723.621, "latencies_ms": [1723.621], "images_per_second": 0.58, "prompt_tokens": 1100, "response_tokens_est": 25, "n_tiles": 1, "output_text": " A tall, colorful vase with a mix of dried flowers and plants sits on a white shelf in front of a brown wall.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 27573.6, "ram_available_mb": 98198.6, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 27581.3, "ram_available_mb": 98190.9, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.22, "peak": 40.16, "min": 16.95}, "VIN": {"avg": 75.22, "peak": 125.64, "min": 30.05}, "VIN_SYS_5V0": {"avg": 14.84, "peak": 16.05, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 16.0, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 30.22, "energy_joules_est": 52.11, "sample_count": 17, "duration_seconds": 1.724}, "timestamp": "2026-01-19T15:02:13.064103"}
{"image_index": 284, "image_name": "000000031050.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031050.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2086.46, "latencies_ms": [2086.46], "images_per_second": 0.479, "prompt_tokens": 1114, "response_tokens_est": 39, "n_tiles": 1, "output_text": " vase: 1, pot: 1, wall: 1, wall art: 1, plant: 1, potted plant: 1, vase on table: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27581.3, "ram_available_mb": 98190.9, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 27582.3, "ram_available_mb": 98189.8, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.95, "peak": 40.57, "min": 18.53}, "VIN": {"avg": 71.65, "peak": 105.89, "min": 30.13}, "VIN_SYS_5V0": {"avg": 15.02, "peak": 16.15, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 16.07, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 28.95, "energy_joules_est": 60.41, "sample_count": 21, "duration_seconds": 2.087}, "timestamp": "2026-01-19T15:02:15.248652"}
{"image_index": 284, "image_name": "000000031050.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031050.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2054.89, "latencies_ms": [2054.89], "images_per_second": 0.487, "prompt_tokens": 1118, "response_tokens_est": 37, "n_tiles": 1, "output_text": " The vase is positioned in the foreground, with the wall and other objects in the background. The vase is placed on a white shelf, which is situated to the left of the wall.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27582.3, "ram_available_mb": 98189.8, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 27594.4, "ram_available_mb": 98177.8, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.74, "peak": 40.18, "min": 15.77}, "VIN": {"avg": 69.78, "peak": 119.92, "min": 29.59}, "VIN_SYS_5V0": {"avg": 14.86, "peak": 16.15, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 16.0, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 28.74, "energy_joules_est": 59.07, "sample_count": 20, "duration_seconds": 2.055}, "timestamp": "2026-01-19T15:02:17.332571"}
{"image_index": 284, "image_name": "000000031050.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031050.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1497.302, "latencies_ms": [1497.302], "images_per_second": 0.668, "prompt_tokens": 1112, "response_tokens_est": 17, "n_tiles": 1, "output_text": " A vase with dried flowers sits on a white shelf in a room with brown walls.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27594.4, "ram_available_mb": 98177.8, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 6.9, "ram_used_mb": 27543.3, "ram_available_mb": 98228.9, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.25, "peak": 39.77, "min": 17.35}, "VIN": {"avg": 73.36, "peak": 105.99, "min": 28.94}, "VIN_SYS_5V0": {"avg": 15.19, "peak": 16.36, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 16.61, "peak": 18.11, "min": 14.18}}, "power_watts_avg": 31.25, "energy_joules_est": 46.8, "sample_count": 15, "duration_seconds": 1.498}, "timestamp": "2026-01-19T15:02:18.902761"}
{"image_index": 284, "image_name": "000000031050.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031050.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1940.099, "latencies_ms": [1940.099], "images_per_second": 0.515, "prompt_tokens": 1110, "response_tokens_est": 33, "n_tiles": 1, "output_text": " The vase is made of ceramic and has a variety of colors, including blue, green, and brown. The lighting is bright and natural, coming from the window.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27543.3, "ram_available_mb": 98228.9, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 27543.6, "ram_available_mb": 98228.6, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.98, "peak": 40.56, "min": 19.71}, "VIN": {"avg": 72.14, "peak": 126.22, "min": 28.92}, "VIN_SYS_5V0": {"avg": 14.95, "peak": 16.15, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 16.14, "peak": 17.71, "min": 13.79}}, "power_watts_avg": 29.98, "energy_joules_est": 58.19, "sample_count": 19, "duration_seconds": 1.941}, "timestamp": "2026-01-19T15:02:20.885527"}
{"image_index": 285, "image_name": "000000031093.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031093.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1655.565, "latencies_ms": [1655.565], "images_per_second": 0.604, "prompt_tokens": 1099, "response_tokens_est": 22, "n_tiles": 1, "output_text": " A skateboarder wearing a helmet and knee pads is performing a trick on a ramp in a skate park.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27543.6, "ram_available_mb": 98228.6, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 27544.2, "ram_available_mb": 98228.0, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.12, "peak": 40.57, "min": 17.35}, "VIN": {"avg": 72.63, "peak": 123.68, "min": 33.72}, "VIN_SYS_5V0": {"avg": 15.01, "peak": 16.05, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 16.14, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 31.12, "energy_joules_est": 51.55, "sample_count": 16, "duration_seconds": 1.656}, "timestamp": "2026-01-19T15:02:22.555795"}
{"image_index": 285, "image_name": "000000031093.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031093.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2843.386, "latencies_ms": [2843.386], "images_per_second": 0.352, "prompt_tokens": 1113, "response_tokens_est": 69, "n_tiles": 1, "output_text": " 1. skateboard: 1\n2. helmet: 1\n3. knee pads: 1\n4. skateboarder: 1\n5. skateboard wheels: 2\n6. skateboard deck: 1\n7. skateboard trucks: 1\n8. skateboard trucks bolts: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 27544.2, "ram_available_mb": 98228.0, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 27551.6, "ram_available_mb": 98220.5, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.11, "peak": 40.95, "min": 18.13}, "VIN": {"avg": 68.16, "peak": 125.26, "min": 29.28}, "VIN_SYS_5V0": {"avg": 15.12, "peak": 16.26, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 16.18, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 27.11, "energy_joules_est": 77.1, "sample_count": 28, "duration_seconds": 2.844}, "timestamp": "2026-01-19T15:02:25.466980"}
{"image_index": 285, "image_name": "000000031093.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031093.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2077.405, "latencies_ms": [2077.405], "images_per_second": 0.481, "prompt_tokens": 1117, "response_tokens_est": 38, "n_tiles": 1, "output_text": " The skateboarder is in the foreground, performing a trick in the skatepark. The ramp is in the background, and the skateboarder is positioned to the left of the ramp.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 27551.6, "ram_available_mb": 98220.5, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 27551.7, "ram_available_mb": 98220.5, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.82, "peak": 40.16, "min": 15.77}, "VIN": {"avg": 76.86, "peak": 126.61, "min": 35.2}, "VIN_SYS_5V0": {"avg": 14.93, "peak": 16.05, "min": 12.15}, "VDD_CPU_SOC_MSS": {"avg": 16.0, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 28.82, "energy_joules_est": 59.88, "sample_count": 20, "duration_seconds": 2.078}, "timestamp": "2026-01-19T15:02:27.550811"}
{"image_index": 285, "image_name": "000000031093.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031093.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1606.691, "latencies_ms": [1606.691], "images_per_second": 0.622, "prompt_tokens": 1111, "response_tokens_est": 21, "n_tiles": 1, "output_text": " A skateboarder wearing a helmet and knee pads is performing a trick on a concrete skateboard ramp.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27551.7, "ram_available_mb": 98220.5, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 27562.9, "ram_available_mb": 98209.3, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.32, "peak": 40.56, "min": 18.14}, "VIN": {"avg": 73.95, "peak": 121.04, "min": 30.05}, "VIN_SYS_5V0": {"avg": 15.25, "peak": 16.26, "min": 13.29}, "VDD_CPU_SOC_MSS": {"avg": 16.39, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 31.32, "energy_joules_est": 50.34, "sample_count": 16, "duration_seconds": 1.607}, "timestamp": "2026-01-19T15:02:29.221020"}
{"image_index": 285, "image_name": "000000031093.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031093.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1631.697, "latencies_ms": [1631.697], "images_per_second": 0.613, "prompt_tokens": 1109, "response_tokens_est": 21, "n_tiles": 1, "output_text": " The skateboarder is wearing a helmet and knee pads, and the skate park is made of concrete.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27562.9, "ram_available_mb": 98209.3, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 27562.7, "ram_available_mb": 98209.5, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.49, "peak": 40.56, "min": 18.91}, "VIN": {"avg": 71.28, "peak": 124.06, "min": 29.91}, "VIN_SYS_5V0": {"avg": 14.93, "peak": 16.05, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 16.09, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 31.49, "energy_joules_est": 51.4, "sample_count": 16, "duration_seconds": 1.632}, "timestamp": "2026-01-19T15:02:30.891453"}
{"image_index": 286, "image_name": "000000031118.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031118.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 3163.017, "latencies_ms": [3163.017], "images_per_second": 0.316, "prompt_tokens": 1099, "response_tokens_est": 81, "n_tiles": 1, "output_text": " The image captures a vibrant night scene in a bustling city square, where a majestic clock tower stands tall, adorned with a festive blue and white Christmas tree, and is beautifully illuminated by strings of lights. The square is teeming with people, adding a lively atmosphere to the scene. In the background, a large building with a red and white sign can be seen, adding to the urban charm of the setting.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 27562.7, "ram_available_mb": 98209.5, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 27568.7, "ram_available_mb": 98203.5, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.23, "peak": 40.95, "min": 18.13}, "VIN": {"avg": 69.76, "peak": 117.34, "min": 29.03}, "VIN_SYS_5V0": {"avg": 15.03, "peak": 16.05, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 16.04, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 26.23, "energy_joules_est": 82.98, "sample_count": 31, "duration_seconds": 3.164}, "timestamp": "2026-01-19T15:02:34.114601"}
{"image_index": 286, "image_name": "000000031118.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031118.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1968.641, "latencies_ms": [1968.641], "images_per_second": 0.508, "prompt_tokens": 1113, "response_tokens_est": 34, "n_tiles": 1, "output_text": " clock: 1, tower: 1, tree: 1, building: 1, lights: 1, people: 1, street: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27568.7, "ram_available_mb": 98203.5, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 27569.6, "ram_available_mb": 98202.5, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.34, "peak": 40.18, "min": 15.77}, "VIN": {"avg": 77.65, "peak": 128.01, "min": 32.89}, "VIN_SYS_5V0": {"avg": 15.01, "peak": 16.05, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 16.08, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 29.34, "energy_joules_est": 57.77, "sample_count": 19, "duration_seconds": 1.969}, "timestamp": "2026-01-19T15:02:36.088757"}
{"image_index": 286, "image_name": "000000031118.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031118.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2613.283, "latencies_ms": [2613.283], "images_per_second": 0.383, "prompt_tokens": 1117, "response_tokens_est": 60, "n_tiles": 1, "output_text": " The clock tower is positioned in the center of the image, with the Christmas tree to its right. The street is in the foreground, with the buildings and shops in the background. The clock tower is the main focal point of the image, with the Christmas tree adding a festive touch to the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27569.6, "ram_available_mb": 98202.5, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 27576.2, "ram_available_mb": 98196.0, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.35, "peak": 40.57, "min": 17.73}, "VIN": {"avg": 68.72, "peak": 121.15, "min": 27.38}, "VIN_SYS_5V0": {"avg": 15.16, "peak": 16.26, "min": 13.29}, "VDD_CPU_SOC_MSS": {"avg": 16.23, "peak": 17.71, "min": 14.18}}, "power_watts_avg": 27.35, "energy_joules_est": 71.48, "sample_count": 26, "duration_seconds": 2.614}, "timestamp": "2026-01-19T15:02:38.791497"}
{"image_index": 286, "image_name": "000000031118.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031118.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2705.893, "latencies_ms": [2705.893], "images_per_second": 0.37, "prompt_tokens": 1111, "response_tokens_est": 62, "n_tiles": 1, "output_text": " The image captures a vibrant night scene in a bustling city square, where a majestic clock tower stands tall, adorned with twinkling lights that add a magical touch to the urban landscape. The square is alive with people, some walking, others gathered around a large, snow-covered Christmas tree, creating a festive atmosphere.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 27576.2, "ram_available_mb": 98196.0, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 6.3, "ram_used_mb": 27541.5, "ram_available_mb": 98230.7, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.66, "peak": 39.78, "min": 15.38}, "VIN": {"avg": 71.72, "peak": 114.65, "min": 34.58}, "VIN_SYS_5V0": {"avg": 14.9, "peak": 16.05, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 16.13, "peak": 18.11, "min": 13.0}}, "power_watts_avg": 26.66, "energy_joules_est": 72.15, "sample_count": 26, "duration_seconds": 2.706}, "timestamp": "2026-01-19T15:02:41.503557"}
{"image_index": 286, "image_name": "000000031118.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031118.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2535.125, "latencies_ms": [2535.125], "images_per_second": 0.394, "prompt_tokens": 1109, "response_tokens_est": 57, "n_tiles": 1, "output_text": " The image features a nighttime scene with a prominent clock tower adorned with lights, a snow-covered Christmas tree, and a bustling street lined with shops and pedestrians. The sky is dark, and the street is wet, reflecting the lights and creating a glistening effect on the pavement.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 27541.5, "ram_available_mb": 98230.7, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 27538.9, "ram_available_mb": 98233.3, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.44, "peak": 40.56, "min": 16.95}, "VIN": {"avg": 68.81, "peak": 120.83, "min": 29.08}, "VIN_SYS_5V0": {"avg": 15.15, "peak": 16.15, "min": 13.29}, "VDD_CPU_SOC_MSS": {"avg": 16.27, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 27.44, "energy_joules_est": 69.57, "sample_count": 25, "duration_seconds": 2.535}, "timestamp": "2026-01-19T15:02:44.097224"}
{"image_index": 287, "image_name": "000000031217.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031217.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1729.286, "latencies_ms": [1729.286], "images_per_second": 0.578, "prompt_tokens": 1099, "response_tokens_est": 25, "n_tiles": 1, "output_text": " A young man wearing a blue shirt and black shorts is playing tennis on a court with a yellow and black tennis racket.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27538.9, "ram_available_mb": 98233.3, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 27540.8, "ram_available_mb": 98231.4, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.13, "peak": 40.57, "min": 15.77}, "VIN": {"avg": 79.41, "peak": 123.74, "min": 29.25}, "VIN_SYS_5V0": {"avg": 14.9, "peak": 16.05, "min": 12.25}, "VDD_CPU_SOC_MSS": {"avg": 16.05, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 30.13, "energy_joules_est": 52.13, "sample_count": 17, "duration_seconds": 1.73}, "timestamp": "2026-01-19T15:02:45.880096"}
{"image_index": 287, "image_name": "000000031217.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031217.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2682.661, "latencies_ms": [2682.661], "images_per_second": 0.373, "prompt_tokens": 1113, "response_tokens_est": 62, "n_tiles": 1, "output_text": " 1. person: 1\n2. tennis racket: 1\n3. tennis ball: 1\n4. chain link fence: 1\n5. tennis court: 1\n6. trees: 1\n7. grass: 1\n8. logo: 1", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 27540.8, "ram_available_mb": 98231.4, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 27553.1, "ram_available_mb": 98219.1, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.37, "peak": 40.57, "min": 18.52}, "VIN": {"avg": 73.26, "peak": 127.94, "min": 30.14}, "VIN_SYS_5V0": {"avg": 15.0, "peak": 16.15, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 16.11, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 27.37, "energy_joules_est": 73.43, "sample_count": 26, "duration_seconds": 2.683}, "timestamp": "2026-01-19T15:02:48.588001"}
{"image_index": 287, "image_name": "000000031217.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031217.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2349.54, "latencies_ms": [2349.54], "images_per_second": 0.426, "prompt_tokens": 1117, "response_tokens_est": 49, "n_tiles": 1, "output_text": " The tennis player is in the foreground, holding a tennis racket and preparing to hit the ball. The tennis ball is in the middle ground, suspended in the air. The tennis court is in the background, enclosed by a chain link fence.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 27553.1, "ram_available_mb": 98219.1, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 27555.4, "ram_available_mb": 98216.8, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.95, "peak": 40.56, "min": 16.56}, "VIN": {"avg": 67.55, "peak": 105.34, "min": 29.07}, "VIN_SYS_5V0": {"avg": 15.02, "peak": 16.15, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 16.16, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 27.95, "energy_joules_est": 65.68, "sample_count": 23, "duration_seconds": 2.35}, "timestamp": "2026-01-19T15:02:50.975912"}
{"image_index": 287, "image_name": "000000031217.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031217.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1515.304, "latencies_ms": [1515.304], "images_per_second": 0.66, "prompt_tokens": 1111, "response_tokens_est": 17, "n_tiles": 1, "output_text": " A young man is playing tennis on a court with a green fence in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27555.4, "ram_available_mb": 98216.8, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 6.4, "ram_used_mb": 27545.9, "ram_available_mb": 98226.3, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.31, "peak": 40.57, "min": 16.56}, "VIN": {"avg": 71.82, "peak": 109.75, "min": 27.85}, "VIN_SYS_5V0": {"avg": 15.11, "peak": 16.15, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 16.48, "peak": 17.71, "min": 13.79}}, "power_watts_avg": 31.31, "energy_joules_est": 47.45, "sample_count": 15, "duration_seconds": 1.516}, "timestamp": "2026-01-19T15:02:52.539533"}
{"image_index": 287, "image_name": "000000031217.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031217.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2456.639, "latencies_ms": [2456.639], "images_per_second": 0.407, "prompt_tokens": 1109, "response_tokens_est": 53, "n_tiles": 1, "output_text": " The image shows a young man playing tennis on a court with a green fence in the background. The court is made of concrete and has white lines marking the boundaries. The sky is overcast, and the lighting is natural, suggesting it might be a cloudy day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27545.9, "ram_available_mb": 98226.3, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 27537.8, "ram_available_mb": 98234.3, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.23, "peak": 40.97, "min": 18.91}, "VIN": {"avg": 68.22, "peak": 120.07, "min": 29.21}, "VIN_SYS_5V0": {"avg": 15.04, "peak": 16.15, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 16.14, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 28.23, "energy_joules_est": 69.36, "sample_count": 24, "duration_seconds": 2.457}, "timestamp": "2026-01-19T15:02:55.034958"}
{"image_index": 288, "image_name": "000000031248.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031248.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1722.79, "latencies_ms": [1722.79], "images_per_second": 0.58, "prompt_tokens": 1099, "response_tokens_est": 25, "n_tiles": 1, "output_text": " A well-lit living room with a fireplace, two lamps, a sofa, and a bookshelf filled with books.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27537.8, "ram_available_mb": 98234.3, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 27545.9, "ram_available_mb": 98226.3, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.29, "peak": 40.56, "min": 16.16}, "VIN": {"avg": 72.11, "peak": 100.92, "min": 30.69}, "VIN_SYS_5V0": {"avg": 15.07, "peak": 16.15, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 16.19, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 30.29, "energy_joules_est": 52.2, "sample_count": 17, "duration_seconds": 1.723}, "timestamp": "2026-01-19T15:02:56.811443"}
{"image_index": 288, "image_name": "000000031248.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031248.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2606.18, "latencies_ms": [2606.18], "images_per_second": 0.384, "prompt_tokens": 1113, "response_tokens_est": 59, "n_tiles": 1, "output_text": " 1. sofa: 1\n2. bookshelves: 2\n3. lamp: 2\n4. fireplace: 1\n5. plant: 2\n6. chair: 1\n7. table: 2\n8. picture: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 27545.9, "ram_available_mb": 98226.3, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 27562.1, "ram_available_mb": 98210.0, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.38, "peak": 40.57, "min": 17.73}, "VIN": {"avg": 62.17, "peak": 105.96, "min": 28.66}, "VIN_SYS_5V0": {"avg": 14.97, "peak": 16.15, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 16.08, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 27.38, "energy_joules_est": 71.37, "sample_count": 26, "duration_seconds": 2.607}, "timestamp": "2026-01-19T15:02:59.500529"}
{"image_index": 288, "image_name": "000000031248.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031248.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3021.84, "latencies_ms": [3021.84], "images_per_second": 0.331, "prompt_tokens": 1117, "response_tokens_est": 71, "n_tiles": 1, "output_text": " The beige armchair is positioned in the foreground, to the right of the fireplace, with the green plant to its left. The white bookshelves are located behind the armchair, with the green plant to their left. The white fireplace mantel is centrally located between the two side tables, with the beige armchair in front of it.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 27562.1, "ram_available_mb": 98210.0, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 27562.5, "ram_available_mb": 98209.7, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 25.55, "peak": 40.18, "min": 15.38}, "VIN": {"avg": 68.78, "peak": 116.08, "min": 27.53}, "VIN_SYS_5V0": {"avg": 14.76, "peak": 15.95, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 15.72, "peak": 16.92, "min": 13.0}}, "power_watts_avg": 25.55, "energy_joules_est": 77.22, "sample_count": 30, "duration_seconds": 3.022}, "timestamp": "2026-01-19T15:03:02.621480"}
{"image_index": 288, "image_name": "000000031248.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031248.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1624.615, "latencies_ms": [1624.615], "images_per_second": 0.616, "prompt_tokens": 1111, "response_tokens_est": 20, "n_tiles": 1, "output_text": " A cozy living room with a fireplace, two lamps, and a chair is shown in the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27562.5, "ram_available_mb": 98209.7, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 27569.2, "ram_available_mb": 98203.0, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.09, "peak": 40.18, "min": 14.59}, "VIN": {"avg": 71.89, "peak": 105.81, "min": 29.29}, "VIN_SYS_5V0": {"avg": 14.66, "peak": 15.95, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 15.75, "peak": 17.32, "min": 12.61}}, "power_watts_avg": 30.09, "energy_joules_est": 48.9, "sample_count": 16, "duration_seconds": 1.625}, "timestamp": "2026-01-19T15:03:04.297682"}
{"image_index": 288, "image_name": "000000031248.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031248.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1525.655, "latencies_ms": [1525.655], "images_per_second": 0.655, "prompt_tokens": 1109, "response_tokens_est": 17, "n_tiles": 1, "output_text": " The room is well-lit with natural light, and the walls are painted white.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27569.2, "ram_available_mb": 98203.0, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 27569.5, "ram_available_mb": 98202.7, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.25, "peak": 40.97, "min": 19.7}, "VIN": {"avg": 74.26, "peak": 126.68, "min": 30.24}, "VIN_SYS_5V0": {"avg": 15.0, "peak": 16.15, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 16.14, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 32.25, "energy_joules_est": 49.21, "sample_count": 15, "duration_seconds": 1.526}, "timestamp": "2026-01-19T15:03:05.862312"}
{"image_index": 289, "image_name": "000000031269.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031269.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2089.853, "latencies_ms": [2089.853], "images_per_second": 0.479, "prompt_tokens": 1099, "response_tokens_est": 40, "n_tiles": 1, "output_text": " In the image, a group of zebras is grazing on the grass in a field, with one zebra standing on the left side, another in the middle, and the third on the right side.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 27569.5, "ram_available_mb": 98202.7, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 27585.8, "ram_available_mb": 98186.4, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.48, "peak": 40.97, "min": 18.53}, "VIN": {"avg": 68.1, "peak": 118.76, "min": 28.8}, "VIN_SYS_5V0": {"avg": 15.04, "peak": 16.15, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 16.22, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 29.48, "energy_joules_est": 61.63, "sample_count": 21, "duration_seconds": 2.09}, "timestamp": "2026-01-19T15:03:08.055162"}
{"image_index": 289, "image_name": "000000031269.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031269.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1192.913, "latencies_ms": [1192.913], "images_per_second": 0.838, "prompt_tokens": 1113, "response_tokens_est": 4, "n_tiles": 1, "output_text": " zebra: 3", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27585.8, "ram_available_mb": 98186.4, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 27599.4, "ram_available_mb": 98172.8, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.78, "peak": 40.18, "min": 15.77}, "VIN": {"avg": 82.56, "peak": 126.65, "min": 29.18}, "VIN_SYS_5V0": {"avg": 14.65, "peak": 16.05, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.85, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 31.78, "energy_joules_est": 37.93, "sample_count": 12, "duration_seconds": 1.194}, "timestamp": "2026-01-19T15:03:09.310687"}
{"image_index": 289, "image_name": "000000031269.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031269.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2879.035, "latencies_ms": [2879.035], "images_per_second": 0.347, "prompt_tokens": 1117, "response_tokens_est": 66, "n_tiles": 1, "output_text": " The zebras are positioned in the foreground of the image, with the one on the left slightly ahead of the other two. The zebras are facing towards the right side of the image, with the one on the left facing the camera. The zebras are standing on a grassy hill, with the dirt path in the foreground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27599.4, "ram_available_mb": 98172.8, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 27606.2, "ram_available_mb": 98165.9, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.75, "peak": 40.95, "min": 18.52}, "VIN": {"avg": 69.48, "peak": 124.45, "min": 29.13}, "VIN_SYS_5V0": {"avg": 14.93, "peak": 16.15, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.9, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 27.75, "energy_joules_est": 79.92, "sample_count": 28, "duration_seconds": 2.88}, "timestamp": "2026-01-19T15:03:12.226494"}
{"image_index": 289, "image_name": "000000031269.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031269.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1513.82, "latencies_ms": [1513.82], "images_per_second": 0.661, "prompt_tokens": 1111, "response_tokens_est": 17, "n_tiles": 1, "output_text": " A group of zebras are grazing in a grassy field with trees in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27606.2, "ram_available_mb": 98165.9, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 27611.9, "ram_available_mb": 98160.2, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.09, "peak": 40.16, "min": 15.77}, "VIN": {"avg": 75.54, "peak": 125.91, "min": 30.29}, "VIN_SYS_5V0": {"avg": 15.04, "peak": 16.15, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 16.17, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 31.09, "energy_joules_est": 47.09, "sample_count": 15, "duration_seconds": 1.515}, "timestamp": "2026-01-19T15:03:13.801434"}
{"image_index": 289, "image_name": "000000031269.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031269.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2266.317, "latencies_ms": [2266.317], "images_per_second": 0.441, "prompt_tokens": 1109, "response_tokens_est": 46, "n_tiles": 1, "output_text": " The image features a group of zebras in a natural setting with a clear blue sky and green grass. The zebras are standing on a dirt path, with their black and white stripes contrasting against the natural colors of the landscape.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27611.9, "ram_available_mb": 98160.2, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 27620.4, "ram_available_mb": 98151.8, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.94, "peak": 40.97, "min": 19.71}, "VIN": {"avg": 71.43, "peak": 105.69, "min": 29.13}, "VIN_SYS_5V0": {"avg": 14.99, "peak": 16.15, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 16.14, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 28.94, "energy_joules_est": 65.6, "sample_count": 22, "duration_seconds": 2.267}, "timestamp": "2026-01-19T15:03:16.099835"}
{"image_index": 290, "image_name": "000000031296.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031296.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1623.617, "latencies_ms": [1623.617], "images_per_second": 0.616, "prompt_tokens": 1099, "response_tokens_est": 21, "n_tiles": 1, "output_text": " A group of people are sitting at a table in a restaurant, wearing hats and smiling at the camera.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 27620.4, "ram_available_mb": 98151.8, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 27628.5, "ram_available_mb": 98143.7, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.8, "peak": 40.57, "min": 16.56}, "VIN": {"avg": 75.15, "peak": 119.86, "min": 29.56}, "VIN_SYS_5V0": {"avg": 15.0, "peak": 16.15, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 16.19, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 30.8, "energy_joules_est": 50.03, "sample_count": 16, "duration_seconds": 1.624}, "timestamp": "2026-01-19T15:03:17.779902"}
{"image_index": 290, "image_name": "000000031296.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031296.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2630.551, "latencies_ms": [2630.551], "images_per_second": 0.38, "prompt_tokens": 1113, "response_tokens_est": 59, "n_tiles": 1, "output_text": " 1. chairs: 10\n2. people: 10\n3. table: 1\n4. bottles: 1\n5. napkins: 1\n6. plates: 1\n7. cups: 1\n8. glasses: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27628.5, "ram_available_mb": 98143.7, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 27557.3, "ram_available_mb": 98214.8, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.31, "peak": 40.57, "min": 17.73}, "VIN": {"avg": 69.45, "peak": 120.9, "min": 29.87}, "VIN_SYS_5V0": {"avg": 15.0, "peak": 16.05, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 16.17, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 27.31, "energy_joules_est": 71.85, "sample_count": 26, "duration_seconds": 2.631}, "timestamp": "2026-01-19T15:03:20.494567"}
{"image_index": 290, "image_name": "000000031296.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031296.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2287.888, "latencies_ms": [2287.888], "images_per_second": 0.437, "prompt_tokens": 1117, "response_tokens_est": 46, "n_tiles": 1, "output_text": " The group of people is seated around a table in the foreground of the image, with the restaurant's interior and other patrons in the background. The table is positioned in the center of the image, with the people seated around it.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 27557.3, "ram_available_mb": 98214.8, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 27556.6, "ram_available_mb": 98215.5, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.58, "peak": 40.18, "min": 15.38}, "VIN": {"avg": 70.03, "peak": 121.16, "min": 28.89}, "VIN_SYS_5V0": {"avg": 14.8, "peak": 16.05, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.95, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 27.58, "energy_joules_est": 63.12, "sample_count": 23, "duration_seconds": 2.289}, "timestamp": "2026-01-19T15:03:22.889435"}
{"image_index": 290, "image_name": "000000031296.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031296.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1570.253, "latencies_ms": [1570.253], "images_per_second": 0.637, "prompt_tokens": 1111, "response_tokens_est": 18, "n_tiles": 1, "output_text": " A group of people are sitting at a table in a restaurant, wearing hats and smiling.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27556.6, "ram_available_mb": 98215.5, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 27556.9, "ram_available_mb": 98215.3, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.14, "peak": 40.18, "min": 15.38}, "VIN": {"avg": 73.15, "peak": 120.6, "min": 28.04}, "VIN_SYS_5V0": {"avg": 14.6, "peak": 15.95, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 15.82, "peak": 17.32, "min": 12.61}}, "power_watts_avg": 30.14, "energy_joules_est": 47.34, "sample_count": 16, "duration_seconds": 1.571}, "timestamp": "2026-01-19T15:03:24.562498"}
{"image_index": 290, "image_name": "000000031296.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031296.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2337.308, "latencies_ms": [2337.308], "images_per_second": 0.428, "prompt_tokens": 1109, "response_tokens_est": 48, "n_tiles": 1, "output_text": " The image is taken in a restaurant with a warm and inviting atmosphere, characterized by the rich brown color of the wooden floor and the warm lighting that illuminates the space. The walls are adorned with brick, adding a rustic charm to the setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27556.9, "ram_available_mb": 98215.3, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 27557.4, "ram_available_mb": 98214.7, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.19, "peak": 40.18, "min": 18.14}, "VIN": {"avg": 68.12, "peak": 105.42, "min": 29.93}, "VIN_SYS_5V0": {"avg": 14.76, "peak": 15.95, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.97, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 28.19, "energy_joules_est": 65.91, "sample_count": 23, "duration_seconds": 2.338}, "timestamp": "2026-01-19T15:03:26.943855"}
{"image_index": 291, "image_name": "000000031322.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031322.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1724.968, "latencies_ms": [1724.968], "images_per_second": 0.58, "prompt_tokens": 1099, "response_tokens_est": 25, "n_tiles": 1, "output_text": " In the image, a group of swans is seen swimming in a marina, with boats docked in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27557.4, "ram_available_mb": 98214.7, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 27564.6, "ram_available_mb": 98207.6, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.34, "peak": 40.57, "min": 16.56}, "VIN": {"avg": 73.76, "peak": 113.51, "min": 29.42}, "VIN_SYS_5V0": {"avg": 14.98, "peak": 16.05, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 16.12, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 30.34, "energy_joules_est": 52.36, "sample_count": 17, "duration_seconds": 1.726}, "timestamp": "2026-01-19T15:03:28.722022"}
{"image_index": 291, "image_name": "000000031322.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031322.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1785.159, "latencies_ms": [1785.159], "images_per_second": 0.56, "prompt_tokens": 1113, "response_tokens_est": 27, "n_tiles": 1, "output_text": " swan: 12, boat: 10, water: 1, dock: 1, purple: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27564.6, "ram_available_mb": 98207.6, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 27568.8, "ram_available_mb": 98203.4, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.31, "peak": 40.57, "min": 18.53}, "VIN": {"avg": 68.78, "peak": 120.74, "min": 30.24}, "VIN_SYS_5V0": {"avg": 14.96, "peak": 16.15, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 16.08, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 30.31, "energy_joules_est": 54.13, "sample_count": 18, "duration_seconds": 1.786}, "timestamp": "2026-01-19T15:03:30.599908"}
{"image_index": 291, "image_name": "000000031322.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031322.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2216.914, "latencies_ms": [2216.914], "images_per_second": 0.451, "prompt_tokens": 1117, "response_tokens_est": 43, "n_tiles": 1, "output_text": " The swans are positioned in the foreground of the image, with the boats in the background. The boats are docked on the left side of the image, while the swans are swimming in the right side.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 27568.8, "ram_available_mb": 98203.4, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 27585.4, "ram_available_mb": 98186.8, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.26, "peak": 40.16, "min": 17.35}, "VIN": {"avg": 70.34, "peak": 104.06, "min": 29.04}, "VIN_SYS_5V0": {"avg": 14.75, "peak": 15.95, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.93, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 28.26, "energy_joules_est": 62.66, "sample_count": 22, "duration_seconds": 2.217}, "timestamp": "2026-01-19T15:03:32.892647"}
{"image_index": 291, "image_name": "000000031322.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031322.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1592.477, "latencies_ms": [1592.477], "images_per_second": 0.628, "prompt_tokens": 1111, "response_tokens_est": 19, "n_tiles": 1, "output_text": " A group of swans are swimming in a marina with boats docked in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27585.4, "ram_available_mb": 98186.8, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 27585.1, "ram_available_mb": 98187.0, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.36, "peak": 40.18, "min": 15.77}, "VIN": {"avg": 70.94, "peak": 123.4, "min": 28.58}, "VIN_SYS_5V0": {"avg": 14.76, "peak": 16.05, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 15.87, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 30.36, "energy_joules_est": 48.36, "sample_count": 16, "duration_seconds": 1.593}, "timestamp": "2026-01-19T15:03:34.571197"}
{"image_index": 291, "image_name": "000000031322.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031322.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2153.942, "latencies_ms": [2153.942], "images_per_second": 0.464, "prompt_tokens": 1109, "response_tokens_est": 41, "n_tiles": 1, "output_text": " The image features a group of swans in a marina with purple-tinted water, reflecting the sunlight. The swans are white, and the water is calm, creating a serene atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27585.1, "ram_available_mb": 98187.0, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 27592.9, "ram_available_mb": 98179.3, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.02, "peak": 40.16, "min": 18.53}, "VIN": {"avg": 72.1, "peak": 118.35, "min": 29.3}, "VIN_SYS_5V0": {"avg": 14.84, "peak": 16.05, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.97, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 29.02, "energy_joules_est": 62.52, "sample_count": 21, "duration_seconds": 2.154}, "timestamp": "2026-01-19T15:03:36.760008"}
{"image_index": 292, "image_name": "000000031620.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031620.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1653.518, "latencies_ms": [1653.518], "images_per_second": 0.605, "prompt_tokens": 1100, "response_tokens_est": 23, "n_tiles": 1, "output_text": " A man and woman are cutting a wedding cake together in a tent with a table covered in a white tablecloth.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 27592.9, "ram_available_mb": 98179.3, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 27600.6, "ram_available_mb": 98171.6, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.95, "peak": 40.18, "min": 16.95}, "VIN": {"avg": 68.77, "peak": 126.91, "min": 29.56}, "VIN_SYS_5V0": {"avg": 15.11, "peak": 16.15, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 16.32, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 30.95, "energy_joules_est": 51.19, "sample_count": 16, "duration_seconds": 1.654}, "timestamp": "2026-01-19T15:03:38.431438"}
{"image_index": 292, "image_name": "000000031620.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031620.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2492.29, "latencies_ms": [2492.29], "images_per_second": 0.401, "prompt_tokens": 1114, "response_tokens_est": 56, "n_tiles": 1, "output_text": " 1. man: 1\n2. woman: 1\n3. table: 1\n4. chair: 1\n5. cake: 1\n6. person: 1\n7. speaker: 1\n8. tent: 1", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 27600.6, "ram_available_mb": 98171.6, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 27607.8, "ram_available_mb": 98164.4, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.3, "peak": 40.16, "min": 19.7}, "VIN": {"avg": 69.43, "peak": 121.54, "min": 32.05}, "VIN_SYS_5V0": {"avg": 15.25, "peak": 16.26, "min": 13.29}, "VDD_CPU_SOC_MSS": {"avg": 16.4, "peak": 17.71, "min": 14.57}}, "power_watts_avg": 28.3, "energy_joules_est": 70.54, "sample_count": 24, "duration_seconds": 2.493}, "timestamp": "2026-01-19T15:03:40.933317"}
{"image_index": 292, "image_name": "000000031620.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031620.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2524.609, "latencies_ms": [2524.609], "images_per_second": 0.396, "prompt_tokens": 1118, "response_tokens_est": 57, "n_tiles": 1, "output_text": " The bride and groom are standing close to each other, with the bride on the left and the groom on the right. The cake is placed in the middle of the table, which is located in the foreground. The background of the image shows a stage with musical instruments and a speaker.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27607.8, "ram_available_mb": 98164.4, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 27623.9, "ram_available_mb": 98148.2, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.34, "peak": 39.78, "min": 17.73}, "VIN": {"avg": 70.52, "peak": 105.02, "min": 28.71}, "VIN_SYS_5V0": {"avg": 15.17, "peak": 16.26, "min": 13.29}, "VDD_CPU_SOC_MSS": {"avg": 16.31, "peak": 17.71, "min": 14.57}}, "power_watts_avg": 27.34, "energy_joules_est": 69.03, "sample_count": 25, "duration_seconds": 2.525}, "timestamp": "2026-01-19T15:03:43.538323"}
{"image_index": 292, "image_name": "000000031620.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031620.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1643.947, "latencies_ms": [1643.947], "images_per_second": 0.608, "prompt_tokens": 1112, "response_tokens_est": 21, "n_tiles": 1, "output_text": " A newlywed couple is cutting their wedding cake together in a tent, surrounded by guests and a band.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27623.9, "ram_available_mb": 98148.2, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 27626.3, "ram_available_mb": 98145.9, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.19, "peak": 39.78, "min": 15.38}, "VIN": {"avg": 76.27, "peak": 130.78, "min": 27.6}, "VIN_SYS_5V0": {"avg": 14.85, "peak": 16.15, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 16.04, "peak": 17.71, "min": 13.0}}, "power_watts_avg": 30.19, "energy_joules_est": 49.65, "sample_count": 16, "duration_seconds": 1.645}, "timestamp": "2026-01-19T15:03:45.205083"}
{"image_index": 292, "image_name": "000000031620.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031620.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1572.38, "latencies_ms": [1572.38], "images_per_second": 0.636, "prompt_tokens": 1110, "response_tokens_est": 20, "n_tiles": 1, "output_text": " The wedding reception is well-lit with natural light, and the tent is decorated with colorful flags.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27626.3, "ram_available_mb": 98145.9, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 27632.2, "ram_available_mb": 98140.0, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.49, "peak": 40.18, "min": 19.31}, "VIN": {"avg": 77.18, "peak": 127.13, "min": 27.61}, "VIN_SYS_5V0": {"avg": 15.17, "peak": 16.15, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 16.31, "peak": 17.71, "min": 14.18}}, "power_watts_avg": 31.49, "energy_joules_est": 49.53, "sample_count": 16, "duration_seconds": 1.573}, "timestamp": "2026-01-19T15:03:46.872694"}
{"image_index": 293, "image_name": "000000031735.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031735.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1466.474, "latencies_ms": [1466.474], "images_per_second": 0.682, "prompt_tokens": 1099, "response_tokens_est": 15, "n_tiles": 1, "output_text": " A living room with a blue couch, a table, and a lamp.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 27632.2, "ram_available_mb": 98140.0, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 27645.4, "ram_available_mb": 98126.8, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.7, "peak": 40.57, "min": 18.14}, "VIN": {"avg": 73.43, "peak": 116.55, "min": 27.66}, "VIN_SYS_5V0": {"avg": 14.76, "peak": 16.05, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.93, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 31.7, "energy_joules_est": 46.51, "sample_count": 15, "duration_seconds": 1.467}, "timestamp": "2026-01-19T15:03:48.443512"}
{"image_index": 293, "image_name": "000000031735.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031735.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2045.551, "latencies_ms": [2045.551], "images_per_second": 0.489, "prompt_tokens": 1113, "response_tokens_est": 37, "n_tiles": 1, "output_text": " sofa: 1, coffee table: 1, side table: 2, lamp: 1, plant: 2, rug: 1, painting: 1", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 27645.4, "ram_available_mb": 98126.8, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 6.9, "ram_used_mb": 27557.3, "ram_available_mb": 98214.9, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.66, "peak": 40.57, "min": 19.32}, "VIN": {"avg": 70.36, "peak": 102.19, "min": 28.41}, "VIN_SYS_5V0": {"avg": 14.87, "peak": 16.05, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 16.16, "peak": 18.11, "min": 13.0}}, "power_watts_avg": 29.66, "energy_joules_est": 60.68, "sample_count": 20, "duration_seconds": 2.046}, "timestamp": "2026-01-19T15:03:50.533715"}
{"image_index": 293, "image_name": "000000031735.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031735.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2552.039, "latencies_ms": [2552.039], "images_per_second": 0.392, "prompt_tokens": 1117, "response_tokens_est": 57, "n_tiles": 1, "output_text": " The sofa is positioned in the center of the room, with the coffee table in front of it. The side table is located to the left of the sofa, while the lamp is situated to the right. The rug is placed on the floor, covering the entire floor space.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27557.3, "ram_available_mb": 98214.9, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 27556.1, "ram_available_mb": 98216.1, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.26, "peak": 40.56, "min": 16.95}, "VIN": {"avg": 68.67, "peak": 118.4, "min": 29.28}, "VIN_SYS_5V0": {"avg": 15.02, "peak": 16.15, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 16.09, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 27.26, "energy_joules_est": 69.58, "sample_count": 25, "duration_seconds": 2.552}, "timestamp": "2026-01-19T15:03:53.147401"}
{"image_index": 293, "image_name": "000000031735.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031735.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1564.074, "latencies_ms": [1564.074], "images_per_second": 0.639, "prompt_tokens": 1111, "response_tokens_est": 19, "n_tiles": 1, "output_text": " A living room with a blue couch, a coffee table, and a rug on the floor.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 27556.1, "ram_available_mb": 98216.1, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 27563.9, "ram_available_mb": 98208.3, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.61, "peak": 40.57, "min": 15.77}, "VIN": {"avg": 72.41, "peak": 118.9, "min": 28.22}, "VIN_SYS_5V0": {"avg": 14.94, "peak": 16.05, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 16.04, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 30.61, "energy_joules_est": 47.89, "sample_count": 16, "duration_seconds": 1.564}, "timestamp": "2026-01-19T15:03:54.819538"}
{"image_index": 293, "image_name": "000000031735.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031735.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1611.996, "latencies_ms": [1611.996], "images_per_second": 0.62, "prompt_tokens": 1109, "response_tokens_est": 20, "n_tiles": 1, "output_text": " The room is painted red and has a red carpet. The furniture is made of wood and glass.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 27563.9, "ram_available_mb": 98208.3, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 27573.9, "ram_available_mb": 98198.2, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.17, "peak": 40.16, "min": 18.14}, "VIN": {"avg": 78.12, "peak": 123.42, "min": 30.73}, "VIN_SYS_5V0": {"avg": 14.74, "peak": 15.95, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.92, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 31.17, "energy_joules_est": 50.26, "sample_count": 16, "duration_seconds": 1.612}, "timestamp": "2026-01-19T15:03:56.486209"}
{"image_index": 294, "image_name": "000000031749.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031749.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2283.389, "latencies_ms": [2283.389], "images_per_second": 0.438, "prompt_tokens": 1432, "response_tokens_est": 30, "n_tiles": 1, "output_text": " The image features a doll with a clock face on its face, which is positioned in front of a mirror, creating a reflection of the clock face.", "error": null, "sys_before": {"cpu_percent": 6.7, "ram_used_mb": 27573.9, "ram_available_mb": 98198.2, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 27577.7, "ram_available_mb": 98194.5, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.2, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.29, "peak": 40.18, "min": 18.92}, "VIN": {"avg": 73.51, "peak": 119.53, "min": 32.6}, "VIN_SYS_5V0": {"avg": 15.35, "peak": 16.66, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 16.52, "peak": 18.11, "min": 13.79}}, "power_watts_avg": 31.29, "energy_joules_est": 71.46, "sample_count": 22, "duration_seconds": 2.284}, "timestamp": "2026-01-19T15:03:58.784943"}
{"image_index": 294, "image_name": "000000031749.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031749.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3331.098, "latencies_ms": [3331.098], "images_per_second": 0.3, "prompt_tokens": 1446, "response_tokens_est": 68, "n_tiles": 1, "output_text": " 1. Clock: 1\n2. Doll: 1\n3. Doll's hair: 1\n4. Doll's eyes: 1\n5. Doll's mouth: 1\n6. Doll's nose: 1\n7. Doll's hands: 1\n8. Doll's feet: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27577.7, "ram_available_mb": 98194.5, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 27587.5, "ram_available_mb": 98184.7, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.7, "peak": 40.95, "min": 17.35}, "VIN": {"avg": 69.23, "peak": 124.5, "min": 28.59}, "VIN_SYS_5V0": {"avg": 15.21, "peak": 16.76, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 16.17, "peak": 18.11, "min": 13.79}}, "power_watts_avg": 27.7, "energy_joules_est": 92.28, "sample_count": 33, "duration_seconds": 3.331}, "timestamp": "2026-01-19T15:04:02.213573"}
{"image_index": 294, "image_name": "000000031749.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031749.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2683.701, "latencies_ms": [2683.701], "images_per_second": 0.373, "prompt_tokens": 1450, "response_tokens_est": 46, "n_tiles": 1, "output_text": " The clock is positioned on the left side of the image, while the doll is on the right side. The doll is in the foreground, with the clock in the background. The doll is closer to the viewer than the clock.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27587.5, "ram_available_mb": 98184.7, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 27602.0, "ram_available_mb": 98170.2, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.26, "peak": 40.18, "min": 14.98}, "VIN": {"avg": 72.04, "peak": 118.52, "min": 28.95}, "VIN_SYS_5V0": {"avg": 15.1, "peak": 16.56, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 16.16, "peak": 17.71, "min": 12.61}}, "power_watts_avg": 29.26, "energy_joules_est": 78.54, "sample_count": 26, "duration_seconds": 2.684}, "timestamp": "2026-01-19T15:04:04.912635"}
{"image_index": 294, "image_name": "000000031749.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031749.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1827.378, "latencies_ms": [1827.378], "images_per_second": 0.547, "prompt_tokens": 1444, "response_tokens_est": 15, "n_tiles": 1, "output_text": " A doll with a clock face on its face is sitting on a table.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27602.0, "ram_available_mb": 98170.2, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 27603.2, "ram_available_mb": 98169.0, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 33.04, "peak": 40.56, "min": 17.34}, "VIN": {"avg": 76.11, "peak": 129.2, "min": 30.21}, "VIN_SYS_5V0": {"avg": 15.52, "peak": 16.76, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 16.67, "peak": 18.11, "min": 14.18}}, "power_watts_avg": 33.04, "energy_joules_est": 60.39, "sample_count": 18, "duration_seconds": 1.828}, "timestamp": "2026-01-19T15:04:06.791228"}
{"image_index": 294, "image_name": "000000031749.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031749.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2803.145, "latencies_ms": [2803.145], "images_per_second": 0.357, "prompt_tokens": 1442, "response_tokens_est": 51, "n_tiles": 1, "output_text": " The image features a doll with a clock face on its face, which is a combination of yellow and black. The doll is made of plastic and has a red and black striped scarf. The lighting in the image is bright and the colors are vibrant.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 27603.2, "ram_available_mb": 98169.0, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 27599.9, "ram_available_mb": 98172.3, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.54, "peak": 40.56, "min": 18.13}, "VIN": {"avg": 69.77, "peak": 116.3, "min": 29.02}, "VIN_SYS_5V0": {"avg": 15.22, "peak": 16.66, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 16.25, "peak": 17.71, "min": 13.79}}, "power_watts_avg": 29.54, "energy_joules_est": 82.82, "sample_count": 28, "duration_seconds": 2.804}, "timestamp": "2026-01-19T15:04:09.698146"}
{"image_index": 295, "image_name": "000000031817.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031817.jpg", "image_width": 334, "image_height": 640, "image_resolution": "334x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1453.774, "latencies_ms": [1453.774], "images_per_second": 0.688, "prompt_tokens": 1100, "response_tokens_est": 14, "n_tiles": 1, "output_text": " A man wearing a helmet and a jacket is sitting on a motorcycle.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 27599.9, "ram_available_mb": 98172.3, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 27599.9, "ram_available_mb": 98172.3, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.98, "peak": 39.77, "min": 15.38}, "VIN": {"avg": 77.64, "peak": 125.18, "min": 35.49}, "VIN_SYS_5V0": {"avg": 14.7, "peak": 16.05, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 15.92, "peak": 17.32, "min": 12.61}}, "power_watts_avg": 30.98, "energy_joules_est": 45.07, "sample_count": 14, "duration_seconds": 1.455}, "timestamp": "2026-01-19T15:04:11.168977"}
{"image_index": 295, "image_name": "000000031817.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031817.jpg", "image_width": 334, "image_height": 640, "image_resolution": "334x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2697.998, "latencies_ms": [2697.998], "images_per_second": 0.371, "prompt_tokens": 1114, "response_tokens_est": 57, "n_tiles": 1, "output_text": " 1. person: 1\n2. helmet: 1\n3. motorcycle: 1\n4. jacket: 1\n5. pants: 1\n6. shoes: 1\n7. seat: 1\n8. seatbelt: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27599.9, "ram_available_mb": 98172.3, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 27607.6, "ram_available_mb": 98164.6, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.4, "peak": 40.57, "min": 19.31}, "VIN": {"avg": 69.7, "peak": 128.72, "min": 30.7}, "VIN_SYS_5V0": {"avg": 14.89, "peak": 16.26, "min": 13.29}, "VDD_CPU_SOC_MSS": {"avg": 16.01, "peak": 17.71, "min": 14.57}}, "power_watts_avg": 27.4, "energy_joules_est": 73.94, "sample_count": 26, "duration_seconds": 2.698}, "timestamp": "2026-01-19T15:04:13.875362"}
{"image_index": 295, "image_name": "000000031817.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031817.jpg", "image_width": 334, "image_height": 640, "image_resolution": "334x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2158.687, "latencies_ms": [2158.687], "images_per_second": 0.463, "prompt_tokens": 1118, "response_tokens_est": 42, "n_tiles": 1, "output_text": " The man is sitting on the left side of the motorcycle, which is positioned in the middle of the image. The motorcycle is in the foreground of the image, while the background is occupied by a red car.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 27607.6, "ram_available_mb": 98164.6, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 6.5, "ram_used_mb": 27542.5, "ram_available_mb": 98229.7, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.55, "peak": 40.18, "min": 16.55}, "VIN": {"avg": 70.79, "peak": 124.16, "min": 30.13}, "VIN_SYS_5V0": {"avg": 15.21, "peak": 16.15, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 16.52, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 28.55, "energy_joules_est": 61.64, "sample_count": 21, "duration_seconds": 2.159}, "timestamp": "2026-01-19T15:04:16.066810"}
{"image_index": 295, "image_name": "000000031817.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031817.jpg", "image_width": 334, "image_height": 640, "image_resolution": "334x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1339.623, "latencies_ms": [1339.623], "images_per_second": 0.746, "prompt_tokens": 1112, "response_tokens_est": 11, "n_tiles": 1, "output_text": " A man wearing a helmet is sitting on a motorcycle.", "error": null, "sys_before": {"cpu_percent": 40.0, "ram_used_mb": 27542.5, "ram_available_mb": 98229.7, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 27538.3, "ram_available_mb": 98233.9, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.06, "peak": 39.39, "min": 16.95}, "VIN": {"avg": 73.59, "peak": 111.5, "min": 28.8}, "VIN_SYS_5V0": {"avg": 15.25, "peak": 16.26, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 16.54, "peak": 17.71, "min": 14.18}}, "power_watts_avg": 32.06, "energy_joules_est": 42.96, "sample_count": 13, "duration_seconds": 1.34}, "timestamp": "2026-01-19T15:04:17.421963"}
{"image_index": 295, "image_name": "000000031817.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031817.jpg", "image_width": 334, "image_height": 640, "image_resolution": "334x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1783.689, "latencies_ms": [1783.689], "images_per_second": 0.561, "prompt_tokens": 1110, "response_tokens_est": 28, "n_tiles": 1, "output_text": " The image is taken in a dimly lit room with a red background. The man is wearing a black helmet and a beige jacket.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27538.3, "ram_available_mb": 98233.9, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 27545.8, "ram_available_mb": 98226.4, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.47, "peak": 40.95, "min": 20.1}, "VIN": {"avg": 70.83, "peak": 116.29, "min": 29.23}, "VIN_SYS_5V0": {"avg": 15.18, "peak": 16.26, "min": 13.29}, "VDD_CPU_SOC_MSS": {"avg": 16.36, "peak": 17.71, "min": 14.57}}, "power_watts_avg": 31.47, "energy_joules_est": 56.15, "sample_count": 18, "duration_seconds": 1.784}, "timestamp": "2026-01-19T15:04:19.300364"}
{"image_index": 296, "image_name": "000000032038.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032038.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1477.597, "latencies_ms": [1477.597], "images_per_second": 0.677, "prompt_tokens": 1100, "response_tokens_est": 15, "n_tiles": 1, "output_text": " A person is spreading cheese on a pizza that is on a wooden table.", "error": null, "sys_before": {"cpu_percent": 16.7, "ram_used_mb": 27545.8, "ram_available_mb": 98226.4, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 27548.6, "ram_available_mb": 98223.5, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.97, "peak": 40.18, "min": 16.95}, "VIN": {"avg": 69.57, "peak": 90.89, "min": 28.92}, "VIN_SYS_5V0": {"avg": 14.72, "peak": 16.05, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.88, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 30.97, "energy_joules_est": 45.79, "sample_count": 15, "duration_seconds": 1.478}, "timestamp": "2026-01-19T15:04:20.859473"}
{"image_index": 296, "image_name": "000000032038.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032038.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2637.1, "latencies_ms": [2637.1], "images_per_second": 0.379, "prompt_tokens": 1114, "response_tokens_est": 60, "n_tiles": 1, "output_text": " 1. pizza: 1\n2. person: 1\n3. spatula: 1\n4. wooden table: 1\n5. metal container: 1\n6. metal pan: 1\n7. cheese: 1\n8. sauce: 1", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 27548.6, "ram_available_mb": 98223.5, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 27553.5, "ram_available_mb": 98218.7, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.41, "peak": 40.57, "min": 18.14}, "VIN": {"avg": 68.36, "peak": 125.72, "min": 29.99}, "VIN_SYS_5V0": {"avg": 14.91, "peak": 16.15, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.96, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 27.41, "energy_joules_est": 72.3, "sample_count": 26, "duration_seconds": 2.638}, "timestamp": "2026-01-19T15:04:23.568079"}
{"image_index": 296, "image_name": "000000032038.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032038.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2115.11, "latencies_ms": [2115.11], "images_per_second": 0.473, "prompt_tokens": 1118, "response_tokens_est": 38, "n_tiles": 1, "output_text": " The pizza is in the foreground, with the person's hand in the background. The pizza is on the left side of the image, while the person's hand is on the right side.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27553.5, "ram_available_mb": 98218.7, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 27554.0, "ram_available_mb": 98218.2, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.1, "peak": 40.18, "min": 15.77}, "VIN": {"avg": 68.18, "peak": 122.45, "min": 30.2}, "VIN_SYS_5V0": {"avg": 14.81, "peak": 16.05, "min": 12.15}, "VDD_CPU_SOC_MSS": {"avg": 15.81, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 28.1, "energy_joules_est": 59.45, "sample_count": 21, "duration_seconds": 2.116}, "timestamp": "2026-01-19T15:04:25.755488"}
{"image_index": 296, "image_name": "000000032038.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032038.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1446.94, "latencies_ms": [1446.94], "images_per_second": 0.691, "prompt_tokens": 1112, "response_tokens_est": 14, "n_tiles": 1, "output_text": " A person is preparing a pizza on a wooden table in a kitchen.", "error": null, "sys_before": {"cpu_percent": 50.0, "ram_used_mb": 27554.0, "ram_available_mb": 98218.2, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 27553.3, "ram_available_mb": 98218.9, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.15, "peak": 39.39, "min": 16.16}, "VIN": {"avg": 81.19, "peak": 125.64, "min": 29.19}, "VIN_SYS_5V0": {"avg": 14.87, "peak": 16.15, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 15.89, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 31.15, "energy_joules_est": 45.09, "sample_count": 14, "duration_seconds": 1.447}, "timestamp": "2026-01-19T15:04:27.219343"}
{"image_index": 296, "image_name": "000000032038.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032038.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1707.332, "latencies_ms": [1707.332], "images_per_second": 0.586, "prompt_tokens": 1110, "response_tokens_est": 25, "n_tiles": 1, "output_text": " The pizza is golden brown and has a creamy white sauce. The lighting is dim and the pizza is on a wooden table.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27553.3, "ram_available_mb": 98218.9, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 27553.3, "ram_available_mb": 98218.9, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.68, "peak": 40.97, "min": 21.67}, "VIN": {"avg": 64.59, "peak": 95.1, "min": 29.39}, "VIN_SYS_5V0": {"avg": 15.21, "peak": 16.26, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 16.23, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 31.68, "energy_joules_est": 54.11, "sample_count": 17, "duration_seconds": 1.708}, "timestamp": "2026-01-19T15:04:28.996610"}
{"image_index": 297, "image_name": "000000032081.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032081.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1507.613, "latencies_ms": [1507.613], "images_per_second": 0.663, "prompt_tokens": 1100, "response_tokens_est": 17, "n_tiles": 1, "output_text": " A woman in a white dress and white shoes is playing tennis on a grass court.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27553.3, "ram_available_mb": 98218.9, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 27553.3, "ram_available_mb": 98218.9, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.65, "peak": 40.57, "min": 18.14}, "VIN": {"avg": 67.25, "peak": 104.85, "min": 29.46}, "VIN_SYS_5V0": {"avg": 14.9, "peak": 16.05, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.96, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 31.65, "energy_joules_est": 47.74, "sample_count": 15, "duration_seconds": 1.508}, "timestamp": "2026-01-19T15:04:30.567697"}
{"image_index": 297, "image_name": "000000032081.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032081.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2584.703, "latencies_ms": [2584.703], "images_per_second": 0.387, "prompt_tokens": 1114, "response_tokens_est": 59, "n_tiles": 1, "output_text": " 1. woman: 1\n2. tennis racket: 1\n3. tennis ball: 1\n4. grass: 1\n5. net: 1\n6. white: 1\n7. white: 1\n8. white: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 27553.3, "ram_available_mb": 98218.9, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 27568.5, "ram_available_mb": 98203.6, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.86, "peak": 40.97, "min": 19.31}, "VIN": {"avg": 64.84, "peak": 92.27, "min": 28.96}, "VIN_SYS_5V0": {"avg": 15.03, "peak": 16.15, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 16.13, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 27.86, "energy_joules_est": 72.02, "sample_count": 25, "duration_seconds": 2.585}, "timestamp": "2026-01-19T15:04:33.175490"}
{"image_index": 297, "image_name": "000000032081.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032081.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1905.75, "latencies_ms": [1905.75], "images_per_second": 0.525, "prompt_tokens": 1118, "response_tokens_est": 33, "n_tiles": 1, "output_text": " The tennis player is positioned in the foreground of the image, with the tennis racket held high above her head, and the tennis court stretching out into the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27568.5, "ram_available_mb": 98203.6, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 27566.5, "ram_available_mb": 98205.7, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.55, "peak": 39.78, "min": 16.95}, "VIN": {"avg": 77.79, "peak": 123.58, "min": 29.25}, "VIN_SYS_5V0": {"avg": 15.16, "peak": 16.26, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 16.27, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 29.55, "energy_joules_est": 56.33, "sample_count": 19, "duration_seconds": 1.906}, "timestamp": "2026-01-19T15:04:35.155419"}
{"image_index": 297, "image_name": "000000032081.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032081.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1437.976, "latencies_ms": [1437.976], "images_per_second": 0.695, "prompt_tokens": 1112, "response_tokens_est": 14, "n_tiles": 1, "output_text": " A woman in a white dress is playing tennis on a grass court.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27566.5, "ram_available_mb": 98205.7, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 27572.0, "ram_available_mb": 98200.2, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.51, "peak": 39.77, "min": 16.95}, "VIN": {"avg": 78.17, "peak": 122.74, "min": 29.22}, "VIN_SYS_5V0": {"avg": 14.94, "peak": 16.26, "min": 12.15}, "VDD_CPU_SOC_MSS": {"avg": 16.09, "peak": 17.71, "min": 13.39}}, "power_watts_avg": 31.51, "energy_joules_est": 45.33, "sample_count": 14, "duration_seconds": 1.438}, "timestamp": "2026-01-19T15:04:36.620464"}
{"image_index": 297, "image_name": "000000032081.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032081.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1597.202, "latencies_ms": [1597.202], "images_per_second": 0.626, "prompt_tokens": 1110, "response_tokens_est": 21, "n_tiles": 1, "output_text": " The tennis player is wearing a white outfit and white shoes, and the grass on the court is green.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27572.0, "ram_available_mb": 98200.2, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 27578.7, "ram_available_mb": 98193.5, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.13, "peak": 40.18, "min": 22.07}, "VIN": {"avg": 75.46, "peak": 120.33, "min": 29.7}, "VIN_SYS_5V0": {"avg": 15.18, "peak": 16.15, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 16.31, "peak": 17.71, "min": 14.18}}, "power_watts_avg": 32.13, "energy_joules_est": 51.33, "sample_count": 16, "duration_seconds": 1.598}, "timestamp": "2026-01-19T15:04:38.299741"}
{"image_index": 298, "image_name": "000000032285.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032285.jpg", "image_width": 640, "image_height": 423, "image_resolution": "640x423", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1793.928, "latencies_ms": [1793.928], "images_per_second": 0.557, "prompt_tokens": 1099, "response_tokens_est": 27, "n_tiles": 1, "output_text": " The image depicts a small bathroom with a white toilet, a white bathtub, and a white door, all set against a yellow wall.", "error": null, "sys_before": {"cpu_percent": 14.3, "ram_used_mb": 27578.7, "ram_available_mb": 98193.5, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 27592.8, "ram_available_mb": 98179.4, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.25, "peak": 40.57, "min": 18.53}, "VIN": {"avg": 72.58, "peak": 129.32, "min": 27.37}, "VIN_SYS_5V0": {"avg": 14.82, "peak": 16.05, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.99, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 30.25, "energy_joules_est": 54.29, "sample_count": 18, "duration_seconds": 1.795}, "timestamp": "2026-01-19T15:04:40.184720"}
{"image_index": 298, "image_name": "000000032285.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032285.jpg", "image_width": 640, "image_height": 423, "image_resolution": "640x423", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2093.301, "latencies_ms": [2093.301], "images_per_second": 0.478, "prompt_tokens": 1113, "response_tokens_est": 39, "n_tiles": 1, "output_text": " toilet: 1, towel: 1, closet: 1, shelf: 1, door: 1, bathtub: 1, curtain: 1, wall: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27592.8, "ram_available_mb": 98179.4, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 27606.2, "ram_available_mb": 98166.0, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.76, "peak": 40.57, "min": 17.35}, "VIN": {"avg": 75.4, "peak": 132.29, "min": 28.05}, "VIN_SYS_5V0": {"avg": 14.86, "peak": 16.05, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.88, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 28.76, "energy_joules_est": 60.21, "sample_count": 21, "duration_seconds": 2.094}, "timestamp": "2026-01-19T15:04:42.374952"}
{"image_index": 298, "image_name": "000000032285.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032285.jpg", "image_width": 640, "image_height": 423, "image_resolution": "640x423", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2184.748, "latencies_ms": [2184.748], "images_per_second": 0.458, "prompt_tokens": 1117, "response_tokens_est": 42, "n_tiles": 1, "output_text": " The toilet is located on the left side of the image, while the shower curtain is on the right side. The closet is positioned in the background, and the towel is hanging on the wall above the toilet.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27606.2, "ram_available_mb": 98166.0, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 6.5, "ram_used_mb": 27537.4, "ram_available_mb": 98234.8, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.48, "peak": 40.16, "min": 15.77}, "VIN": {"avg": 70.02, "peak": 128.36, "min": 33.12}, "VIN_SYS_5V0": {"avg": 14.97, "peak": 16.05, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 16.14, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 28.48, "energy_joules_est": 62.23, "sample_count": 21, "duration_seconds": 2.185}, "timestamp": "2026-01-19T15:04:44.568812"}
{"image_index": 298, "image_name": "000000032285.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032285.jpg", "image_width": 640, "image_height": 423, "image_resolution": "640x423", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1515.628, "latencies_ms": [1515.628], "images_per_second": 0.66, "prompt_tokens": 1111, "response_tokens_est": 17, "n_tiles": 1, "output_text": " A small bathroom with a white toilet, a shower curtain, and a towel rack.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27537.4, "ram_available_mb": 98234.8, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 27537.4, "ram_available_mb": 98234.8, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.67, "peak": 40.57, "min": 17.73}, "VIN": {"avg": 75.83, "peak": 112.95, "min": 30.09}, "VIN_SYS_5V0": {"avg": 15.26, "peak": 16.26, "min": 13.29}, "VDD_CPU_SOC_MSS": {"avg": 16.4, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 31.67, "energy_joules_est": 48.01, "sample_count": 15, "duration_seconds": 1.516}, "timestamp": "2026-01-19T15:04:46.128452"}
{"image_index": 298, "image_name": "000000032285.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032285.jpg", "image_width": 640, "image_height": 423, "image_resolution": "640x423", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1565.34, "latencies_ms": [1565.34], "images_per_second": 0.639, "prompt_tokens": 1109, "response_tokens_est": 19, "n_tiles": 1, "output_text": " The bathroom is painted in a light yellow color, and the floor is covered with brown tiles.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27537.4, "ram_available_mb": 98234.8, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 27537.4, "ram_available_mb": 98234.8, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.93, "peak": 40.95, "min": 20.49}, "VIN": {"avg": 75.22, "peak": 119.78, "min": 29.02}, "VIN_SYS_5V0": {"avg": 15.06, "peak": 16.15, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 16.09, "peak": 17.32, "min": 13.78}}, "power_watts_avg": 31.93, "energy_joules_est": 49.99, "sample_count": 16, "duration_seconds": 1.566}, "timestamp": "2026-01-19T15:04:47.793202"}
{"image_index": 299, "image_name": "000000032334.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032334.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1388.515, "latencies_ms": [1388.515], "images_per_second": 0.72, "prompt_tokens": 1099, "response_tokens_est": 12, "n_tiles": 1, "output_text": " Two men are holding wine glasses and smiling at the camera.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 27537.4, "ram_available_mb": 98234.8, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 27552.9, "ram_available_mb": 98219.3, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.36, "peak": 40.56, "min": 18.52}, "VIN": {"avg": 78.69, "peak": 123.12, "min": 29.53}, "VIN_SYS_5V0": {"avg": 14.77, "peak": 16.05, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.89, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 32.36, "energy_joules_est": 44.95, "sample_count": 14, "duration_seconds": 1.389}, "timestamp": "2026-01-19T15:04:49.259232"}
{"image_index": 299, "image_name": "000000032334.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032334.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2531.56, "latencies_ms": [2531.56], "images_per_second": 0.395, "prompt_tokens": 1113, "response_tokens_est": 56, "n_tiles": 1, "output_text": " 1. glasses: 2\n2. people: 3\n3. table: 1\n4. chair: 1\n5. wall: 1\n6. door: 1\n7. window: 1\n8. light: 1", "error": null, "sys_before": {"cpu_percent": 40.0, "ram_used_mb": 27552.9, "ram_available_mb": 98219.3, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 27556.0, "ram_available_mb": 98216.2, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.18, "peak": 40.97, "min": 18.13}, "VIN": {"avg": 71.33, "peak": 107.58, "min": 28.77}, "VIN_SYS_5V0": {"avg": 14.95, "peak": 16.05, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 16.13, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 28.18, "energy_joules_est": 71.36, "sample_count": 25, "duration_seconds": 2.532}, "timestamp": "2026-01-19T15:04:51.869810"}
{"image_index": 299, "image_name": "000000032334.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032334.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3102.863, "latencies_ms": [3102.863], "images_per_second": 0.322, "prompt_tokens": 1117, "response_tokens_est": 78, "n_tiles": 1, "output_text": " The two people are in the foreground, with the man on the right holding a wine glass and the woman on the left holding a wine glass. The wine glasses are in the middle of the image, with the man on the right holding a wine glass and the woman on the left holding a wine glass. The man on the right is closer to the camera than the woman on the left.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27556.0, "ram_available_mb": 98216.2, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 27555.8, "ram_available_mb": 98216.4, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 25.83, "peak": 40.18, "min": 15.38}, "VIN": {"avg": 74.05, "peak": 130.72, "min": 29.0}, "VIN_SYS_5V0": {"avg": 14.96, "peak": 16.05, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 15.91, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 25.83, "energy_joules_est": 80.15, "sample_count": 30, "duration_seconds": 3.103}, "timestamp": "2026-01-19T15:04:54.998264"}
{"image_index": 299, "image_name": "000000032334.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032334.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1659.633, "latencies_ms": [1659.633], "images_per_second": 0.603, "prompt_tokens": 1111, "response_tokens_est": 23, "n_tiles": 1, "output_text": " Two people are holding wine glasses and smiling at the camera. They are in a room with a table and chairs.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27555.8, "ram_available_mb": 98216.4, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 27555.2, "ram_available_mb": 98217.0, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.05, "peak": 40.56, "min": 16.56}, "VIN": {"avg": 71.21, "peak": 121.32, "min": 28.9}, "VIN_SYS_5V0": {"avg": 15.23, "peak": 16.15, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 16.29, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 31.05, "energy_joules_est": 51.54, "sample_count": 16, "duration_seconds": 1.66}, "timestamp": "2026-01-19T15:04:56.670648"}
{"image_index": 299, "image_name": "000000032334.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032334.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1929.255, "latencies_ms": [1929.255], "images_per_second": 0.518, "prompt_tokens": 1109, "response_tokens_est": 34, "n_tiles": 1, "output_text": " The image is taken in a well-lit room with natural light coming through the windows. The colors in the image are vibrant and the materials are mostly wood and metal.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27555.2, "ram_available_mb": 98217.0, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 27565.4, "ram_available_mb": 98206.8, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.4, "peak": 40.16, "min": 20.11}, "VIN": {"avg": 68.45, "peak": 99.31, "min": 29.6}, "VIN_SYS_5V0": {"avg": 15.23, "peak": 16.26, "min": 13.29}, "VDD_CPU_SOC_MSS": {"avg": 16.37, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 30.4, "energy_joules_est": 58.66, "sample_count": 19, "duration_seconds": 1.93}, "timestamp": "2026-01-19T15:04:58.664887"}
{"image_index": 300, "image_name": "000000032570.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032570.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1704.358, "latencies_ms": [1704.358], "images_per_second": 0.587, "prompt_tokens": 1099, "response_tokens_est": 23, "n_tiles": 1, "output_text": " A surfer is riding a wave in the ocean, with a clear blue sky and a beach in the background.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 27565.4, "ram_available_mb": 98206.8, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 27578.4, "ram_available_mb": 98193.8, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.24, "peak": 40.56, "min": 16.95}, "VIN": {"avg": 75.72, "peak": 123.75, "min": 29.72}, "VIN_SYS_5V0": {"avg": 14.81, "peak": 16.05, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.96, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 30.24, "energy_joules_est": 51.55, "sample_count": 17, "duration_seconds": 1.705}, "timestamp": "2026-01-19T15:05:00.447884"}
{"image_index": 300, "image_name": "000000032570.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032570.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2595.565, "latencies_ms": [2595.565], "images_per_second": 0.385, "prompt_tokens": 1113, "response_tokens_est": 58, "n_tiles": 1, "output_text": " 1. surfer: 1\n2. surfboard: 1\n3. wave: 1\n4. ocean: 1\n5. sky: 1\n6. beach: 1\n7. sand: 1\n8. clouds: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 27578.4, "ram_available_mb": 98193.8, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 27582.1, "ram_available_mb": 98190.1, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.52, "peak": 40.18, "min": 18.14}, "VIN": {"avg": 72.85, "peak": 120.3, "min": 32.89}, "VIN_SYS_5V0": {"avg": 14.93, "peak": 16.05, "min": 12.15}, "VDD_CPU_SOC_MSS": {"avg": 16.01, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 27.52, "energy_joules_est": 71.44, "sample_count": 25, "duration_seconds": 2.596}, "timestamp": "2026-01-19T15:05:03.048447"}
{"image_index": 300, "image_name": "000000032570.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032570.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2768.515, "latencies_ms": [2768.515], "images_per_second": 0.361, "prompt_tokens": 1117, "response_tokens_est": 66, "n_tiles": 1, "output_text": " The surfer is positioned in the foreground, riding a wave that dominates the majority of the image. The wave is in the middle ground, with the surfer's body positioned in the center of the wave. The background features a sandy beach and a clear sky, providing a serene contrast to the dynamic action of the surfer.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27582.1, "ram_available_mb": 98190.1, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 27593.7, "ram_available_mb": 98178.5, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.92, "peak": 40.56, "min": 17.34}, "VIN": {"avg": 68.57, "peak": 125.25, "min": 29.22}, "VIN_SYS_5V0": {"avg": 15.15, "peak": 16.26, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 16.26, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 26.92, "energy_joules_est": 74.54, "sample_count": 27, "duration_seconds": 2.769}, "timestamp": "2026-01-19T15:05:05.856404"}
{"image_index": 300, "image_name": "000000032570.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032570.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1653.659, "latencies_ms": [1653.659], "images_per_second": 0.605, "prompt_tokens": 1111, "response_tokens_est": 22, "n_tiles": 1, "output_text": " A surfer is riding a wave in the ocean, with a beach and clear blue sky in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27593.7, "ram_available_mb": 98178.5, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 27593.7, "ram_available_mb": 98178.5, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.8, "peak": 40.16, "min": 16.16}, "VIN": {"avg": 71.44, "peak": 95.09, "min": 29.24}, "VIN_SYS_5V0": {"avg": 15.05, "peak": 16.15, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 16.17, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 30.8, "energy_joules_est": 50.95, "sample_count": 16, "duration_seconds": 1.654}, "timestamp": "2026-01-19T15:05:07.526535"}
{"image_index": 300, "image_name": "000000032570.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032570.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2290.473, "latencies_ms": [2290.473], "images_per_second": 0.437, "prompt_tokens": 1109, "response_tokens_est": 47, "n_tiles": 1, "output_text": " The image captures a surfer riding a wave in a clear blue ocean, with the sun shining brightly in the background. The surfer is wearing a black wetsuit and blue shorts, and the wave is a light blue color.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27593.7, "ram_available_mb": 98178.5, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 27593.7, "ram_available_mb": 98178.5, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.57, "peak": 40.95, "min": 17.73}, "VIN": {"avg": 70.73, "peak": 99.61, "min": 28.71}, "VIN_SYS_5V0": {"avg": 15.05, "peak": 16.15, "min": 13.06}, "VDD_CPU_SOC_MSS": {"avg": 16.17, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 28.57, "energy_joules_est": 65.46, "sample_count": 23, "duration_seconds": 2.291}, "timestamp": "2026-01-19T15:05:09.922622"}
{"image_index": 301, "image_name": "000000032610.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032610.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1603.886, "latencies_ms": [1603.886], "images_per_second": 0.623, "prompt_tokens": 1099, "response_tokens_est": 20, "n_tiles": 1, "output_text": " A collection of laptops, tablets, and a red and black backpack are arranged on a wooden table.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27593.7, "ram_available_mb": 98178.5, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 27597.7, "ram_available_mb": 98174.5, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.36, "peak": 40.18, "min": 15.38}, "VIN": {"avg": 69.14, "peak": 127.17, "min": 29.46}, "VIN_SYS_5V0": {"avg": 14.7, "peak": 15.95, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 15.82, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 30.36, "energy_joules_est": 48.71, "sample_count": 16, "duration_seconds": 1.604}, "timestamp": "2026-01-19T15:05:11.594330"}
{"image_index": 301, "image_name": "000000032610.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032610.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 4402.672, "latencies_ms": [4402.672], "images_per_second": 0.227, "prompt_tokens": 1113, "response_tokens_est": 128, "n_tiles": 1, "output_text": " laptop: 3, laptop: 1, laptop: 1, laptop: 1, laptop: 1, laptop: 1, laptop: 1, laptop: 1, backpack: 1, cables: 1, cables: 1, cables: 1, cables: 1, cables: 1, cables: 1, cables: 1, cables: 1, cables: 1, cables: 1, cables: 1, cables: 1, cables: 1, cables: 1, cables: 1, cables: 1, cables: ", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27597.7, "ram_available_mb": 98174.5, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 27598.7, "ram_available_mb": 98173.5, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 24.16, "peak": 40.56, "min": 17.73}, "VIN": {"avg": 67.62, "peak": 128.74, "min": 28.95}, "VIN_SYS_5V0": {"avg": 14.93, "peak": 16.05, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.9, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 24.16, "energy_joules_est": 106.38, "sample_count": 43, "duration_seconds": 4.403}, "timestamp": "2026-01-19T15:05:16.066783"}
{"image_index": 301, "image_name": "000000032610.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032610.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2405.413, "latencies_ms": [2405.413], "images_per_second": 0.416, "prompt_tokens": 1117, "response_tokens_est": 51, "n_tiles": 1, "output_text": " The laptop computer is positioned to the left of the backpack, while the tablet computer is situated to the right of the backpack. The cables are spread out in front of the backpack, with some extending towards the laptop computer and others extending towards the tablet computer.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27598.7, "ram_available_mb": 98173.5, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 27599.0, "ram_available_mb": 98173.2, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.28, "peak": 40.18, "min": 15.38}, "VIN": {"avg": 63.07, "peak": 118.24, "min": 28.28}, "VIN_SYS_5V0": {"avg": 14.86, "peak": 16.05, "min": 12.15}, "VDD_CPU_SOC_MSS": {"avg": 15.85, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 27.28, "energy_joules_est": 65.63, "sample_count": 24, "duration_seconds": 2.406}, "timestamp": "2026-01-19T15:05:18.562730"}
{"image_index": 301, "image_name": "000000032610.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032610.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1487.035, "latencies_ms": [1487.035], "images_per_second": 0.672, "prompt_tokens": 1111, "response_tokens_est": 14, "n_tiles": 1, "output_text": " A collection of laptops and a backpack are placed on a wooden table.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27599.0, "ram_available_mb": 98173.2, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 27608.4, "ram_available_mb": 98163.8, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.57, "peak": 40.18, "min": 15.38}, "VIN": {"avg": 71.28, "peak": 102.72, "min": 27.97}, "VIN_SYS_5V0": {"avg": 14.58, "peak": 15.95, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.83, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 30.57, "energy_joules_est": 45.48, "sample_count": 15, "duration_seconds": 1.488}, "timestamp": "2026-01-19T15:05:20.133234"}
{"image_index": 301, "image_name": "000000032610.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032610.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2259.168, "latencies_ms": [2259.168], "images_per_second": 0.443, "prompt_tokens": 1109, "response_tokens_est": 43, "n_tiles": 1, "output_text": " The image is taken in a room with a brown carpet and a white wall. The lighting is dim, and the objects are illuminated by artificial light. The materials of the objects are metal, plastic, and fabric.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27608.4, "ram_available_mb": 98163.8, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 6.4, "ram_used_mb": 27576.0, "ram_available_mb": 98196.2, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.65, "peak": 40.57, "min": 19.31}, "VIN": {"avg": 67.43, "peak": 119.01, "min": 29.0}, "VIN_SYS_5V0": {"avg": 14.78, "peak": 15.85, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.86, "peak": 16.92, "min": 13.0}}, "power_watts_avg": 28.65, "energy_joules_est": 64.74, "sample_count": 22, "duration_seconds": 2.26}, "timestamp": "2026-01-19T15:05:22.424432"}
{"image_index": 302, "image_name": "000000032735.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032735.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1898.055, "latencies_ms": [1898.055], "images_per_second": 0.527, "prompt_tokens": 1099, "response_tokens_est": 32, "n_tiles": 1, "output_text": " A skier in a vibrant orange and green suit is captured mid-air, performing a jump on a snowy mountain, with a blurred figure in the background.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 27576.0, "ram_available_mb": 98196.2, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 27583.2, "ram_available_mb": 98188.9, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.38, "peak": 40.56, "min": 16.56}, "VIN": {"avg": 70.73, "peak": 107.62, "min": 27.43}, "VIN_SYS_5V0": {"avg": 15.01, "peak": 16.05, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 16.16, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 29.38, "energy_joules_est": 55.78, "sample_count": 19, "duration_seconds": 1.899}, "timestamp": "2026-01-19T15:05:24.409530"}
{"image_index": 302, "image_name": "000000032735.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032735.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2671.998, "latencies_ms": [2671.998], "images_per_second": 0.374, "prompt_tokens": 1113, "response_tokens_est": 58, "n_tiles": 1, "output_text": " 1. skier: 1\n2. skis: 2\n3. poles: 2\n4. helmet: 1\n5. jacket: 1\n6. pants: 1\n7. snow: 1\n8. sky: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27583.2, "ram_available_mb": 98188.9, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 27572.5, "ram_available_mb": 98199.7, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.72, "peak": 40.18, "min": 16.56}, "VIN": {"avg": 65.62, "peak": 117.11, "min": 28.89}, "VIN_SYS_5V0": {"avg": 14.79, "peak": 16.05, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.87, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 26.72, "energy_joules_est": 71.41, "sample_count": 26, "duration_seconds": 2.672}, "timestamp": "2026-01-19T15:05:27.110371"}
{"image_index": 302, "image_name": "000000032735.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032735.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2224.554, "latencies_ms": [2224.554], "images_per_second": 0.45, "prompt_tokens": 1117, "response_tokens_est": 44, "n_tiles": 1, "output_text": " The skier is in the foreground, with the snowboarder in the background. The skier is to the left of the snowboarder. The skier is closer to the camera than the snowboarder.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27572.5, "ram_available_mb": 98199.7, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 6.7, "ram_used_mb": 27546.9, "ram_available_mb": 98225.2, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.06, "peak": 40.18, "min": 16.16}, "VIN": {"avg": 69.86, "peak": 92.29, "min": 28.82}, "VIN_SYS_5V0": {"avg": 15.01, "peak": 16.15, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 16.3, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 28.06, "energy_joules_est": 62.44, "sample_count": 22, "duration_seconds": 2.225}, "timestamp": "2026-01-19T15:05:29.409792"}
{"image_index": 302, "image_name": "000000032735.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032735.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1796.598, "latencies_ms": [1796.598], "images_per_second": 0.557, "prompt_tokens": 1111, "response_tokens_est": 27, "n_tiles": 1, "output_text": " A skier in a vibrant orange and green suit is performing a jump in the snow, while another skier stands in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27546.9, "ram_available_mb": 98225.2, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 27547.6, "ram_available_mb": 98224.6, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.61, "peak": 40.18, "min": 16.16}, "VIN": {"avg": 77.06, "peak": 123.06, "min": 28.23}, "VIN_SYS_5V0": {"avg": 14.79, "peak": 16.05, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 15.99, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 29.61, "energy_joules_est": 53.21, "sample_count": 18, "duration_seconds": 1.797}, "timestamp": "2026-01-19T15:05:31.286311"}
{"image_index": 302, "image_name": "000000032735.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032735.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2337.947, "latencies_ms": [2337.947], "images_per_second": 0.428, "prompt_tokens": 1109, "response_tokens_est": 48, "n_tiles": 1, "output_text": " The skier is wearing a vibrant orange and green outfit, and the snow is a pristine white. The skier is in mid-air, performing a jump, and the sun is shining brightly, casting a warm glow on the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27547.6, "ram_available_mb": 98224.6, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 27556.0, "ram_available_mb": 98216.2, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.78, "peak": 40.16, "min": 16.95}, "VIN": {"avg": 67.77, "peak": 119.26, "min": 28.96}, "VIN_SYS_5V0": {"avg": 14.79, "peak": 16.05, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 15.92, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 27.78, "energy_joules_est": 64.97, "sample_count": 23, "duration_seconds": 2.339}, "timestamp": "2026-01-19T15:05:33.688206"}
{"image_index": 303, "image_name": "000000032811.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032811.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1482.413, "latencies_ms": [1482.413], "images_per_second": 0.675, "prompt_tokens": 1100, "response_tokens_est": 16, "n_tiles": 1, "output_text": " A bird is perched on a window sill, looking out at the water.", "error": null, "sys_before": {"cpu_percent": 14.3, "ram_used_mb": 27556.0, "ram_available_mb": 98216.2, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 27563.8, "ram_available_mb": 98208.4, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.81, "peak": 40.18, "min": 16.16}, "VIN": {"avg": 68.91, "peak": 105.77, "min": 30.42}, "VIN_SYS_5V0": {"avg": 14.87, "peak": 16.05, "min": 12.25}, "VDD_CPU_SOC_MSS": {"avg": 16.01, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 30.81, "energy_joules_est": 45.7, "sample_count": 15, "duration_seconds": 1.483}, "timestamp": "2026-01-19T15:05:35.264806"}
{"image_index": 303, "image_name": "000000032811.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032811.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2684.942, "latencies_ms": [2684.942], "images_per_second": 0.372, "prompt_tokens": 1114, "response_tokens_est": 62, "n_tiles": 1, "output_text": " 1. window: 1\n2. bird: 1\n3. door: 1\n4. wall: 1\n5. door handle: 1\n6. window frame: 1\n7. window latch: 1\n8. bird's beak: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27563.8, "ram_available_mb": 98208.4, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 27578.2, "ram_available_mb": 98193.9, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.29, "peak": 40.18, "min": 19.31}, "VIN": {"avg": 69.26, "peak": 118.62, "min": 29.16}, "VIN_SYS_5V0": {"avg": 14.89, "peak": 16.05, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.98, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 27.29, "energy_joules_est": 73.28, "sample_count": 26, "duration_seconds": 2.685}, "timestamp": "2026-01-19T15:05:37.974116"}
{"image_index": 303, "image_name": "000000032811.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032811.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2761.586, "latencies_ms": [2761.586], "images_per_second": 0.362, "prompt_tokens": 1118, "response_tokens_est": 66, "n_tiles": 1, "output_text": " The bird is positioned in the foreground, on the right side of the window, and looking out towards the water. The window is located on the left side of the image, and the bird is positioned in the center of the window. The background of the image is a body of water, with a shoreline visible in the distance.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27578.2, "ram_available_mb": 98193.9, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 27580.2, "ram_available_mb": 98192.0, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.66, "peak": 39.77, "min": 16.95}, "VIN": {"avg": 73.33, "peak": 127.24, "min": 29.84}, "VIN_SYS_5V0": {"avg": 15.13, "peak": 16.15, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 16.17, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 26.66, "energy_joules_est": 73.64, "sample_count": 27, "duration_seconds": 2.762}, "timestamp": "2026-01-19T15:05:40.789846"}
{"image_index": 303, "image_name": "000000032811.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032811.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1486.516, "latencies_ms": [1486.516], "images_per_second": 0.673, "prompt_tokens": 1112, "response_tokens_est": 16, "n_tiles": 1, "output_text": " A bird is perched on a window sill, looking out at the water.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27580.2, "ram_available_mb": 98192.0, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 27583.5, "ram_available_mb": 98188.7, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.7, "peak": 39.78, "min": 15.77}, "VIN": {"avg": 77.15, "peak": 120.52, "min": 29.59}, "VIN_SYS_5V0": {"avg": 14.96, "peak": 16.26, "min": 12.46}, "VDD_CPU_SOC_MSS": {"avg": 16.01, "peak": 17.71, "min": 13.39}}, "power_watts_avg": 30.7, "energy_joules_est": 45.64, "sample_count": 15, "duration_seconds": 1.487}, "timestamp": "2026-01-19T15:05:42.355570"}
{"image_index": 303, "image_name": "000000032811.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032811.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2646.34, "latencies_ms": [2646.34], "images_per_second": 0.378, "prompt_tokens": 1110, "response_tokens_est": 61, "n_tiles": 1, "output_text": " The image features a bird perched on a window sill, with a view of a body of water in the background. The bird is black, and the window sill is made of wood. The lighting in the image is natural, coming from the outside, and the weather appears to be overcast.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27583.5, "ram_available_mb": 98188.7, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 27591.6, "ram_available_mb": 98180.6, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.38, "peak": 40.56, "min": 18.52}, "VIN": {"avg": 70.77, "peak": 122.54, "min": 30.87}, "VIN_SYS_5V0": {"avg": 14.95, "peak": 16.15, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 16.01, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 27.38, "energy_joules_est": 72.46, "sample_count": 26, "duration_seconds": 2.647}, "timestamp": "2026-01-19T15:05:45.056859"}
{"image_index": 304, "image_name": "000000032817.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032817.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1660.973, "latencies_ms": [1660.973], "images_per_second": 0.602, "prompt_tokens": 1100, "response_tokens_est": 23, "n_tiles": 1, "output_text": " Two men are cleaning a bathroom with a bucket and a mop, and there is a toilet in the room.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 27591.6, "ram_available_mb": 98180.6, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 27592.4, "ram_available_mb": 98179.7, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.48, "peak": 39.77, "min": 15.77}, "VIN": {"avg": 69.97, "peak": 100.6, "min": 32.68}, "VIN_SYS_5V0": {"avg": 15.03, "peak": 16.26, "min": 12.36}, "VDD_CPU_SOC_MSS": {"avg": 16.12, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 30.48, "energy_joules_est": 50.65, "sample_count": 16, "duration_seconds": 1.662}, "timestamp": "2026-01-19T15:05:46.729260"}
{"image_index": 304, "image_name": "000000032817.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032817.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2135.561, "latencies_ms": [2135.561], "images_per_second": 0.468, "prompt_tokens": 1114, "response_tokens_est": 42, "n_tiles": 1, "output_text": " toilet: 1, trash can: 1, trash bag: 1, bucket: 1, person: 2, shelf: 1, bottle: 1, cupboard: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27592.4, "ram_available_mb": 98179.7, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 6.6, "ram_used_mb": 27556.2, "ram_available_mb": 98215.9, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.36, "peak": 40.16, "min": 19.31}, "VIN": {"avg": 71.88, "peak": 105.88, "min": 30.82}, "VIN_SYS_5V0": {"avg": 15.31, "peak": 16.26, "min": 13.39}, "VDD_CPU_SOC_MSS": {"avg": 16.61, "peak": 17.71, "min": 14.57}}, "power_watts_avg": 29.36, "energy_joules_est": 62.71, "sample_count": 21, "duration_seconds": 2.136}, "timestamp": "2026-01-19T15:05:48.918967"}
{"image_index": 304, "image_name": "000000032817.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032817.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2236.631, "latencies_ms": [2236.631], "images_per_second": 0.447, "prompt_tokens": 1118, "response_tokens_est": 45, "n_tiles": 1, "output_text": " The toilet is located in the foreground, with the trash can and cleaning supplies in the background. The person in the orange hat is standing near the toilet, while the other person is standing in the background, near the shelf.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 27556.2, "ram_available_mb": 98215.9, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 27553.2, "ram_available_mb": 98219.0, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.24, "peak": 40.18, "min": 16.56}, "VIN": {"avg": 70.57, "peak": 117.51, "min": 28.91}, "VIN_SYS_5V0": {"avg": 14.99, "peak": 16.15, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 16.14, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 28.24, "energy_joules_est": 63.19, "sample_count": 22, "duration_seconds": 2.237}, "timestamp": "2026-01-19T15:05:51.199457"}
{"image_index": 304, "image_name": "000000032817.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032817.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1430.125, "latencies_ms": [1430.125], "images_per_second": 0.699, "prompt_tokens": 1112, "response_tokens_est": 14, "n_tiles": 1, "output_text": " Two men are cleaning a bathroom with a bucket and a mop.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 27553.2, "ram_available_mb": 98219.0, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 27553.6, "ram_available_mb": 98218.6, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.46, "peak": 39.77, "min": 16.56}, "VIN": {"avg": 65.05, "peak": 102.39, "min": 29.31}, "VIN_SYS_5V0": {"avg": 15.08, "peak": 16.26, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 16.28, "peak": 17.71, "min": 13.79}}, "power_watts_avg": 31.46, "energy_joules_est": 45.01, "sample_count": 14, "duration_seconds": 1.431}, "timestamp": "2026-01-19T15:05:52.662369"}
{"image_index": 304, "image_name": "000000032817.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032817.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1688.829, "latencies_ms": [1688.829], "images_per_second": 0.592, "prompt_tokens": 1110, "response_tokens_est": 24, "n_tiles": 1, "output_text": " The image is taken in a dimly lit room with a yellowish hue, and the floor is covered in blue paint.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27553.6, "ram_available_mb": 98218.6, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 27553.8, "ram_available_mb": 98218.3, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.61, "peak": 40.95, "min": 21.28}, "VIN": {"avg": 66.5, "peak": 88.75, "min": 29.57}, "VIN_SYS_5V0": {"avg": 15.11, "peak": 16.26, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 16.26, "peak": 17.71, "min": 14.18}}, "power_watts_avg": 31.61, "energy_joules_est": 53.41, "sample_count": 17, "duration_seconds": 1.69}, "timestamp": "2026-01-19T15:05:54.435625"}
{"image_index": 305, "image_name": "000000032861.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032861.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2014.016, "latencies_ms": [2014.016], "images_per_second": 0.497, "prompt_tokens": 1100, "response_tokens_est": 36, "n_tiles": 1, "output_text": " In the image, a person is seen walking down a hallway, holding an umbrella with the word \"opera\" written on it, under a rain-soaked sky.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 27553.8, "ram_available_mb": 98218.3, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 27561.6, "ram_available_mb": 98210.6, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.07, "peak": 40.56, "min": 17.74}, "VIN": {"avg": 63.36, "peak": 95.12, "min": 27.98}, "VIN_SYS_5V0": {"avg": 14.78, "peak": 16.05, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.98, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 29.07, "energy_joules_est": 58.57, "sample_count": 20, "duration_seconds": 2.015}, "timestamp": "2026-01-19T15:05:56.522147"}
{"image_index": 305, "image_name": "000000032861.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032861.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2527.169, "latencies_ms": [2527.169], "images_per_second": 0.396, "prompt_tokens": 1114, "response_tokens_est": 56, "n_tiles": 1, "output_text": " 1. person: 1\n2. umbrella: 1\n3. wall: 2\n4. door: 1\n5. floor: 1\n6. light: 1\n7. picture: 1\n8. text: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27561.6, "ram_available_mb": 98210.6, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 27560.9, "ram_available_mb": 98211.3, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.06, "peak": 40.18, "min": 16.56}, "VIN": {"avg": 72.47, "peak": 132.6, "min": 29.43}, "VIN_SYS_5V0": {"avg": 14.9, "peak": 16.15, "min": 12.25}, "VDD_CPU_SOC_MSS": {"avg": 16.02, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 27.06, "energy_joules_est": 68.39, "sample_count": 25, "duration_seconds": 2.527}, "timestamp": "2026-01-19T15:05:59.127978"}
{"image_index": 305, "image_name": "000000032861.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032861.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2308.131, "latencies_ms": [2308.131], "images_per_second": 0.433, "prompt_tokens": 1118, "response_tokens_est": 45, "n_tiles": 1, "output_text": " The person is standing in the middle of the hallway, with the door on the right and the wall on the left. The person is holding the umbrella in front of them, which is blocking the view of the door.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27560.9, "ram_available_mb": 98211.3, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 27570.1, "ram_available_mb": 98202.1, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.37, "peak": 39.78, "min": 15.38}, "VIN": {"avg": 70.55, "peak": 119.9, "min": 29.4}, "VIN_SYS_5V0": {"avg": 14.77, "peak": 16.05, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 15.87, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 27.37, "energy_joules_est": 63.18, "sample_count": 23, "duration_seconds": 2.308}, "timestamp": "2026-01-19T15:06:01.516505"}
{"image_index": 305, "image_name": "000000032861.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032861.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1715.768, "latencies_ms": [1715.768], "images_per_second": 0.583, "prompt_tokens": 1112, "response_tokens_est": 23, "n_tiles": 1, "output_text": " A person is walking down a hallway with a black umbrella that has the word \"opera\" on it.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 27570.1, "ram_available_mb": 98202.1, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 27583.9, "ram_available_mb": 98188.3, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.5, "peak": 39.39, "min": 15.77}, "VIN": {"avg": 65.77, "peak": 101.03, "min": 28.92}, "VIN_SYS_5V0": {"avg": 14.71, "peak": 16.15, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.82, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 29.5, "energy_joules_est": 50.63, "sample_count": 17, "duration_seconds": 1.716}, "timestamp": "2026-01-19T15:06:03.292292"}
{"image_index": 305, "image_name": "000000032861.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032861.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2402.469, "latencies_ms": [2402.469], "images_per_second": 0.416, "prompt_tokens": 1110, "response_tokens_est": 49, "n_tiles": 1, "output_text": " The image features a person holding an umbrella with the word \"opera\" written on it, standing in a hallway with red walls and a white door. The lighting is dim, and the rain is falling outside, creating a dramatic atmosphere.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 27583.9, "ram_available_mb": 98188.3, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 27584.2, "ram_available_mb": 98187.9, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.48, "peak": 40.18, "min": 17.34}, "VIN": {"avg": 68.75, "peak": 125.88, "min": 28.26}, "VIN_SYS_5V0": {"avg": 14.82, "peak": 16.05, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.83, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 27.48, "energy_joules_est": 66.03, "sample_count": 24, "duration_seconds": 2.403}, "timestamp": "2026-01-19T15:06:05.795683"}
{"image_index": 306, "image_name": "000000032887.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032887.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1910.139, "latencies_ms": [1910.139], "images_per_second": 0.524, "prompt_tokens": 1099, "response_tokens_est": 32, "n_tiles": 1, "output_text": " A man wearing a red shirt and a backpack is standing on a path in the woods, while another man in a yellow raincoat is standing on a staircase.", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 27584.2, "ram_available_mb": 98187.9, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 27588.8, "ram_available_mb": 98183.4, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.93, "peak": 40.18, "min": 14.59}, "VIN": {"avg": 77.39, "peak": 126.09, "min": 28.65}, "VIN_SYS_5V0": {"avg": 14.7, "peak": 15.95, "min": 11.54}, "VDD_CPU_SOC_MSS": {"avg": 15.81, "peak": 17.32, "min": 12.61}}, "power_watts_avg": 28.93, "energy_joules_est": 55.29, "sample_count": 19, "duration_seconds": 1.911}, "timestamp": "2026-01-19T15:06:07.782473"}
{"image_index": 306, "image_name": "000000032887.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032887.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2549.455, "latencies_ms": [2549.455], "images_per_second": 0.392, "prompt_tokens": 1113, "response_tokens_est": 57, "n_tiles": 1, "output_text": " 1. man: 1\n2. backpack: 1\n3. signpost: 1\n4. man: 1\n5. man: 1\n6. man: 1\n7. man: 1\n8. man: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27588.8, "ram_available_mb": 98183.4, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 27596.4, "ram_available_mb": 98175.8, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.39, "peak": 40.57, "min": 16.95}, "VIN": {"avg": 70.8, "peak": 113.89, "min": 29.78}, "VIN_SYS_5V0": {"avg": 14.93, "peak": 16.05, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.95, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 27.39, "energy_joules_est": 69.84, "sample_count": 25, "duration_seconds": 2.55}, "timestamp": "2026-01-19T15:06:10.385549"}
{"image_index": 306, "image_name": "000000032887.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032887.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2346.942, "latencies_ms": [2346.942], "images_per_second": 0.426, "prompt_tokens": 1117, "response_tokens_est": 49, "n_tiles": 1, "output_text": " The man in the red shirt is standing in the foreground, while the man in the yellow raincoat is standing on the steps in the background. The man in the red shirt is closer to the camera than the man in the yellow raincoat.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27596.4, "ram_available_mb": 98175.8, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 27598.3, "ram_available_mb": 98173.8, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.85, "peak": 40.18, "min": 16.16}, "VIN": {"avg": 68.87, "peak": 93.77, "min": 30.52}, "VIN_SYS_5V0": {"avg": 14.99, "peak": 16.05, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.97, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 27.85, "energy_joules_est": 65.38, "sample_count": 23, "duration_seconds": 2.347}, "timestamp": "2026-01-19T15:06:12.781218"}
{"image_index": 306, "image_name": "000000032887.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032887.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1913.112, "latencies_ms": [1913.112], "images_per_second": 0.523, "prompt_tokens": 1111, "response_tokens_est": 32, "n_tiles": 1, "output_text": " A man in a red shirt and backpack stands next to a signpost in a forest, while another man in a yellow jacket walks up a set of stairs.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27598.3, "ram_available_mb": 98173.8, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 27610.0, "ram_available_mb": 98162.2, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.36, "peak": 40.57, "min": 16.16}, "VIN": {"avg": 69.84, "peak": 127.42, "min": 28.78}, "VIN_SYS_5V0": {"avg": 14.91, "peak": 16.05, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 16.0, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 29.36, "energy_joules_est": 56.18, "sample_count": 19, "duration_seconds": 1.913}, "timestamp": "2026-01-19T15:06:14.758953"}
{"image_index": 306, "image_name": "000000032887.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032887.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2065.5, "latencies_ms": [2065.5], "images_per_second": 0.484, "prompt_tokens": 1109, "response_tokens_est": 38, "n_tiles": 1, "output_text": " The image features a man wearing a red shirt and blue jeans, standing on a rocky path in a lush green forest. The lighting is natural, and the weather appears to be overcast.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27610.0, "ram_available_mb": 98162.2, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 27612.2, "ram_available_mb": 98160.0, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.17, "peak": 40.56, "min": 17.35}, "VIN": {"avg": 77.05, "peak": 122.95, "min": 29.06}, "VIN_SYS_5V0": {"avg": 14.94, "peak": 16.05, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 16.02, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 29.17, "energy_joules_est": 60.27, "sample_count": 20, "duration_seconds": 2.066}, "timestamp": "2026-01-19T15:06:16.855459"}
{"image_index": 307, "image_name": "000000032901.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032901.jpg", "image_width": 640, "image_height": 546, "image_resolution": "640x546", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2074.457, "latencies_ms": [2074.457], "images_per_second": 0.482, "prompt_tokens": 1432, "response_tokens_est": 23, "n_tiles": 1, "output_text": " Five men are standing together in a room, with one man wearing a red tie and another wearing a white shirt.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 27612.2, "ram_available_mb": 98160.0, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 27621.5, "ram_available_mb": 98150.7, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.2, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.81, "peak": 40.16, "min": 17.35}, "VIN": {"avg": 77.0, "peak": 125.68, "min": 33.72}, "VIN_SYS_5V0": {"avg": 15.53, "peak": 16.76, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 16.71, "peak": 18.11, "min": 14.18}}, "power_watts_avg": 31.81, "energy_joules_est": 66.0, "sample_count": 20, "duration_seconds": 2.075}, "timestamp": "2026-01-19T15:06:18.944706"}
{"image_index": 307, "image_name": "000000032901.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032901.jpg", "image_width": 640, "image_height": 546, "image_resolution": "640x546", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2953.584, "latencies_ms": [2953.584], "images_per_second": 0.339, "prompt_tokens": 1446, "response_tokens_est": 56, "n_tiles": 1, "output_text": " 1. chair: 2\n2. man: 5\n3. bottle: 1\n4. man: 1\n5. man: 1\n6. man: 1\n7. man: 1\n8. man: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27621.5, "ram_available_mb": 98150.7, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 27546.8, "ram_available_mb": 98225.4, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.11, "peak": 40.56, "min": 18.91}, "VIN": {"avg": 70.41, "peak": 120.1, "min": 29.9}, "VIN_SYS_5V0": {"avg": 15.31, "peak": 16.76, "min": 13.29}, "VDD_CPU_SOC_MSS": {"avg": 16.66, "peak": 18.89, "min": 14.57}}, "power_watts_avg": 29.11, "energy_joules_est": 85.99, "sample_count": 29, "duration_seconds": 2.954}, "timestamp": "2026-01-19T15:06:21.953555"}
{"image_index": 307, "image_name": "000000032901.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032901.jpg", "image_width": 640, "image_height": 546, "image_resolution": "640x546", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3012.374, "latencies_ms": [3012.374], "images_per_second": 0.332, "prompt_tokens": 1450, "response_tokens_est": 58, "n_tiles": 1, "output_text": " The man on the left is standing closer to the camera than the man on the right. The man in the middle is standing between the two men on the left and the man on the right. The man on the right is standing further away from the camera than the man on the left.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27546.8, "ram_available_mb": 98225.4, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 27539.6, "ram_available_mb": 98232.6, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.54, "peak": 40.18, "min": 16.16}, "VIN": {"avg": 70.08, "peak": 116.14, "min": 32.18}, "VIN_SYS_5V0": {"avg": 15.2, "peak": 16.56, "min": 12.36}, "VDD_CPU_SOC_MSS": {"avg": 16.32, "peak": 17.71, "min": 13.79}}, "power_watts_avg": 28.54, "energy_joules_est": 85.98, "sample_count": 29, "duration_seconds": 3.013}, "timestamp": "2026-01-19T15:06:24.972897"}
{"image_index": 307, "image_name": "000000032901.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032901.jpg", "image_width": 640, "image_height": 546, "image_resolution": "640x546", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1892.967, "latencies_ms": [1892.967], "images_per_second": 0.528, "prompt_tokens": 1444, "response_tokens_est": 17, "n_tiles": 1, "output_text": " Five men are posing for a picture in a room with a bar in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27539.6, "ram_available_mb": 98232.6, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 27536.7, "ram_available_mb": 98235.5, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.53, "peak": 40.18, "min": 17.34}, "VIN": {"avg": 74.04, "peak": 118.85, "min": 28.78}, "VIN_SYS_5V0": {"avg": 15.49, "peak": 16.76, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 16.66, "peak": 18.11, "min": 14.57}}, "power_watts_avg": 32.53, "energy_joules_est": 61.59, "sample_count": 19, "duration_seconds": 1.893}, "timestamp": "2026-01-19T15:06:26.946536"}
{"image_index": 307, "image_name": "000000032901.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032901.jpg", "image_width": 640, "image_height": 546, "image_resolution": "640x546", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2649.39, "latencies_ms": [2649.39], "images_per_second": 0.377, "prompt_tokens": 1442, "response_tokens_est": 44, "n_tiles": 1, "output_text": " The image is taken in a well-lit room with natural light coming from the windows. The colors in the image are mostly neutral, with the exception of the red chairs and the red and white banner in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27536.7, "ram_available_mb": 98235.5, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 27559.3, "ram_available_mb": 98212.9, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.92, "peak": 40.18, "min": 19.32}, "VIN": {"avg": 72.48, "peak": 117.42, "min": 28.99}, "VIN_SYS_5V0": {"avg": 15.13, "peak": 16.56, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 16.31, "peak": 17.71, "min": 13.39}}, "power_watts_avg": 29.92, "energy_joules_est": 79.29, "sample_count": 26, "duration_seconds": 2.65}, "timestamp": "2026-01-19T15:06:29.652343"}
{"image_index": 308, "image_name": "000000032941.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032941.jpg", "image_width": 458, "image_height": 640, "image_resolution": "458x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2132.754, "latencies_ms": [2132.754], "images_per_second": 0.469, "prompt_tokens": 1100, "response_tokens_est": 41, "n_tiles": 1, "output_text": " The image captures a bustling city street corner, where a yellow traffic sign stands out against the backdrop of a red traffic light, a black pole, and a white car parked on the side of the road.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 27559.3, "ram_available_mb": 98212.9, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 27564.3, "ram_available_mb": 98207.8, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.37, "peak": 39.77, "min": 16.56}, "VIN": {"avg": 72.37, "peak": 120.21, "min": 29.2}, "VIN_SYS_5V0": {"avg": 14.98, "peak": 16.15, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 16.09, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 28.37, "energy_joules_est": 60.53, "sample_count": 21, "duration_seconds": 2.134}, "timestamp": "2026-01-19T15:06:31.843412"}
{"image_index": 308, "image_name": "000000032941.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032941.jpg", "image_width": 458, "image_height": 640, "image_resolution": "458x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2801.577, "latencies_ms": [2801.577], "images_per_second": 0.357, "prompt_tokens": 1114, "response_tokens_est": 67, "n_tiles": 1, "output_text": " 1. pole: 1\n2. traffic light: 1\n3. street sign: 1\n4. yellow sign: 1\n5. black pole: 1\n6. black box: 1\n7. black box on pole: 1\n8. black box on ground: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27564.3, "ram_available_mb": 98207.8, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 27571.1, "ram_available_mb": 98201.1, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.62, "peak": 40.16, "min": 16.56}, "VIN": {"avg": 66.13, "peak": 112.53, "min": 29.64}, "VIN_SYS_5V0": {"avg": 15.03, "peak": 16.15, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 16.1, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 26.62, "energy_joules_est": 74.59, "sample_count": 27, "duration_seconds": 2.802}, "timestamp": "2026-01-19T15:06:34.659537"}
{"image_index": 308, "image_name": "000000032941.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032941.jpg", "image_width": 458, "image_height": 640, "image_resolution": "458x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2225.076, "latencies_ms": [2225.076], "images_per_second": 0.449, "prompt_tokens": 1118, "response_tokens_est": 45, "n_tiles": 1, "output_text": " The yellow sign is positioned to the right of the black pole, which is located in the foreground of the image. The sign is situated near the edge of the road, while the pole extends into the background of the scene.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 27571.1, "ram_available_mb": 98201.1, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 6.4, "ram_used_mb": 27556.2, "ram_available_mb": 98216.0, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.13, "peak": 39.77, "min": 16.55}, "VIN": {"avg": 72.13, "peak": 127.44, "min": 28.8}, "VIN_SYS_5V0": {"avg": 15.2, "peak": 16.26, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 16.41, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 28.13, "energy_joules_est": 62.6, "sample_count": 22, "duration_seconds": 2.225}, "timestamp": "2026-01-19T15:06:36.953422"}
{"image_index": 308, "image_name": "000000032941.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032941.jpg", "image_width": 458, "image_height": 640, "image_resolution": "458x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2277.695, "latencies_ms": [2277.695], "images_per_second": 0.439, "prompt_tokens": 1112, "response_tokens_est": 47, "n_tiles": 1, "output_text": " The image captures a bustling city street corner, where a yellow traffic sign stands out against the backdrop of a red traffic light and a blue bus. The street is lined with buildings, shops, and cars, creating a lively urban atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27556.2, "ram_available_mb": 98216.0, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 27550.1, "ram_available_mb": 98222.1, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.63, "peak": 39.78, "min": 16.16}, "VIN": {"avg": 68.38, "peak": 105.15, "min": 27.48}, "VIN_SYS_5V0": {"avg": 14.97, "peak": 16.26, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 16.0, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 27.63, "energy_joules_est": 62.94, "sample_count": 23, "duration_seconds": 2.278}, "timestamp": "2026-01-19T15:06:39.334164"}
{"image_index": 308, "image_name": "000000032941.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032941.jpg", "image_width": 458, "image_height": 640, "image_resolution": "458x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2088.861, "latencies_ms": [2088.861], "images_per_second": 0.479, "prompt_tokens": 1110, "response_tokens_est": 39, "n_tiles": 1, "output_text": " The image features a street corner with a yellow and black traffic sign, a black pole, and a red traffic light. The sky is cloudy, and the street is wet, suggesting recent rain.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27550.1, "ram_available_mb": 98222.1, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 27561.0, "ram_available_mb": 98211.2, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.07, "peak": 40.16, "min": 14.98}, "VIN": {"avg": 70.23, "peak": 127.51, "min": 29.34}, "VIN_SYS_5V0": {"avg": 14.79, "peak": 16.15, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.88, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 28.07, "energy_joules_est": 58.64, "sample_count": 21, "duration_seconds": 2.089}, "timestamp": "2026-01-19T15:06:41.514340"}
{"image_index": 309, "image_name": "000000033005.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033005.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1597.19, "latencies_ms": [1597.19], "images_per_second": 0.626, "prompt_tokens": 1099, "response_tokens_est": 19, "n_tiles": 1, "output_text": " A man wearing a white shirt and a white hat is playing tennis on a court at night.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 27561.0, "ram_available_mb": 98211.2, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 27574.9, "ram_available_mb": 98197.3, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.36, "peak": 40.16, "min": 15.77}, "VIN": {"avg": 75.94, "peak": 121.98, "min": 27.38}, "VIN_SYS_5V0": {"avg": 14.71, "peak": 16.05, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.82, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 30.36, "energy_joules_est": 48.51, "sample_count": 16, "duration_seconds": 1.598}, "timestamp": "2026-01-19T15:06:43.190022"}
{"image_index": 309, "image_name": "000000033005.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033005.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2677.177, "latencies_ms": [2677.177], "images_per_second": 0.374, "prompt_tokens": 1113, "response_tokens_est": 59, "n_tiles": 1, "output_text": " 1. person: 1\n2. tennis racket: 1\n3. tennis ball: 1\n4. fence: 1\n5. sign: 1\n6. net: 1\n7. court: 1\n8. player: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27574.9, "ram_available_mb": 98197.3, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 27574.3, "ram_available_mb": 98197.8, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.18, "peak": 40.57, "min": 18.53}, "VIN": {"avg": 68.0, "peak": 119.49, "min": 30.43}, "VIN_SYS_5V0": {"avg": 14.87, "peak": 16.05, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.83, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 27.18, "energy_joules_est": 72.77, "sample_count": 26, "duration_seconds": 2.677}, "timestamp": "2026-01-19T15:06:45.894848"}
{"image_index": 309, "image_name": "000000033005.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033005.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2240.727, "latencies_ms": [2240.727], "images_per_second": 0.446, "prompt_tokens": 1117, "response_tokens_est": 45, "n_tiles": 1, "output_text": " The tennis player is positioned on the left side of the image, with the tennis racket held in his right hand. The tennis court is located in the middle of the image, with the fence and advertisements in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27574.3, "ram_available_mb": 98197.8, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 27583.9, "ram_available_mb": 98188.3, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.15, "peak": 40.18, "min": 16.16}, "VIN": {"avg": 72.15, "peak": 108.59, "min": 30.15}, "VIN_SYS_5V0": {"avg": 15.02, "peak": 16.15, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 16.14, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 28.15, "energy_joules_est": 63.09, "sample_count": 22, "duration_seconds": 2.241}, "timestamp": "2026-01-19T15:06:48.192060"}
{"image_index": 309, "image_name": "000000033005.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033005.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1371.059, "latencies_ms": [1371.059], "images_per_second": 0.729, "prompt_tokens": 1111, "response_tokens_est": 11, "n_tiles": 1, "output_text": " A man is playing tennis on a court at night.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 27583.9, "ram_available_mb": 98188.3, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 27588.0, "ram_available_mb": 98184.2, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.52, "peak": 40.18, "min": 16.56}, "VIN": {"avg": 77.13, "peak": 128.85, "min": 28.16}, "VIN_SYS_5V0": {"avg": 14.86, "peak": 16.05, "min": 12.36}, "VDD_CPU_SOC_MSS": {"avg": 15.97, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 31.52, "energy_joules_est": 43.23, "sample_count": 14, "duration_seconds": 1.371}, "timestamp": "2026-01-19T15:06:49.648380"}
{"image_index": 309, "image_name": "000000033005.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033005.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2110.652, "latencies_ms": [2110.652], "images_per_second": 0.474, "prompt_tokens": 1109, "response_tokens_est": 40, "n_tiles": 1, "output_text": " The image features a tennis court with a green surface, a red fence, and a player wearing a white shirt and black shorts. The lighting is dim, and the court is illuminated by artificial lights.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27588.0, "ram_available_mb": 98184.2, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 27611.2, "ram_available_mb": 98161.0, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.62, "peak": 40.97, "min": 18.92}, "VIN": {"avg": 74.22, "peak": 128.59, "min": 30.54}, "VIN_SYS_5V0": {"avg": 14.87, "peak": 16.05, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 16.01, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 29.62, "energy_joules_est": 62.53, "sample_count": 21, "duration_seconds": 2.111}, "timestamp": "2026-01-19T15:06:51.834698"}
{"image_index": 310, "image_name": "000000033104.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033104.jpg", "image_width": 428, "image_height": 500, "image_resolution": "428x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2452.439, "latencies_ms": [2452.439], "images_per_second": 0.408, "prompt_tokens": 1432, "response_tokens_est": 37, "n_tiles": 1, "output_text": " A group of people, including a man in a black jacket and a woman in a white helmet, are standing on a snowy hill, with a blue fence separating them from the slope.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 27611.2, "ram_available_mb": 98161.0, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 27610.9, "ram_available_mb": 98161.3, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.2, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.86, "peak": 40.18, "min": 16.16}, "VIN": {"avg": 74.12, "peak": 121.18, "min": 29.02}, "VIN_SYS_5V0": {"avg": 15.26, "peak": 16.76, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 16.3, "peak": 18.11, "min": 13.0}}, "power_watts_avg": 29.86, "energy_joules_est": 73.25, "sample_count": 24, "duration_seconds": 2.453}, "timestamp": "2026-01-19T15:06:54.339901"}
{"image_index": 310, "image_name": "000000033104.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033104.jpg", "image_width": 428, "image_height": 500, "image_resolution": "428x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3138.846, "latencies_ms": [3138.846], "images_per_second": 0.319, "prompt_tokens": 1446, "response_tokens_est": 58, "n_tiles": 1, "output_text": " 1. person: 2\n2. helmet: 2\n3. goggles: 2\n4. skis: 2\n5. ski poles: 2\n6. net: 1\n7. person: 1\n8. person: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27610.9, "ram_available_mb": 98161.3, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 27561.9, "ram_available_mb": 98210.3, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.73, "peak": 40.16, "min": 17.34}, "VIN": {"avg": 70.28, "peak": 118.81, "min": 27.41}, "VIN_SYS_5V0": {"avg": 14.98, "peak": 16.66, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 16.18, "peak": 18.89, "min": 13.39}}, "power_watts_avg": 27.73, "energy_joules_est": 87.05, "sample_count": 31, "duration_seconds": 3.139}, "timestamp": "2026-01-19T15:06:57.563364"}
{"image_index": 310, "image_name": "000000033104.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033104.jpg", "image_width": 428, "image_height": 500, "image_resolution": "428x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2825.235, "latencies_ms": [2825.235], "images_per_second": 0.354, "prompt_tokens": 1450, "response_tokens_est": 49, "n_tiles": 1, "output_text": " The skier in the foreground is wearing a white helmet and goggles, while the skier in the background is wearing a red helmet and goggles. The skier in the foreground is standing closer to the camera than the skier in the background.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 27561.9, "ram_available_mb": 98210.3, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 27561.9, "ram_available_mb": 98210.3, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.47, "peak": 40.18, "min": 14.98}, "VIN": {"avg": 71.73, "peak": 115.0, "min": 30.28}, "VIN_SYS_5V0": {"avg": 14.95, "peak": 16.46, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 16.0, "peak": 17.71, "min": 12.61}}, "power_watts_avg": 28.47, "energy_joules_est": 80.44, "sample_count": 28, "duration_seconds": 2.826}, "timestamp": "2026-01-19T15:07:00.469912"}
{"image_index": 310, "image_name": "000000033104.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033104.jpg", "image_width": 428, "image_height": 500, "image_resolution": "428x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2148.793, "latencies_ms": [2148.793], "images_per_second": 0.465, "prompt_tokens": 1444, "response_tokens_est": 25, "n_tiles": 1, "output_text": " A group of people are standing on a snowy hill, wearing helmets and goggles, and some of them are holding ski poles.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27561.9, "ram_available_mb": 98210.3, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 27563.7, "ram_available_mb": 98208.4, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.1, "peak": 40.18, "min": 15.77}, "VIN": {"avg": 74.73, "peak": 125.41, "min": 28.99}, "VIN_SYS_5V0": {"avg": 15.08, "peak": 16.56, "min": 11.87}, "VDD_CPU_SOC_MSS": {"avg": 16.24, "peak": 17.71, "min": 13.0}}, "power_watts_avg": 31.1, "energy_joules_est": 66.84, "sample_count": 21, "duration_seconds": 2.149}, "timestamp": "2026-01-19T15:07:02.656651"}
{"image_index": 310, "image_name": "000000033104.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033104.jpg", "image_width": 428, "image_height": 500, "image_resolution": "428x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2774.747, "latencies_ms": [2774.747], "images_per_second": 0.36, "prompt_tokens": 1442, "response_tokens_est": 50, "n_tiles": 1, "output_text": " The image features a group of people wearing helmets and goggles, with one person wearing a white helmet and another wearing a red helmet. The scene is set in a snowy environment, with the ground covered in snow and the sky appearing to be overcast.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27563.7, "ram_available_mb": 98208.4, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 27578.7, "ram_available_mb": 98193.4, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.6, "peak": 40.56, "min": 18.91}, "VIN": {"avg": 70.8, "peak": 121.26, "min": 28.98}, "VIN_SYS_5V0": {"avg": 15.26, "peak": 16.66, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 16.36, "peak": 17.71, "min": 13.79}}, "power_watts_avg": 29.6, "energy_joules_est": 82.14, "sample_count": 27, "duration_seconds": 2.775}, "timestamp": "2026-01-19T15:07:05.458553"}
{"image_index": 311, "image_name": "000000033109.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033109.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1571.251, "latencies_ms": [1571.251], "images_per_second": 0.636, "prompt_tokens": 1099, "response_tokens_est": 20, "n_tiles": 1, "output_text": " A blue truck with a trailer is driving down a wet street with houses and trees on either side.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 27578.7, "ram_available_mb": 98193.4, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 27579.0, "ram_available_mb": 98193.1, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.14, "peak": 40.18, "min": 17.34}, "VIN": {"avg": 76.05, "peak": 120.0, "min": 29.34}, "VIN_SYS_5V0": {"avg": 15.08, "peak": 16.15, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 16.22, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 31.14, "energy_joules_est": 48.95, "sample_count": 16, "duration_seconds": 1.572}, "timestamp": "2026-01-19T15:07:07.134268"}
{"image_index": 311, "image_name": "000000033109.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033109.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2549.749, "latencies_ms": [2549.749], "images_per_second": 0.392, "prompt_tokens": 1113, "response_tokens_est": 57, "n_tiles": 1, "output_text": " 1. truck: 1\n2. street: 1\n3. road: 1\n4. sidewalk: 1\n5. grass: 1\n6. trees: 1\n7. house: 1\n8. street light: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27579.0, "ram_available_mb": 98193.1, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 27597.1, "ram_available_mb": 98175.1, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.53, "peak": 40.57, "min": 18.14}, "VIN": {"avg": 69.23, "peak": 107.14, "min": 29.22}, "VIN_SYS_5V0": {"avg": 14.87, "peak": 16.05, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.92, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 27.53, "energy_joules_est": 70.21, "sample_count": 25, "duration_seconds": 2.55}, "timestamp": "2026-01-19T15:07:09.733671"}
{"image_index": 311, "image_name": "000000033109.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033109.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1916.641, "latencies_ms": [1916.641], "images_per_second": 0.522, "prompt_tokens": 1117, "response_tokens_est": 33, "n_tiles": 1, "output_text": " The blue truck is on the left side of the road, while the white car is on the right side. The truck is closer to the camera than the car.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27597.1, "ram_available_mb": 98175.1, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 27597.4, "ram_available_mb": 98174.8, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.44, "peak": 40.56, "min": 16.16}, "VIN": {"avg": 70.4, "peak": 115.34, "min": 28.75}, "VIN_SYS_5V0": {"avg": 15.01, "peak": 16.15, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 16.18, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 29.44, "energy_joules_est": 56.44, "sample_count": 19, "duration_seconds": 1.917}, "timestamp": "2026-01-19T15:07:11.711959"}
{"image_index": 311, "image_name": "000000033109.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033109.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1458.859, "latencies_ms": [1458.859], "images_per_second": 0.685, "prompt_tokens": 1111, "response_tokens_est": 15, "n_tiles": 1, "output_text": " A blue truck is driving down a wet street with houses on the side.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 27597.4, "ram_available_mb": 98174.8, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 27599.5, "ram_available_mb": 98172.7, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.08, "peak": 40.57, "min": 17.35}, "VIN": {"avg": 76.11, "peak": 105.99, "min": 38.03}, "VIN_SYS_5V0": {"avg": 15.07, "peak": 16.15, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 16.17, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 32.08, "energy_joules_est": 46.81, "sample_count": 14, "duration_seconds": 1.459}, "timestamp": "2026-01-19T15:07:13.174373"}
{"image_index": 311, "image_name": "000000033109.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033109.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1384.306, "latencies_ms": [1384.306], "images_per_second": 0.722, "prompt_tokens": 1109, "response_tokens_est": 13, "n_tiles": 1, "output_text": " The truck is blue and the road is wet from the rain.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27599.5, "ram_available_mb": 98172.7, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 27606.6, "ram_available_mb": 98165.6, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 34.05, "peak": 40.95, "min": 23.64}, "VIN": {"avg": 73.58, "peak": 106.14, "min": 28.59}, "VIN_SYS_5V0": {"avg": 15.42, "peak": 16.36, "min": 13.69}, "VDD_CPU_SOC_MSS": {"avg": 16.53, "peak": 17.32, "min": 14.96}}, "power_watts_avg": 34.05, "energy_joules_est": 47.16, "sample_count": 14, "duration_seconds": 1.385}, "timestamp": "2026-01-19T15:07:14.630968"}
{"image_index": 312, "image_name": "000000033114.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033114.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1749.112, "latencies_ms": [1749.112], "images_per_second": 0.572, "prompt_tokens": 1099, "response_tokens_est": 26, "n_tiles": 1, "output_text": " In the image, a large airplane is seen on a runway, with a mountainous backdrop and a few red lights in the foreground.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 27606.6, "ram_available_mb": 98165.6, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 6.7, "ram_used_mb": 27581.1, "ram_available_mb": 98191.1, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.63, "peak": 41.34, "min": 21.28}, "VIN": {"avg": 76.09, "peak": 106.85, "min": 29.35}, "VIN_SYS_5V0": {"avg": 15.04, "peak": 16.05, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 16.21, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 31.63, "energy_joules_est": 55.34, "sample_count": 17, "duration_seconds": 1.75}, "timestamp": "2026-01-19T15:07:16.408305"}
{"image_index": 312, "image_name": "000000033114.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033114.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1596.73, "latencies_ms": [1596.73], "images_per_second": 0.626, "prompt_tokens": 1113, "response_tokens_est": 21, "n_tiles": 1, "output_text": " airplane: 1\nrunway: 1\nmountains: 1\nlight: 4", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27581.1, "ram_available_mb": 98191.1, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 27585.2, "ram_available_mb": 98187.0, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.76, "peak": 40.57, "min": 19.7}, "VIN": {"avg": 75.09, "peak": 128.37, "min": 30.98}, "VIN_SYS_5V0": {"avg": 15.25, "peak": 16.15, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 16.41, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 31.76, "energy_joules_est": 50.73, "sample_count": 16, "duration_seconds": 1.597}, "timestamp": "2026-01-19T15:07:18.076359"}
{"image_index": 312, "image_name": "000000033114.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033114.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2338.42, "latencies_ms": [2338.42], "images_per_second": 0.428, "prompt_tokens": 1117, "response_tokens_est": 49, "n_tiles": 1, "output_text": " The airplane is positioned in the middle of the runway, with the runway extending towards the foreground and the mountains in the background. The airport lights are located in the foreground, with the airplane in the middle, and the mountains are in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27585.2, "ram_available_mb": 98187.0, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 27585.5, "ram_available_mb": 98186.7, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.48, "peak": 40.56, "min": 18.91}, "VIN": {"avg": 71.24, "peak": 105.66, "min": 29.96}, "VIN_SYS_5V0": {"avg": 14.97, "peak": 16.05, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 16.0, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 28.48, "energy_joules_est": 66.61, "sample_count": 23, "duration_seconds": 2.339}, "timestamp": "2026-01-19T15:07:20.467261"}
{"image_index": 312, "image_name": "000000033114.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033114.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1491.105, "latencies_ms": [1491.105], "images_per_second": 0.671, "prompt_tokens": 1111, "response_tokens_est": 16, "n_tiles": 1, "output_text": " An airplane is on the runway at an airport, with mountains in the background.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 27585.5, "ram_available_mb": 98186.7, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 27584.3, "ram_available_mb": 98187.9, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.22, "peak": 40.56, "min": 16.16}, "VIN": {"avg": 72.64, "peak": 97.74, "min": 28.17}, "VIN_SYS_5V0": {"avg": 14.97, "peak": 16.15, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 16.01, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 31.22, "energy_joules_est": 46.57, "sample_count": 15, "duration_seconds": 1.492}, "timestamp": "2026-01-19T15:07:22.032568"}
{"image_index": 312, "image_name": "000000033114.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033114.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2009.474, "latencies_ms": [2009.474], "images_per_second": 0.498, "prompt_tokens": 1109, "response_tokens_est": 36, "n_tiles": 1, "output_text": " The image features a large airplane on a runway with a hazy sky in the background. The airplane is white with blue accents, and the runway is wet, reflecting the light.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27584.3, "ram_available_mb": 98187.9, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 27588.1, "ram_available_mb": 98184.1, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.76, "peak": 40.56, "min": 19.7}, "VIN": {"avg": 74.05, "peak": 126.89, "min": 28.76}, "VIN_SYS_5V0": {"avg": 14.93, "peak": 16.05, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.9, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 29.76, "energy_joules_est": 59.82, "sample_count": 20, "duration_seconds": 2.01}, "timestamp": "2026-01-19T15:07:24.113328"}
{"image_index": 313, "image_name": "000000033221.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033221.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1882.887, "latencies_ms": [1882.887], "images_per_second": 0.531, "prompt_tokens": 1099, "response_tokens_est": 31, "n_tiles": 1, "output_text": " In the image, a group of people are enjoying a day at the beach, with one person holding a tennis racket and another holding a blue bag.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27588.1, "ram_available_mb": 98184.1, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 27582.2, "ram_available_mb": 98190.0, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.34, "peak": 40.57, "min": 16.56}, "VIN": {"avg": 75.34, "peak": 121.48, "min": 28.92}, "VIN_SYS_5V0": {"avg": 14.86, "peak": 16.05, "min": 12.25}, "VDD_CPU_SOC_MSS": {"avg": 15.96, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 29.34, "energy_joules_est": 55.25, "sample_count": 19, "duration_seconds": 1.883}, "timestamp": "2026-01-19T15:07:26.099397"}
{"image_index": 313, "image_name": "000000033221.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033221.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2743.909, "latencies_ms": [2743.909], "images_per_second": 0.364, "prompt_tokens": 1113, "response_tokens_est": 64, "n_tiles": 1, "output_text": " 1. beach umbrella: 1\n2. beach umbrella: 1\n3. beach umbrella: 1\n4. beach umbrella: 1\n5. beach umbrella: 1\n6. beach umbrella: 1\n7. beach umbrella: 1\n8. beach umbrella: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 27582.2, "ram_available_mb": 98190.0, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 27602.3, "ram_available_mb": 98169.9, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.69, "peak": 40.18, "min": 16.56}, "VIN": {"avg": 67.12, "peak": 118.56, "min": 28.88}, "VIN_SYS_5V0": {"avg": 14.85, "peak": 16.05, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.88, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 26.69, "energy_joules_est": 73.24, "sample_count": 27, "duration_seconds": 2.744}, "timestamp": "2026-01-19T15:07:28.907771"}
{"image_index": 313, "image_name": "000000033221.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033221.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2840.575, "latencies_ms": [2840.575], "images_per_second": 0.352, "prompt_tokens": 1117, "response_tokens_est": 68, "n_tiles": 1, "output_text": " The woman in the orange shirt is standing closer to the camera than the woman in the yellow hoodie. The woman in the orange shirt is standing in the foreground, while the woman in the yellow hoodie is standing in the background. The woman in the orange shirt is also standing closer to the camera than the woman in the blue shirt.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27602.3, "ram_available_mb": 98169.9, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 27609.1, "ram_available_mb": 98163.1, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.3, "peak": 40.16, "min": 15.77}, "VIN": {"avg": 66.47, "peak": 126.66, "min": 29.5}, "VIN_SYS_5V0": {"avg": 14.88, "peak": 16.05, "min": 12.15}, "VDD_CPU_SOC_MSS": {"avg": 15.87, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 26.3, "energy_joules_est": 74.72, "sample_count": 28, "duration_seconds": 2.841}, "timestamp": "2026-01-19T15:07:31.818376"}
{"image_index": 313, "image_name": "000000033221.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033221.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1451.178, "latencies_ms": [1451.178], "images_per_second": 0.689, "prompt_tokens": 1111, "response_tokens_est": 14, "n_tiles": 1, "output_text": " A group of people are on a beach with mountains in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27609.1, "ram_available_mb": 98163.1, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 27617.4, "ram_available_mb": 98154.8, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.37, "peak": 40.16, "min": 15.38}, "VIN": {"avg": 82.4, "peak": 118.32, "min": 32.44}, "VIN_SYS_5V0": {"avg": 14.91, "peak": 16.05, "min": 12.15}, "VDD_CPU_SOC_MSS": {"avg": 16.06, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 31.37, "energy_joules_est": 45.54, "sample_count": 14, "duration_seconds": 1.452}, "timestamp": "2026-01-19T15:07:33.275703"}
{"image_index": 313, "image_name": "000000033221.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033221.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2013.95, "latencies_ms": [2013.95], "images_per_second": 0.497, "prompt_tokens": 1109, "response_tokens_est": 37, "n_tiles": 1, "output_text": " The image features a group of people on a beach with a clear blue sky and a mountain range in the background. The sand is light brown, and the people are wearing casual clothing.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27617.4, "ram_available_mb": 98154.8, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 27625.4, "ram_available_mb": 98146.8, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.55, "peak": 41.36, "min": 19.7}, "VIN": {"avg": 76.01, "peak": 118.7, "min": 28.73}, "VIN_SYS_5V0": {"avg": 15.25, "peak": 16.26, "min": 13.39}, "VDD_CPU_SOC_MSS": {"avg": 16.34, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 30.55, "energy_joules_est": 61.54, "sample_count": 20, "duration_seconds": 2.014}, "timestamp": "2026-01-19T15:07:35.355095"}
{"image_index": 314, "image_name": "000000033368.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033368.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1717.17, "latencies_ms": [1717.17], "images_per_second": 0.582, "prompt_tokens": 1100, "response_tokens_est": 25, "n_tiles": 1, "output_text": " A tennis player is standing on a blue court with a white logo on it, holding a tennis racket and looking down.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27625.4, "ram_available_mb": 98146.8, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 27636.0, "ram_available_mb": 98136.1, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.31, "peak": 40.57, "min": 16.95}, "VIN": {"avg": 74.72, "peak": 114.2, "min": 29.17}, "VIN_SYS_5V0": {"avg": 14.91, "peak": 16.15, "min": 12.15}, "VDD_CPU_SOC_MSS": {"avg": 16.03, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 30.31, "energy_joules_est": 52.07, "sample_count": 17, "duration_seconds": 1.718}, "timestamp": "2026-01-19T15:07:37.133413"}
{"image_index": 314, "image_name": "000000033368.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033368.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2684.408, "latencies_ms": [2684.408], "images_per_second": 0.373, "prompt_tokens": 1114, "response_tokens_est": 62, "n_tiles": 1, "output_text": " 1. man: 1\n2. tennis racket: 1\n3. tennis ball: 1\n4. tennis shoes: 2\n5. shorts: 1\n6. shirt: 1\n7. headband: 1\n8. wristband: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27636.0, "ram_available_mb": 98136.1, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 6.4, "ram_used_mb": 27563.9, "ram_available_mb": 98208.3, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.2, "peak": 40.57, "min": 18.53}, "VIN": {"avg": 71.6, "peak": 127.53, "min": 28.85}, "VIN_SYS_5V0": {"avg": 14.99, "peak": 16.15, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 16.14, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 27.2, "energy_joules_est": 73.03, "sample_count": 26, "duration_seconds": 2.685}, "timestamp": "2026-01-19T15:07:39.835256"}
{"image_index": 314, "image_name": "000000033368.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033368.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2454.744, "latencies_ms": [2454.744], "images_per_second": 0.407, "prompt_tokens": 1118, "response_tokens_est": 54, "n_tiles": 1, "output_text": " The tennis player is positioned in the foreground of the image, with the blue tennis court serving as the background. The player is standing on the court, with the shadow of the player and the tennis racket cast on the court, indicating the direction of the light source.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27563.9, "ram_available_mb": 98208.3, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 27560.6, "ram_available_mb": 98211.6, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.51, "peak": 40.16, "min": 16.56}, "VIN": {"avg": 64.46, "peak": 120.13, "min": 28.58}, "VIN_SYS_5V0": {"avg": 15.09, "peak": 16.26, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 16.13, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 27.51, "energy_joules_est": 67.55, "sample_count": 24, "duration_seconds": 2.455}, "timestamp": "2026-01-19T15:07:42.333678"}
{"image_index": 314, "image_name": "000000033368.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033368.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1558.686, "latencies_ms": [1558.686], "images_per_second": 0.642, "prompt_tokens": 1112, "response_tokens_est": 19, "n_tiles": 1, "output_text": " A tennis player is standing on a blue court, holding a tennis racket and looking down.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27560.6, "ram_available_mb": 98211.6, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 27560.6, "ram_available_mb": 98211.6, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.96, "peak": 39.78, "min": 16.16}, "VIN": {"avg": 75.74, "peak": 117.67, "min": 33.76}, "VIN_SYS_5V0": {"avg": 15.08, "peak": 16.26, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 16.11, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 30.96, "energy_joules_est": 48.27, "sample_count": 15, "duration_seconds": 1.559}, "timestamp": "2026-01-19T15:07:43.897288"}
{"image_index": 314, "image_name": "000000033368.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033368.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2287.506, "latencies_ms": [2287.506], "images_per_second": 0.437, "prompt_tokens": 1110, "response_tokens_est": 48, "n_tiles": 1, "output_text": " The image features a tennis player in a blue shirt and white shorts, standing on a blue tennis court with a white logo on the ground. The lighting is bright and the shadows are sharp, indicating that the photo was taken during the day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27560.6, "ram_available_mb": 98211.6, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 27561.6, "ram_available_mb": 98210.6, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.71, "peak": 40.18, "min": 17.74}, "VIN": {"avg": 73.55, "peak": 120.92, "min": 28.43}, "VIN_SYS_5V0": {"avg": 15.22, "peak": 16.26, "min": 13.06}, "VDD_CPU_SOC_MSS": {"avg": 16.31, "peak": 17.71, "min": 14.18}}, "power_watts_avg": 28.71, "energy_joules_est": 65.68, "sample_count": 23, "duration_seconds": 2.288}, "timestamp": "2026-01-19T15:07:46.289572"}
{"image_index": 315, "image_name": "000000033638.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033638.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1789.896, "latencies_ms": [1789.896], "images_per_second": 0.559, "prompt_tokens": 1100, "response_tokens_est": 28, "n_tiles": 1, "output_text": " A woman in a floral shirt and beige shorts is standing in a kitchen with a large black stove and a white cabinet in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27561.6, "ram_available_mb": 98210.6, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 27561.8, "ram_available_mb": 98210.3, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.46, "peak": 40.57, "min": 14.98}, "VIN": {"avg": 71.17, "peak": 118.53, "min": 30.54}, "VIN_SYS_5V0": {"avg": 14.76, "peak": 16.05, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 15.9, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 29.46, "energy_joules_est": 52.75, "sample_count": 18, "duration_seconds": 1.791}, "timestamp": "2026-01-19T15:07:48.169955"}
{"image_index": 315, "image_name": "000000033638.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033638.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2552.757, "latencies_ms": [2552.757], "images_per_second": 0.392, "prompt_tokens": 1114, "response_tokens_est": 57, "n_tiles": 1, "output_text": " 1. woman: 1\n2. stove: 1\n3. cupboard: 1\n4. shelf: 1\n5. dish: 1\n6. pot: 1\n7. kettle: 1\n8. pan: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27561.8, "ram_available_mb": 98210.3, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 27567.9, "ram_available_mb": 98204.3, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.22, "peak": 40.16, "min": 17.35}, "VIN": {"avg": 71.29, "peak": 125.78, "min": 31.83}, "VIN_SYS_5V0": {"avg": 14.88, "peak": 16.05, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 15.97, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 27.22, "energy_joules_est": 69.49, "sample_count": 25, "duration_seconds": 2.553}, "timestamp": "2026-01-19T15:07:50.771480"}
{"image_index": 315, "image_name": "000000033638.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033638.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2230.485, "latencies_ms": [2230.485], "images_per_second": 0.448, "prompt_tokens": 1118, "response_tokens_est": 45, "n_tiles": 1, "output_text": " The woman is standing to the left of the stove, which is positioned in the center of the image. The stove is located in the foreground of the image, with the woman's body and the kitchen counter in the background.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 27567.9, "ram_available_mb": 98204.3, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 27576.5, "ram_available_mb": 98195.7, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.17, "peak": 40.16, "min": 16.16}, "VIN": {"avg": 69.64, "peak": 115.72, "min": 28.86}, "VIN_SYS_5V0": {"avg": 15.03, "peak": 16.15, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 16.2, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 28.17, "energy_joules_est": 62.85, "sample_count": 22, "duration_seconds": 2.231}, "timestamp": "2026-01-19T15:07:53.060432"}
{"image_index": 315, "image_name": "000000033638.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033638.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1507.663, "latencies_ms": [1507.663], "images_per_second": 0.663, "prompt_tokens": 1112, "response_tokens_est": 17, "n_tiles": 1, "output_text": " A woman is standing in a kitchen with a large black stove and a white cabinet.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 27576.5, "ram_available_mb": 98195.7, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 27595.6, "ram_available_mb": 98176.6, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.91, "peak": 39.77, "min": 16.56}, "VIN": {"avg": 71.8, "peak": 124.31, "min": 28.77}, "VIN_SYS_5V0": {"avg": 14.97, "peak": 16.26, "min": 12.36}, "VDD_CPU_SOC_MSS": {"avg": 15.98, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 30.91, "energy_joules_est": 46.62, "sample_count": 15, "duration_seconds": 1.508}, "timestamp": "2026-01-19T15:07:54.629641"}
{"image_index": 315, "image_name": "000000033638.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033638.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2207.108, "latencies_ms": [2207.108], "images_per_second": 0.453, "prompt_tokens": 1110, "response_tokens_est": 44, "n_tiles": 1, "output_text": " The image is a black and white photo with a vintage feel, capturing a woman in a kitchen with a large black stove and a white wall. The lighting is soft and natural, suggesting it was taken during the day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27595.6, "ram_available_mb": 98176.6, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 6.6, "ram_used_mb": 27568.2, "ram_available_mb": 98204.0, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.79, "peak": 40.56, "min": 18.52}, "VIN": {"avg": 71.81, "peak": 104.36, "min": 27.99}, "VIN_SYS_5V0": {"avg": 15.06, "peak": 16.15, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 16.3, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 28.79, "energy_joules_est": 63.55, "sample_count": 22, "duration_seconds": 2.207}, "timestamp": "2026-01-19T15:07:56.917102"}
{"image_index": 316, "image_name": "000000033707.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033707.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1999.213, "latencies_ms": [1999.213], "images_per_second": 0.5, "prompt_tokens": 1432, "response_tokens_est": 20, "n_tiles": 1, "output_text": " Two giraffes are standing in front of a building, with one of them eating leaves from a tree.", "error": null, "sys_before": {"cpu_percent": 6.7, "ram_used_mb": 27568.2, "ram_available_mb": 98204.0, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 27570.3, "ram_available_mb": 98201.9, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.2, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.26, "peak": 40.18, "min": 15.38}, "VIN": {"avg": 74.75, "peak": 120.27, "min": 29.3}, "VIN_SYS_5V0": {"avg": 15.26, "peak": 16.76, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 16.38, "peak": 17.71, "min": 13.39}}, "power_watts_avg": 31.26, "energy_joules_est": 62.52, "sample_count": 20, "duration_seconds": 2.0}, "timestamp": "2026-01-19T15:07:59.016240"}
{"image_index": 316, "image_name": "000000033707.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033707.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3009.5, "latencies_ms": [3009.5], "images_per_second": 0.332, "prompt_tokens": 1446, "response_tokens_est": 56, "n_tiles": 1, "output_text": " 1. giraffe: 2\n2. giraffe: 2\n3. giraffe: 2\n4. giraffe: 2\n5. giraffe: 2\n6. giraffe: 2\n7. giraffe: 2\n8. giraffe: 2", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 27570.3, "ram_available_mb": 98201.9, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 27570.4, "ram_available_mb": 98201.7, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.44, "peak": 40.57, "min": 17.73}, "VIN": {"avg": 70.45, "peak": 114.21, "min": 29.47}, "VIN_SYS_5V0": {"avg": 15.06, "peak": 16.56, "min": 12.15}, "VDD_CPU_SOC_MSS": {"avg": 15.99, "peak": 17.71, "min": 13.0}}, "power_watts_avg": 28.44, "energy_joules_est": 85.6, "sample_count": 30, "duration_seconds": 3.01}, "timestamp": "2026-01-19T15:08:02.115532"}
{"image_index": 316, "image_name": "000000033707.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033707.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2361.497, "latencies_ms": [2361.497], "images_per_second": 0.423, "prompt_tokens": 1450, "response_tokens_est": 34, "n_tiles": 1, "output_text": " The giraffes are positioned in the foreground of the image, with the building in the background. The giraffes are standing on a rocky terrain, with the building situated behind them.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 27570.4, "ram_available_mb": 98201.7, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 27569.4, "ram_available_mb": 98202.8, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.42, "peak": 40.18, "min": 15.38}, "VIN": {"avg": 73.73, "peak": 132.52, "min": 28.84}, "VIN_SYS_5V0": {"avg": 15.13, "peak": 16.56, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 16.23, "peak": 17.71, "min": 12.61}}, "power_watts_avg": 30.42, "energy_joules_est": 71.85, "sample_count": 23, "duration_seconds": 2.362}, "timestamp": "2026-01-19T15:08:04.514760"}
{"image_index": 316, "image_name": "000000033707.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033707.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1907.641, "latencies_ms": [1907.641], "images_per_second": 0.524, "prompt_tokens": 1444, "response_tokens_est": 17, "n_tiles": 1, "output_text": " Two giraffes are standing in a zoo enclosure, one of them eating from a tree.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27569.4, "ram_available_mb": 98202.8, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 27570.6, "ram_available_mb": 98201.6, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.49, "peak": 40.18, "min": 17.74}, "VIN": {"avg": 74.9, "peak": 123.56, "min": 28.15}, "VIN_SYS_5V0": {"avg": 15.34, "peak": 16.66, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 16.41, "peak": 17.71, "min": 13.79}}, "power_watts_avg": 32.49, "energy_joules_est": 62.0, "sample_count": 19, "duration_seconds": 1.908}, "timestamp": "2026-01-19T15:08:06.500620"}
{"image_index": 316, "image_name": "000000033707.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033707.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1800.593, "latencies_ms": [1800.593], "images_per_second": 0.555, "prompt_tokens": 1442, "response_tokens_est": 13, "n_tiles": 1, "output_text": " The giraffes are brown and white, and the sky is cloudy.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27570.6, "ram_available_mb": 98201.6, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 2.3, "ram_used_mb": 27570.8, "ram_available_mb": 98201.3, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 33.29, "peak": 40.57, "min": 18.92}, "VIN": {"avg": 75.53, "peak": 130.58, "min": 27.28}, "VIN_SYS_5V0": {"avg": 15.2, "peak": 16.56, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 16.32, "peak": 17.71, "min": 13.39}}, "power_watts_avg": 33.29, "energy_joules_est": 59.95, "sample_count": 18, "duration_seconds": 1.801}, "timestamp": "2026-01-19T15:08:08.374104"}
{"image_index": 317, "image_name": "000000033759.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033759.jpg", "image_width": 640, "image_height": 457, "image_resolution": "640x457", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1675.911, "latencies_ms": [1675.911], "images_per_second": 0.597, "prompt_tokens": 1099, "response_tokens_est": 22, "n_tiles": 1, "output_text": " A young boy wearing a green and yellow baseball uniform is swinging a blue and white baseball bat at a baseball.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 27570.8, "ram_available_mb": 98201.3, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 27586.3, "ram_available_mb": 98185.8, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.14, "peak": 40.56, "min": 20.11}, "VIN": {"avg": 74.68, "peak": 119.77, "min": 29.21}, "VIN_SYS_5V0": {"avg": 14.78, "peak": 16.05, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.89, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 31.14, "energy_joules_est": 52.21, "sample_count": 17, "duration_seconds": 1.677}, "timestamp": "2026-01-19T15:08:10.154458"}
{"image_index": 317, "image_name": "000000033759.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033759.jpg", "image_width": 640, "image_height": 457, "image_resolution": "640x457", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2687.554, "latencies_ms": [2687.554], "images_per_second": 0.372, "prompt_tokens": 1113, "response_tokens_est": 62, "n_tiles": 1, "output_text": " 1. boy: 1\n2. helmet: 1\n3. bat: 1\n4. ball: 1\n5. chain link fence: 1\n6. grass: 1\n7. chain link fence: 1\n8. chain link fence: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27586.3, "ram_available_mb": 98185.8, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 27592.4, "ram_available_mb": 98179.7, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.89, "peak": 40.57, "min": 17.34}, "VIN": {"avg": 72.26, "peak": 114.31, "min": 28.56}, "VIN_SYS_5V0": {"avg": 14.84, "peak": 16.05, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.91, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 26.89, "energy_joules_est": 72.29, "sample_count": 27, "duration_seconds": 2.688}, "timestamp": "2026-01-19T15:08:12.948488"}
{"image_index": 317, "image_name": "000000033759.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033759.jpg", "image_width": 640, "image_height": 457, "image_resolution": "640x457", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2052.66, "latencies_ms": [2052.66], "images_per_second": 0.487, "prompt_tokens": 1117, "response_tokens_est": 37, "n_tiles": 1, "output_text": " The baseball player is in the foreground, holding a blue bat and swinging it towards the ball. The ball is in the middle ground, and the chain link fence is in the background.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 27592.4, "ram_available_mb": 98179.7, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 27599.7, "ram_available_mb": 98172.5, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.66, "peak": 40.18, "min": 14.98}, "VIN": {"avg": 69.95, "peak": 120.9, "min": 28.95}, "VIN_SYS_5V0": {"avg": 14.85, "peak": 16.05, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.93, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 28.66, "energy_joules_est": 58.84, "sample_count": 20, "duration_seconds": 2.053}, "timestamp": "2026-01-19T15:08:15.035294"}
{"image_index": 317, "image_name": "000000033759.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033759.jpg", "image_width": 640, "image_height": 457, "image_resolution": "640x457", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1495.849, "latencies_ms": [1495.849], "images_per_second": 0.669, "prompt_tokens": 1111, "response_tokens_est": 16, "n_tiles": 1, "output_text": " A young boy is playing baseball in a field with a fence in the background.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 27599.7, "ram_available_mb": 98172.5, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 27595.9, "ram_available_mb": 98176.2, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.49, "peak": 40.57, "min": 17.35}, "VIN": {"avg": 75.2, "peak": 119.16, "min": 29.31}, "VIN_SYS_5V0": {"avg": 15.06, "peak": 16.15, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 16.14, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 31.49, "energy_joules_est": 47.13, "sample_count": 15, "duration_seconds": 1.497}, "timestamp": "2026-01-19T15:08:16.607074"}
{"image_index": 317, "image_name": "000000033759.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033759.jpg", "image_width": 640, "image_height": 457, "image_resolution": "640x457", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2197.167, "latencies_ms": [2197.167], "images_per_second": 0.455, "prompt_tokens": 1109, "response_tokens_est": 43, "n_tiles": 1, "output_text": " The image features a young boy wearing a green and yellow baseball uniform, swinging a blue bat at a white baseball. The scene is set on a sunny day with a clear blue sky and green grass in the background.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 27595.9, "ram_available_mb": 98176.2, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 27601.4, "ram_available_mb": 98170.8, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.94, "peak": 40.97, "min": 18.53}, "VIN": {"avg": 69.94, "peak": 117.3, "min": 29.07}, "VIN_SYS_5V0": {"avg": 14.85, "peak": 16.05, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.91, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 28.94, "energy_joules_est": 63.6, "sample_count": 22, "duration_seconds": 2.198}, "timestamp": "2026-01-19T15:08:18.897051"}
{"image_index": 318, "image_name": "000000033854.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033854.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2317.475, "latencies_ms": [2317.475], "images_per_second": 0.432, "prompt_tokens": 1099, "response_tokens_est": 48, "n_tiles": 1, "output_text": " The image captures a bustling scene at a vintage car show, with a variety of classic vehicles parked on cobblestone streets, including a red bus, a black car, and a motorcycle, all under the watchful eyes of onlookers.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27601.4, "ram_available_mb": 98170.8, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 27601.4, "ram_available_mb": 98170.8, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.73, "peak": 40.18, "min": 15.77}, "VIN": {"avg": 73.5, "peak": 127.27, "min": 28.75}, "VIN_SYS_5V0": {"avg": 14.85, "peak": 15.95, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.78, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 27.73, "energy_joules_est": 64.29, "sample_count": 23, "duration_seconds": 2.318}, "timestamp": "2026-01-19T15:08:21.298381"}
{"image_index": 318, "image_name": "000000033854.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033854.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2738.031, "latencies_ms": [2738.031], "images_per_second": 0.365, "prompt_tokens": 1113, "response_tokens_est": 64, "n_tiles": 1, "output_text": " 1. black car: 2\n2. red bus: 1\n3. black motorcycle: 1\n4. green tank: 1\n5. red flag: 1\n6. black car: 1\n7. black motorcycle: 1\n8. red flag: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27601.4, "ram_available_mb": 98170.8, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 6.3, "ram_used_mb": 27545.7, "ram_available_mb": 98226.5, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.53, "peak": 40.16, "min": 15.77}, "VIN": {"avg": 70.02, "peak": 116.08, "min": 28.55}, "VIN_SYS_5V0": {"avg": 14.91, "peak": 16.05, "min": 12.15}, "VDD_CPU_SOC_MSS": {"avg": 16.08, "peak": 17.71, "min": 13.0}}, "power_watts_avg": 26.53, "energy_joules_est": 72.66, "sample_count": 27, "duration_seconds": 2.739}, "timestamp": "2026-01-19T15:08:24.113523"}
{"image_index": 318, "image_name": "000000033854.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033854.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2378.146, "latencies_ms": [2378.146], "images_per_second": 0.42, "prompt_tokens": 1117, "response_tokens_est": 50, "n_tiles": 1, "output_text": " The motorcycles are positioned in the foreground, with the cars parked behind them. The buses are located in the background, with the motorcycles and cars in the middle ground. The red flag is positioned in the middle ground, between the motorcycles and the buses.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27545.7, "ram_available_mb": 98226.5, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 27549.8, "ram_available_mb": 98222.3, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.64, "peak": 40.16, "min": 15.38}, "VIN": {"avg": 72.51, "peak": 120.61, "min": 29.75}, "VIN_SYS_5V0": {"avg": 14.91, "peak": 16.05, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 15.97, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 27.64, "energy_joules_est": 65.74, "sample_count": 23, "duration_seconds": 2.378}, "timestamp": "2026-01-19T15:08:26.510450"}
{"image_index": 318, "image_name": "000000033854.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033854.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1586.119, "latencies_ms": [1586.119], "images_per_second": 0.63, "prompt_tokens": 1111, "response_tokens_est": 20, "n_tiles": 1, "output_text": " A group of people are gathered in a parking lot, with vintage cars and motorcycles parked around them.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27549.8, "ram_available_mb": 98222.3, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 27562.7, "ram_available_mb": 98209.5, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.93, "peak": 40.56, "min": 16.95}, "VIN": {"avg": 73.96, "peak": 110.62, "min": 28.94}, "VIN_SYS_5V0": {"avg": 15.13, "peak": 16.15, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 16.24, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 30.93, "energy_joules_est": 49.08, "sample_count": 16, "duration_seconds": 1.587}, "timestamp": "2026-01-19T15:08:28.180751"}
{"image_index": 318, "image_name": "000000033854.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033854.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2421.703, "latencies_ms": [2421.703], "images_per_second": 0.413, "prompt_tokens": 1109, "response_tokens_est": 52, "n_tiles": 1, "output_text": " The image features a cobblestone street with a variety of vehicles, including cars and motorcycles, parked on the side. The sky is clear and blue, indicating a sunny day. The vehicles are mostly black and silver, with some having red and white accents.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27562.7, "ram_available_mb": 98209.5, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 27567.5, "ram_available_mb": 98204.6, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.94, "peak": 40.57, "min": 18.52}, "VIN": {"avg": 67.47, "peak": 122.03, "min": 28.94}, "VIN_SYS_5V0": {"avg": 14.93, "peak": 16.05, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.93, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 27.94, "energy_joules_est": 67.67, "sample_count": 24, "duration_seconds": 2.422}, "timestamp": "2026-01-19T15:08:30.667125"}
{"image_index": 319, "image_name": "000000034071.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034071.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2200.178, "latencies_ms": [2200.178], "images_per_second": 0.455, "prompt_tokens": 1099, "response_tokens_est": 43, "n_tiles": 1, "output_text": " The image captures a close-up view of two parking meters, their backs facing the camera, with the sun setting in the background, casting a warm glow and creating a bokeh effect with the surrounding lights.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27567.5, "ram_available_mb": 98204.6, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 27578.8, "ram_available_mb": 98193.4, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.06, "peak": 40.16, "min": 16.16}, "VIN": {"avg": 70.43, "peak": 121.91, "min": 27.36}, "VIN_SYS_5V0": {"avg": 14.93, "peak": 16.05, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 16.0, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 28.06, "energy_joules_est": 61.75, "sample_count": 22, "duration_seconds": 2.201}, "timestamp": "2026-01-19T15:08:32.953374"}
{"image_index": 319, "image_name": "000000034071.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034071.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1305.802, "latencies_ms": [1305.802], "images_per_second": 0.766, "prompt_tokens": 1113, "response_tokens_est": 8, "n_tiles": 1, "output_text": " 1. parking meter: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27578.8, "ram_available_mb": 98193.4, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 27590.8, "ram_available_mb": 98181.3, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.49, "peak": 40.16, "min": 15.77}, "VIN": {"avg": 73.47, "peak": 110.46, "min": 30.03}, "VIN_SYS_5V0": {"avg": 14.75, "peak": 16.05, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 15.93, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 31.49, "energy_joules_est": 41.13, "sample_count": 13, "duration_seconds": 1.306}, "timestamp": "2026-01-19T15:08:34.309181"}
{"image_index": 319, "image_name": "000000034071.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034071.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2400.603, "latencies_ms": [2400.603], "images_per_second": 0.417, "prompt_tokens": 1117, "response_tokens_est": 51, "n_tiles": 1, "output_text": " The two parking meters are positioned in the foreground, with the sun setting in the background, creating a warm and inviting atmosphere. The parking meters are relatively close to the camera, while the sun is farther away, creating a sense of depth in the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27590.8, "ram_available_mb": 98181.3, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 27595.1, "ram_available_mb": 98177.1, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.92, "peak": 40.95, "min": 17.74}, "VIN": {"avg": 69.19, "peak": 126.13, "min": 28.53}, "VIN_SYS_5V0": {"avg": 14.97, "peak": 16.15, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 16.16, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 28.92, "energy_joules_est": 69.43, "sample_count": 24, "duration_seconds": 2.401}, "timestamp": "2026-01-19T15:08:36.804599"}
{"image_index": 319, "image_name": "000000034071.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034071.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1849.732, "latencies_ms": [1849.732], "images_per_second": 0.541, "prompt_tokens": 1111, "response_tokens_est": 29, "n_tiles": 1, "output_text": " The image captures a serene moment at a parking lot during sunset, with two parking meters in the foreground and a blurred cityscape in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27595.1, "ram_available_mb": 98177.1, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 27601.8, "ram_available_mb": 98170.4, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.44, "peak": 40.18, "min": 15.38}, "VIN": {"avg": 70.88, "peak": 113.46, "min": 29.47}, "VIN_SYS_5V0": {"avg": 14.83, "peak": 16.05, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.97, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 29.44, "energy_joules_est": 54.47, "sample_count": 18, "duration_seconds": 1.85}, "timestamp": "2026-01-19T15:08:38.681508"}
{"image_index": 319, "image_name": "000000034071.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034071.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2291.537, "latencies_ms": [2291.537], "images_per_second": 0.436, "prompt_tokens": 1109, "response_tokens_est": 47, "n_tiles": 1, "output_text": " The image features two parking meters with a warm, golden hue from the setting sun in the background. The parking meters are black and have a metallic finish, and the sun is setting behind them, casting a soft glow on the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27601.8, "ram_available_mb": 98170.4, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 27609.5, "ram_available_mb": 98162.7, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.61, "peak": 40.56, "min": 18.13}, "VIN": {"avg": 72.77, "peak": 119.05, "min": 35.17}, "VIN_SYS_5V0": {"avg": 15.2, "peak": 16.15, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 16.32, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 28.61, "energy_joules_est": 65.57, "sample_count": 22, "duration_seconds": 2.292}, "timestamp": "2026-01-19T15:08:40.976588"}
{"image_index": 320, "image_name": "000000034139.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034139.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1588.036, "latencies_ms": [1588.036], "images_per_second": 0.63, "prompt_tokens": 1099, "response_tokens_est": 21, "n_tiles": 1, "output_text": " A large brown suitcase with stickers on it is sitting on the sidewalk next to a group of people.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 27609.5, "ram_available_mb": 98162.7, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 27625.1, "ram_available_mb": 98147.1, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.29, "peak": 40.16, "min": 18.52}, "VIN": {"avg": 73.15, "peak": 106.18, "min": 28.7}, "VIN_SYS_5V0": {"avg": 15.3, "peak": 16.26, "min": 13.39}, "VDD_CPU_SOC_MSS": {"avg": 16.54, "peak": 17.71, "min": 14.57}}, "power_watts_avg": 31.29, "energy_joules_est": 49.71, "sample_count": 16, "duration_seconds": 1.589}, "timestamp": "2026-01-19T15:08:42.648912"}
{"image_index": 320, "image_name": "000000034139.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034139.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2916.636, "latencies_ms": [2916.636], "images_per_second": 0.343, "prompt_tokens": 1113, "response_tokens_est": 71, "n_tiles": 1, "output_text": " 1. suitcase: 1\n2. man: 1\n3. woman: 1\n4. man in glasses: 1\n5. woman with blue jacket: 1\n6. man in yellow shirt: 1\n7. man in black jacket: 1\n8. man in blue shirt: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27625.1, "ram_available_mb": 98147.1, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 27627.7, "ram_available_mb": 98144.5, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.63, "peak": 40.56, "min": 17.73}, "VIN": {"avg": 66.39, "peak": 125.39, "min": 29.12}, "VIN_SYS_5V0": {"avg": 14.89, "peak": 16.05, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.98, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 26.63, "energy_joules_est": 77.68, "sample_count": 29, "duration_seconds": 2.917}, "timestamp": "2026-01-19T15:08:45.647195"}
{"image_index": 320, "image_name": "000000034139.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034139.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2409.014, "latencies_ms": [2409.014], "images_per_second": 0.415, "prompt_tokens": 1117, "response_tokens_est": 51, "n_tiles": 1, "output_text": " The suitcase is positioned to the left of the couple, who are standing in the foreground of the image. The suitcase is in front of the couple, and the couple is standing on a sidewalk, with the suitcase placed on a pedestal.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 27627.7, "ram_available_mb": 98144.5, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 27636.7, "ram_available_mb": 98135.5, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.36, "peak": 40.16, "min": 15.38}, "VIN": {"avg": 71.09, "peak": 128.49, "min": 28.7}, "VIN_SYS_5V0": {"avg": 14.87, "peak": 16.05, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 15.94, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 27.36, "energy_joules_est": 65.92, "sample_count": 24, "duration_seconds": 2.409}, "timestamp": "2026-01-19T15:08:48.140755"}
{"image_index": 320, "image_name": "000000034139.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034139.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1933.122, "latencies_ms": [1933.122], "images_per_second": 0.517, "prompt_tokens": 1111, "response_tokens_est": 33, "n_tiles": 1, "output_text": " A large brown suitcase with stickers on it is sitting on the sidewalk in front of a building. A man and a woman are standing next to the suitcase.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27636.7, "ram_available_mb": 98135.5, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 6.6, "ram_used_mb": 27558.6, "ram_available_mb": 98213.6, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.23, "peak": 40.16, "min": 15.77}, "VIN": {"avg": 74.94, "peak": 130.49, "min": 29.55}, "VIN_SYS_5V0": {"avg": 14.9, "peak": 16.05, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 16.2, "peak": 18.11, "min": 13.0}}, "power_watts_avg": 29.23, "energy_joules_est": 56.51, "sample_count": 19, "duration_seconds": 1.933}, "timestamp": "2026-01-19T15:08:50.123268"}
{"image_index": 320, "image_name": "000000034139.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034139.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1567.893, "latencies_ms": [1567.893], "images_per_second": 0.638, "prompt_tokens": 1109, "response_tokens_est": 19, "n_tiles": 1, "output_text": " The suitcase is brown and white, and the suitcase is in front of a building.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27558.6, "ram_available_mb": 98213.6, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 27555.5, "ram_available_mb": 98216.7, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.0, "peak": 40.56, "min": 17.35}, "VIN": {"avg": 74.61, "peak": 109.64, "min": 30.31}, "VIN_SYS_5V0": {"avg": 14.96, "peak": 16.15, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 16.14, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 31.0, "energy_joules_est": 48.63, "sample_count": 16, "duration_seconds": 1.569}, "timestamp": "2026-01-19T15:08:51.792194"}
{"image_index": 321, "image_name": "000000034205.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034205.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1842.567, "latencies_ms": [1842.567], "images_per_second": 0.543, "prompt_tokens": 1099, "response_tokens_est": 29, "n_tiles": 1, "output_text": " The image features a plate of food that includes a variety of mushrooms and broccoli, with the mushrooms being cooked and the broccoli being fresh and green.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 27555.5, "ram_available_mb": 98216.7, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 27560.6, "ram_available_mb": 98211.5, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.2, "peak": 40.16, "min": 18.14}, "VIN": {"avg": 70.54, "peak": 113.29, "min": 30.37}, "VIN_SYS_5V0": {"avg": 14.76, "peak": 15.95, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.95, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 30.2, "energy_joules_est": 55.66, "sample_count": 18, "duration_seconds": 1.843}, "timestamp": "2026-01-19T15:08:53.673945"}
{"image_index": 321, "image_name": "000000034205.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034205.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1498.137, "latencies_ms": [1498.137], "images_per_second": 0.667, "prompt_tokens": 1113, "response_tokens_est": 16, "n_tiles": 1, "output_text": " mushroom: 10, broccoli: 2, parsley: 10", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27560.6, "ram_available_mb": 98211.5, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 27571.6, "ram_available_mb": 98200.6, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.75, "peak": 40.56, "min": 18.14}, "VIN": {"avg": 75.98, "peak": 123.94, "min": 30.13}, "VIN_SYS_5V0": {"avg": 14.99, "peak": 16.15, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 16.17, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 31.75, "energy_joules_est": 47.58, "sample_count": 15, "duration_seconds": 1.499}, "timestamp": "2026-01-19T15:08:55.240998"}
{"image_index": 321, "image_name": "000000034205.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034205.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1973.837, "latencies_ms": [1973.837], "images_per_second": 0.507, "prompt_tokens": 1117, "response_tokens_est": 34, "n_tiles": 1, "output_text": " The mushrooms are located in the foreground, with the broccoli situated in the background. The parsley is sprinkled on top of the mushrooms and broccoli, creating a visually appealing contrast.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27571.6, "ram_available_mb": 98200.6, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 27580.7, "ram_available_mb": 98191.4, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.15, "peak": 40.57, "min": 20.11}, "VIN": {"avg": 72.88, "peak": 124.48, "min": 29.49}, "VIN_SYS_5V0": {"avg": 14.92, "peak": 16.05, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 16.0, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 30.15, "energy_joules_est": 59.53, "sample_count": 19, "duration_seconds": 1.974}, "timestamp": "2026-01-19T15:08:57.233599"}
{"image_index": 321, "image_name": "000000034205.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034205.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1946.605, "latencies_ms": [1946.605], "images_per_second": 0.514, "prompt_tokens": 1111, "response_tokens_est": 34, "n_tiles": 1, "output_text": " In a close-up shot, a plate of food is presented, featuring a variety of ingredients including mushrooms, broccoli, and a piece of meat garnished with parsley.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27580.7, "ram_available_mb": 98191.4, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 27589.5, "ram_available_mb": 98182.7, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.88, "peak": 40.56, "min": 18.13}, "VIN": {"avg": 69.27, "peak": 116.31, "min": 29.92}, "VIN_SYS_5V0": {"avg": 15.19, "peak": 16.15, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 16.27, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 29.88, "energy_joules_est": 58.18, "sample_count": 19, "duration_seconds": 1.947}, "timestamp": "2026-01-19T15:08:59.214833"}
{"image_index": 321, "image_name": "000000034205.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034205.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2159.199, "latencies_ms": [2159.199], "images_per_second": 0.463, "prompt_tokens": 1109, "response_tokens_est": 40, "n_tiles": 1, "output_text": " The image features a plate of food with a variety of colors, including green broccoli, brown mushrooms, and white chicken. The lighting is bright and natural, highlighting the textures and colors of the food.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 27589.5, "ram_available_mb": 98182.7, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 27604.9, "ram_available_mb": 98167.3, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.89, "peak": 40.16, "min": 17.74}, "VIN": {"avg": 69.35, "peak": 127.41, "min": 30.13}, "VIN_SYS_5V0": {"avg": 15.04, "peak": 16.05, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 16.05, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 28.89, "energy_joules_est": 62.4, "sample_count": 21, "duration_seconds": 2.16}, "timestamp": "2026-01-19T15:09:01.388667"}
{"image_index": 322, "image_name": "000000034257.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034257.jpg", "image_width": 500, "image_height": 309, "image_resolution": "500x309", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2015.45, "latencies_ms": [2015.45], "images_per_second": 0.496, "prompt_tokens": 1099, "response_tokens_est": 37, "n_tiles": 1, "output_text": " The image captures a vibrant display of fresh produce, with a variety of vegetables and fruits neatly arranged on wooden crates and baskets, creating a visually appealing and healthy assortment of food items.", "error": null, "sys_before": {"cpu_percent": 14.3, "ram_used_mb": 27604.9, "ram_available_mb": 98167.3, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 27608.2, "ram_available_mb": 98163.9, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.17, "peak": 40.56, "min": 17.34}, "VIN": {"avg": 67.1, "peak": 117.9, "min": 29.44}, "VIN_SYS_5V0": {"avg": 15.08, "peak": 16.15, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 16.18, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 29.17, "energy_joules_est": 58.8, "sample_count": 20, "duration_seconds": 2.016}, "timestamp": "2026-01-19T15:09:03.478595"}
{"image_index": 322, "image_name": "000000034257.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034257.jpg", "image_width": 500, "image_height": 309, "image_resolution": "500x309", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2026.122, "latencies_ms": [2026.122], "images_per_second": 0.494, "prompt_tokens": 1113, "response_tokens_est": 36, "n_tiles": 1, "output_text": " carrot: 12, cabbage: 1, broccoli: 1, cauliflower: 1, kale: 1, lettuce: 1, fennel: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27608.2, "ram_available_mb": 98163.9, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 27617.3, "ram_available_mb": 98154.9, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.03, "peak": 40.16, "min": 16.56}, "VIN": {"avg": 72.34, "peak": 120.65, "min": 28.03}, "VIN_SYS_5V0": {"avg": 14.86, "peak": 16.05, "min": 12.15}, "VDD_CPU_SOC_MSS": {"avg": 16.0, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 29.03, "energy_joules_est": 58.84, "sample_count": 20, "duration_seconds": 2.027}, "timestamp": "2026-01-19T15:09:05.560677"}
{"image_index": 322, "image_name": "000000034257.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034257.jpg", "image_width": 500, "image_height": 309, "image_resolution": "500x309", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1992.923, "latencies_ms": [1992.923], "images_per_second": 0.502, "prompt_tokens": 1117, "response_tokens_est": 35, "n_tiles": 1, "output_text": " The carrots are in the foreground, while the broccoli is in the background. The leafy greens are in the middle ground, with the radishes being the closest to the camera.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27617.3, "ram_available_mb": 98154.9, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 27627.9, "ram_available_mb": 98144.3, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.09, "peak": 40.56, "min": 16.56}, "VIN": {"avg": 70.78, "peak": 125.94, "min": 27.84}, "VIN_SYS_5V0": {"avg": 14.93, "peak": 16.05, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 16.02, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 29.09, "energy_joules_est": 57.99, "sample_count": 20, "duration_seconds": 1.993}, "timestamp": "2026-01-19T15:09:07.642731"}
{"image_index": 322, "image_name": "000000034257.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034257.jpg", "image_width": 500, "image_height": 309, "image_resolution": "500x309", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2465.512, "latencies_ms": [2465.512], "images_per_second": 0.406, "prompt_tokens": 1111, "response_tokens_est": 53, "n_tiles": 1, "output_text": " The image captures a vibrant display of fresh produce at a market stall, with a variety of vegetables and fruits neatly arranged in baskets and crates. The colors and textures of the produce create a visually appealing scene, showcasing the freshness and abundance of the items on display.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 27627.9, "ram_available_mb": 98144.3, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 27631.7, "ram_available_mb": 98140.5, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.55, "peak": 40.16, "min": 16.16}, "VIN": {"avg": 72.5, "peak": 121.36, "min": 30.67}, "VIN_SYS_5V0": {"avg": 14.87, "peak": 16.05, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.96, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 27.55, "energy_joules_est": 67.94, "sample_count": 24, "duration_seconds": 2.466}, "timestamp": "2026-01-19T15:09:10.140953"}
{"image_index": 322, "image_name": "000000034257.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034257.jpg", "image_width": 500, "image_height": 309, "image_resolution": "500x309", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2623.133, "latencies_ms": [2623.133], "images_per_second": 0.381, "prompt_tokens": 1109, "response_tokens_est": 60, "n_tiles": 1, "output_text": " The image features a variety of fresh vegetables, including carrots, broccoli, and cauliflower, displayed in a rustic wooden crate. The lighting is natural and soft, highlighting the vibrant colors of the produce. The vegetables are arranged in a way that showcases their textures and colors, creating a visually appealing display.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27631.7, "ram_available_mb": 98140.5, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 27638.5, "ram_available_mb": 98133.6, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.94, "peak": 40.18, "min": 16.56}, "VIN": {"avg": 69.02, "peak": 99.49, "min": 28.2}, "VIN_SYS_5V0": {"avg": 15.03, "peak": 16.15, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 16.01, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 26.94, "energy_joules_est": 70.68, "sample_count": 26, "duration_seconds": 2.624}, "timestamp": "2026-01-19T15:09:12.848989"}
{"image_index": 323, "image_name": "000000034417.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034417.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2128.284, "latencies_ms": [2128.284], "images_per_second": 0.47, "prompt_tokens": 1099, "response_tokens_est": 41, "n_tiles": 1, "output_text": " The image captures a bustling scene inside a donut shop, where a variety of donuts are being prepared on a conveyor belt, with workers diligently handling the doughnuts and ensuring they are cooked to perfection.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27638.5, "ram_available_mb": 98133.6, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 27638.5, "ram_available_mb": 98133.7, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.42, "peak": 40.16, "min": 14.98}, "VIN": {"avg": 75.64, "peak": 122.51, "min": 28.74}, "VIN_SYS_5V0": {"avg": 14.88, "peak": 16.05, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.97, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 28.42, "energy_joules_est": 60.5, "sample_count": 21, "duration_seconds": 2.129}, "timestamp": "2026-01-19T15:09:15.036449"}
{"image_index": 323, "image_name": "000000034417.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034417.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1489.338, "latencies_ms": [1489.338], "images_per_second": 0.671, "prompt_tokens": 1113, "response_tokens_est": 16, "n_tiles": 1, "output_text": " donut: 10\nmachine: 2\nperson: 3", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 27638.5, "ram_available_mb": 98133.7, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 27652.8, "ram_available_mb": 98119.4, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.36, "peak": 40.57, "min": 16.56}, "VIN": {"avg": 75.43, "peak": 118.01, "min": 29.05}, "VIN_SYS_5V0": {"avg": 14.95, "peak": 16.05, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 16.22, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 31.36, "energy_joules_est": 46.71, "sample_count": 15, "duration_seconds": 1.49}, "timestamp": "2026-01-19T15:09:16.604616"}
{"image_index": 323, "image_name": "000000034417.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034417.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2801.36, "latencies_ms": [2801.36], "images_per_second": 0.357, "prompt_tokens": 1117, "response_tokens_est": 66, "n_tiles": 1, "output_text": " The donut machine is located in the foreground, with the donuts being processed on the left side. The donut machine is positioned in the middle of the image, with the donuts being processed on the right side. The donut machine is located in the foreground, with the donuts being processed on the left side.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27652.8, "ram_available_mb": 98119.4, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 6.4, "ram_used_mb": 27569.1, "ram_available_mb": 98203.0, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.24, "peak": 40.97, "min": 19.31}, "VIN": {"avg": 73.93, "peak": 120.9, "min": 29.85}, "VIN_SYS_5V0": {"avg": 14.94, "peak": 16.05, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 16.24, "peak": 17.72, "min": 13.39}}, "power_watts_avg": 27.24, "energy_joules_est": 76.32, "sample_count": 27, "duration_seconds": 2.802}, "timestamp": "2026-01-19T15:09:19.428173"}
{"image_index": 323, "image_name": "000000034417.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034417.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2250.659, "latencies_ms": [2250.659], "images_per_second": 0.444, "prompt_tokens": 1111, "response_tokens_est": 46, "n_tiles": 1, "output_text": " The image captures a bustling scene inside a donut shop, where a variety of donuts are being prepared on a conveyor belt. The shop is well-lit, with customers waiting in line to purchase the freshly made donuts.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27569.1, "ram_available_mb": 98203.0, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 27567.6, "ram_available_mb": 98204.6, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.33, "peak": 40.18, "min": 16.95}, "VIN": {"avg": 66.64, "peak": 129.11, "min": 28.5}, "VIN_SYS_5V0": {"avg": 15.14, "peak": 16.15, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 16.28, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 28.33, "energy_joules_est": 63.78, "sample_count": 22, "duration_seconds": 2.251}, "timestamp": "2026-01-19T15:09:21.717693"}
{"image_index": 323, "image_name": "000000034417.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034417.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2174.82, "latencies_ms": [2174.82], "images_per_second": 0.46, "prompt_tokens": 1109, "response_tokens_est": 43, "n_tiles": 1, "output_text": " The image is taken in a brightly lit bakery with a green wall and a red sign that reads \"HOT\". The bakery is filled with people and various equipment, including a conveyor belt with donuts on it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27567.6, "ram_available_mb": 98204.6, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 27568.1, "ram_available_mb": 98204.1, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.74, "peak": 40.56, "min": 16.56}, "VIN": {"avg": 72.82, "peak": 128.82, "min": 28.86}, "VIN_SYS_5V0": {"avg": 15.14, "peak": 16.15, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 16.31, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 28.74, "energy_joules_est": 62.52, "sample_count": 21, "duration_seconds": 2.175}, "timestamp": "2026-01-19T15:09:23.903329"}
{"image_index": 324, "image_name": "000000034452.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034452.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1645.886, "latencies_ms": [1645.886], "images_per_second": 0.608, "prompt_tokens": 1099, "response_tokens_est": 23, "n_tiles": 1, "output_text": " A man in a green jacket and white shoes is bending over in a forest, holding an orange frisbee.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 27568.1, "ram_available_mb": 98204.1, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 27569.8, "ram_available_mb": 98202.3, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.32, "peak": 39.77, "min": 18.13}, "VIN": {"avg": 72.02, "peak": 99.88, "min": 30.01}, "VIN_SYS_5V0": {"avg": 15.29, "peak": 16.15, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 16.44, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 31.32, "energy_joules_est": 51.56, "sample_count": 16, "duration_seconds": 1.646}, "timestamp": "2026-01-19T15:09:25.577979"}
{"image_index": 324, "image_name": "000000034452.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034452.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2571.686, "latencies_ms": [2571.686], "images_per_second": 0.389, "prompt_tokens": 1113, "response_tokens_est": 58, "n_tiles": 1, "output_text": " 1. person: 1\n2. tree: 1\n3. frisbee: 1\n4. ground: 1\n5. leaves: 1\n6. sticks: 1\n7. grass: 1\n8. sky: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27569.8, "ram_available_mb": 98202.3, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 27579.4, "ram_available_mb": 98192.8, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.94, "peak": 40.57, "min": 19.31}, "VIN": {"avg": 72.43, "peak": 127.48, "min": 28.88}, "VIN_SYS_5V0": {"avg": 15.17, "peak": 16.15, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 16.33, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 27.94, "energy_joules_est": 71.86, "sample_count": 25, "duration_seconds": 2.572}, "timestamp": "2026-01-19T15:09:28.174748"}
{"image_index": 324, "image_name": "000000034452.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034452.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2080.831, "latencies_ms": [2080.831], "images_per_second": 0.481, "prompt_tokens": 1117, "response_tokens_est": 39, "n_tiles": 1, "output_text": " The man is standing in the middle of the forest, with the trees surrounding him. The orange frisbee is in front of him, and the white frisbee is in his hand.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 27579.4, "ram_available_mb": 98192.8, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 27591.3, "ram_available_mb": 98180.9, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.05, "peak": 40.16, "min": 16.55}, "VIN": {"avg": 77.08, "peak": 126.02, "min": 59.55}, "VIN_SYS_5V0": {"avg": 15.17, "peak": 16.15, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 16.3, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 29.05, "energy_joules_est": 60.46, "sample_count": 20, "duration_seconds": 2.081}, "timestamp": "2026-01-19T15:09:30.260528"}
{"image_index": 324, "image_name": "000000034452.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034452.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1501.195, "latencies_ms": [1501.195], "images_per_second": 0.666, "prompt_tokens": 1111, "response_tokens_est": 18, "n_tiles": 1, "output_text": " A man in a green jacket and white shoes is playing frisbee in a forest.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27591.3, "ram_available_mb": 98180.9, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 27595.4, "ram_available_mb": 98176.8, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.98, "peak": 40.16, "min": 18.91}, "VIN": {"avg": 69.29, "peak": 89.07, "min": 29.83}, "VIN_SYS_5V0": {"avg": 15.37, "peak": 16.26, "min": 13.39}, "VDD_CPU_SOC_MSS": {"avg": 16.64, "peak": 17.71, "min": 14.97}}, "power_watts_avg": 31.98, "energy_joules_est": 48.02, "sample_count": 15, "duration_seconds": 1.502}, "timestamp": "2026-01-19T15:09:31.828585"}
{"image_index": 324, "image_name": "000000034452.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034452.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2250.32, "latencies_ms": [2250.32], "images_per_second": 0.444, "prompt_tokens": 1109, "response_tokens_est": 45, "n_tiles": 1, "output_text": " The image features a person in a green jacket and white shoes, standing in a forest with a brown tree trunk and a white frisbee in their hand. The lighting is natural, and the weather appears to be clear.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27595.4, "ram_available_mb": 98176.8, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 27602.9, "ram_available_mb": 98169.3, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.92, "peak": 40.56, "min": 19.7}, "VIN": {"avg": 62.65, "peak": 127.32, "min": 30.8}, "VIN_SYS_5V0": {"avg": 14.99, "peak": 16.15, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 16.18, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 28.92, "energy_joules_est": 65.09, "sample_count": 22, "duration_seconds": 2.251}, "timestamp": "2026-01-19T15:09:34.114158"}
{"image_index": 325, "image_name": "000000034760.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034760.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1677.477, "latencies_ms": [1677.477], "images_per_second": 0.596, "prompt_tokens": 1100, "response_tokens_est": 24, "n_tiles": 1, "output_text": " The image depicts a bathroom with a white sink, a white toilet, and a shower curtain with blue and green stripes.", "error": null, "sys_before": {"cpu_percent": 14.3, "ram_used_mb": 27602.9, "ram_available_mb": 98169.3, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 27614.9, "ram_available_mb": 98157.3, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.2, "peak": 39.77, "min": 16.95}, "VIN": {"avg": 74.15, "peak": 121.96, "min": 28.22}, "VIN_SYS_5V0": {"avg": 15.06, "peak": 16.15, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 16.33, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 30.2, "energy_joules_est": 50.67, "sample_count": 17, "duration_seconds": 1.678}, "timestamp": "2026-01-19T15:09:35.889960"}
{"image_index": 325, "image_name": "000000034760.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034760.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2823.944, "latencies_ms": [2823.944], "images_per_second": 0.354, "prompt_tokens": 1114, "response_tokens_est": 67, "n_tiles": 1, "output_text": " 1. sink: 1\n2. mirror: 1\n3. toilet: 1\n4. shower curtain: 1\n5. can of Ajax: 1\n6. bottle of blue liquid: 1\n7. bottle of blue liquid: 1\n8. toothbrush holder: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27614.9, "ram_available_mb": 98157.3, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 27624.8, "ram_available_mb": 98147.3, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.31, "peak": 40.16, "min": 17.34}, "VIN": {"avg": 64.87, "peak": 124.54, "min": 29.12}, "VIN_SYS_5V0": {"avg": 14.84, "peak": 16.05, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 16.0, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 26.31, "energy_joules_est": 74.31, "sample_count": 28, "duration_seconds": 2.824}, "timestamp": "2026-01-19T15:09:38.804384"}
{"image_index": 325, "image_name": "000000034760.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034760.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2412.681, "latencies_ms": [2412.681], "images_per_second": 0.414, "prompt_tokens": 1118, "response_tokens_est": 51, "n_tiles": 1, "output_text": " The sink is located to the left of the mirror, and the toilet is situated to the right of the sink. The shower curtain is hanging above the toilet, and the can of Ajax is placed on the countertop to the left of the sink.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27624.8, "ram_available_mb": 98147.3, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 27635.8, "ram_available_mb": 98136.4, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.05, "peak": 40.16, "min": 14.98}, "VIN": {"avg": 73.67, "peak": 128.36, "min": 29.55}, "VIN_SYS_5V0": {"avg": 14.82, "peak": 16.05, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.85, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 27.05, "energy_joules_est": 65.27, "sample_count": 24, "duration_seconds": 2.413}, "timestamp": "2026-01-19T15:09:41.308981"}
{"image_index": 325, "image_name": "000000034760.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034760.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1529.646, "latencies_ms": [1529.646], "images_per_second": 0.654, "prompt_tokens": 1112, "response_tokens_est": 17, "n_tiles": 1, "output_text": " A bathroom with a sink, toilet, and shower curtain is shown in the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27635.8, "ram_available_mb": 98136.4, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 27637.2, "ram_available_mb": 98135.0, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.41, "peak": 39.78, "min": 15.38}, "VIN": {"avg": 73.18, "peak": 114.74, "min": 28.97}, "VIN_SYS_5V0": {"avg": 14.77, "peak": 16.15, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.93, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 30.41, "energy_joules_est": 46.53, "sample_count": 15, "duration_seconds": 1.53}, "timestamp": "2026-01-19T15:09:42.876048"}
{"image_index": 325, "image_name": "000000034760.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034760.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1869.718, "latencies_ms": [1869.718], "images_per_second": 0.535, "prompt_tokens": 1110, "response_tokens_est": 26, "n_tiles": 1, "output_text": " The bathroom is well-lit with natural light coming from a window, and the walls are painted in a light beige color.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27637.2, "ram_available_mb": 98135.0, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 6.9, "ram_used_mb": 27558.7, "ram_available_mb": 98213.4, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.48, "peak": 40.56, "min": 20.1}, "VIN": {"avg": 72.47, "peak": 105.86, "min": 34.65}, "VIN_SYS_5V0": {"avg": 15.01, "peak": 16.26, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 16.36, "peak": 17.71, "min": 14.18}}, "power_watts_avg": 30.48, "energy_joules_est": 57.0, "sample_count": 18, "duration_seconds": 1.87}, "timestamp": "2026-01-19T15:09:44.751255"}
{"image_index": 326, "image_name": "000000034873.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034873.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1858.291, "latencies_ms": [1858.291], "images_per_second": 0.538, "prompt_tokens": 1099, "response_tokens_est": 30, "n_tiles": 1, "output_text": " The image depicts a modern kitchen with a large island, a sink, and a dining table with chairs, all set against a white wall and window.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27558.7, "ram_available_mb": 98213.4, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 27548.5, "ram_available_mb": 98223.7, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.12, "peak": 40.18, "min": 18.13}, "VIN": {"avg": 70.82, "peak": 102.53, "min": 29.92}, "VIN_SYS_5V0": {"avg": 15.1, "peak": 16.15, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 16.25, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 30.12, "energy_joules_est": 55.99, "sample_count": 18, "duration_seconds": 1.859}, "timestamp": "2026-01-19T15:09:46.630875"}
{"image_index": 326, "image_name": "000000034873.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034873.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1966.863, "latencies_ms": [1966.863], "images_per_second": 0.508, "prompt_tokens": 1113, "response_tokens_est": 35, "n_tiles": 1, "output_text": " sink: 1, countertop: 1, cabinet: 1, window: 1, chair: 1, table: 1, door: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27548.5, "ram_available_mb": 98223.7, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 27552.4, "ram_available_mb": 98219.8, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.0, "peak": 40.18, "min": 18.52}, "VIN": {"avg": 74.11, "peak": 126.59, "min": 29.45}, "VIN_SYS_5V0": {"avg": 15.2, "peak": 16.15, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 16.28, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 30.0, "energy_joules_est": 59.02, "sample_count": 19, "duration_seconds": 1.967}, "timestamp": "2026-01-19T15:09:48.612170"}
{"image_index": 326, "image_name": "000000034873.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034873.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2014.292, "latencies_ms": [2014.292], "images_per_second": 0.496, "prompt_tokens": 1117, "response_tokens_est": 37, "n_tiles": 1, "output_text": " The sink is located to the left of the stove, and the dining table is situated in the background. The countertop is in the foreground, and the window is in the background.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 27552.4, "ram_available_mb": 98219.8, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 27572.3, "ram_available_mb": 98199.9, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.61, "peak": 40.18, "min": 18.52}, "VIN": {"avg": 71.05, "peak": 124.39, "min": 29.51}, "VIN_SYS_5V0": {"avg": 15.19, "peak": 16.15, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 16.28, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 29.61, "energy_joules_est": 59.66, "sample_count": 20, "duration_seconds": 2.015}, "timestamp": "2026-01-19T15:09:50.697527"}
{"image_index": 326, "image_name": "000000034873.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034873.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1440.033, "latencies_ms": [1440.033], "images_per_second": 0.694, "prompt_tokens": 1111, "response_tokens_est": 14, "n_tiles": 1, "output_text": " A kitchen with a large island and a dining table in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27572.3, "ram_available_mb": 98199.9, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 27591.8, "ram_available_mb": 98180.4, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.83, "peak": 40.57, "min": 16.56}, "VIN": {"avg": 78.96, "peak": 120.57, "min": 29.48}, "VIN_SYS_5V0": {"avg": 14.98, "peak": 16.05, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 16.11, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 31.83, "energy_joules_est": 45.85, "sample_count": 14, "duration_seconds": 1.44}, "timestamp": "2026-01-19T15:09:52.160831"}
{"image_index": 326, "image_name": "000000034873.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034873.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1453.882, "latencies_ms": [1453.882], "images_per_second": 0.688, "prompt_tokens": 1109, "response_tokens_est": 15, "n_tiles": 1, "output_text": " The kitchen has a black countertop, white walls, and wooden chairs.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27591.8, "ram_available_mb": 98180.4, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 27584.1, "ram_available_mb": 98188.0, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 33.85, "peak": 40.97, "min": 22.46}, "VIN": {"avg": 83.96, "peak": 126.16, "min": 33.44}, "VIN_SYS_5V0": {"avg": 15.3, "peak": 16.15, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 16.42, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 33.85, "energy_joules_est": 49.24, "sample_count": 14, "duration_seconds": 1.455}, "timestamp": "2026-01-19T15:09:53.620446"}
{"image_index": 327, "image_name": "000000035062.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035062.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1573.18, "latencies_ms": [1573.18], "images_per_second": 0.636, "prompt_tokens": 1100, "response_tokens_est": 20, "n_tiles": 1, "output_text": " A person is lying in bed with a blanket that has a pattern of daisies on it.", "error": null, "sys_before": {"cpu_percent": 14.3, "ram_used_mb": 27584.1, "ram_available_mb": 98188.0, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 27591.8, "ram_available_mb": 98180.4, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.4, "peak": 40.56, "min": 21.67}, "VIN": {"avg": 79.84, "peak": 130.43, "min": 27.36}, "VIN_SYS_5V0": {"avg": 15.21, "peak": 16.26, "min": 13.29}, "VDD_CPU_SOC_MSS": {"avg": 16.44, "peak": 17.71, "min": 14.57}}, "power_watts_avg": 32.4, "energy_joules_est": 50.99, "sample_count": 16, "duration_seconds": 1.574}, "timestamp": "2026-01-19T15:09:55.296238"}
{"image_index": 327, "image_name": "000000035062.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035062.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2661.125, "latencies_ms": [2661.125], "images_per_second": 0.376, "prompt_tokens": 1114, "response_tokens_est": 58, "n_tiles": 1, "output_text": " 1. person: 1\n2. bed: 1\n3. blanket: 1\n4. pillow: 1\n5. wall: 1\n6. curtain: 1\n7. bedside table: 1\n8. light: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27591.8, "ram_available_mb": 98180.4, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 27607.7, "ram_available_mb": 98164.5, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.94, "peak": 40.18, "min": 18.14}, "VIN": {"avg": 65.3, "peak": 84.64, "min": 29.4}, "VIN_SYS_5V0": {"avg": 14.77, "peak": 16.05, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.79, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 26.94, "energy_joules_est": 71.7, "sample_count": 26, "duration_seconds": 2.662}, "timestamp": "2026-01-19T15:09:57.996100"}
{"image_index": 327, "image_name": "000000035062.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035062.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2076.091, "latencies_ms": [2076.091], "images_per_second": 0.482, "prompt_tokens": 1118, "response_tokens_est": 39, "n_tiles": 1, "output_text": " The person is lying on the bed, which is positioned in the foreground of the image. The bed is located in the middle of the room, with the wall and a window in the background.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 27607.7, "ram_available_mb": 98164.5, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 27612.3, "ram_available_mb": 98159.9, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.84, "peak": 40.18, "min": 16.16}, "VIN": {"avg": 67.04, "peak": 116.13, "min": 31.32}, "VIN_SYS_5V0": {"avg": 15.07, "peak": 16.15, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 16.2, "peak": 17.71, "min": 13.39}}, "power_watts_avg": 28.84, "energy_joules_est": 59.89, "sample_count": 20, "duration_seconds": 2.077}, "timestamp": "2026-01-19T15:10:00.083457"}
{"image_index": 327, "image_name": "000000035062.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035062.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2244.053, "latencies_ms": [2244.053], "images_per_second": 0.446, "prompt_tokens": 1112, "response_tokens_est": 46, "n_tiles": 1, "output_text": " In a dimly lit room, a child lies in bed, covered with a blanket adorned with daisy patterns. The child's face is obscured, and the room is shrouded in darkness, creating an atmosphere of tranquility and solitude.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 27612.3, "ram_available_mb": 98159.9, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 27623.4, "ram_available_mb": 98148.8, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.26, "peak": 39.39, "min": 17.74}, "VIN": {"avg": 74.66, "peak": 125.12, "min": 29.34}, "VIN_SYS_5V0": {"avg": 15.19, "peak": 16.26, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 16.34, "peak": 17.71, "min": 14.18}}, "power_watts_avg": 28.26, "energy_joules_est": 63.43, "sample_count": 22, "duration_seconds": 2.244}, "timestamp": "2026-01-19T15:10:02.376928"}
{"image_index": 327, "image_name": "000000035062.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035062.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2445.795, "latencies_ms": [2445.795], "images_per_second": 0.409, "prompt_tokens": 1110, "response_tokens_est": 53, "n_tiles": 1, "output_text": " The image depicts a child sleeping in a bed with a dark blue comforter adorned with white daisy patterns. The room is dimly lit, with only a small amount of light coming from the window, casting a soft glow on the child and the bed.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27623.4, "ram_available_mb": 98148.8, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 27627.8, "ram_available_mb": 98144.4, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.43, "peak": 40.16, "min": 16.56}, "VIN": {"avg": 73.19, "peak": 121.04, "min": 29.24}, "VIN_SYS_5V0": {"avg": 14.98, "peak": 16.15, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 16.06, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 27.43, "energy_joules_est": 67.1, "sample_count": 24, "duration_seconds": 2.446}, "timestamp": "2026-01-19T15:10:04.879644"}
{"image_index": 328, "image_name": "000000035197.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035197.jpg", "image_width": 429, "image_height": 640, "image_resolution": "429x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1874.829, "latencies_ms": [1874.829], "images_per_second": 0.533, "prompt_tokens": 1100, "response_tokens_est": 31, "n_tiles": 1, "output_text": " A person is riding a skateboard on a ramp, wearing shorts and sneakers, with a man sitting on the side of the ramp in the background.", "error": null, "sys_before": {"cpu_percent": 30.0, "ram_used_mb": 27627.8, "ram_available_mb": 98144.4, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 6.6, "ram_used_mb": 27575.1, "ram_available_mb": 98197.0, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.09, "peak": 40.16, "min": 16.16}, "VIN": {"avg": 75.23, "peak": 124.18, "min": 28.46}, "VIN_SYS_5V0": {"avg": 14.91, "peak": 16.15, "min": 12.25}, "VDD_CPU_SOC_MSS": {"avg": 16.18, "peak": 18.51, "min": 13.39}}, "power_watts_avg": 29.09, "energy_joules_est": 54.55, "sample_count": 19, "duration_seconds": 1.875}, "timestamp": "2026-01-19T15:10:06.859572"}
{"image_index": 328, "image_name": "000000035197.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035197.jpg", "image_width": 429, "image_height": 640, "image_resolution": "429x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2679.202, "latencies_ms": [2679.202], "images_per_second": 0.373, "prompt_tokens": 1114, "response_tokens_est": 61, "n_tiles": 1, "output_text": " 1. skateboard: 1\n2. person: 1\n3. shorts: 1\n4. socks: 1\n5. shoes: 1\n6. skateboard wheels: 2\n7. skateboard deck: 1\n8. shadow: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27575.1, "ram_available_mb": 98197.0, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 27571.0, "ram_available_mb": 98201.2, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.82, "peak": 39.78, "min": 16.56}, "VIN": {"avg": 68.19, "peak": 95.24, "min": 30.54}, "VIN_SYS_5V0": {"avg": 14.89, "peak": 16.05, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.86, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 26.82, "energy_joules_est": 71.87, "sample_count": 26, "duration_seconds": 2.68}, "timestamp": "2026-01-19T15:10:09.565706"}
{"image_index": 328, "image_name": "000000035197.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035197.jpg", "image_width": 429, "image_height": 640, "image_resolution": "429x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2444.619, "latencies_ms": [2444.619], "images_per_second": 0.409, "prompt_tokens": 1118, "response_tokens_est": 53, "n_tiles": 1, "output_text": " The skateboarder's feet are positioned on the skateboard, with the left foot on the tail and the right foot on the nose, in the foreground. The skateboarder is in the middle of the image, with the ramp and buildings in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27571.0, "ram_available_mb": 98201.2, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 27574.4, "ram_available_mb": 98197.8, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.45, "peak": 40.18, "min": 16.56}, "VIN": {"avg": 69.09, "peak": 104.42, "min": 28.93}, "VIN_SYS_5V0": {"avg": 15.03, "peak": 16.26, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 16.13, "peak": 17.71, "min": 13.79}}, "power_watts_avg": 27.45, "energy_joules_est": 67.11, "sample_count": 24, "duration_seconds": 2.445}, "timestamp": "2026-01-19T15:10:12.063129"}
{"image_index": 328, "image_name": "000000035197.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035197.jpg", "image_width": 429, "image_height": 640, "image_resolution": "429x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1674.311, "latencies_ms": [1674.311], "images_per_second": 0.597, "prompt_tokens": 1112, "response_tokens_est": 23, "n_tiles": 1, "output_text": " A person is riding a skateboard on a ramp, while another person is sitting on the edge of the ramp.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27574.4, "ram_available_mb": 98197.8, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 27577.9, "ram_available_mb": 98194.2, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.82, "peak": 39.78, "min": 16.16}, "VIN": {"avg": 77.38, "peak": 129.22, "min": 29.56}, "VIN_SYS_5V0": {"avg": 14.91, "peak": 16.26, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 16.0, "peak": 17.71, "min": 13.4}}, "power_watts_avg": 29.82, "energy_joules_est": 49.94, "sample_count": 17, "duration_seconds": 1.675}, "timestamp": "2026-01-19T15:10:13.839977"}
{"image_index": 328, "image_name": "000000035197.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035197.jpg", "image_width": 429, "image_height": 640, "image_resolution": "429x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1931.178, "latencies_ms": [1931.178], "images_per_second": 0.518, "prompt_tokens": 1110, "response_tokens_est": 33, "n_tiles": 1, "output_text": " The image is in black and white, with the skateboarder's legs and shoes being the main focus, and the background showing a clear sky and some buildings.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 27577.9, "ram_available_mb": 98194.2, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 27581.4, "ram_available_mb": 98190.7, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.52, "peak": 40.16, "min": 17.35}, "VIN": {"avg": 73.55, "peak": 124.92, "min": 28.91}, "VIN_SYS_5V0": {"avg": 14.83, "peak": 16.05, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.98, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 29.52, "energy_joules_est": 57.02, "sample_count": 19, "duration_seconds": 1.931}, "timestamp": "2026-01-19T15:10:15.823725"}
{"image_index": 329, "image_name": "000000035279.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035279.jpg", "image_width": 640, "image_height": 463, "image_resolution": "640x463", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1681.311, "latencies_ms": [1681.311], "images_per_second": 0.595, "prompt_tokens": 1099, "response_tokens_est": 23, "n_tiles": 1, "output_text": " A man is lying on the floor with a laptop, a tennis racket, and a book on the floor.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 27581.4, "ram_available_mb": 98190.7, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 27581.5, "ram_available_mb": 98190.7, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.38, "peak": 40.16, "min": 17.34}, "VIN": {"avg": 72.18, "peak": 124.29, "min": 27.72}, "VIN_SYS_5V0": {"avg": 14.86, "peak": 16.05, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.96, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 30.38, "energy_joules_est": 51.09, "sample_count": 17, "duration_seconds": 1.682}, "timestamp": "2026-01-19T15:10:17.605817"}
{"image_index": 329, "image_name": "000000035279.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035279.jpg", "image_width": 640, "image_height": 463, "image_resolution": "640x463", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2197.51, "latencies_ms": [2197.51], "images_per_second": 0.455, "prompt_tokens": 1113, "response_tokens_est": 43, "n_tiles": 1, "output_text": " laptop: 1\ncamera: 1\nphone: 1\nracket: 1\nbottle: 1\nbook: 1\ncamera lens: 1\ncomputer mouse: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27581.5, "ram_available_mb": 98190.7, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 27592.6, "ram_available_mb": 98179.5, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.53, "peak": 40.57, "min": 17.74}, "VIN": {"avg": 70.55, "peak": 130.15, "min": 29.44}, "VIN_SYS_5V0": {"avg": 14.83, "peak": 16.05, "min": 11.87}, "VDD_CPU_SOC_MSS": {"avg": 15.84, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 28.53, "energy_joules_est": 62.7, "sample_count": 22, "duration_seconds": 2.198}, "timestamp": "2026-01-19T15:10:19.897607"}
{"image_index": 329, "image_name": "000000035279.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035279.jpg", "image_width": 640, "image_height": 463, "image_resolution": "640x463", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2447.844, "latencies_ms": [2447.844], "images_per_second": 0.409, "prompt_tokens": 1117, "response_tokens_est": 52, "n_tiles": 1, "output_text": " The laptop is in the middle of the image, with the tennis racket and camera to its right. The books are in the foreground, with the laptop and camera to its left. The man is lying on the floor, with his head near the laptop.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27592.6, "ram_available_mb": 98179.5, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 27604.3, "ram_available_mb": 98167.8, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.4, "peak": 40.16, "min": 15.77}, "VIN": {"avg": 69.32, "peak": 98.95, "min": 29.25}, "VIN_SYS_5V0": {"avg": 14.84, "peak": 16.05, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.85, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 27.4, "energy_joules_est": 67.08, "sample_count": 24, "duration_seconds": 2.448}, "timestamp": "2026-01-19T15:10:22.395977"}
{"image_index": 329, "image_name": "000000035279.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035279.jpg", "image_width": 640, "image_height": 463, "image_resolution": "640x463", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1606.402, "latencies_ms": [1606.402], "images_per_second": 0.623, "prompt_tokens": 1111, "response_tokens_est": 20, "n_tiles": 1, "output_text": " A man is lying on the floor with a laptop, a tennis racket, and a book.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27604.3, "ram_available_mb": 98167.8, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 27608.7, "ram_available_mb": 98163.5, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.7, "peak": 40.18, "min": 16.16}, "VIN": {"avg": 72.87, "peak": 124.35, "min": 29.96}, "VIN_SYS_5V0": {"avg": 14.91, "peak": 16.05, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 16.04, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 30.7, "energy_joules_est": 49.33, "sample_count": 16, "duration_seconds": 1.607}, "timestamp": "2026-01-19T15:10:24.066637"}
{"image_index": 329, "image_name": "000000035279.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035279.jpg", "image_width": 640, "image_height": 463, "image_resolution": "640x463", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1960.51, "latencies_ms": [1960.51], "images_per_second": 0.51, "prompt_tokens": 1109, "response_tokens_est": 34, "n_tiles": 1, "output_text": " The image is taken in a well-lit room with natural light coming from the window. The colors in the image are vibrant and the materials are mostly plastic and metal.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27608.7, "ram_available_mb": 98163.5, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 27621.0, "ram_available_mb": 98151.2, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.12, "peak": 40.56, "min": 19.31}, "VIN": {"avg": 74.88, "peak": 124.53, "min": 29.12}, "VIN_SYS_5V0": {"avg": 14.97, "peak": 16.05, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 16.14, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 30.12, "energy_joules_est": 59.06, "sample_count": 19, "duration_seconds": 1.961}, "timestamp": "2026-01-19T15:10:26.047435"}
{"image_index": 330, "image_name": "000000035326.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035326.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1700.061, "latencies_ms": [1700.061], "images_per_second": 0.588, "prompt_tokens": 1099, "response_tokens_est": 25, "n_tiles": 1, "output_text": " The image depicts a kitchen with a black stove top, a black range hood above it, and a marble backsplash.", "error": null, "sys_before": {"cpu_percent": 22.2, "ram_used_mb": 27621.0, "ram_available_mb": 98151.2, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 27627.4, "ram_available_mb": 98144.8, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.98, "peak": 40.16, "min": 18.52}, "VIN": {"avg": 74.66, "peak": 123.53, "min": 28.64}, "VIN_SYS_5V0": {"avg": 15.18, "peak": 16.15, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 16.26, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 30.98, "energy_joules_est": 52.68, "sample_count": 17, "duration_seconds": 1.701}, "timestamp": "2026-01-19T15:10:27.824473"}
{"image_index": 330, "image_name": "000000035326.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035326.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2287.102, "latencies_ms": [2287.102], "images_per_second": 0.437, "prompt_tokens": 1113, "response_tokens_est": 47, "n_tiles": 1, "output_text": " 1. black stove top\n2. black range hood\n3. black knife block\n4. black pot\n5. black pot lid\n6. black pot handle\n7. black pot base\n8. black pot ring", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 27627.4, "ram_available_mb": 98144.8, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 27647.8, "ram_available_mb": 98124.4, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.18, "peak": 40.57, "min": 17.73}, "VIN": {"avg": 69.44, "peak": 87.35, "min": 28.6}, "VIN_SYS_5V0": {"avg": 14.97, "peak": 16.05, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 16.06, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 28.18, "energy_joules_est": 64.46, "sample_count": 23, "duration_seconds": 2.287}, "timestamp": "2026-01-19T15:10:30.216534"}
{"image_index": 330, "image_name": "000000035326.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035326.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2152.45, "latencies_ms": [2152.45], "images_per_second": 0.465, "prompt_tokens": 1117, "response_tokens_est": 41, "n_tiles": 1, "output_text": " The stove is located in the foreground, with the spice rack and hanging towels positioned further back. The spice rack is situated to the left of the stove, while the hanging towels are located to the right.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27647.8, "ram_available_mb": 98124.4, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 6.6, "ram_used_mb": 27557.4, "ram_available_mb": 98214.8, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.36, "peak": 40.18, "min": 14.98}, "VIN": {"avg": 70.27, "peak": 112.75, "min": 29.36}, "VIN_SYS_5V0": {"avg": 14.89, "peak": 16.05, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 16.16, "peak": 17.71, "min": 13.0}}, "power_watts_avg": 28.36, "energy_joules_est": 61.05, "sample_count": 21, "duration_seconds": 2.153}, "timestamp": "2026-01-19T15:10:32.397414"}
{"image_index": 330, "image_name": "000000035326.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035326.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1404.057, "latencies_ms": [1404.057], "images_per_second": 0.712, "prompt_tokens": 1111, "response_tokens_est": 13, "n_tiles": 1, "output_text": " A kitchen with a stove and a spice rack on the counter.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27557.4, "ram_available_mb": 98214.8, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 27550.7, "ram_available_mb": 98221.5, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.99, "peak": 40.56, "min": 17.34}, "VIN": {"avg": 72.82, "peak": 122.53, "min": 29.61}, "VIN_SYS_5V0": {"avg": 15.13, "peak": 16.15, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 16.28, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 31.99, "energy_joules_est": 44.93, "sample_count": 14, "duration_seconds": 1.405}, "timestamp": "2026-01-19T15:10:33.863128"}
{"image_index": 330, "image_name": "000000035326.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035326.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1587.492, "latencies_ms": [1587.492], "images_per_second": 0.63, "prompt_tokens": 1109, "response_tokens_est": 20, "n_tiles": 1, "output_text": " The kitchen has a marble backsplash, a black stove top, and a black range hood.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 27550.7, "ram_available_mb": 98221.5, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 27556.4, "ram_available_mb": 98215.7, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.21, "peak": 40.97, "min": 21.29}, "VIN": {"avg": 73.27, "peak": 129.68, "min": 28.97}, "VIN_SYS_5V0": {"avg": 14.99, "peak": 16.05, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 16.14, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 32.21, "energy_joules_est": 51.14, "sample_count": 16, "duration_seconds": 1.588}, "timestamp": "2026-01-19T15:10:35.538847"}
{"image_index": 331, "image_name": "000000035682.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035682.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1735.427, "latencies_ms": [1735.427], "images_per_second": 0.576, "prompt_tokens": 1100, "response_tokens_est": 26, "n_tiles": 1, "output_text": " A person is sitting at a table in a coffee shop, eating a chocolate donut with sprinkles and a cup of coffee.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27556.4, "ram_available_mb": 98215.7, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 27565.6, "ram_available_mb": 98206.6, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.7, "peak": 40.56, "min": 18.14}, "VIN": {"avg": 63.74, "peak": 106.97, "min": 27.96}, "VIN_SYS_5V0": {"avg": 14.87, "peak": 16.05, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 16.03, "peak": 17.71, "min": 13.0}}, "power_watts_avg": 30.7, "energy_joules_est": 53.29, "sample_count": 17, "duration_seconds": 1.736}, "timestamp": "2026-01-19T15:10:37.321761"}
{"image_index": 331, "image_name": "000000035682.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035682.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2674.278, "latencies_ms": [2674.278], "images_per_second": 0.374, "prompt_tokens": 1114, "response_tokens_est": 62, "n_tiles": 1, "output_text": " 1. cup: 1\n2. plate: 1\n3. donut: 1\n4. donut: 1\n5. donut: 1\n6. donut: 1\n7. donut: 1\n8. donut: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27565.6, "ram_available_mb": 98206.6, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 27577.3, "ram_available_mb": 98194.8, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.23, "peak": 40.56, "min": 18.52}, "VIN": {"avg": 64.25, "peak": 104.07, "min": 30.47}, "VIN_SYS_5V0": {"avg": 15.06, "peak": 16.26, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 16.16, "peak": 17.71, "min": 13.79}}, "power_watts_avg": 27.23, "energy_joules_est": 72.83, "sample_count": 26, "duration_seconds": 2.675}, "timestamp": "2026-01-19T15:10:40.026718"}
{"image_index": 331, "image_name": "000000035682.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035682.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2478.982, "latencies_ms": [2478.982], "images_per_second": 0.403, "prompt_tokens": 1118, "response_tokens_est": 55, "n_tiles": 1, "output_text": " The main objects are positioned in the foreground, with the person in the foreground holding a donut and the table in the background. The donut is placed in the center of the table, with the coffee cup to the left and the plate of donuts to the right.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27577.3, "ram_available_mb": 98194.8, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 27583.2, "ram_available_mb": 98189.0, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.5, "peak": 39.77, "min": 16.55}, "VIN": {"avg": 74.25, "peak": 123.07, "min": 29.65}, "VIN_SYS_5V0": {"avg": 15.12, "peak": 16.15, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 16.22, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 27.5, "energy_joules_est": 68.18, "sample_count": 24, "duration_seconds": 2.479}, "timestamp": "2026-01-19T15:10:42.534224"}
{"image_index": 331, "image_name": "000000035682.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035682.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1693.658, "latencies_ms": [1693.658], "images_per_second": 0.59, "prompt_tokens": 1112, "response_tokens_est": 25, "n_tiles": 1, "output_text": " A young man is sitting at a table in a coffee shop, eating a chocolate donut and drinking a cup of coffee.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27583.2, "ram_available_mb": 98189.0, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 27596.6, "ram_available_mb": 98175.5, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.2, "peak": 39.39, "min": 16.95}, "VIN": {"avg": 78.61, "peak": 121.72, "min": 28.63}, "VIN_SYS_5V0": {"avg": 15.13, "peak": 16.26, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 16.3, "peak": 17.71, "min": 14.18}}, "power_watts_avg": 30.2, "energy_joules_est": 51.16, "sample_count": 17, "duration_seconds": 1.694}, "timestamp": "2026-01-19T15:10:44.312201"}
{"image_index": 331, "image_name": "000000035682.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035682.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1732.448, "latencies_ms": [1732.448], "images_per_second": 0.577, "prompt_tokens": 1110, "response_tokens_est": 25, "n_tiles": 1, "output_text": " The image is taken in a dimly lit cafe with warm lighting, and the food is presented on a brown tray.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27596.6, "ram_available_mb": 98175.5, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 27603.1, "ram_available_mb": 98169.0, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.5, "peak": 40.18, "min": 17.74}, "VIN": {"avg": 73.27, "peak": 124.32, "min": 29.33}, "VIN_SYS_5V0": {"avg": 14.9, "peak": 16.05, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 15.96, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 30.5, "energy_joules_est": 52.85, "sample_count": 17, "duration_seconds": 1.733}, "timestamp": "2026-01-19T15:10:46.075670"}
{"image_index": 332, "image_name": "000000035770.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035770.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1726.259, "latencies_ms": [1726.259], "images_per_second": 0.579, "prompt_tokens": 1100, "response_tokens_est": 26, "n_tiles": 1, "output_text": " The image depicts a bathroom with a white sink, a black and white toilet seat, and a glass-topped countertop.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27603.1, "ram_available_mb": 98169.0, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 27617.3, "ram_available_mb": 98154.9, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.96, "peak": 40.18, "min": 18.91}, "VIN": {"avg": 77.4, "peak": 129.21, "min": 28.73}, "VIN_SYS_5V0": {"avg": 15.17, "peak": 16.15, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 16.3, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 30.96, "energy_joules_est": 53.46, "sample_count": 17, "duration_seconds": 1.727}, "timestamp": "2026-01-19T15:10:47.851836"}
{"image_index": 332, "image_name": "000000035770.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035770.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2664.48, "latencies_ms": [2664.48], "images_per_second": 0.375, "prompt_tokens": 1114, "response_tokens_est": 62, "n_tiles": 1, "output_text": " 1. Toilet: 1\n2. Sink: 1\n3. Bathtub: 1\n4. Tile: 1\n5. Toilet seat: 1\n6. Bowl: 1\n7. Food: 1\n8. Glass: 1", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 27617.3, "ram_available_mb": 98154.9, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 27623.9, "ram_available_mb": 98148.2, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.42, "peak": 40.57, "min": 18.52}, "VIN": {"avg": 70.12, "peak": 104.01, "min": 28.96}, "VIN_SYS_5V0": {"avg": 15.04, "peak": 16.26, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 16.2, "peak": 17.71, "min": 13.79}}, "power_watts_avg": 27.42, "energy_joules_est": 73.07, "sample_count": 26, "duration_seconds": 2.665}, "timestamp": "2026-01-19T15:10:50.546320"}
{"image_index": 332, "image_name": "000000035770.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035770.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2837.228, "latencies_ms": [2837.228], "images_per_second": 0.352, "prompt_tokens": 1118, "response_tokens_est": 69, "n_tiles": 1, "output_text": " The sink is located to the left of the toilet, which is situated in the foreground of the image. The toilet lid is black and white, with a pattern resembling a cow's face, and is positioned near the center of the image. The sink is white and has a transparent glass top, and is situated to the right of the toilet.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27623.9, "ram_available_mb": 98148.2, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 27630.8, "ram_available_mb": 98141.3, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.37, "peak": 40.18, "min": 16.55}, "VIN": {"avg": 64.35, "peak": 128.63, "min": 30.07}, "VIN_SYS_5V0": {"avg": 15.04, "peak": 16.26, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 16.07, "peak": 17.71, "min": 14.18}}, "power_watts_avg": 26.37, "energy_joules_est": 74.83, "sample_count": 28, "duration_seconds": 2.838}, "timestamp": "2026-01-19T15:10:53.451070"}
{"image_index": 332, "image_name": "000000035770.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035770.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1590.037, "latencies_ms": [1590.037], "images_per_second": 0.629, "prompt_tokens": 1112, "response_tokens_est": 20, "n_tiles": 1, "output_text": " A bathroom with a black and white toilet, a sink, and a blue and white tile wall.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27630.8, "ram_available_mb": 98141.3, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 27635.1, "ram_available_mb": 98137.0, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.24, "peak": 39.39, "min": 15.77}, "VIN": {"avg": 77.71, "peak": 122.19, "min": 27.19}, "VIN_SYS_5V0": {"avg": 14.9, "peak": 16.26, "min": 12.15}, "VDD_CPU_SOC_MSS": {"avg": 15.99, "peak": 17.71, "min": 13.39}}, "power_watts_avg": 30.24, "energy_joules_est": 48.1, "sample_count": 16, "duration_seconds": 1.59}, "timestamp": "2026-01-19T15:10:55.113836"}
{"image_index": 332, "image_name": "000000035770.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035770.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1673.386, "latencies_ms": [1673.386], "images_per_second": 0.598, "prompt_tokens": 1110, "response_tokens_est": 23, "n_tiles": 1, "output_text": " The bathroom has a black and white checkered floor, a white sink, and a black and white toilet.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27635.1, "ram_available_mb": 98137.0, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 6.8, "ram_used_mb": 27561.1, "ram_available_mb": 98211.1, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.68, "peak": 40.16, "min": 18.53}, "VIN": {"avg": 73.33, "peak": 125.26, "min": 28.31}, "VIN_SYS_5V0": {"avg": 14.9, "peak": 16.15, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 16.19, "peak": 18.11, "min": 13.39}}, "power_watts_avg": 30.68, "energy_joules_est": 51.35, "sample_count": 17, "duration_seconds": 1.674}, "timestamp": "2026-01-19T15:10:56.888090"}
{"image_index": 333, "image_name": "000000035963.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035963.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 3293.504, "latencies_ms": [3293.504], "images_per_second": 0.304, "prompt_tokens": 1100, "response_tokens_est": 87, "n_tiles": 1, "output_text": " In the image, there is a memorial for Joseph Panis, who passed away on May 11, 1998, at the age of 41. The memorial is adorned with teddy bears and a sign that reads \"Joseph Panis\" and \"Mysti Mallean.\" The memorial is situated in a garden, surrounded by a fence and various plants, including a large green plant in the background.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 27561.1, "ram_available_mb": 98211.1, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 27561.7, "ram_available_mb": 98210.5, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 25.61, "peak": 40.16, "min": 17.35}, "VIN": {"avg": 72.47, "peak": 121.52, "min": 29.01}, "VIN_SYS_5V0": {"avg": 14.94, "peak": 16.05, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.9, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 25.61, "energy_joules_est": 84.37, "sample_count": 32, "duration_seconds": 3.294}, "timestamp": "2026-01-19T15:11:00.203467"}
{"image_index": 333, "image_name": "000000035963.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035963.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2509.994, "latencies_ms": [2509.994], "images_per_second": 0.398, "prompt_tokens": 1114, "response_tokens_est": 57, "n_tiles": 1, "output_text": " 1. teddy bear: 3\n2. sign: 1\n3. grass: 1\n4. tree: 1\n5. fence: 1\n6. wall: 1\n7. house: 1\n8. path: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27561.7, "ram_available_mb": 98210.5, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 27561.8, "ram_available_mb": 98210.3, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.31, "peak": 39.78, "min": 16.95}, "VIN": {"avg": 69.56, "peak": 118.16, "min": 28.21}, "VIN_SYS_5V0": {"avg": 15.15, "peak": 16.26, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 16.11, "peak": 17.71, "min": 14.18}}, "power_watts_avg": 27.31, "energy_joules_est": 68.56, "sample_count": 25, "duration_seconds": 2.51}, "timestamp": "2026-01-19T15:11:02.808212"}
{"image_index": 333, "image_name": "000000035963.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035963.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2241.811, "latencies_ms": [2241.811], "images_per_second": 0.446, "prompt_tokens": 1118, "response_tokens_est": 45, "n_tiles": 1, "output_text": " The teddy bears are positioned on the right side of the cross, which is located in the middle of the image. The cross is situated in the foreground of the image, with the background featuring a grassy area and a building.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 27561.8, "ram_available_mb": 98210.3, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 27565.2, "ram_available_mb": 98206.9, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.94, "peak": 40.16, "min": 15.38}, "VIN": {"avg": 69.23, "peak": 126.27, "min": 30.37}, "VIN_SYS_5V0": {"avg": 14.84, "peak": 16.15, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.84, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 27.94, "energy_joules_est": 62.64, "sample_count": 22, "duration_seconds": 2.242}, "timestamp": "2026-01-19T15:11:05.090593"}
{"image_index": 333, "image_name": "000000035963.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035963.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1724.49, "latencies_ms": [1724.49], "images_per_second": 0.58, "prompt_tokens": 1112, "response_tokens_est": 26, "n_tiles": 1, "output_text": " A teddy bear is placed on a grave with a sign that reads \"Joseph Pans\" and \"Mysti Malain\".", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27565.2, "ram_available_mb": 98206.9, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 27572.7, "ram_available_mb": 98199.4, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.08, "peak": 39.39, "min": 16.56}, "VIN": {"avg": 73.97, "peak": 119.16, "min": 29.51}, "VIN_SYS_5V0": {"avg": 15.12, "peak": 16.15, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 16.21, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 30.08, "energy_joules_est": 51.89, "sample_count": 17, "duration_seconds": 1.725}, "timestamp": "2026-01-19T15:11:06.867924"}
{"image_index": 333, "image_name": "000000035963.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035963.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3144.799, "latencies_ms": [3144.799], "images_per_second": 0.318, "prompt_tokens": 1110, "response_tokens_est": 82, "n_tiles": 1, "output_text": " The image depicts a teddy bear cross with a white teddy bear on top, placed in a grassy area with a wooden sign that reads \"Joseph Panis\" and \"Mysti Mallean.\" The teddy bear is positioned on a stone pedestal, and the cross is surrounded by a small garden with plants and a pathway. The lighting appears to be natural daylight, and the weather seems to be clear.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27572.7, "ram_available_mb": 98199.4, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 27582.1, "ram_available_mb": 98190.1, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 25.95, "peak": 40.57, "min": 17.74}, "VIN": {"avg": 67.2, "peak": 102.44, "min": 28.77}, "VIN_SYS_5V0": {"avg": 15.04, "peak": 16.26, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 16.01, "peak": 17.71, "min": 13.79}}, "power_watts_avg": 25.95, "energy_joules_est": 81.62, "sample_count": 31, "duration_seconds": 3.145}, "timestamp": "2026-01-19T15:11:10.084078"}
{"image_index": 334, "image_name": "000000036494.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036494.jpg", "image_width": 640, "image_height": 434, "image_resolution": "640x434", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1959.36, "latencies_ms": [1959.36], "images_per_second": 0.51, "prompt_tokens": 1099, "response_tokens_est": 34, "n_tiles": 1, "output_text": " The image captures a bustling restaurant with a large clock on the wall, where people are seated at tables, enjoying their meals and drinks, with a warm and inviting atmosphere.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 27582.1, "ram_available_mb": 98190.1, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 27586.0, "ram_available_mb": 98186.2, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.22, "peak": 40.16, "min": 15.38}, "VIN": {"avg": 72.73, "peak": 127.51, "min": 29.28}, "VIN_SYS_5V0": {"avg": 14.91, "peak": 16.05, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 15.89, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 29.22, "energy_joules_est": 57.28, "sample_count": 19, "duration_seconds": 1.96}, "timestamp": "2026-01-19T15:11:12.062985"}
{"image_index": 334, "image_name": "000000036494.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036494.jpg", "image_width": 640, "image_height": 434, "image_resolution": "640x434", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2017.344, "latencies_ms": [2017.344], "images_per_second": 0.496, "prompt_tokens": 1113, "response_tokens_est": 37, "n_tiles": 1, "output_text": " table: 10, chair: 20, person: 20, clock: 1, wall: 1, ceiling: 1, floor: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27586.0, "ram_available_mb": 98186.2, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 27595.9, "ram_available_mb": 98176.3, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.47, "peak": 40.56, "min": 18.14}, "VIN": {"avg": 70.77, "peak": 120.82, "min": 29.79}, "VIN_SYS_5V0": {"avg": 15.18, "peak": 16.15, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 16.3, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 29.47, "energy_joules_est": 59.47, "sample_count": 20, "duration_seconds": 2.018}, "timestamp": "2026-01-19T15:11:14.143629"}
{"image_index": 334, "image_name": "000000036494.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036494.jpg", "image_width": 640, "image_height": 434, "image_resolution": "640x434", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2334.773, "latencies_ms": [2334.773], "images_per_second": 0.428, "prompt_tokens": 1117, "response_tokens_est": 48, "n_tiles": 1, "output_text": " The clock is positioned in the background, far from the camera, and is surrounded by a dining area with tables and chairs. The dining area is located in the foreground, close to the camera, and is filled with people sitting at tables.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27595.9, "ram_available_mb": 98176.3, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 27603.3, "ram_available_mb": 98168.9, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.99, "peak": 40.16, "min": 16.95}, "VIN": {"avg": 74.12, "peak": 123.05, "min": 29.9}, "VIN_SYS_5V0": {"avg": 14.95, "peak": 16.05, "min": 12.25}, "VDD_CPU_SOC_MSS": {"avg": 16.02, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 27.99, "energy_joules_est": 65.36, "sample_count": 23, "duration_seconds": 2.335}, "timestamp": "2026-01-19T15:11:16.536238"}
{"image_index": 334, "image_name": "000000036494.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036494.jpg", "image_width": 640, "image_height": 434, "image_resolution": "640x434", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1712.226, "latencies_ms": [1712.226], "images_per_second": 0.584, "prompt_tokens": 1111, "response_tokens_est": 23, "n_tiles": 1, "output_text": " The image captures a bustling restaurant with a large clock on the wall, where people are enjoying their meals and drinks.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27603.3, "ram_available_mb": 98168.9, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 27609.5, "ram_available_mb": 98162.7, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.13, "peak": 40.18, "min": 16.16}, "VIN": {"avg": 77.54, "peak": 129.27, "min": 28.71}, "VIN_SYS_5V0": {"avg": 14.92, "peak": 16.05, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.91, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 30.13, "energy_joules_est": 51.6, "sample_count": 17, "duration_seconds": 1.712}, "timestamp": "2026-01-19T15:11:18.310050"}
{"image_index": 334, "image_name": "000000036494.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036494.jpg", "image_width": 640, "image_height": 434, "image_resolution": "640x434", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2105.896, "latencies_ms": [2105.896], "images_per_second": 0.475, "prompt_tokens": 1109, "response_tokens_est": 37, "n_tiles": 1, "output_text": " The image is taken in a restaurant with a warm and inviting atmosphere, characterized by the soft lighting and the rich colors of the wooden furniture and the warm glow of the chandelier.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27609.5, "ram_available_mb": 98162.7, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 27622.5, "ram_available_mb": 98149.7, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.59, "peak": 40.18, "min": 17.74}, "VIN": {"avg": 73.57, "peak": 127.89, "min": 28.59}, "VIN_SYS_5V0": {"avg": 14.75, "peak": 15.95, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.8, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 28.59, "energy_joules_est": 60.21, "sample_count": 21, "duration_seconds": 2.106}, "timestamp": "2026-01-19T15:11:20.506400"}
{"image_index": 335, "image_name": "000000036539.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036539.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1883.378, "latencies_ms": [1883.378], "images_per_second": 0.531, "prompt_tokens": 1100, "response_tokens_est": 31, "n_tiles": 1, "output_text": " A man and a child are standing on a snow-covered slope, with the man wearing a black jacket and the child wearing a pink snowsuit.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 27622.5, "ram_available_mb": 98149.7, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 27637.4, "ram_available_mb": 98134.8, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.03, "peak": 40.16, "min": 15.77}, "VIN": {"avg": 72.38, "peak": 117.35, "min": 28.55}, "VIN_SYS_5V0": {"avg": 14.75, "peak": 15.95, "min": 11.87}, "VDD_CPU_SOC_MSS": {"avg": 15.85, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 29.03, "energy_joules_est": 54.69, "sample_count": 19, "duration_seconds": 1.884}, "timestamp": "2026-01-19T15:11:22.493475"}
{"image_index": 335, "image_name": "000000036539.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036539.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2661.519, "latencies_ms": [2661.519], "images_per_second": 0.376, "prompt_tokens": 1114, "response_tokens_est": 58, "n_tiles": 1, "output_text": " 1. person: 1\n2. snow: 1\n3. trees: 1\n4. rocks: 1\n5. skis: 1\n6. goggles: 1\n7. snowboard: 1\n8. backpack: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27637.4, "ram_available_mb": 98134.8, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 6.4, "ram_used_mb": 27552.5, "ram_available_mb": 98219.7, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.62, "peak": 39.78, "min": 16.16}, "VIN": {"avg": 68.07, "peak": 127.53, "min": 28.83}, "VIN_SYS_5V0": {"avg": 14.82, "peak": 16.05, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 16.02, "peak": 18.11, "min": 13.0}}, "power_watts_avg": 26.62, "energy_joules_est": 70.86, "sample_count": 26, "duration_seconds": 2.662}, "timestamp": "2026-01-19T15:11:25.192655"}
{"image_index": 335, "image_name": "000000036539.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036539.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1886.139, "latencies_ms": [1886.139], "images_per_second": 0.53, "prompt_tokens": 1118, "response_tokens_est": 31, "n_tiles": 1, "output_text": " The child in the foreground is standing on the snow, while the adult is standing in the background. The child is closer to the camera than the adult.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27552.5, "ram_available_mb": 98219.7, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 27547.4, "ram_available_mb": 98224.8, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.11, "peak": 40.18, "min": 16.16}, "VIN": {"avg": 73.18, "peak": 105.27, "min": 28.45}, "VIN_SYS_5V0": {"avg": 14.97, "peak": 16.15, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 16.1, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 29.11, "energy_joules_est": 54.92, "sample_count": 19, "duration_seconds": 1.886}, "timestamp": "2026-01-19T15:11:27.179453"}
{"image_index": 335, "image_name": "000000036539.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036539.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1556.174, "latencies_ms": [1556.174], "images_per_second": 0.643, "prompt_tokens": 1112, "response_tokens_est": 18, "n_tiles": 1, "output_text": " A man and a child are standing on a snowy mountain, with trees in the background.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 27547.4, "ram_available_mb": 98224.8, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 27553.6, "ram_available_mb": 98218.5, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.68, "peak": 39.39, "min": 16.16}, "VIN": {"avg": 70.25, "peak": 121.32, "min": 34.25}, "VIN_SYS_5V0": {"avg": 14.79, "peak": 16.15, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.99, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 30.68, "energy_joules_est": 47.76, "sample_count": 15, "duration_seconds": 1.557}, "timestamp": "2026-01-19T15:11:28.745870"}
{"image_index": 335, "image_name": "000000036539.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036539.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2064.969, "latencies_ms": [2064.969], "images_per_second": 0.484, "prompt_tokens": 1110, "response_tokens_est": 39, "n_tiles": 1, "output_text": " The image features a snowy mountain with a child wearing a black jacket and a man wearing a black jacket standing on the snow. The sky is clear and blue, and the sun is shining brightly.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 27553.6, "ram_available_mb": 98218.5, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 27560.9, "ram_available_mb": 98211.2, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.0, "peak": 40.56, "min": 20.89}, "VIN": {"avg": 69.57, "peak": 118.21, "min": 30.0}, "VIN_SYS_5V0": {"avg": 15.26, "peak": 16.26, "min": 13.29}, "VDD_CPU_SOC_MSS": {"avg": 16.48, "peak": 17.71, "min": 14.57}}, "power_watts_avg": 30.0, "energy_joules_est": 61.96, "sample_count": 20, "duration_seconds": 2.065}, "timestamp": "2026-01-19T15:11:30.824646"}
{"image_index": 336, "image_name": "000000036660.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036660.jpg", "image_width": 500, "image_height": 323, "image_resolution": "500x323", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1762.641, "latencies_ms": [1762.641], "images_per_second": 0.567, "prompt_tokens": 1099, "response_tokens_est": 27, "n_tiles": 1, "output_text": " A pair of feet wearing flip-flops is standing on a wooden floor with a broken mobile phone and its components scattered around them.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27560.9, "ram_available_mb": 98211.2, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 27573.3, "ram_available_mb": 98198.9, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.16, "peak": 40.56, "min": 17.74}, "VIN": {"avg": 75.15, "peak": 117.77, "min": 28.81}, "VIN_SYS_5V0": {"avg": 15.17, "peak": 16.15, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 16.27, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 30.16, "energy_joules_est": 53.17, "sample_count": 18, "duration_seconds": 1.763}, "timestamp": "2026-01-19T15:11:32.691881"}
{"image_index": 336, "image_name": "000000036660.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036660.jpg", "image_width": 500, "image_height": 323, "image_resolution": "500x323", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1547.819, "latencies_ms": [1547.819], "images_per_second": 0.646, "prompt_tokens": 1113, "response_tokens_est": 17, "n_tiles": 1, "output_text": " 1. flip phone: 2\n2. person's feet: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27573.3, "ram_available_mb": 98198.9, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 27579.0, "ram_available_mb": 98193.2, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.2, "peak": 40.18, "min": 16.95}, "VIN": {"avg": 79.15, "peak": 123.68, "min": 30.55}, "VIN_SYS_5V0": {"avg": 14.77, "peak": 16.05, "min": 11.87}, "VDD_CPU_SOC_MSS": {"avg": 15.91, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 31.2, "energy_joules_est": 48.3, "sample_count": 15, "duration_seconds": 1.548}, "timestamp": "2026-01-19T15:11:34.263637"}
{"image_index": 336, "image_name": "000000036660.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036660.jpg", "image_width": 500, "image_height": 323, "image_resolution": "500x323", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2737.758, "latencies_ms": [2737.758], "images_per_second": 0.365, "prompt_tokens": 1117, "response_tokens_est": 64, "n_tiles": 1, "output_text": " The two flip phones are positioned in the foreground, with the front of the phones facing upwards. The person's feet are in the foreground, with the left foot wearing black flip-flops and the right foot wearing white flip-flops. The two flip phones are positioned to the left of the person's feet.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 27579.0, "ram_available_mb": 98193.2, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 27590.2, "ram_available_mb": 98182.0, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.42, "peak": 40.95, "min": 17.73}, "VIN": {"avg": 72.32, "peak": 117.69, "min": 27.96}, "VIN_SYS_5V0": {"avg": 15.1, "peak": 16.15, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 16.1, "peak": 17.71, "min": 14.18}}, "power_watts_avg": 27.42, "energy_joules_est": 75.08, "sample_count": 27, "duration_seconds": 2.738}, "timestamp": "2026-01-19T15:11:37.083774"}
{"image_index": 336, "image_name": "000000036660.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036660.jpg", "image_width": 500, "image_height": 323, "image_resolution": "500x323", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1768.738, "latencies_ms": [1768.738], "images_per_second": 0.565, "prompt_tokens": 1111, "response_tokens_est": 26, "n_tiles": 1, "output_text": " A pair of feet wearing flip flops are standing on a wooden floor with a broken cell phone and its parts scattered around them.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27590.2, "ram_available_mb": 98182.0, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 27593.3, "ram_available_mb": 98178.9, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.87, "peak": 39.78, "min": 14.97}, "VIN": {"avg": 72.92, "peak": 116.32, "min": 33.84}, "VIN_SYS_5V0": {"avg": 14.85, "peak": 16.05, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.96, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 29.87, "energy_joules_est": 52.85, "sample_count": 17, "duration_seconds": 1.769}, "timestamp": "2026-01-19T15:11:38.858868"}
{"image_index": 336, "image_name": "000000036660.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036660.jpg", "image_width": 500, "image_height": 323, "image_resolution": "500x323", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2024.829, "latencies_ms": [2024.829], "images_per_second": 0.494, "prompt_tokens": 1109, "response_tokens_est": 37, "n_tiles": 1, "output_text": " The wooden floor is brown with a glossy finish, and the person's feet are wearing black flip flops. The lighting is natural and bright, and the weather appears to be clear.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 27593.3, "ram_available_mb": 98178.9, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 27606.4, "ram_available_mb": 98165.7, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.79, "peak": 40.95, "min": 19.7}, "VIN": {"avg": 71.65, "peak": 117.74, "min": 30.37}, "VIN_SYS_5V0": {"avg": 15.18, "peak": 16.26, "min": 13.29}, "VDD_CPU_SOC_MSS": {"avg": 16.3, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 29.79, "energy_joules_est": 60.34, "sample_count": 20, "duration_seconds": 2.025}, "timestamp": "2026-01-19T15:11:40.942682"}
{"image_index": 337, "image_name": "000000036678.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036678.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2858.456, "latencies_ms": [2858.456], "images_per_second": 0.35, "prompt_tokens": 1099, "response_tokens_est": 69, "n_tiles": 1, "output_text": " The image captures the majestic Palace of Westminster, also known as the Houses of Parliament, bathed in the warm glow of the setting sun, with the iconic Big Ben clock tower standing tall in the foreground, while a boat with a red and white flag floats on the river in the foreground, and a few other boats are visible in the background.", "error": null, "sys_before": {"cpu_percent": 18.2, "ram_used_mb": 27606.4, "ram_available_mb": 98165.7, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 27606.5, "ram_available_mb": 98165.7, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.47, "peak": 40.57, "min": 16.56}, "VIN": {"avg": 73.4, "peak": 122.98, "min": 30.65}, "VIN_SYS_5V0": {"avg": 14.96, "peak": 16.05, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.93, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 26.47, "energy_joules_est": 75.68, "sample_count": 28, "duration_seconds": 2.859}, "timestamp": "2026-01-19T15:11:43.863974"}
{"image_index": 337, "image_name": "000000036678.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036678.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2529.86, "latencies_ms": [2529.86], "images_per_second": 0.395, "prompt_tokens": 1113, "response_tokens_est": 56, "n_tiles": 1, "output_text": " 1. Tower: 1\n2. Boat: 2\n3. Building: 1\n4. Bridge: 1\n5. Flag: 1\n6. Boat: 2\n7. Boat: 2\n8. Boat: 2", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 27606.5, "ram_available_mb": 98165.7, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 27626.4, "ram_available_mb": 98145.8, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.06, "peak": 40.18, "min": 15.77}, "VIN": {"avg": 71.71, "peak": 120.48, "min": 28.83}, "VIN_SYS_5V0": {"avg": 14.9, "peak": 16.05, "min": 12.36}, "VDD_CPU_SOC_MSS": {"avg": 15.94, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 27.06, "energy_joules_est": 68.47, "sample_count": 25, "duration_seconds": 2.53}, "timestamp": "2026-01-19T15:11:46.470646"}
{"image_index": 337, "image_name": "000000036678.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036678.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2440.339, "latencies_ms": [2440.339], "images_per_second": 0.41, "prompt_tokens": 1117, "response_tokens_est": 52, "n_tiles": 1, "output_text": " The clock tower is located on the left side of the image, while the Palace of Westminster is situated in the background. The foreground features a barge with a red and white striped awning, and the background includes a smaller boat with a white roof.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27626.4, "ram_available_mb": 98145.8, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 27636.8, "ram_available_mb": 98135.4, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.32, "peak": 40.18, "min": 15.38}, "VIN": {"avg": 69.46, "peak": 123.05, "min": 29.49}, "VIN_SYS_5V0": {"avg": 14.83, "peak": 16.05, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.86, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 27.32, "energy_joules_est": 66.68, "sample_count": 24, "duration_seconds": 2.441}, "timestamp": "2026-01-19T15:11:48.971319"}
{"image_index": 337, "image_name": "000000036678.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036678.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2924.266, "latencies_ms": [2924.266], "images_per_second": 0.342, "prompt_tokens": 1111, "response_tokens_est": 71, "n_tiles": 1, "output_text": " The image captures the majestic Palace of Westminster, also known as the Houses of Parliament, in London, England. The scene is set on a river, with boats gently floating on the water, adding a sense of tranquility to the historic landmark. The sky overhead is overcast, casting a soft light over the scene and highlighting the intricate details of the buildings.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 27636.8, "ram_available_mb": 98135.4, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 27551.6, "ram_available_mb": 98220.6, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.03, "peak": 40.18, "min": 15.77}, "VIN": {"avg": 65.06, "peak": 131.57, "min": 28.61}, "VIN_SYS_5V0": {"avg": 14.92, "peak": 16.05, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 16.12, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 26.03, "energy_joules_est": 76.14, "sample_count": 29, "duration_seconds": 2.925}, "timestamp": "2026-01-19T15:11:51.979040"}
{"image_index": 337, "image_name": "000000036678.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036678.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2077.959, "latencies_ms": [2077.959], "images_per_second": 0.481, "prompt_tokens": 1109, "response_tokens_est": 38, "n_tiles": 1, "output_text": " The sky is overcast with a grayish hue, and the buildings are bathed in a warm, golden light. The water is a deep blue, reflecting the city's architectural beauty.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27548.5, "ram_available_mb": 98223.7, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 27546.1, "ram_available_mb": 98226.1, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.2, "peak": 39.78, "min": 15.38}, "VIN": {"avg": 73.39, "peak": 121.91, "min": 27.76}, "VIN_SYS_5V0": {"avg": 14.78, "peak": 16.05, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.94, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 28.2, "energy_joules_est": 58.62, "sample_count": 21, "duration_seconds": 2.079}, "timestamp": "2026-01-19T15:11:54.158906"}
{"image_index": 338, "image_name": "000000036844.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036844.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2034.026, "latencies_ms": [2034.026], "images_per_second": 0.492, "prompt_tokens": 1099, "response_tokens_est": 37, "n_tiles": 1, "output_text": " The image depicts a spacious living room with a red carpet, a green sofa, a black leather chair, and a wooden floor, with a ceiling fan and several potted plants.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 27546.1, "ram_available_mb": 98226.1, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 27545.5, "ram_available_mb": 98226.7, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.74, "peak": 40.18, "min": 15.77}, "VIN": {"avg": 73.45, "peak": 128.7, "min": 29.84}, "VIN_SYS_5V0": {"avg": 14.79, "peak": 15.95, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.91, "peak": 17.32, "min": 12.61}}, "power_watts_avg": 28.74, "energy_joules_est": 58.48, "sample_count": 20, "duration_seconds": 2.035}, "timestamp": "2026-01-19T15:11:56.250600"}
{"image_index": 338, "image_name": "000000036844.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036844.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2219.267, "latencies_ms": [2219.267], "images_per_second": 0.451, "prompt_tokens": 1113, "response_tokens_est": 44, "n_tiles": 1, "output_text": " chair: 1, sofa: 1, television: 1, potted plant: 1, mirror: 1, television stand: 1, sofa: 1, chair: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27545.5, "ram_available_mb": 98226.7, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 27549.5, "ram_available_mb": 98222.7, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.38, "peak": 40.57, "min": 16.95}, "VIN": {"avg": 69.54, "peak": 104.12, "min": 30.08}, "VIN_SYS_5V0": {"avg": 14.95, "peak": 16.05, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 16.07, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 28.38, "energy_joules_est": 63.0, "sample_count": 22, "duration_seconds": 2.22}, "timestamp": "2026-01-19T15:11:58.545154"}
{"image_index": 338, "image_name": "000000036844.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036844.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2690.795, "latencies_ms": [2690.795], "images_per_second": 0.372, "prompt_tokens": 1117, "response_tokens_est": 60, "n_tiles": 1, "output_text": " The living room is situated in the center of the image, with the furniture arranged around it. The foreground features a coffee table and a chair, while the background includes a television and a bookshelf. The living room is positioned between the windows, with the furniture placed in the middle of the room.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27549.5, "ram_available_mb": 98222.7, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 27557.2, "ram_available_mb": 98215.0, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.82, "peak": 40.16, "min": 16.16}, "VIN": {"avg": 67.78, "peak": 108.61, "min": 31.83}, "VIN_SYS_5V0": {"avg": 14.87, "peak": 16.05, "min": 12.15}, "VDD_CPU_SOC_MSS": {"avg": 15.9, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 26.82, "energy_joules_est": 72.18, "sample_count": 26, "duration_seconds": 2.691}, "timestamp": "2026-01-19T15:12:01.247210"}
{"image_index": 338, "image_name": "000000036844.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036844.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1448.877, "latencies_ms": [1448.877], "images_per_second": 0.69, "prompt_tokens": 1111, "response_tokens_est": 15, "n_tiles": 1, "output_text": " A living room with a red carpet, a couch, and a chair.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27557.2, "ram_available_mb": 98215.0, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 27576.8, "ram_available_mb": 98195.3, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.11, "peak": 40.16, "min": 16.95}, "VIN": {"avg": 75.1, "peak": 119.37, "min": 29.02}, "VIN_SYS_5V0": {"avg": 15.26, "peak": 16.15, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 16.4, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 32.11, "energy_joules_est": 46.53, "sample_count": 14, "duration_seconds": 1.449}, "timestamp": "2026-01-19T15:12:02.709834"}
{"image_index": 338, "image_name": "000000036844.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036844.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1603.303, "latencies_ms": [1603.303], "images_per_second": 0.624, "prompt_tokens": 1109, "response_tokens_est": 21, "n_tiles": 1, "output_text": " The room is well lit with natural light coming in from the windows, and the walls are painted white.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27576.8, "ram_available_mb": 98195.3, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 6.7, "ram_used_mb": 27528.2, "ram_available_mb": 98244.0, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.55, "peak": 40.56, "min": 22.46}, "VIN": {"avg": 70.59, "peak": 121.42, "min": 28.78}, "VIN_SYS_5V0": {"avg": 15.29, "peak": 16.15, "min": 13.29}, "VDD_CPU_SOC_MSS": {"avg": 16.61, "peak": 18.11, "min": 14.57}}, "power_watts_avg": 32.55, "energy_joules_est": 52.2, "sample_count": 16, "duration_seconds": 1.604}, "timestamp": "2026-01-19T15:12:04.379200"}
{"image_index": 339, "image_name": "000000036861.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036861.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1981.584, "latencies_ms": [1981.584], "images_per_second": 0.505, "prompt_tokens": 1100, "response_tokens_est": 34, "n_tiles": 1, "output_text": " Two parking meters are attached to a red pole on a sidewalk, with a building in the background displaying a sign that reads \"40 Years of Living LIVES.\"", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 27528.2, "ram_available_mb": 98244.0, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 27532.4, "ram_available_mb": 98239.8, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.25, "peak": 40.18, "min": 18.52}, "VIN": {"avg": 67.05, "peak": 103.69, "min": 27.21}, "VIN_SYS_5V0": {"avg": 14.86, "peak": 16.05, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 16.01, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 29.25, "energy_joules_est": 57.98, "sample_count": 20, "duration_seconds": 1.982}, "timestamp": "2026-01-19T15:12:06.469613"}
{"image_index": 339, "image_name": "000000036861.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036861.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2163.048, "latencies_ms": [2163.048], "images_per_second": 0.462, "prompt_tokens": 1114, "response_tokens_est": 42, "n_tiles": 1, "output_text": " 1. red post\n2. parking meter\n3. red circular holder\n4. parking meter\n5. red post\n6. red circular holder\n7. parking meter\n8. red post", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27532.4, "ram_available_mb": 98239.8, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 27544.5, "ram_available_mb": 98227.7, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.29, "peak": 39.78, "min": 15.77}, "VIN": {"avg": 69.39, "peak": 119.66, "min": 29.92}, "VIN_SYS_5V0": {"avg": 14.82, "peak": 16.05, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 15.94, "peak": 17.32, "min": 12.61}}, "power_watts_avg": 28.29, "energy_joules_est": 61.21, "sample_count": 21, "duration_seconds": 2.164}, "timestamp": "2026-01-19T15:12:08.653080"}
{"image_index": 339, "image_name": "000000036861.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036861.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2221.672, "latencies_ms": [2221.672], "images_per_second": 0.45, "prompt_tokens": 1118, "response_tokens_est": 45, "n_tiles": 1, "output_text": " The two parking meters are positioned on the left side of the image, with the red pole in the foreground and the building in the background. The parking meters are closer to the camera than the building, which is further away.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27544.5, "ram_available_mb": 98227.7, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 27556.1, "ram_available_mb": 98216.1, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.47, "peak": 39.78, "min": 17.73}, "VIN": {"avg": 75.66, "peak": 127.98, "min": 28.64}, "VIN_SYS_5V0": {"avg": 15.19, "peak": 16.26, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 16.25, "peak": 17.71, "min": 14.18}}, "power_watts_avg": 28.47, "energy_joules_est": 63.26, "sample_count": 22, "duration_seconds": 2.222}, "timestamp": "2026-01-19T15:12:10.942360"}
{"image_index": 339, "image_name": "000000036861.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036861.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1666.42, "latencies_ms": [1666.42], "images_per_second": 0.6, "prompt_tokens": 1112, "response_tokens_est": 23, "n_tiles": 1, "output_text": " A red pole with two parking meters on top stands on a sidewalk in front of a building with a large window.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27556.1, "ram_available_mb": 98216.1, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 27571.0, "ram_available_mb": 98201.2, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.9, "peak": 39.77, "min": 16.16}, "VIN": {"avg": 73.18, "peak": 132.26, "min": 28.47}, "VIN_SYS_5V0": {"avg": 14.93, "peak": 16.26, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 16.03, "peak": 17.71, "min": 13.39}}, "power_watts_avg": 29.9, "energy_joules_est": 49.84, "sample_count": 17, "duration_seconds": 1.667}, "timestamp": "2026-01-19T15:12:12.711005"}
{"image_index": 339, "image_name": "000000036861.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036861.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2663.159, "latencies_ms": [2663.159], "images_per_second": 0.375, "prompt_tokens": 1110, "response_tokens_est": 61, "n_tiles": 1, "output_text": " The image features a red parking meter with two meters on top, standing on a sidewalk. The parking meter is located in front of a building with a large window that has a banner with the text \"40 Years of Living LIVES\". The sky is clear and blue, indicating a sunny day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27571.0, "ram_available_mb": 98201.2, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 27576.7, "ram_available_mb": 98195.4, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.01, "peak": 40.18, "min": 17.35}, "VIN": {"avg": 67.87, "peak": 124.55, "min": 30.33}, "VIN_SYS_5V0": {"avg": 14.84, "peak": 16.05, "min": 11.87}, "VDD_CPU_SOC_MSS": {"avg": 15.99, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 27.01, "energy_joules_est": 71.94, "sample_count": 26, "duration_seconds": 2.663}, "timestamp": "2026-01-19T15:12:15.411846"}
{"image_index": 340, "image_name": "000000036936.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036936.jpg", "image_width": 640, "image_height": 404, "image_resolution": "640x404", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1616.915, "latencies_ms": [1616.915], "images_per_second": 0.618, "prompt_tokens": 1099, "response_tokens_est": 21, "n_tiles": 1, "output_text": " A man and a woman are sitting on a couch in a living room, watching TV and eating snacks.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 27576.7, "ram_available_mb": 98195.4, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 27588.6, "ram_available_mb": 98183.6, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.73, "peak": 40.16, "min": 16.16}, "VIN": {"avg": 71.39, "peak": 101.63, "min": 29.63}, "VIN_SYS_5V0": {"avg": 15.06, "peak": 16.15, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 16.19, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 30.73, "energy_joules_est": 49.7, "sample_count": 16, "duration_seconds": 1.617}, "timestamp": "2026-01-19T15:12:17.076681"}
{"image_index": 340, "image_name": "000000036936.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036936.jpg", "image_width": 640, "image_height": 404, "image_resolution": "640x404", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2720.848, "latencies_ms": [2720.848], "images_per_second": 0.368, "prompt_tokens": 1113, "response_tokens_est": 63, "n_tiles": 1, "output_text": " 1. sofa: 2\n2. television: 1\n3. coffee table: 1\n4. dining table: 1\n5. dining chairs: 2\n6. dining table: 1\n7. dining table: 1\n8. dining table: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27588.6, "ram_available_mb": 98183.6, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 27595.2, "ram_available_mb": 98177.0, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.06, "peak": 40.56, "min": 17.34}, "VIN": {"avg": 71.05, "peak": 115.21, "min": 28.62}, "VIN_SYS_5V0": {"avg": 14.93, "peak": 16.15, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 16.06, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 27.06, "energy_joules_est": 73.64, "sample_count": 27, "duration_seconds": 2.721}, "timestamp": "2026-01-19T15:12:19.889941"}
{"image_index": 340, "image_name": "000000036936.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036936.jpg", "image_width": 640, "image_height": 404, "image_resolution": "640x404", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2237.082, "latencies_ms": [2237.082], "images_per_second": 0.447, "prompt_tokens": 1117, "response_tokens_est": 44, "n_tiles": 1, "output_text": " The television is on the left side of the room, the couch is in the middle, and the coffee table is in the foreground. The person on the couch is closer to the camera than the person on the floor.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27595.2, "ram_available_mb": 98177.0, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 27606.0, "ram_available_mb": 98166.2, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.92, "peak": 40.16, "min": 14.98}, "VIN": {"avg": 70.17, "peak": 106.21, "min": 28.85}, "VIN_SYS_5V0": {"avg": 14.83, "peak": 16.05, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.96, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 27.92, "energy_joules_est": 62.47, "sample_count": 22, "duration_seconds": 2.238}, "timestamp": "2026-01-19T15:12:22.171624"}
{"image_index": 340, "image_name": "000000036936.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036936.jpg", "image_width": 640, "image_height": 404, "image_resolution": "640x404", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2221.314, "latencies_ms": [2221.314], "images_per_second": 0.45, "prompt_tokens": 1111, "response_tokens_est": 44, "n_tiles": 1, "output_text": " A young man and woman are sitting on a couch in a living room, watching TV. The TV is on a wooden stand, and there is a coffee table in front of them with some snacks and drinks on it.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 27606.0, "ram_available_mb": 98166.2, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 27607.4, "ram_available_mb": 98164.8, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.24, "peak": 40.57, "min": 16.95}, "VIN": {"avg": 72.73, "peak": 127.38, "min": 30.03}, "VIN_SYS_5V0": {"avg": 15.0, "peak": 16.15, "min": 12.56}, "VDD_CPU_SOC_MSS": {"avg": 15.98, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 28.24, "energy_joules_est": 62.74, "sample_count": 22, "duration_seconds": 2.222}, "timestamp": "2026-01-19T15:12:24.465242"}
{"image_index": 340, "image_name": "000000036936.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036936.jpg", "image_width": 640, "image_height": 404, "image_resolution": "640x404", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1801.726, "latencies_ms": [1801.726], "images_per_second": 0.555, "prompt_tokens": 1109, "response_tokens_est": 22, "n_tiles": 1, "output_text": " The room is lit by a warm yellow light, and the walls are painted in a light beige color.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27607.4, "ram_available_mb": 98164.8, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 27619.4, "ram_available_mb": 98152.8, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.98, "peak": 38.6, "min": 16.16}, "VIN": {"avg": 72.41, "peak": 124.29, "min": 27.97}, "VIN_SYS_5V0": {"avg": 14.48, "peak": 15.95, "min": 12.15}, "VDD_CPU_SOC_MSS": {"avg": 15.46, "peak": 16.92, "min": 13.39}}, "power_watts_avg": 28.98, "energy_joules_est": 52.23, "sample_count": 18, "duration_seconds": 1.802}, "timestamp": "2026-01-19T15:12:26.338099"}
{"image_index": 341, "image_name": "000000037670.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037670.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1777.586, "latencies_ms": [1777.586], "images_per_second": 0.563, "prompt_tokens": 1099, "response_tokens_est": 26, "n_tiles": 1, "output_text": " A person is holding a white electrical device with multiple buttons and a red light on it, which is attached to a white toilet.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 27619.4, "ram_available_mb": 98152.8, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 27623.8, "ram_available_mb": 98148.4, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.88, "peak": 40.57, "min": 16.95}, "VIN": {"avg": 73.31, "peak": 116.78, "min": 29.24}, "VIN_SYS_5V0": {"avg": 14.68, "peak": 15.85, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.71, "peak": 16.92, "min": 13.0}}, "power_watts_avg": 29.88, "energy_joules_est": 53.14, "sample_count": 18, "duration_seconds": 1.778}, "timestamp": "2026-01-19T15:12:28.223405"}
{"image_index": 341, "image_name": "000000037670.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037670.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2022.811, "latencies_ms": [2022.811], "images_per_second": 0.494, "prompt_tokens": 1113, "response_tokens_est": 36, "n_tiles": 1, "output_text": " toilet: 1, person: 1, floor: 1, wall: 1, box: 1, floor lamp: 1, electrical outlet: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 27623.8, "ram_available_mb": 98148.4, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 6.4, "ram_used_mb": 27555.5, "ram_available_mb": 98216.7, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.02, "peak": 40.18, "min": 16.56}, "VIN": {"avg": 69.79, "peak": 120.97, "min": 29.02}, "VIN_SYS_5V0": {"avg": 14.8, "peak": 16.05, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 16.06, "peak": 18.11, "min": 12.61}}, "power_watts_avg": 29.02, "energy_joules_est": 58.72, "sample_count": 20, "duration_seconds": 2.023}, "timestamp": "2026-01-19T15:12:30.307422"}
{"image_index": 341, "image_name": "000000037670.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037670.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2266.879, "latencies_ms": [2266.879], "images_per_second": 0.441, "prompt_tokens": 1117, "response_tokens_est": 46, "n_tiles": 1, "output_text": " The person's hand is positioned to the right of the toilet, with the toilet paper holder being in the foreground. The person is holding the power strip in front of the toilet, which is located in the middle of the image.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 27555.5, "ram_available_mb": 98216.7, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 27569.3, "ram_available_mb": 98202.9, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.31, "peak": 40.16, "min": 16.95}, "VIN": {"avg": 70.31, "peak": 108.9, "min": 30.57}, "VIN_SYS_5V0": {"avg": 14.99, "peak": 16.05, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 16.0, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 28.31, "energy_joules_est": 64.19, "sample_count": 22, "duration_seconds": 2.267}, "timestamp": "2026-01-19T15:12:32.603817"}
{"image_index": 341, "image_name": "000000037670.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037670.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1404.109, "latencies_ms": [1404.109], "images_per_second": 0.712, "prompt_tokens": 1111, "response_tokens_est": 13, "n_tiles": 1, "output_text": " A person is holding a power strip in front of a toilet.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27569.3, "ram_available_mb": 98202.9, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 27569.2, "ram_available_mb": 98203.0, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.86, "peak": 40.57, "min": 16.95}, "VIN": {"avg": 75.23, "peak": 131.09, "min": 28.67}, "VIN_SYS_5V0": {"avg": 15.13, "peak": 16.15, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 16.25, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 31.86, "energy_joules_est": 44.75, "sample_count": 14, "duration_seconds": 1.405}, "timestamp": "2026-01-19T15:12:34.070240"}
{"image_index": 341, "image_name": "000000037670.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037670.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1991.497, "latencies_ms": [1991.497], "images_per_second": 0.502, "prompt_tokens": 1109, "response_tokens_est": 34, "n_tiles": 1, "output_text": " The image is a photograph of a white toilet with a silver metal stand next to it. The toilet is located in a room with a white wall and a wooden floor.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27569.2, "ram_available_mb": 98203.0, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 27585.6, "ram_available_mb": 98186.6, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.0, "peak": 40.95, "min": 18.91}, "VIN": {"avg": 66.76, "peak": 106.11, "min": 27.62}, "VIN_SYS_5V0": {"avg": 14.91, "peak": 16.05, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 16.01, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 30.0, "energy_joules_est": 59.76, "sample_count": 20, "duration_seconds": 1.992}, "timestamp": "2026-01-19T15:12:36.146539"}
{"image_index": 342, "image_name": "000000037689.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037689.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1769.274, "latencies_ms": [1769.274], "images_per_second": 0.565, "prompt_tokens": 1099, "response_tokens_est": 26, "n_tiles": 1, "output_text": " A snowboarder is performing a trick in the air above a snow ramp, with a crowd of spectators watching from the side.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 27585.6, "ram_available_mb": 98186.6, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 6.7, "ram_used_mb": 27573.1, "ram_available_mb": 98199.0, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.06, "peak": 40.18, "min": 16.16}, "VIN": {"avg": 75.77, "peak": 123.87, "min": 34.56}, "VIN_SYS_5V0": {"avg": 14.87, "peak": 16.05, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 16.05, "peak": 17.32, "min": 12.61}}, "power_watts_avg": 30.06, "energy_joules_est": 53.2, "sample_count": 17, "duration_seconds": 1.77}, "timestamp": "2026-01-19T15:12:37.926073"}
{"image_index": 342, "image_name": "000000037689.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037689.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2833.767, "latencies_ms": [2833.767], "images_per_second": 0.353, "prompt_tokens": 1113, "response_tokens_est": 69, "n_tiles": 1, "output_text": " 1. snowboard: 1\n2. snowboarder: 1\n3. crowd: 1\n4. snowboarder: 1\n5. snowboarder: 1\n6. snowboarder: 1\n7. snowboarder: 1\n8. snowboarder: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27573.1, "ram_available_mb": 98199.0, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 27560.4, "ram_available_mb": 98211.8, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.0, "peak": 40.56, "min": 18.13}, "VIN": {"avg": 68.23, "peak": 120.72, "min": 28.53}, "VIN_SYS_5V0": {"avg": 15.18, "peak": 16.26, "min": 13.39}, "VDD_CPU_SOC_MSS": {"avg": 16.18, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 27.0, "energy_joules_est": 76.52, "sample_count": 28, "duration_seconds": 2.834}, "timestamp": "2026-01-19T15:12:40.833489"}
{"image_index": 342, "image_name": "000000037689.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037689.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2455.34, "latencies_ms": [2455.34], "images_per_second": 0.407, "prompt_tokens": 1117, "response_tokens_est": 53, "n_tiles": 1, "output_text": " The snowboarder is in the foreground, performing a trick in the air. The crowd is in the background, watching the event. The snow ramp is in the middle ground, with the snowboarder's trick taking place on the right side of the ramp.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27560.4, "ram_available_mb": 98211.8, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 27574.2, "ram_available_mb": 98197.9, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.42, "peak": 40.18, "min": 15.38}, "VIN": {"avg": 72.97, "peak": 132.42, "min": 30.38}, "VIN_SYS_5V0": {"avg": 14.88, "peak": 16.05, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 16.04, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 27.42, "energy_joules_est": 67.33, "sample_count": 24, "duration_seconds": 2.456}, "timestamp": "2026-01-19T15:12:43.319541"}
{"image_index": 342, "image_name": "000000037689.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037689.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1827.169, "latencies_ms": [1827.169], "images_per_second": 0.547, "prompt_tokens": 1111, "response_tokens_est": 29, "n_tiles": 1, "output_text": " A snowboarder is performing a trick in the air above a snow ramp. The ramp is surrounded by a crowd of spectators watching the event.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27574.2, "ram_available_mb": 98197.9, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 27575.9, "ram_available_mb": 98196.2, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.85, "peak": 40.16, "min": 16.56}, "VIN": {"avg": 70.64, "peak": 104.92, "min": 29.83}, "VIN_SYS_5V0": {"avg": 15.06, "peak": 16.15, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 16.21, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 29.85, "energy_joules_est": 54.55, "sample_count": 18, "duration_seconds": 1.828}, "timestamp": "2026-01-19T15:12:45.203707"}
{"image_index": 342, "image_name": "000000037689.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037689.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2182.734, "latencies_ms": [2182.734], "images_per_second": 0.458, "prompt_tokens": 1109, "response_tokens_est": 43, "n_tiles": 1, "output_text": " The image features a snowboarder performing a trick in the air, with a clear blue sky in the background. The snowboarder is wearing a red and white outfit, and the snow is white and pristine.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 27575.9, "ram_available_mb": 98196.2, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 27590.4, "ram_available_mb": 98181.7, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.54, "peak": 40.56, "min": 17.74}, "VIN": {"avg": 73.29, "peak": 125.7, "min": 28.11}, "VIN_SYS_5V0": {"avg": 14.94, "peak": 16.05, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 16.0, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 28.54, "energy_joules_est": 62.3, "sample_count": 22, "duration_seconds": 2.183}, "timestamp": "2026-01-19T15:12:47.489412"}
{"image_index": 343, "image_name": "000000037740.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037740.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1442.613, "latencies_ms": [1442.613], "images_per_second": 0.693, "prompt_tokens": 1099, "response_tokens_est": 14, "n_tiles": 1, "output_text": " A room with a desk, chair, and a plant in it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27590.4, "ram_available_mb": 98181.7, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 27586.7, "ram_available_mb": 98185.4, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.32, "peak": 40.56, "min": 15.38}, "VIN": {"avg": 80.5, "peak": 124.7, "min": 27.74}, "VIN_SYS_5V0": {"avg": 14.72, "peak": 15.95, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.83, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 31.32, "energy_joules_est": 45.2, "sample_count": 14, "duration_seconds": 1.443}, "timestamp": "2026-01-19T15:12:48.966193"}
{"image_index": 343, "image_name": "000000037740.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037740.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2643.359, "latencies_ms": [2643.359], "images_per_second": 0.378, "prompt_tokens": 1113, "response_tokens_est": 61, "n_tiles": 1, "output_text": " 1. white chair: 1\n2. desk: 1\n3. computer monitor: 1\n4. keyboard: 1\n5. mouse: 1\n6. lamp: 1\n7. bookshelf: 1\n8. potted plant: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27586.7, "ram_available_mb": 98185.4, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 27602.1, "ram_available_mb": 98170.1, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.9, "peak": 40.97, "min": 18.13}, "VIN": {"avg": 72.08, "peak": 122.7, "min": 29.03}, "VIN_SYS_5V0": {"avg": 15.09, "peak": 16.15, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 16.16, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 27.9, "energy_joules_est": 73.76, "sample_count": 26, "duration_seconds": 2.644}, "timestamp": "2026-01-19T15:12:51.679587"}
{"image_index": 343, "image_name": "000000037740.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037740.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2498.439, "latencies_ms": [2498.439], "images_per_second": 0.4, "prompt_tokens": 1117, "response_tokens_est": 55, "n_tiles": 1, "output_text": " The desk is positioned to the left of the chair, with the chair in front of the desk. The desk is situated in the middle of the room, with the chair placed in front of it. The plant is located to the right of the desk, near the wall.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 27602.1, "ram_available_mb": 98170.1, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 27603.9, "ram_available_mb": 98168.3, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.09, "peak": 40.18, "min": 15.77}, "VIN": {"avg": 68.93, "peak": 86.29, "min": 27.96}, "VIN_SYS_5V0": {"avg": 14.92, "peak": 16.15, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 16.06, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 27.09, "energy_joules_est": 67.69, "sample_count": 25, "duration_seconds": 2.499}, "timestamp": "2026-01-19T15:12:54.269832"}
{"image_index": 343, "image_name": "000000037740.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037740.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1531.604, "latencies_ms": [1531.604], "images_per_second": 0.653, "prompt_tokens": 1111, "response_tokens_est": 17, "n_tiles": 1, "output_text": " A room with a desk, chair, and computer setup, with a plant nearby.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27603.9, "ram_available_mb": 98168.3, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 27613.3, "ram_available_mb": 98158.9, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.86, "peak": 40.16, "min": 14.98}, "VIN": {"avg": 72.21, "peak": 124.94, "min": 29.33}, "VIN_SYS_5V0": {"avg": 14.79, "peak": 16.05, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.93, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 30.86, "energy_joules_est": 47.28, "sample_count": 15, "duration_seconds": 1.532}, "timestamp": "2026-01-19T15:12:55.833543"}
{"image_index": 343, "image_name": "000000037740.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037740.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1505.919, "latencies_ms": [1505.919], "images_per_second": 0.664, "prompt_tokens": 1109, "response_tokens_est": 17, "n_tiles": 1, "output_text": " The room is well-lit with natural light, and the walls are painted white.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27613.3, "ram_available_mb": 98158.9, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 27613.8, "ram_available_mb": 98158.4, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.7, "peak": 40.95, "min": 20.89}, "VIN": {"avg": 85.23, "peak": 124.55, "min": 28.98}, "VIN_SYS_5V0": {"avg": 15.21, "peak": 16.15, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 16.27, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 32.7, "energy_joules_est": 49.26, "sample_count": 15, "duration_seconds": 1.506}, "timestamp": "2026-01-19T15:12:57.396195"}
{"image_index": 344, "image_name": "000000037751.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037751.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1771.409, "latencies_ms": [1771.409], "images_per_second": 0.565, "prompt_tokens": 1099, "response_tokens_est": 27, "n_tiles": 1, "output_text": " A person is standing on a dirt road with a motorcycle parked beside them, with a scenic view of mountains and trees in the background.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 27613.8, "ram_available_mb": 98158.4, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 27636.1, "ram_available_mb": 98136.1, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.69, "peak": 40.97, "min": 19.7}, "VIN": {"avg": 71.87, "peak": 120.59, "min": 30.36}, "VIN_SYS_5V0": {"avg": 15.0, "peak": 16.15, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 16.12, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 30.69, "energy_joules_est": 54.38, "sample_count": 18, "duration_seconds": 1.772}, "timestamp": "2026-01-19T15:12:59.279984"}
{"image_index": 344, "image_name": "000000037751.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037751.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2799.276, "latencies_ms": [2799.276], "images_per_second": 0.357, "prompt_tokens": 1113, "response_tokens_est": 64, "n_tiles": 1, "output_text": " 1. Motorcycle: 1\n2. Motorcycle: 1\n3. Motorcycle: 1\n4. Motorcycle: 1\n5. Motorcycle: 1\n6. Motorcycle: 1\n7. Motorcycle: 1\n8. Motorcycle: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27636.1, "ram_available_mb": 98136.1, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 27632.7, "ram_available_mb": 98139.5, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.33, "peak": 40.18, "min": 16.95}, "VIN": {"avg": 67.06, "peak": 98.05, "min": 28.51}, "VIN_SYS_5V0": {"avg": 14.75, "peak": 15.95, "min": 11.87}, "VDD_CPU_SOC_MSS": {"avg": 15.72, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 26.33, "energy_joules_est": 73.72, "sample_count": 28, "duration_seconds": 2.8}, "timestamp": "2026-01-19T15:13:02.186307"}
{"image_index": 344, "image_name": "000000037751.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037751.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2246.011, "latencies_ms": [2246.011], "images_per_second": 0.445, "prompt_tokens": 1117, "response_tokens_est": 45, "n_tiles": 1, "output_text": " The motorcycle is positioned on the left side of the image, with the rider standing on the right side. The foreground of the image features the motorcycle and the rider, while the background showcases a scenic landscape with mountains and trees.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27632.7, "ram_available_mb": 98139.5, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 27643.9, "ram_available_mb": 98128.3, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.99, "peak": 40.18, "min": 14.59}, "VIN": {"avg": 64.46, "peak": 106.65, "min": 29.96}, "VIN_SYS_5V0": {"avg": 14.8, "peak": 15.95, "min": 11.54}, "VDD_CPU_SOC_MSS": {"avg": 15.78, "peak": 17.32, "min": 12.61}}, "power_watts_avg": 27.99, "energy_joules_est": 62.88, "sample_count": 22, "duration_seconds": 2.247}, "timestamp": "2026-01-19T15:13:04.469250"}
{"image_index": 344, "image_name": "000000037751.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037751.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1589.601, "latencies_ms": [1589.601], "images_per_second": 0.629, "prompt_tokens": 1111, "response_tokens_est": 20, "n_tiles": 1, "output_text": " A motorcyclist is standing on a dirt road in the mountains, looking out at the scenery.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27643.9, "ram_available_mb": 98128.3, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 6.7, "ram_used_mb": 27552.6, "ram_available_mb": 98219.6, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.88, "peak": 40.57, "min": 16.95}, "VIN": {"avg": 77.34, "peak": 124.97, "min": 27.64}, "VIN_SYS_5V0": {"avg": 15.12, "peak": 16.15, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 16.34, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 30.88, "energy_joules_est": 49.1, "sample_count": 16, "duration_seconds": 1.59}, "timestamp": "2026-01-19T15:13:06.136177"}
{"image_index": 344, "image_name": "000000037751.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037751.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1908.716, "latencies_ms": [1908.716], "images_per_second": 0.524, "prompt_tokens": 1109, "response_tokens_est": 32, "n_tiles": 1, "output_text": " The image features a dirt road with a motorcyclist and a backpack, set against a backdrop of a mountain range under a clear blue sky with scattered clouds.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27552.6, "ram_available_mb": 98219.6, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 27549.1, "ram_available_mb": 98223.1, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.84, "peak": 40.56, "min": 18.53}, "VIN": {"avg": 73.2, "peak": 116.95, "min": 30.01}, "VIN_SYS_5V0": {"avg": 14.91, "peak": 16.05, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 16.08, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 29.84, "energy_joules_est": 56.97, "sample_count": 19, "duration_seconds": 1.909}, "timestamp": "2026-01-19T15:13:08.124718"}
{"image_index": 345, "image_name": "000000037777.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037777.jpg", "image_width": 352, "image_height": 230, "image_resolution": "352x230", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1930.571, "latencies_ms": [1930.571], "images_per_second": 0.518, "prompt_tokens": 1099, "response_tokens_est": 33, "n_tiles": 1, "output_text": " The image shows a kitchen with wooden cabinets, a white stove, and a white refrigerator, with a dining table in the center and a bowl of oranges on it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27549.1, "ram_available_mb": 98223.1, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 27557.7, "ram_available_mb": 98214.5, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.48, "peak": 40.57, "min": 16.95}, "VIN": {"avg": 74.68, "peak": 122.79, "min": 28.85}, "VIN_SYS_5V0": {"avg": 14.88, "peak": 16.05, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 15.91, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 29.48, "energy_joules_est": 56.92, "sample_count": 19, "duration_seconds": 1.931}, "timestamp": "2026-01-19T15:13:10.103016"}
{"image_index": 345, "image_name": "000000037777.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037777.jpg", "image_width": 352, "image_height": 230, "image_resolution": "352x230", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2338.605, "latencies_ms": [2338.605], "images_per_second": 0.428, "prompt_tokens": 1113, "response_tokens_est": 49, "n_tiles": 1, "output_text": " table: 1, chair: 1, bananas: 1, oranges: 1, bowl: 1, stove: 1, refrigerator: 1, cabinet: 1, door: 1, light: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27557.7, "ram_available_mb": 98214.5, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 27569.8, "ram_available_mb": 98202.4, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.25, "peak": 40.57, "min": 17.73}, "VIN": {"avg": 68.88, "peak": 128.43, "min": 29.97}, "VIN_SYS_5V0": {"avg": 15.02, "peak": 16.15, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 16.09, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 28.25, "energy_joules_est": 66.09, "sample_count": 23, "duration_seconds": 2.339}, "timestamp": "2026-01-19T15:13:12.489092"}
{"image_index": 345, "image_name": "000000037777.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037777.jpg", "image_width": 352, "image_height": 230, "image_resolution": "352x230", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2091.928, "latencies_ms": [2091.928], "images_per_second": 0.478, "prompt_tokens": 1117, "response_tokens_est": 39, "n_tiles": 1, "output_text": " The kitchen is located in the center of the house, with the dining table and chairs placed in the foreground. The refrigerator is positioned in the background, while the sink is situated near the stove.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27569.8, "ram_available_mb": 98202.4, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 27578.3, "ram_available_mb": 98193.9, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.68, "peak": 40.16, "min": 16.55}, "VIN": {"avg": 66.81, "peak": 102.35, "min": 27.76}, "VIN_SYS_5V0": {"avg": 15.0, "peak": 16.15, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 16.12, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 28.68, "energy_joules_est": 60.01, "sample_count": 21, "duration_seconds": 2.092}, "timestamp": "2026-01-19T15:13:14.662790"}
{"image_index": 345, "image_name": "000000037777.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037777.jpg", "image_width": 352, "image_height": 230, "image_resolution": "352x230", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1818.751, "latencies_ms": [1818.751], "images_per_second": 0.55, "prompt_tokens": 1111, "response_tokens_est": 28, "n_tiles": 1, "output_text": " A kitchen with white appliances and wooden cabinets, a dining table with a bowl of oranges on it, and a window with a green curtain.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27578.3, "ram_available_mb": 98193.9, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 27599.0, "ram_available_mb": 98173.2, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.65, "peak": 40.56, "min": 16.16}, "VIN": {"avg": 71.81, "peak": 114.27, "min": 28.93}, "VIN_SYS_5V0": {"avg": 14.86, "peak": 16.05, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 15.95, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 29.65, "energy_joules_est": 53.94, "sample_count": 18, "duration_seconds": 1.819}, "timestamp": "2026-01-19T15:13:16.538711"}
{"image_index": 345, "image_name": "000000037777.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037777.jpg", "image_width": 352, "image_height": 230, "image_resolution": "352x230", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1669.189, "latencies_ms": [1669.189], "images_per_second": 0.599, "prompt_tokens": 1109, "response_tokens_est": 22, "n_tiles": 1, "output_text": " The kitchen is well lit with natural light coming in from the windows, and the cabinets are made of wood.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27599.0, "ram_available_mb": 98173.2, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 27599.1, "ram_available_mb": 98173.1, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.59, "peak": 40.57, "min": 17.74}, "VIN": {"avg": 70.12, "peak": 106.28, "min": 27.72}, "VIN_SYS_5V0": {"avg": 14.93, "peak": 16.05, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 16.0, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 30.59, "energy_joules_est": 51.07, "sample_count": 17, "duration_seconds": 1.669}, "timestamp": "2026-01-19T15:13:18.304268"}
{"image_index": 346, "image_name": "000000037988.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037988.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1505.894, "latencies_ms": [1505.894], "images_per_second": 0.664, "prompt_tokens": 1099, "response_tokens_est": 16, "n_tiles": 1, "output_text": " A female tennis player is preparing to serve the ball on a green tennis court.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 27599.1, "ram_available_mb": 98173.1, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 27599.1, "ram_available_mb": 98173.1, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.25, "peak": 40.57, "min": 16.95}, "VIN": {"avg": 68.76, "peak": 102.98, "min": 28.97}, "VIN_SYS_5V0": {"avg": 14.73, "peak": 15.95, "min": 11.87}, "VDD_CPU_SOC_MSS": {"avg": 15.69, "peak": 16.92, "min": 13.0}}, "power_watts_avg": 31.25, "energy_joules_est": 47.07, "sample_count": 15, "duration_seconds": 1.506}, "timestamp": "2026-01-19T15:13:19.889130"}
{"image_index": 346, "image_name": "000000037988.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037988.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2685.128, "latencies_ms": [2685.128], "images_per_second": 0.372, "prompt_tokens": 1113, "response_tokens_est": 62, "n_tiles": 1, "output_text": " 1. woman: 1\n2. racket: 1\n3. tennis ball: 1\n4. person: 1\n5. blue wall: 1\n6. white line: 1\n7. green surface: 1\n8. white cap: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27599.1, "ram_available_mb": 98173.1, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 27629.0, "ram_available_mb": 98143.2, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.5, "peak": 40.95, "min": 18.91}, "VIN": {"avg": 64.55, "peak": 101.93, "min": 29.01}, "VIN_SYS_5V0": {"avg": 14.96, "peak": 16.05, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.98, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 27.5, "energy_joules_est": 73.85, "sample_count": 26, "duration_seconds": 2.685}, "timestamp": "2026-01-19T15:13:22.602614"}
{"image_index": 346, "image_name": "000000037988.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037988.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2363.95, "latencies_ms": [2363.95], "images_per_second": 0.423, "prompt_tokens": 1117, "response_tokens_est": 50, "n_tiles": 1, "output_text": " The tennis player is positioned in the foreground of the image, with the ball slightly above her head, indicating she is in the process of serving. The background features a blue wall with a \"POLO\" logo, suggesting the location of the match.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27629.0, "ram_available_mb": 98143.2, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 27628.8, "ram_available_mb": 98143.4, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.94, "peak": 40.56, "min": 16.55}, "VIN": {"avg": 72.15, "peak": 121.89, "min": 28.99}, "VIN_SYS_5V0": {"avg": 15.13, "peak": 16.15, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 16.16, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 27.94, "energy_joules_est": 66.07, "sample_count": 23, "duration_seconds": 2.365}, "timestamp": "2026-01-19T15:13:24.996972"}
{"image_index": 346, "image_name": "000000037988.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037988.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1487.186, "latencies_ms": [1487.186], "images_per_second": 0.672, "prompt_tokens": 1111, "response_tokens_est": 16, "n_tiles": 1, "output_text": " A female tennis player is preparing to serve the ball on a green tennis court.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27628.8, "ram_available_mb": 98143.4, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 27627.0, "ram_available_mb": 98145.2, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.44, "peak": 40.56, "min": 16.95}, "VIN": {"avg": 79.24, "peak": 120.65, "min": 28.2}, "VIN_SYS_5V0": {"avg": 15.1, "peak": 16.15, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 16.14, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 31.44, "energy_joules_est": 46.77, "sample_count": 15, "duration_seconds": 1.488}, "timestamp": "2026-01-19T15:13:26.563909"}
{"image_index": 346, "image_name": "000000037988.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037988.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2059.86, "latencies_ms": [2059.86], "images_per_second": 0.485, "prompt_tokens": 1109, "response_tokens_est": 38, "n_tiles": 1, "output_text": " The image features a female tennis player in a white outfit, with a green tennis court and a blue wall in the background. The lighting is natural, and the weather appears to be clear.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27627.0, "ram_available_mb": 98145.2, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 27625.9, "ram_available_mb": 98146.3, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.74, "peak": 40.57, "min": 19.71}, "VIN": {"avg": 72.23, "peak": 126.82, "min": 30.08}, "VIN_SYS_5V0": {"avg": 14.91, "peak": 16.05, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 16.04, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 29.74, "energy_joules_est": 61.27, "sample_count": 20, "duration_seconds": 2.06}, "timestamp": "2026-01-19T15:13:28.648116"}
{"image_index": 347, "image_name": "000000038048.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038048.jpg", "image_width": 299, "image_height": 500, "image_resolution": "299x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1495.64, "latencies_ms": [1495.64], "images_per_second": 0.669, "prompt_tokens": 1100, "response_tokens_est": 17, "n_tiles": 1, "output_text": " A red fire hydrant is on the sidewalk, and a man is standing nearby.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27625.9, "ram_available_mb": 98146.3, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 27626.1, "ram_available_mb": 98146.1, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.51, "peak": 40.16, "min": 17.74}, "VIN": {"avg": 75.13, "peak": 127.69, "min": 29.27}, "VIN_SYS_5V0": {"avg": 15.09, "peak": 16.15, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 16.27, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 31.51, "energy_joules_est": 47.15, "sample_count": 15, "duration_seconds": 1.496}, "timestamp": "2026-01-19T15:13:30.217190"}
{"image_index": 347, "image_name": "000000038048.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038048.jpg", "image_width": 299, "image_height": 500, "image_resolution": "299x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2263.944, "latencies_ms": [2263.944], "images_per_second": 0.442, "prompt_tokens": 1114, "response_tokens_est": 46, "n_tiles": 1, "output_text": " 1. red fire hydrant\n2. white manhole cover\n3. yellow pedestrian sign\n4. person in white shirt\n5. person in blue shorts\n6. tree\n7. building\n8. sidewalk", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27626.1, "ram_available_mb": 98146.1, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 27626.4, "ram_available_mb": 98145.8, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.83, "peak": 40.57, "min": 19.7}, "VIN": {"avg": 69.29, "peak": 124.36, "min": 29.43}, "VIN_SYS_5V0": {"avg": 14.95, "peak": 16.15, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.98, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 28.83, "energy_joules_est": 65.28, "sample_count": 22, "duration_seconds": 2.264}, "timestamp": "2026-01-19T15:13:32.516169"}
{"image_index": 347, "image_name": "000000038048.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038048.jpg", "image_width": 299, "image_height": 500, "image_resolution": "299x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2042.482, "latencies_ms": [2042.482], "images_per_second": 0.49, "prompt_tokens": 1118, "response_tokens_est": 36, "n_tiles": 1, "output_text": " The fire hydrant is located on the left side of the sidewalk, with the pedestrian on the right side. The pedestrian is standing closer to the fire hydrant than the building.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 27626.4, "ram_available_mb": 98145.8, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 27626.4, "ram_available_mb": 98145.8, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.9, "peak": 40.18, "min": 16.95}, "VIN": {"avg": 67.69, "peak": 103.68, "min": 30.43}, "VIN_SYS_5V0": {"avg": 15.02, "peak": 16.15, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 16.06, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 28.9, "energy_joules_est": 59.04, "sample_count": 20, "duration_seconds": 2.043}, "timestamp": "2026-01-19T15:13:34.599283"}
{"image_index": 347, "image_name": "000000038048.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038048.jpg", "image_width": 299, "image_height": 500, "image_resolution": "299x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1493.527, "latencies_ms": [1493.527], "images_per_second": 0.67, "prompt_tokens": 1112, "response_tokens_est": 17, "n_tiles": 1, "output_text": " A red fire hydrant is on the sidewalk, and a man is standing nearby.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 27626.4, "ram_available_mb": 98145.8, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 27626.4, "ram_available_mb": 98145.8, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.02, "peak": 39.39, "min": 16.95}, "VIN": {"avg": 68.51, "peak": 93.47, "min": 28.82}, "VIN_SYS_5V0": {"avg": 15.02, "peak": 16.15, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 16.22, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 31.02, "energy_joules_est": 46.34, "sample_count": 15, "duration_seconds": 1.494}, "timestamp": "2026-01-19T15:13:36.167957"}
{"image_index": 347, "image_name": "000000038048.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038048.jpg", "image_width": 299, "image_height": 500, "image_resolution": "299x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1463.266, "latencies_ms": [1463.266], "images_per_second": 0.683, "prompt_tokens": 1110, "response_tokens_est": 15, "n_tiles": 1, "output_text": " The fire hydrant is red and white, and the sky is blue.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27626.4, "ram_available_mb": 98145.8, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 27626.4, "ram_available_mb": 98145.8, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.01, "peak": 40.56, "min": 19.32}, "VIN": {"avg": 74.92, "peak": 119.22, "min": 27.26}, "VIN_SYS_5V0": {"avg": 14.85, "peak": 16.15, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 16.01, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 32.01, "energy_joules_est": 46.85, "sample_count": 15, "duration_seconds": 1.464}, "timestamp": "2026-01-19T15:13:37.736178"}
{"image_index": 348, "image_name": "000000038070.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038070.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1634.173, "latencies_ms": [1634.173], "images_per_second": 0.612, "prompt_tokens": 1099, "response_tokens_est": 22, "n_tiles": 1, "output_text": " A white toilet with a lid and seat is in a small bathroom with a green trash can next to it.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 27626.4, "ram_available_mb": 98145.8, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 27626.6, "ram_available_mb": 98145.6, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.68, "peak": 40.56, "min": 19.31}, "VIN": {"avg": 72.37, "peak": 104.19, "min": 29.68}, "VIN_SYS_5V0": {"avg": 14.88, "peak": 16.05, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.97, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 31.68, "energy_joules_est": 51.8, "sample_count": 16, "duration_seconds": 1.635}, "timestamp": "2026-01-19T15:13:39.394472"}
{"image_index": 348, "image_name": "000000038070.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038070.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2027.103, "latencies_ms": [2027.103], "images_per_second": 0.493, "prompt_tokens": 1113, "response_tokens_est": 36, "n_tiles": 1, "output_text": " toilet: 1, toilet paper: 1, trash can: 1, person: 1, floor: 1, wall: 1, lid: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27626.6, "ram_available_mb": 98145.6, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 27626.6, "ram_available_mb": 98145.6, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.9, "peak": 40.56, "min": 19.31}, "VIN": {"avg": 68.41, "peak": 119.24, "min": 28.82}, "VIN_SYS_5V0": {"avg": 15.14, "peak": 16.15, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 16.2, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 29.9, "energy_joules_est": 60.63, "sample_count": 20, "duration_seconds": 2.028}, "timestamp": "2026-01-19T15:13:41.478308"}
{"image_index": 348, "image_name": "000000038070.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038070.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2220.373, "latencies_ms": [2220.373], "images_per_second": 0.45, "prompt_tokens": 1117, "response_tokens_est": 45, "n_tiles": 1, "output_text": " The toilet is positioned in the center of the image, with the person's feet visible in the foreground. The trash can is located to the left of the toilet, while the paper towel dispenser is situated to the right.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27626.6, "ram_available_mb": 98145.6, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 27626.6, "ram_available_mb": 98145.6, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.35, "peak": 40.56, "min": 16.55}, "VIN": {"avg": 73.94, "peak": 118.17, "min": 29.01}, "VIN_SYS_5V0": {"avg": 15.0, "peak": 16.15, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 16.2, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 28.35, "energy_joules_est": 62.96, "sample_count": 22, "duration_seconds": 2.221}, "timestamp": "2026-01-19T15:13:43.752994"}
{"image_index": 348, "image_name": "000000038070.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038070.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1748.999, "latencies_ms": [1748.999], "images_per_second": 0.572, "prompt_tokens": 1111, "response_tokens_est": 26, "n_tiles": 1, "output_text": " A person is standing in a small bathroom with a white toilet, a green trash can, and a white paper towel dispenser.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27626.6, "ram_available_mb": 98145.6, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 27651.6, "ram_available_mb": 98120.5, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.33, "peak": 40.16, "min": 16.55}, "VIN": {"avg": 70.51, "peak": 89.83, "min": 29.59}, "VIN_SYS_5V0": {"avg": 15.03, "peak": 16.15, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 16.26, "peak": 17.71, "min": 13.79}}, "power_watts_avg": 30.33, "energy_joules_est": 53.05, "sample_count": 17, "duration_seconds": 1.749}, "timestamp": "2026-01-19T15:13:45.521941"}
{"image_index": 348, "image_name": "000000038070.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038070.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1524.804, "latencies_ms": [1524.804], "images_per_second": 0.656, "prompt_tokens": 1109, "response_tokens_est": 17, "n_tiles": 1, "output_text": " The bathroom has a white toilet, a green toilet seat, and a brown carpet.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27651.6, "ram_available_mb": 98120.5, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 27651.6, "ram_available_mb": 98120.5, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.2, "peak": 40.56, "min": 19.31}, "VIN": {"avg": 86.24, "peak": 125.92, "min": 27.43}, "VIN_SYS_5V0": {"avg": 15.23, "peak": 16.15, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 16.32, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 32.2, "energy_joules_est": 49.11, "sample_count": 15, "duration_seconds": 1.525}, "timestamp": "2026-01-19T15:13:47.080975"}
{"image_index": 349, "image_name": "000000038118.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038118.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1751.298, "latencies_ms": [1751.298], "images_per_second": 0.571, "prompt_tokens": 1099, "response_tokens_est": 26, "n_tiles": 1, "output_text": " A person wearing a red jacket and black pants is skiing down a snow-covered mountain with a clear blue sky in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27651.6, "ram_available_mb": 98120.5, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 27651.6, "ram_available_mb": 98120.5, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.17, "peak": 40.95, "min": 20.11}, "VIN": {"avg": 73.82, "peak": 123.16, "min": 30.66}, "VIN_SYS_5V0": {"avg": 15.07, "peak": 16.15, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 16.12, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 31.17, "energy_joules_est": 54.6, "sample_count": 17, "duration_seconds": 1.752}, "timestamp": "2026-01-19T15:13:48.869766"}
{"image_index": 349, "image_name": "000000038118.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038118.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2557.554, "latencies_ms": [2557.554], "images_per_second": 0.391, "prompt_tokens": 1113, "response_tokens_est": 57, "n_tiles": 1, "output_text": " 1. person: 1\n2. skis: 2\n3. poles: 2\n4. backpack: 1\n5. helmet: 1\n6. jacket: 1\n7. pants: 1\n8. snow: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27651.6, "ram_available_mb": 98120.5, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 27651.6, "ram_available_mb": 98120.5, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.59, "peak": 40.56, "min": 18.52}, "VIN": {"avg": 69.09, "peak": 94.81, "min": 28.64}, "VIN_SYS_5V0": {"avg": 15.04, "peak": 16.15, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 16.09, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 27.59, "energy_joules_est": 70.57, "sample_count": 25, "duration_seconds": 2.558}, "timestamp": "2026-01-19T15:13:51.475346"}
{"image_index": 349, "image_name": "000000038118.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038118.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2774.993, "latencies_ms": [2774.993], "images_per_second": 0.36, "prompt_tokens": 1117, "response_tokens_est": 62, "n_tiles": 1, "output_text": " The skier is positioned in the foreground of the image, with the mountain peak in the background. The skier is facing towards the mountain peak, indicating a sense of direction or focus on the destination. The skier is also positioned to the left of the image, with the mountain peak to the right.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27651.6, "ram_available_mb": 98120.5, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 27651.6, "ram_available_mb": 98120.5, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.38, "peak": 40.18, "min": 16.16}, "VIN": {"avg": 68.14, "peak": 127.11, "min": 28.72}, "VIN_SYS_5V0": {"avg": 14.86, "peak": 16.05, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.88, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 26.38, "energy_joules_est": 73.22, "sample_count": 27, "duration_seconds": 2.775}, "timestamp": "2026-01-19T15:13:54.291614"}
{"image_index": 349, "image_name": "000000038118.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038118.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1529.491, "latencies_ms": [1529.491], "images_per_second": 0.654, "prompt_tokens": 1111, "response_tokens_est": 17, "n_tiles": 1, "output_text": " A person is skiing down a snowy mountain with a clear blue sky in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27651.6, "ram_available_mb": 98120.5, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 27651.6, "ram_available_mb": 98120.5, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.94, "peak": 40.18, "min": 15.76}, "VIN": {"avg": 69.84, "peak": 122.56, "min": 30.21}, "VIN_SYS_5V0": {"avg": 14.92, "peak": 16.05, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.98, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 30.94, "energy_joules_est": 47.34, "sample_count": 15, "duration_seconds": 1.53}, "timestamp": "2026-01-19T15:13:55.857782"}
{"image_index": 349, "image_name": "000000038118.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038118.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2144.377, "latencies_ms": [2144.377], "images_per_second": 0.466, "prompt_tokens": 1109, "response_tokens_est": 41, "n_tiles": 1, "output_text": " The image features a person wearing a red jacket and black pants, skiing down a snowy mountain under a clear blue sky. The snow is pristine white, and the person's skis are a vibrant red.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27651.6, "ram_available_mb": 98120.5, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 27651.6, "ram_available_mb": 98120.5, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.42, "peak": 40.56, "min": 19.7}, "VIN": {"avg": 67.81, "peak": 123.89, "min": 29.31}, "VIN_SYS_5V0": {"avg": 15.01, "peak": 16.05, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 16.07, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 29.42, "energy_joules_est": 63.1, "sample_count": 21, "duration_seconds": 2.145}, "timestamp": "2026-01-19T15:13:58.049317"}
{"image_index": 350, "image_name": "000000038210.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038210.jpg", "image_width": 428, "image_height": 640, "image_resolution": "428x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1694.721, "latencies_ms": [1694.721], "images_per_second": 0.59, "prompt_tokens": 1100, "response_tokens_est": 24, "n_tiles": 1, "output_text": " A man wearing a bib with the number 30 is skiing down a snowy mountain with trees in the background.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 27651.6, "ram_available_mb": 98120.5, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 27651.6, "ram_available_mb": 98120.5, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.24, "peak": 40.16, "min": 16.56}, "VIN": {"avg": 74.07, "peak": 126.88, "min": 28.74}, "VIN_SYS_5V0": {"avg": 14.9, "peak": 16.05, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 16.05, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 30.24, "energy_joules_est": 51.27, "sample_count": 17, "duration_seconds": 1.696}, "timestamp": "2026-01-19T15:13:59.826277"}
{"image_index": 350, "image_name": "000000038210.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038210.jpg", "image_width": 428, "image_height": 640, "image_resolution": "428x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2700.98, "latencies_ms": [2700.98], "images_per_second": 0.37, "prompt_tokens": 1114, "response_tokens_est": 59, "n_tiles": 1, "output_text": " 1. person: 1\n2. skis: 2\n3. ski poles: 2\n4. bib: 1\n5. backpack: 1\n6. snow: 1\n7. trees: 1\n8. mountain: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27651.6, "ram_available_mb": 98120.5, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 27651.6, "ram_available_mb": 98120.5, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.57, "peak": 40.18, "min": 16.95}, "VIN": {"avg": 68.47, "peak": 128.45, "min": 28.11}, "VIN_SYS_5V0": {"avg": 14.69, "peak": 15.95, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.59, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 26.57, "energy_joules_est": 71.79, "sample_count": 27, "duration_seconds": 2.702}, "timestamp": "2026-01-19T15:14:02.634705"}
{"image_index": 350, "image_name": "000000038210.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038210.jpg", "image_width": 428, "image_height": 640, "image_resolution": "428x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1928.44, "latencies_ms": [1928.44], "images_per_second": 0.519, "prompt_tokens": 1118, "response_tokens_est": 32, "n_tiles": 1, "output_text": " The skier is in the foreground, with the trees and mountains in the background. The skier is to the left of the skier in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27651.6, "ram_available_mb": 98120.5, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 27651.6, "ram_available_mb": 98120.5, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.78, "peak": 39.77, "min": 14.59}, "VIN": {"avg": 75.07, "peak": 119.52, "min": 30.62}, "VIN_SYS_5V0": {"avg": 14.67, "peak": 15.95, "min": 11.54}, "VDD_CPU_SOC_MSS": {"avg": 15.73, "peak": 17.32, "min": 12.61}}, "power_watts_avg": 28.78, "energy_joules_est": 55.52, "sample_count": 19, "duration_seconds": 1.929}, "timestamp": "2026-01-19T15:14:04.618286"}
{"image_index": 350, "image_name": "000000038210.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038210.jpg", "image_width": 428, "image_height": 640, "image_resolution": "428x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1691.427, "latencies_ms": [1691.427], "images_per_second": 0.591, "prompt_tokens": 1112, "response_tokens_est": 24, "n_tiles": 1, "output_text": " A man wearing a bib with the number 30 is skiing down a snowy mountain with trees in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27651.6, "ram_available_mb": 98120.5, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 27651.6, "ram_available_mb": 98120.5, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.24, "peak": 39.78, "min": 17.35}, "VIN": {"avg": 77.36, "peak": 128.72, "min": 28.97}, "VIN_SYS_5V0": {"avg": 14.93, "peak": 16.26, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 16.0, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 30.24, "energy_joules_est": 51.16, "sample_count": 17, "duration_seconds": 1.692}, "timestamp": "2026-01-19T15:14:06.391202"}
{"image_index": 350, "image_name": "000000038210.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038210.jpg", "image_width": 428, "image_height": 640, "image_resolution": "428x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2269.402, "latencies_ms": [2269.402], "images_per_second": 0.441, "prompt_tokens": 1110, "response_tokens_est": 46, "n_tiles": 1, "output_text": " The image features a man wearing a red and white jacket, black pants, and a black hat, skiing down a snowy mountain with a white background. The lighting is natural, and the weather appears to be cold and overcast.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 27651.6, "ram_available_mb": 98120.5, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 27651.6, "ram_available_mb": 98120.6, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.22, "peak": 40.18, "min": 17.75}, "VIN": {"avg": 70.25, "peak": 126.01, "min": 29.48}, "VIN_SYS_5V0": {"avg": 14.89, "peak": 16.15, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 15.93, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 28.22, "energy_joules_est": 64.06, "sample_count": 22, "duration_seconds": 2.27}, "timestamp": "2026-01-19T15:14:08.687007"}
{"image_index": 351, "image_name": "000000038576.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038576.jpg", "image_width": 438, "image_height": 640, "image_resolution": "438x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1898.931, "latencies_ms": [1898.931], "images_per_second": 0.527, "prompt_tokens": 1100, "response_tokens_est": 32, "n_tiles": 1, "output_text": " A black and white photo of a computer desk with a keyboard, mouse, and monitor, with the word \"WORKPLACE\" in the top right corner.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27651.6, "ram_available_mb": 98120.6, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 27651.6, "ram_available_mb": 98120.6, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.44, "peak": 40.57, "min": 16.95}, "VIN": {"avg": 73.98, "peak": 122.62, "min": 29.18}, "VIN_SYS_5V0": {"avg": 15.05, "peak": 16.26, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 16.18, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 29.44, "energy_joules_est": 55.93, "sample_count": 19, "duration_seconds": 1.9}, "timestamp": "2026-01-19T15:14:10.668183"}
{"image_index": 351, "image_name": "000000038576.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038576.jpg", "image_width": 438, "image_height": 640, "image_resolution": "438x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2588.659, "latencies_ms": [2588.659], "images_per_second": 0.386, "prompt_tokens": 1114, "response_tokens_est": 58, "n_tiles": 1, "output_text": " 1. monitor: 1\n2. keyboard: 1\n3. mouse: 1\n4. desk: 1\n5. wall: 1\n6. computer: 1\n7. desk chair: 1\n8. desk surface: 1", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 27651.6, "ram_available_mb": 98120.6, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 27651.6, "ram_available_mb": 98120.6, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.14, "peak": 40.18, "min": 16.95}, "VIN": {"avg": 68.07, "peak": 121.27, "min": 29.38}, "VIN_SYS_5V0": {"avg": 14.93, "peak": 16.05, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.87, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 27.14, "energy_joules_est": 70.27, "sample_count": 25, "duration_seconds": 2.589}, "timestamp": "2026-01-19T15:14:13.273914"}
{"image_index": 351, "image_name": "000000038576.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038576.jpg", "image_width": 438, "image_height": 640, "image_resolution": "438x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2180.015, "latencies_ms": [2180.015], "images_per_second": 0.459, "prompt_tokens": 1118, "response_tokens_est": 43, "n_tiles": 1, "output_text": " The keyboard is positioned in the foreground, close to the camera, while the computer monitor is in the background, farther away. The mouse is placed near the keyboard, and the computer monitor is positioned above the keyboard.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27651.6, "ram_available_mb": 98120.6, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 27651.6, "ram_available_mb": 98120.6, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.59, "peak": 40.18, "min": 16.56}, "VIN": {"avg": 66.75, "peak": 108.2, "min": 32.24}, "VIN_SYS_5V0": {"avg": 15.14, "peak": 16.26, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 16.14, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 28.59, "energy_joules_est": 62.34, "sample_count": 21, "duration_seconds": 2.18}, "timestamp": "2026-01-19T15:14:15.463384"}
{"image_index": 351, "image_name": "000000038576.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038576.jpg", "image_width": 438, "image_height": 640, "image_resolution": "438x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1493.057, "latencies_ms": [1493.057], "images_per_second": 0.67, "prompt_tokens": 1112, "response_tokens_est": 17, "n_tiles": 1, "output_text": " A black and white photo of a computer desk with a keyboard and mouse on it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27651.6, "ram_available_mb": 98120.6, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 27651.6, "ram_available_mb": 98120.6, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.2, "peak": 39.39, "min": 17.35}, "VIN": {"avg": 79.39, "peak": 121.65, "min": 27.46}, "VIN_SYS_5V0": {"avg": 15.22, "peak": 16.26, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 16.3, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 31.2, "energy_joules_est": 46.6, "sample_count": 15, "duration_seconds": 1.494}, "timestamp": "2026-01-19T15:14:17.032287"}
{"image_index": 351, "image_name": "000000038576.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038576.jpg", "image_width": 438, "image_height": 640, "image_resolution": "438x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2000.176, "latencies_ms": [2000.176], "images_per_second": 0.5, "prompt_tokens": 1110, "response_tokens_est": 35, "n_tiles": 1, "output_text": " The image is in black and white, with a white desk and a white wall in the background. The lighting is bright and even, and the materials are smooth and polished.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27651.6, "ram_available_mb": 98120.6, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 27651.6, "ram_available_mb": 98120.6, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.37, "peak": 40.18, "min": 19.31}, "VIN": {"avg": 68.77, "peak": 102.16, "min": 28.14}, "VIN_SYS_5V0": {"avg": 14.82, "peak": 16.05, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.89, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 29.37, "energy_joules_est": 58.75, "sample_count": 20, "duration_seconds": 2.0}, "timestamp": "2026-01-19T15:14:19.115396"}
{"image_index": 352, "image_name": "000000038678.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038678.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1588.131, "latencies_ms": [1588.131], "images_per_second": 0.63, "prompt_tokens": 1100, "response_tokens_est": 20, "n_tiles": 1, "output_text": " A woman wearing a striped shirt is sitting at a table in a restaurant and eating a bagel.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27651.6, "ram_available_mb": 98120.6, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 27651.6, "ram_available_mb": 98120.6, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.66, "peak": 40.57, "min": 16.16}, "VIN": {"avg": 70.05, "peak": 105.05, "min": 27.34}, "VIN_SYS_5V0": {"avg": 14.77, "peak": 16.15, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.82, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 30.66, "energy_joules_est": 48.72, "sample_count": 16, "duration_seconds": 1.589}, "timestamp": "2026-01-19T15:14:20.797124"}
{"image_index": 352, "image_name": "000000038678.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038678.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2579.859, "latencies_ms": [2579.859], "images_per_second": 0.388, "prompt_tokens": 1114, "response_tokens_est": 58, "n_tiles": 1, "output_text": " 1. person: 1\n2. bagel: 1\n3. coffee cup: 1\n4. box: 1\n5. sandwich: 1\n6. person: 1\n7. person: 1\n8. person: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27651.6, "ram_available_mb": 98120.6, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 27651.6, "ram_available_mb": 98120.6, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.63, "peak": 40.57, "min": 18.53}, "VIN": {"avg": 74.15, "peak": 125.65, "min": 28.95}, "VIN_SYS_5V0": {"avg": 14.91, "peak": 16.15, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.89, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 27.63, "energy_joules_est": 71.29, "sample_count": 25, "duration_seconds": 2.58}, "timestamp": "2026-01-19T15:14:23.403063"}
{"image_index": 352, "image_name": "000000038678.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038678.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1938.294, "latencies_ms": [1938.294], "images_per_second": 0.516, "prompt_tokens": 1118, "response_tokens_est": 34, "n_tiles": 1, "output_text": " The woman is in the foreground, holding a bagel, while the coffee cup is in the background. The bagel is closer to the camera than the coffee cup.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27651.6, "ram_available_mb": 98120.6, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 27651.6, "ram_available_mb": 98120.6, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.42, "peak": 40.16, "min": 16.95}, "VIN": {"avg": 67.73, "peak": 118.73, "min": 29.9}, "VIN_SYS_5V0": {"avg": 15.12, "peak": 16.15, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 16.2, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 29.42, "energy_joules_est": 57.04, "sample_count": 19, "duration_seconds": 1.939}, "timestamp": "2026-01-19T15:14:25.381621"}
{"image_index": 352, "image_name": "000000038678.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038678.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1522.084, "latencies_ms": [1522.084], "images_per_second": 0.657, "prompt_tokens": 1112, "response_tokens_est": 18, "n_tiles": 1, "output_text": " A woman is sitting at a table in a restaurant, holding a bagel and smiling.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27651.6, "ram_available_mb": 98120.6, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 27651.6, "ram_available_mb": 98120.6, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.33, "peak": 39.39, "min": 17.74}, "VIN": {"avg": 72.97, "peak": 120.15, "min": 28.78}, "VIN_SYS_5V0": {"avg": 15.11, "peak": 16.15, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 16.22, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 31.33, "energy_joules_est": 47.7, "sample_count": 15, "duration_seconds": 1.523}, "timestamp": "2026-01-19T15:14:26.952954"}
{"image_index": 352, "image_name": "000000038678.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038678.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2098.186, "latencies_ms": [2098.186], "images_per_second": 0.477, "prompt_tokens": 1110, "response_tokens_est": 40, "n_tiles": 1, "output_text": " The image has a warm color tone, with the person's clothing being a mix of purple and red. The lighting is natural, coming from the window, and the window is covered with blinds.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27651.6, "ram_available_mb": 98120.6, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 27651.6, "ram_available_mb": 98120.6, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.17, "peak": 40.56, "min": 18.52}, "VIN": {"avg": 61.21, "peak": 95.01, "min": 28.92}, "VIN_SYS_5V0": {"avg": 15.01, "peak": 16.26, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 16.03, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 29.17, "energy_joules_est": 61.21, "sample_count": 21, "duration_seconds": 2.098}, "timestamp": "2026-01-19T15:14:29.142568"}
{"image_index": 353, "image_name": "000000038825.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038825.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1557.691, "latencies_ms": [1557.691], "images_per_second": 0.642, "prompt_tokens": 1099, "response_tokens_est": 18, "n_tiles": 1, "output_text": " Two zebras with black and white stripes are grazing on green grass in a grassy field.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 27651.6, "ram_available_mb": 98120.6, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 27651.6, "ram_available_mb": 98120.6, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.89, "peak": 40.18, "min": 15.77}, "VIN": {"avg": 74.02, "peak": 118.33, "min": 30.27}, "VIN_SYS_5V0": {"avg": 14.74, "peak": 16.05, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.85, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 30.89, "energy_joules_est": 48.14, "sample_count": 15, "duration_seconds": 1.559}, "timestamp": "2026-01-19T15:14:30.729395"}
{"image_index": 353, "image_name": "000000038825.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038825.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1176.745, "latencies_ms": [1176.745], "images_per_second": 0.85, "prompt_tokens": 1113, "response_tokens_est": 4, "n_tiles": 1, "output_text": " zebra: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27651.6, "ram_available_mb": 98120.6, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 1.9, "ram_used_mb": 27651.6, "ram_available_mb": 98120.6, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 33.94, "peak": 40.95, "min": 20.89}, "VIN": {"avg": 77.48, "peak": 123.21, "min": 29.88}, "VIN_SYS_5V0": {"avg": 15.08, "peak": 16.15, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 16.27, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 33.94, "energy_joules_est": 39.95, "sample_count": 12, "duration_seconds": 1.177}, "timestamp": "2026-01-19T15:14:31.994922"}
{"image_index": 353, "image_name": "000000038825.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038825.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2369.338, "latencies_ms": [2369.338], "images_per_second": 0.422, "prompt_tokens": 1117, "response_tokens_est": 49, "n_tiles": 1, "output_text": " The two zebras are positioned side by side, with the zebra on the left slightly in front of the one on the right. The zebras are in the foreground of the image, with the background consisting of a grassy field and a fence.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 27651.6, "ram_available_mb": 98120.6, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 27651.6, "ram_available_mb": 98120.6, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.41, "peak": 40.95, "min": 19.7}, "VIN": {"avg": 70.22, "peak": 120.47, "min": 29.32}, "VIN_SYS_5V0": {"avg": 14.89, "peak": 16.05, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.88, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 29.41, "energy_joules_est": 69.69, "sample_count": 23, "duration_seconds": 2.37}, "timestamp": "2026-01-19T15:14:34.398094"}
{"image_index": 353, "image_name": "000000038825.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038825.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1414.294, "latencies_ms": [1414.294], "images_per_second": 0.707, "prompt_tokens": 1111, "response_tokens_est": 13, "n_tiles": 1, "output_text": " Two zebras are grazing on green grass in a grassy field.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27651.6, "ram_available_mb": 98120.6, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 27651.6, "ram_available_mb": 98120.6, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.82, "peak": 40.56, "min": 16.56}, "VIN": {"avg": 72.33, "peak": 104.72, "min": 29.33}, "VIN_SYS_5V0": {"avg": 15.08, "peak": 16.15, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 16.17, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 31.82, "energy_joules_est": 45.02, "sample_count": 14, "duration_seconds": 1.415}, "timestamp": "2026-01-19T15:14:35.855485"}
{"image_index": 353, "image_name": "000000038825.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038825.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2322.252, "latencies_ms": [2322.252], "images_per_second": 0.431, "prompt_tokens": 1109, "response_tokens_est": 48, "n_tiles": 1, "output_text": " The image features two zebras with their black and white stripes grazing on green grass. The lighting is natural and bright, suggesting it is daytime. The zebras are standing close to each other, indicating a sense of companionship or social interaction.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 27651.6, "ram_available_mb": 98120.6, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 27648.1, "ram_available_mb": 98124.1, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.95, "peak": 40.95, "min": 18.91}, "VIN": {"avg": 63.29, "peak": 101.38, "min": 30.13}, "VIN_SYS_5V0": {"avg": 14.97, "peak": 16.15, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 16.11, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 28.95, "energy_joules_est": 67.24, "sample_count": 23, "duration_seconds": 2.323}, "timestamp": "2026-01-19T15:14:38.240113"}
{"image_index": 354, "image_name": "000000038829.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038829.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1660.169, "latencies_ms": [1660.169], "images_per_second": 0.602, "prompt_tokens": 1099, "response_tokens_est": 22, "n_tiles": 1, "output_text": " Two young men are riding a green bicycle on a busy street with a Coca Cola store in the background.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 27648.1, "ram_available_mb": 98124.1, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 8.0, "ram_used_mb": 27609.4, "ram_available_mb": 98162.7, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.71, "peak": 40.18, "min": 16.16}, "VIN": {"avg": 77.43, "peak": 126.53, "min": 29.5}, "VIN_SYS_5V0": {"avg": 14.92, "peak": 16.05, "min": 12.15}, "VDD_CPU_SOC_MSS": {"avg": 16.59, "peak": 18.11, "min": 13.39}}, "power_watts_avg": 30.71, "energy_joules_est": 51.01, "sample_count": 16, "duration_seconds": 1.661}, "timestamp": "2026-01-19T15:14:39.916948"}
{"image_index": 354, "image_name": "000000038829.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038829.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2536.156, "latencies_ms": [2536.156], "images_per_second": 0.394, "prompt_tokens": 1113, "response_tokens_est": 57, "n_tiles": 1, "output_text": " 1. bicycle: 2\n2. person: 2\n3. motorcycle: 2\n4. scooter: 1\n5. store: 1\n6. plant: 1\n7. sign: 1\n8. building: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27609.4, "ram_available_mb": 98162.7, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 27609.4, "ram_available_mb": 98162.8, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.96, "peak": 40.95, "min": 18.13}, "VIN": {"avg": 69.16, "peak": 122.34, "min": 29.07}, "VIN_SYS_5V0": {"avg": 15.16, "peak": 16.26, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 16.2, "peak": 17.71, "min": 14.57}}, "power_watts_avg": 27.96, "energy_joules_est": 70.92, "sample_count": 25, "duration_seconds": 2.537}, "timestamp": "2026-01-19T15:14:42.519868"}
{"image_index": 354, "image_name": "000000038829.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038829.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2228.894, "latencies_ms": [2228.894], "images_per_second": 0.449, "prompt_tokens": 1117, "response_tokens_est": 42, "n_tiles": 1, "output_text": " The two men are positioned in the foreground of the image, with the bicycle in front of them. The motorcycle is parked on the right side of the image, while the storefront is located in the background.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 27609.4, "ram_available_mb": 98162.8, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 27609.7, "ram_available_mb": 98162.5, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.93, "peak": 40.16, "min": 15.77}, "VIN": {"avg": 70.79, "peak": 119.47, "min": 29.96}, "VIN_SYS_5V0": {"avg": 14.82, "peak": 16.05, "min": 12.15}, "VDD_CPU_SOC_MSS": {"avg": 15.89, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 27.93, "energy_joules_est": 62.27, "sample_count": 22, "duration_seconds": 2.229}, "timestamp": "2026-01-19T15:14:44.810057"}
{"image_index": 354, "image_name": "000000038829.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038829.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1478.044, "latencies_ms": [1478.044], "images_per_second": 0.677, "prompt_tokens": 1111, "response_tokens_est": 15, "n_tiles": 1, "output_text": " Two young men are riding a bicycle down a busy street in a city.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27609.7, "ram_available_mb": 98162.5, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 27609.9, "ram_available_mb": 98162.3, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.09, "peak": 40.16, "min": 16.16}, "VIN": {"avg": 74.65, "peak": 123.2, "min": 28.8}, "VIN_SYS_5V0": {"avg": 14.82, "peak": 16.05, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.93, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 31.09, "energy_joules_est": 45.96, "sample_count": 15, "duration_seconds": 1.478}, "timestamp": "2026-01-19T15:14:46.378741"}
{"image_index": 354, "image_name": "000000038829.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038829.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1795.641, "latencies_ms": [1795.641], "images_per_second": 0.557, "prompt_tokens": 1109, "response_tokens_est": 27, "n_tiles": 1, "output_text": " The image is taken during the day with natural light, and the colors are vibrant with a mix of green, blue, and red.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27609.9, "ram_available_mb": 98162.3, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 27610.3, "ram_available_mb": 98161.8, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.51, "peak": 40.56, "min": 19.7}, "VIN": {"avg": 70.46, "peak": 119.79, "min": 28.41}, "VIN_SYS_5V0": {"avg": 14.77, "peak": 16.05, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.92, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 30.51, "energy_joules_est": 54.79, "sample_count": 18, "duration_seconds": 1.796}, "timestamp": "2026-01-19T15:14:48.256644"}
{"image_index": 355, "image_name": "000000039405.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039405.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2703.563, "latencies_ms": [2703.563], "images_per_second": 0.37, "prompt_tokens": 1099, "response_tokens_est": 63, "n_tiles": 1, "output_text": " The image captures a dynamic moment in a tennis match, with two players in white attire actively engaged on a grass court, while a referee and a ball boy are positioned nearby, ready to assist. The spectators, seated in the stands, are attentively watching the unfolding action, adding to the atmosphere of anticipation and excitement.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27610.3, "ram_available_mb": 98161.8, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 27622.1, "ram_available_mb": 98150.0, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.75, "peak": 40.57, "min": 16.95}, "VIN": {"avg": 69.23, "peak": 105.05, "min": 28.65}, "VIN_SYS_5V0": {"avg": 14.87, "peak": 16.05, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 16.03, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 26.75, "energy_joules_est": 72.33, "sample_count": 27, "duration_seconds": 2.704}, "timestamp": "2026-01-19T15:14:51.069115"}
{"image_index": 355, "image_name": "000000039405.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039405.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2786.71, "latencies_ms": [2786.71], "images_per_second": 0.359, "prompt_tokens": 1113, "response_tokens_est": 66, "n_tiles": 1, "output_text": " 1. Tennis racket: 1\n2. Tennis ball: 1\n3. Player: 2\n4. Chair: 1\n5. Ball boy: 1\n6. Umpire: 1\n7. Spectator: 1\n8. Seat: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27622.1, "ram_available_mb": 98150.0, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 27629.8, "ram_available_mb": 98142.4, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.44, "peak": 40.18, "min": 14.59}, "VIN": {"avg": 69.62, "peak": 120.34, "min": 29.26}, "VIN_SYS_5V0": {"avg": 14.93, "peak": 16.05, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.92, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 26.44, "energy_joules_est": 73.69, "sample_count": 27, "duration_seconds": 2.787}, "timestamp": "2026-01-19T15:14:53.883876"}
{"image_index": 355, "image_name": "000000039405.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039405.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2048.837, "latencies_ms": [2048.837], "images_per_second": 0.488, "prompt_tokens": 1117, "response_tokens_est": 38, "n_tiles": 1, "output_text": " The tennis player is positioned on the left side of the image, while the ball is located in the center. The audience is situated in the background, with some spectators closer to the foreground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27629.8, "ram_available_mb": 98142.4, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 27631.6, "ram_available_mb": 98140.5, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.23, "peak": 40.57, "min": 16.55}, "VIN": {"avg": 77.18, "peak": 121.18, "min": 30.24}, "VIN_SYS_5V0": {"avg": 15.18, "peak": 16.15, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 16.26, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 29.23, "energy_joules_est": 59.9, "sample_count": 20, "duration_seconds": 2.049}, "timestamp": "2026-01-19T15:14:55.955107"}
{"image_index": 355, "image_name": "000000039405.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039405.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1499.723, "latencies_ms": [1499.723], "images_per_second": 0.667, "prompt_tokens": 1111, "response_tokens_est": 17, "n_tiles": 1, "output_text": " A tennis match is taking place in a large stadium with a crowd of spectators watching.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 27631.6, "ram_available_mb": 98140.5, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 27643.4, "ram_available_mb": 98128.8, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.72, "peak": 40.18, "min": 18.13}, "VIN": {"avg": 78.42, "peak": 121.29, "min": 30.54}, "VIN_SYS_5V0": {"avg": 15.18, "peak": 16.15, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 16.3, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 31.72, "energy_joules_est": 47.58, "sample_count": 15, "duration_seconds": 1.5}, "timestamp": "2026-01-19T15:14:57.534781"}
{"image_index": 355, "image_name": "000000039405.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039405.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2024.362, "latencies_ms": [2024.362], "images_per_second": 0.494, "prompt_tokens": 1109, "response_tokens_est": 36, "n_tiles": 1, "output_text": " The image is taken during a daytime tennis match, with the players dressed in white and the court's green grass. The lighting is natural, and the weather appears to be clear.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27643.4, "ram_available_mb": 98128.8, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 6.4, "ram_used_mb": 27570.4, "ram_available_mb": 98201.8, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.6, "peak": 40.56, "min": 19.32}, "VIN": {"avg": 71.05, "peak": 126.55, "min": 30.61}, "VIN_SYS_5V0": {"avg": 14.88, "peak": 16.05, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 16.12, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 29.6, "energy_joules_est": 59.95, "sample_count": 20, "duration_seconds": 2.025}, "timestamp": "2026-01-19T15:14:59.624932"}
{"image_index": 356, "image_name": "000000039477.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039477.jpg", "image_width": 640, "image_height": 421, "image_resolution": "640x421", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2073.965, "latencies_ms": [2073.965], "images_per_second": 0.482, "prompt_tokens": 1099, "response_tokens_est": 37, "n_tiles": 1, "output_text": " The image depicts a cozy living room with a brown couch, a television on a wooden stand, and a variety of plants on shelves and the floor, creating a warm and inviting atmosphere.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 27570.4, "ram_available_mb": 98201.8, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 27573.8, "ram_available_mb": 98198.4, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.46, "peak": 40.18, "min": 16.56}, "VIN": {"avg": 66.37, "peak": 93.77, "min": 27.63}, "VIN_SYS_5V0": {"avg": 14.78, "peak": 15.95, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.82, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 28.46, "energy_joules_est": 59.04, "sample_count": 21, "duration_seconds": 2.074}, "timestamp": "2026-01-19T15:15:01.808131"}
{"image_index": 356, "image_name": "000000039477.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039477.jpg", "image_width": 640, "image_height": 421, "image_resolution": "640x421", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2340.577, "latencies_ms": [2340.577], "images_per_second": 0.427, "prompt_tokens": 1113, "response_tokens_est": 48, "n_tiles": 1, "output_text": " television: 1\ncouch: 1\ntelevision stand: 1\ntelevision: 1\ncouch: 1\ntelevision stand: 1\ntelevision: 1\ncouch: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27573.8, "ram_available_mb": 98198.4, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 27567.0, "ram_available_mb": 98205.2, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.61, "peak": 39.78, "min": 15.38}, "VIN": {"avg": 68.68, "peak": 96.89, "min": 28.72}, "VIN_SYS_5V0": {"avg": 14.81, "peak": 15.95, "min": 11.66}, "VDD_CPU_SOC_MSS": {"avg": 15.87, "peak": 17.32, "min": 12.61}}, "power_watts_avg": 27.61, "energy_joules_est": 64.63, "sample_count": 23, "duration_seconds": 2.341}, "timestamp": "2026-01-19T15:15:04.197502"}
{"image_index": 356, "image_name": "000000039477.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039477.jpg", "image_width": 640, "image_height": 421, "image_resolution": "640x421", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2327.866, "latencies_ms": [2327.866], "images_per_second": 0.43, "prompt_tokens": 1117, "response_tokens_est": 48, "n_tiles": 1, "output_text": " The television is located on the left side of the room, with the couch situated on the right side. The plants are placed in the background, near the window, while the rug is positioned in the foreground, in front of the couch.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27567.0, "ram_available_mb": 98205.2, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 27567.5, "ram_available_mb": 98204.7, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.87, "peak": 40.18, "min": 16.56}, "VIN": {"avg": 73.65, "peak": 126.97, "min": 29.89}, "VIN_SYS_5V0": {"avg": 14.93, "peak": 16.15, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 16.02, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 27.87, "energy_joules_est": 64.89, "sample_count": 23, "duration_seconds": 2.328}, "timestamp": "2026-01-19T15:15:06.598909"}
{"image_index": 356, "image_name": "000000039477.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039477.jpg", "image_width": 640, "image_height": 421, "image_resolution": "640x421", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1511.199, "latencies_ms": [1511.199], "images_per_second": 0.662, "prompt_tokens": 1111, "response_tokens_est": 16, "n_tiles": 1, "output_text": " A living room with a brown couch, a TV, and a TV stand.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27567.5, "ram_available_mb": 98204.7, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 27578.8, "ram_available_mb": 98193.4, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.91, "peak": 40.18, "min": 15.77}, "VIN": {"avg": 74.17, "peak": 105.73, "min": 30.62}, "VIN_SYS_5V0": {"avg": 14.82, "peak": 16.05, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 16.01, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 30.91, "energy_joules_est": 46.72, "sample_count": 15, "duration_seconds": 1.512}, "timestamp": "2026-01-19T15:15:08.154307"}
{"image_index": 356, "image_name": "000000039477.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039477.jpg", "image_width": 640, "image_height": 421, "image_resolution": "640x421", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1632.028, "latencies_ms": [1632.028], "images_per_second": 0.613, "prompt_tokens": 1109, "response_tokens_est": 21, "n_tiles": 1, "output_text": " The room is well-lit with natural light coming through the window, and the walls are painted white.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27578.8, "ram_available_mb": 98193.4, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 27581.1, "ram_available_mb": 98191.1, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.01, "peak": 40.95, "min": 20.89}, "VIN": {"avg": 72.94, "peak": 132.0, "min": 29.05}, "VIN_SYS_5V0": {"avg": 15.03, "peak": 16.15, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 16.12, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 32.01, "energy_joules_est": 52.26, "sample_count": 16, "duration_seconds": 1.633}, "timestamp": "2026-01-19T15:15:09.822285"}
{"image_index": 357, "image_name": "000000039480.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039480.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1746.998, "latencies_ms": [1746.998], "images_per_second": 0.572, "prompt_tokens": 1099, "response_tokens_est": 26, "n_tiles": 1, "output_text": " A woman wearing a red dress and a white visor is holding a tennis racket and smiling on a red clay tennis court.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27581.1, "ram_available_mb": 98191.1, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 27591.3, "ram_available_mb": 98180.8, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.19, "peak": 40.57, "min": 19.7}, "VIN": {"avg": 69.63, "peak": 101.93, "min": 29.15}, "VIN_SYS_5V0": {"avg": 15.05, "peak": 16.15, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 16.26, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 31.19, "energy_joules_est": 54.5, "sample_count": 17, "duration_seconds": 1.747}, "timestamp": "2026-01-19T15:15:11.598227"}
{"image_index": 357, "image_name": "000000039480.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039480.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2837.047, "latencies_ms": [2837.047], "images_per_second": 0.352, "prompt_tokens": 1113, "response_tokens_est": 64, "n_tiles": 1, "output_text": " 1. woman: 1\n2. visor: 1\n3. tennis racket: 1\n4. tennis ball: 1\n5. tennis court: 1\n6. green fence: 1\n7. white shoe: 1\n8. white shoe: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27591.3, "ram_available_mb": 98180.8, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 27598.6, "ram_available_mb": 98173.6, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.61, "peak": 40.97, "min": 15.76}, "VIN": {"avg": 64.08, "peak": 98.2, "min": 28.53}, "VIN_SYS_5V0": {"avg": 14.97, "peak": 16.15, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 15.97, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 26.61, "energy_joules_est": 75.5, "sample_count": 28, "duration_seconds": 2.837}, "timestamp": "2026-01-19T15:15:14.513810"}
{"image_index": 357, "image_name": "000000039480.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039480.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2296.978, "latencies_ms": [2296.978], "images_per_second": 0.435, "prompt_tokens": 1117, "response_tokens_est": 47, "n_tiles": 1, "output_text": " The tennis player is positioned in the foreground of the image, with the green fence and spectators in the background. The player is holding a tennis racket in her right hand, and her body language suggests she is celebrating a successful point.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27598.6, "ram_available_mb": 98173.6, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 27606.1, "ram_available_mb": 98166.1, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.39, "peak": 40.18, "min": 13.8}, "VIN": {"avg": 66.65, "peak": 127.52, "min": 29.3}, "VIN_SYS_5V0": {"avg": 14.73, "peak": 15.95, "min": 11.34}, "VDD_CPU_SOC_MSS": {"avg": 15.78, "peak": 17.32, "min": 12.61}}, "power_watts_avg": 27.39, "energy_joules_est": 62.92, "sample_count": 23, "duration_seconds": 2.297}, "timestamp": "2026-01-19T15:15:16.902533"}
{"image_index": 357, "image_name": "000000039480.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039480.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1666.792, "latencies_ms": [1666.792], "images_per_second": 0.6, "prompt_tokens": 1111, "response_tokens_est": 22, "n_tiles": 1, "output_text": " A female tennis player is on a clay court, holding a tennis racket and wearing a white visor.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27606.1, "ram_available_mb": 98166.1, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 27615.1, "ram_available_mb": 98157.1, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.41, "peak": 40.16, "min": 15.38}, "VIN": {"avg": 73.79, "peak": 116.4, "min": 32.37}, "VIN_SYS_5V0": {"avg": 14.81, "peak": 16.05, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.94, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 30.41, "energy_joules_est": 50.71, "sample_count": 16, "duration_seconds": 1.668}, "timestamp": "2026-01-19T15:15:18.573542"}
{"image_index": 357, "image_name": "000000039480.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039480.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2167.075, "latencies_ms": [2167.075], "images_per_second": 0.461, "prompt_tokens": 1109, "response_tokens_est": 43, "n_tiles": 1, "output_text": " The image captures a woman in a vibrant red dress and white visor, holding a black and red tennis racket, on a clay tennis court. The lighting is natural, and the weather appears to be clear.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27615.1, "ram_available_mb": 98157.1, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 27633.5, "ram_available_mb": 98138.7, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.62, "peak": 40.97, "min": 20.49}, "VIN": {"avg": 72.24, "peak": 120.61, "min": 29.95}, "VIN_SYS_5V0": {"avg": 15.2, "peak": 16.26, "min": 13.39}, "VDD_CPU_SOC_MSS": {"avg": 16.38, "peak": 17.71, "min": 14.57}}, "power_watts_avg": 29.62, "energy_joules_est": 64.2, "sample_count": 21, "duration_seconds": 2.167}, "timestamp": "2026-01-19T15:15:20.758231"}
{"image_index": 358, "image_name": "000000039484.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039484.jpg", "image_width": 640, "image_height": 437, "image_resolution": "640x437", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2679.401, "latencies_ms": [2679.401], "images_per_second": 0.373, "prompt_tokens": 1099, "response_tokens_est": 63, "n_tiles": 1, "output_text": " The image captures a bustling city street lined with a variety of buildings, including a prominent red brick building with a sign that reads \"OmniFest\" and a green awning with the word \"HOTEL\" on it. The street is filled with cars and people, creating a lively urban atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27633.5, "ram_available_mb": 98138.7, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 27637.1, "ram_available_mb": 98135.1, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.14, "peak": 40.57, "min": 17.35}, "VIN": {"avg": 71.85, "peak": 115.33, "min": 29.74}, "VIN_SYS_5V0": {"avg": 15.11, "peak": 16.15, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 16.19, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 27.14, "energy_joules_est": 72.74, "sample_count": 26, "duration_seconds": 2.68}, "timestamp": "2026-01-19T15:15:23.470155"}
{"image_index": 358, "image_name": "000000039484.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039484.jpg", "image_width": 640, "image_height": 437, "image_resolution": "640x437", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2552.509, "latencies_ms": [2552.509], "images_per_second": 0.392, "prompt_tokens": 1113, "response_tokens_est": 57, "n_tiles": 1, "output_text": " 1. car: 3\n2. building: 10\n3. street: 1\n4. sign: 1\n5. person: 1\n6. airplane: 1\n7. store: 1\n8. road: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27637.1, "ram_available_mb": 98135.1, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 27551.7, "ram_available_mb": 98220.5, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.23, "peak": 40.56, "min": 16.55}, "VIN": {"avg": 70.84, "peak": 108.93, "min": 29.77}, "VIN_SYS_5V0": {"avg": 15.08, "peak": 16.15, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 16.35, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 27.23, "energy_joules_est": 69.51, "sample_count": 25, "duration_seconds": 2.553}, "timestamp": "2026-01-19T15:15:26.072345"}
{"image_index": 358, "image_name": "000000039484.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039484.jpg", "image_width": 640, "image_height": 437, "image_resolution": "640x437", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2324.612, "latencies_ms": [2324.612], "images_per_second": 0.43, "prompt_tokens": 1117, "response_tokens_est": 48, "n_tiles": 1, "output_text": " The main objects in the image are the cars on the road, the buildings, and the people. The cars are located in the foreground, near the buildings. The buildings are located in the background, with the people located near the buildings.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27551.7, "ram_available_mb": 98220.5, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 27562.2, "ram_available_mb": 98209.9, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.88, "peak": 40.18, "min": 16.16}, "VIN": {"avg": 67.64, "peak": 125.13, "min": 29.69}, "VIN_SYS_5V0": {"avg": 14.94, "peak": 16.05, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 16.13, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 27.88, "energy_joules_est": 64.82, "sample_count": 23, "duration_seconds": 2.325}, "timestamp": "2026-01-19T15:15:28.458890"}
{"image_index": 358, "image_name": "000000039484.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039484.jpg", "image_width": 640, "image_height": 437, "image_resolution": "640x437", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2124.719, "latencies_ms": [2124.719], "images_per_second": 0.471, "prompt_tokens": 1111, "response_tokens_est": 40, "n_tiles": 1, "output_text": " The image captures a bustling city street lined with various shops and restaurants. Cars are parked along the side of the road, and people can be seen walking on the sidewalk, adding to the lively atmosphere.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 27562.2, "ram_available_mb": 98209.9, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 27574.5, "ram_available_mb": 98197.7, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.46, "peak": 40.16, "min": 16.16}, "VIN": {"avg": 70.97, "peak": 123.38, "min": 29.15}, "VIN_SYS_5V0": {"avg": 14.92, "peak": 16.05, "min": 12.25}, "VDD_CPU_SOC_MSS": {"avg": 16.03, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 28.46, "energy_joules_est": 60.48, "sample_count": 21, "duration_seconds": 2.125}, "timestamp": "2026-01-19T15:15:30.650756"}
{"image_index": 358, "image_name": "000000039484.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039484.jpg", "image_width": 640, "image_height": 437, "image_resolution": "640x437", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2741.435, "latencies_ms": [2741.435], "images_per_second": 0.365, "prompt_tokens": 1109, "response_tokens_est": 64, "n_tiles": 1, "output_text": " The image captures a vibrant city street bathed in the soft glow of daylight, with the sky painted in a light blue canvas. The buildings, adorned with a variety of colors, stand tall against the backdrop of the overcast sky. The street is bustling with life, filled with cars and people going about their day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27574.5, "ram_available_mb": 98197.7, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 27574.8, "ram_available_mb": 98197.4, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.66, "peak": 40.18, "min": 16.56}, "VIN": {"avg": 64.93, "peak": 124.32, "min": 28.64}, "VIN_SYS_5V0": {"avg": 14.91, "peak": 16.05, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.91, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 26.66, "energy_joules_est": 73.1, "sample_count": 27, "duration_seconds": 2.742}, "timestamp": "2026-01-19T15:15:33.455279"}
{"image_index": 359, "image_name": "000000039551.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039551.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1681.872, "latencies_ms": [1681.872], "images_per_second": 0.595, "prompt_tokens": 1099, "response_tokens_est": 23, "n_tiles": 1, "output_text": " A female tennis player in a pink outfit is playing on a blue court with a yellow tennis ball in the air.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 27574.8, "ram_available_mb": 98197.4, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 27588.2, "ram_available_mb": 98184.0, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.94, "peak": 40.16, "min": 15.77}, "VIN": {"avg": 76.14, "peak": 122.23, "min": 27.11}, "VIN_SYS_5V0": {"avg": 14.86, "peak": 16.05, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.93, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 29.94, "energy_joules_est": 50.37, "sample_count": 17, "duration_seconds": 1.682}, "timestamp": "2026-01-19T15:15:35.238511"}
{"image_index": 359, "image_name": "000000039551.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039551.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2757.776, "latencies_ms": [2757.776], "images_per_second": 0.363, "prompt_tokens": 1113, "response_tokens_est": 64, "n_tiles": 1, "output_text": " 1. tennis racket: 1\n2. tennis ball: 1\n3. woman: 1\n4. tennis court: 1\n5. white line: 1\n6. blue surface: 1\n7. green surface: 1\n8. white shoes: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27588.2, "ram_available_mb": 98184.0, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 27600.5, "ram_available_mb": 98171.7, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.73, "peak": 40.16, "min": 17.35}, "VIN": {"avg": 67.88, "peak": 106.39, "min": 28.79}, "VIN_SYS_5V0": {"avg": 14.91, "peak": 16.05, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.97, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 26.73, "energy_joules_est": 73.72, "sample_count": 27, "duration_seconds": 2.758}, "timestamp": "2026-01-19T15:15:38.049294"}
{"image_index": 359, "image_name": "000000039551.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039551.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2070.321, "latencies_ms": [2070.321], "images_per_second": 0.483, "prompt_tokens": 1117, "response_tokens_est": 38, "n_tiles": 1, "output_text": " The tennis player is positioned on the left side of the image, with the tennis ball located in the center. The player is in the foreground, while the tennis court extends into the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27600.5, "ram_available_mb": 98171.7, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 27602.0, "ram_available_mb": 98170.2, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.9, "peak": 40.18, "min": 15.76}, "VIN": {"avg": 60.7, "peak": 103.21, "min": 30.25}, "VIN_SYS_5V0": {"avg": 15.02, "peak": 16.15, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 16.2, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 28.9, "energy_joules_est": 59.84, "sample_count": 20, "duration_seconds": 2.071}, "timestamp": "2026-01-19T15:15:40.131503"}
{"image_index": 359, "image_name": "000000039551.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039551.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1460.672, "latencies_ms": [1460.672], "images_per_second": 0.685, "prompt_tokens": 1111, "response_tokens_est": 15, "n_tiles": 1, "output_text": " A female tennis player is playing on a blue court with a green background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27602.0, "ram_available_mb": 98170.2, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 27615.0, "ram_available_mb": 98157.1, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.8, "peak": 40.56, "min": 18.13}, "VIN": {"avg": 77.98, "peak": 122.34, "min": 32.11}, "VIN_SYS_5V0": {"avg": 15.2, "peak": 16.15, "min": 13.29}, "VDD_CPU_SOC_MSS": {"avg": 16.38, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 31.8, "energy_joules_est": 46.46, "sample_count": 15, "duration_seconds": 1.461}, "timestamp": "2026-01-19T15:15:41.697552"}
{"image_index": 359, "image_name": "000000039551.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039551.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1645.921, "latencies_ms": [1645.921], "images_per_second": 0.608, "prompt_tokens": 1109, "response_tokens_est": 21, "n_tiles": 1, "output_text": " The tennis player is wearing a red outfit and white shoes, and the court is blue with white lines.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 27615.0, "ram_available_mb": 98157.1, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 27618.5, "ram_available_mb": 98153.7, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.34, "peak": 40.56, "min": 19.32}, "VIN": {"avg": 70.56, "peak": 103.7, "min": 31.5}, "VIN_SYS_5V0": {"avg": 14.74, "peak": 16.05, "min": 11.87}, "VDD_CPU_SOC_MSS": {"avg": 15.87, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 31.34, "energy_joules_est": 51.59, "sample_count": 16, "duration_seconds": 1.646}, "timestamp": "2026-01-19T15:15:43.374967"}
{"image_index": 360, "image_name": "000000039670.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039670.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1978.599, "latencies_ms": [1978.599], "images_per_second": 0.505, "prompt_tokens": 1432, "response_tokens_est": 20, "n_tiles": 1, "output_text": " A blue and orange train with a white front is traveling on a track through a lush green forest.", "error": null, "sys_before": {"cpu_percent": 6.7, "ram_used_mb": 27618.5, "ram_available_mb": 98153.7, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 27632.7, "ram_available_mb": 98139.5, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.2, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.86, "peak": 40.56, "min": 19.7}, "VIN": {"avg": 77.54, "peak": 120.16, "min": 34.67}, "VIN_SYS_5V0": {"avg": 15.6, "peak": 16.76, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 16.8, "peak": 18.11, "min": 14.18}}, "power_watts_avg": 32.86, "energy_joules_est": 65.03, "sample_count": 19, "duration_seconds": 1.979}, "timestamp": "2026-01-19T15:15:45.366379"}
{"image_index": 360, "image_name": "000000039670.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039670.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2327.714, "latencies_ms": [2327.714], "images_per_second": 0.43, "prompt_tokens": 1446, "response_tokens_est": 34, "n_tiles": 1, "output_text": " train: 1, tracks: 1, bushes: 1, trees: 1, windows: 1, doors: 1, front: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27632.7, "ram_available_mb": 98139.5, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 27644.8, "ram_available_mb": 98127.4, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.56, "peak": 40.95, "min": 20.1}, "VIN": {"avg": 74.67, "peak": 125.75, "min": 28.95}, "VIN_SYS_5V0": {"avg": 15.52, "peak": 16.76, "min": 13.49}, "VDD_CPU_SOC_MSS": {"avg": 16.71, "peak": 18.11, "min": 14.96}}, "power_watts_avg": 31.56, "energy_joules_est": 73.47, "sample_count": 23, "duration_seconds": 2.328}, "timestamp": "2026-01-19T15:15:47.759250"}
{"image_index": 360, "image_name": "000000039670.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039670.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2335.554, "latencies_ms": [2335.554], "images_per_second": 0.428, "prompt_tokens": 1450, "response_tokens_est": 33, "n_tiles": 1, "output_text": " The train is positioned on the left side of the image, moving towards the right. The train is in the foreground, with the background consisting of trees and bushes.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27644.8, "ram_available_mb": 98127.4, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 27647.5, "ram_available_mb": 98124.7, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.76, "peak": 40.56, "min": 17.34}, "VIN": {"avg": 72.68, "peak": 129.5, "min": 30.06}, "VIN_SYS_5V0": {"avg": 15.19, "peak": 16.56, "min": 12.25}, "VDD_CPU_SOC_MSS": {"avg": 16.29, "peak": 17.71, "min": 13.39}}, "power_watts_avg": 30.76, "energy_joules_est": 71.86, "sample_count": 23, "duration_seconds": 2.336}, "timestamp": "2026-01-19T15:15:50.152128"}
{"image_index": 360, "image_name": "000000039670.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039670.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1850.22, "latencies_ms": [1850.22], "images_per_second": 0.54, "prompt_tokens": 1444, "response_tokens_est": 15, "n_tiles": 1, "output_text": " A blue and orange train is traveling on a track through a wooded area.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27647.5, "ram_available_mb": 98124.7, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 27559.5, "ram_available_mb": 98212.7, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.93, "peak": 40.18, "min": 17.35}, "VIN": {"avg": 77.02, "peak": 119.9, "min": 29.48}, "VIN_SYS_5V0": {"avg": 15.39, "peak": 16.66, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 16.69, "peak": 18.11, "min": 13.39}}, "power_watts_avg": 32.93, "energy_joules_est": 60.94, "sample_count": 18, "duration_seconds": 1.85}, "timestamp": "2026-01-19T15:15:52.031813"}
{"image_index": 360, "image_name": "000000039670.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039670.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1783.657, "latencies_ms": [1783.657], "images_per_second": 0.561, "prompt_tokens": 1442, "response_tokens_est": 13, "n_tiles": 1, "output_text": " The train is blue and orange, and the weather is sunny.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 27559.5, "ram_available_mb": 98212.7, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 27565.0, "ram_available_mb": 98207.2, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 33.89, "peak": 40.56, "min": 21.28}, "VIN": {"avg": 77.85, "peak": 121.31, "min": 29.29}, "VIN_SYS_5V0": {"avg": 15.49, "peak": 16.76, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 16.73, "peak": 18.11, "min": 14.57}}, "power_watts_avg": 33.89, "energy_joules_est": 60.46, "sample_count": 18, "duration_seconds": 1.784}, "timestamp": "2026-01-19T15:15:53.908582"}
{"image_index": 361, "image_name": "000000039769.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039769.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1555.54, "latencies_ms": [1555.54], "images_per_second": 0.643, "prompt_tokens": 1099, "response_tokens_est": 18, "n_tiles": 1, "output_text": " Two cats are sleeping on a pink blanket, with a remote control on the left cat.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 27565.0, "ram_available_mb": 98207.2, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 27565.3, "ram_available_mb": 98206.9, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.25, "peak": 40.95, "min": 20.11}, "VIN": {"avg": 76.36, "peak": 127.97, "min": 31.33}, "VIN_SYS_5V0": {"avg": 14.84, "peak": 16.05, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.9, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 32.25, "energy_joules_est": 50.19, "sample_count": 15, "duration_seconds": 1.556}, "timestamp": "2026-01-19T15:15:55.480307"}
{"image_index": 361, "image_name": "000000039769.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039769.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2488.977, "latencies_ms": [2488.977], "images_per_second": 0.402, "prompt_tokens": 1113, "response_tokens_est": 56, "n_tiles": 1, "output_text": " 1. cat: 2\n2. remote: 2\n3. couch: 1\n4. blanket: 1\n5. tail: 1\n6. paw: 1\n7. head: 1\n8. eye: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27565.3, "ram_available_mb": 98206.9, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 27575.4, "ram_available_mb": 98196.8, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.78, "peak": 40.56, "min": 20.1}, "VIN": {"avg": 66.44, "peak": 95.25, "min": 30.05}, "VIN_SYS_5V0": {"avg": 15.3, "peak": 16.26, "min": 13.39}, "VDD_CPU_SOC_MSS": {"avg": 16.34, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 28.78, "energy_joules_est": 71.64, "sample_count": 24, "duration_seconds": 2.489}, "timestamp": "2026-01-19T15:15:57.980838"}
{"image_index": 361, "image_name": "000000039769.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039769.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2552.007, "latencies_ms": [2552.007], "images_per_second": 0.392, "prompt_tokens": 1117, "response_tokens_est": 57, "n_tiles": 1, "output_text": " The two cats are lying on a pink blanket, with the cat on the left being closer to the camera and the cat on the right being farther away. The remote control is placed near the cat on the left, while the other remote control is located near the cat on the right.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27575.4, "ram_available_mb": 98196.8, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 27583.6, "ram_available_mb": 98188.5, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.51, "peak": 40.16, "min": 17.74}, "VIN": {"avg": 67.79, "peak": 103.66, "min": 29.09}, "VIN_SYS_5V0": {"avg": 15.21, "peak": 16.26, "min": 13.29}, "VDD_CPU_SOC_MSS": {"avg": 16.17, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 27.51, "energy_joules_est": 70.22, "sample_count": 25, "duration_seconds": 2.552}, "timestamp": "2026-01-19T15:16:00.587410"}
{"image_index": 361, "image_name": "000000039769.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039769.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1463.488, "latencies_ms": [1463.488], "images_per_second": 0.683, "prompt_tokens": 1111, "response_tokens_est": 15, "n_tiles": 1, "output_text": " Two cats are sleeping on a pink blanket, with a remote control nearby.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27583.6, "ram_available_mb": 98188.5, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 27595.5, "ram_available_mb": 98176.7, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.57, "peak": 40.56, "min": 15.77}, "VIN": {"avg": 75.97, "peak": 107.02, "min": 36.66}, "VIN_SYS_5V0": {"avg": 15.04, "peak": 16.05, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 16.11, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 31.57, "energy_joules_est": 46.21, "sample_count": 14, "duration_seconds": 1.464}, "timestamp": "2026-01-19T15:16:02.053925"}
{"image_index": 361, "image_name": "000000039769.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039769.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1819.037, "latencies_ms": [1819.037], "images_per_second": 0.55, "prompt_tokens": 1109, "response_tokens_est": 30, "n_tiles": 1, "output_text": " The image features two cats sleeping on a pink blanket, with a remote control nearby. The lighting is natural, and the weather appears to be clear.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27595.5, "ram_available_mb": 98176.7, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 27611.1, "ram_available_mb": 98161.1, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.65, "peak": 40.57, "min": 20.89}, "VIN": {"avg": 69.34, "peak": 104.66, "min": 29.32}, "VIN_SYS_5V0": {"avg": 15.32, "peak": 16.26, "min": 13.59}, "VDD_CPU_SOC_MSS": {"avg": 16.4, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 31.65, "energy_joules_est": 57.58, "sample_count": 18, "duration_seconds": 1.819}, "timestamp": "2026-01-19T15:16:03.934996"}
{"image_index": 362, "image_name": "000000039785.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039785.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1713.008, "latencies_ms": [1713.008], "images_per_second": 0.584, "prompt_tokens": 1099, "response_tokens_est": 25, "n_tiles": 1, "output_text": " A person is surfing on a river with a blue surfboard, while another person is standing on a bridge in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27611.1, "ram_available_mb": 98161.1, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 27619.4, "ram_available_mb": 98152.8, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.7, "peak": 40.56, "min": 17.74}, "VIN": {"avg": 71.63, "peak": 105.55, "min": 30.53}, "VIN_SYS_5V0": {"avg": 14.99, "peak": 16.05, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 16.0, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 30.7, "energy_joules_est": 52.61, "sample_count": 17, "duration_seconds": 1.714}, "timestamp": "2026-01-19T15:16:05.707536"}
{"image_index": 362, "image_name": "000000039785.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039785.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2593.927, "latencies_ms": [2593.927], "images_per_second": 0.386, "prompt_tokens": 1113, "response_tokens_est": 59, "n_tiles": 1, "output_text": " 1. Surfer: 1\n2. Surfboard: 1\n3. River: 1\n4. Bridge: 1\n5. Tree: 1\n6. Bench: 1\n7. Wall: 1\n8. Bench: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27619.4, "ram_available_mb": 98152.8, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 27627.6, "ram_available_mb": 98144.5, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.66, "peak": 40.56, "min": 18.53}, "VIN": {"avg": 67.91, "peak": 121.35, "min": 31.39}, "VIN_SYS_5V0": {"avg": 15.1, "peak": 16.15, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 16.01, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 27.66, "energy_joules_est": 71.76, "sample_count": 25, "duration_seconds": 2.594}, "timestamp": "2026-01-19T15:16:08.317304"}
{"image_index": 362, "image_name": "000000039785.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039785.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2016.241, "latencies_ms": [2016.241], "images_per_second": 0.496, "prompt_tokens": 1117, "response_tokens_est": 37, "n_tiles": 1, "output_text": " The surfer is positioned in the foreground, riding a wave in the river, while the bridge is located in the background. The surfer is closer to the camera than the bridge.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27627.6, "ram_available_mb": 98144.5, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 27633.6, "ram_available_mb": 98138.5, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.29, "peak": 40.56, "min": 17.34}, "VIN": {"avg": 70.61, "peak": 121.39, "min": 29.65}, "VIN_SYS_5V0": {"avg": 15.22, "peak": 16.26, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 16.24, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 29.29, "energy_joules_est": 59.06, "sample_count": 20, "duration_seconds": 2.017}, "timestamp": "2026-01-19T15:16:10.394670"}
{"image_index": 362, "image_name": "000000039785.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039785.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1799.87, "latencies_ms": [1799.87], "images_per_second": 0.556, "prompt_tokens": 1111, "response_tokens_est": 28, "n_tiles": 1, "output_text": " A man in a wetsuit is surfing on a river with a blue surfboard. The river is surrounded by trees and a bridge.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 27633.6, "ram_available_mb": 98138.5, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 27637.1, "ram_available_mb": 98135.1, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.96, "peak": 40.56, "min": 16.95}, "VIN": {"avg": 73.42, "peak": 106.32, "min": 30.37}, "VIN_SYS_5V0": {"avg": 14.97, "peak": 16.15, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.94, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 29.96, "energy_joules_est": 53.93, "sample_count": 18, "duration_seconds": 1.8}, "timestamp": "2026-01-19T15:16:12.272115"}
{"image_index": 362, "image_name": "000000039785.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039785.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2496.442, "latencies_ms": [2496.442], "images_per_second": 0.401, "prompt_tokens": 1109, "response_tokens_est": 55, "n_tiles": 1, "output_text": " The image features a surfer in a black wetsuit riding a wave in a river, with a blue surfboard visible in the background. The lighting is natural, and the water appears to be a muddy brown color, suggesting it might be a river or a lake.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27637.1, "ram_available_mb": 98135.1, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 27645.3, "ram_available_mb": 98126.9, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.41, "peak": 40.56, "min": 17.34}, "VIN": {"avg": 71.08, "peak": 120.64, "min": 27.06}, "VIN_SYS_5V0": {"avg": 14.87, "peak": 16.05, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 15.87, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 27.41, "energy_joules_est": 68.44, "sample_count": 25, "duration_seconds": 2.497}, "timestamp": "2026-01-19T15:16:14.874340"}
{"image_index": 363, "image_name": "000000039914.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039914.jpg", "image_width": 487, "image_height": 640, "image_resolution": "487x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1959.739, "latencies_ms": [1959.739], "images_per_second": 0.51, "prompt_tokens": 1432, "response_tokens_est": 18, "n_tiles": 1, "output_text": " A woman and a child are flying a kite in a park with trees in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27645.3, "ram_available_mb": 98126.9, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 27656.4, "ram_available_mb": 98115.8, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.2, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.45, "peak": 39.39, "min": 14.98}, "VIN": {"avg": 77.12, "peak": 114.51, "min": 30.22}, "VIN_SYS_5V0": {"avg": 15.18, "peak": 16.76, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 16.33, "peak": 18.11, "min": 12.61}}, "power_watts_avg": 31.45, "energy_joules_est": 61.66, "sample_count": 19, "duration_seconds": 1.961}, "timestamp": "2026-01-19T15:16:16.859489"}
{"image_index": 363, "image_name": "000000039914.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039914.jpg", "image_width": 487, "image_height": 640, "image_resolution": "487x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3039.069, "latencies_ms": [3039.069], "images_per_second": 0.329, "prompt_tokens": 1446, "response_tokens_est": 60, "n_tiles": 1, "output_text": " 1. person: 2\n2. child: 1\n3. kite: 1\n4. grass: 1\n5. trees: 1\n6. sky: 1\n7. person's hand: 1\n8. person's leg: 1", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 27656.4, "ram_available_mb": 98115.8, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 27668.2, "ram_available_mb": 98104.0, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.94, "peak": 40.57, "min": 18.13}, "VIN": {"avg": 71.52, "peak": 128.2, "min": 28.06}, "VIN_SYS_5V0": {"avg": 15.31, "peak": 16.76, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 16.34, "peak": 17.71, "min": 14.18}}, "power_watts_avg": 28.94, "energy_joules_est": 87.96, "sample_count": 30, "duration_seconds": 3.039}, "timestamp": "2026-01-19T15:16:19.980954"}
{"image_index": 363, "image_name": "000000039914.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039914.jpg", "image_width": 487, "image_height": 640, "image_resolution": "487x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2752.79, "latencies_ms": [2752.79], "images_per_second": 0.363, "prompt_tokens": 1450, "response_tokens_est": 48, "n_tiles": 1, "output_text": " The kite is in the foreground, flying high in the sky, while the person is in the background, standing on the grass. The person is holding the kite string, which is in the foreground, and the kite is in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27668.2, "ram_available_mb": 98104.0, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 27676.8, "ram_available_mb": 98095.4, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.96, "peak": 40.18, "min": 15.77}, "VIN": {"avg": 72.06, "peak": 115.3, "min": 30.59}, "VIN_SYS_5V0": {"avg": 15.11, "peak": 16.56, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 16.16, "peak": 17.71, "min": 13.0}}, "power_watts_avg": 28.96, "energy_joules_est": 79.73, "sample_count": 27, "duration_seconds": 2.753}, "timestamp": "2026-01-19T15:16:22.795218"}
{"image_index": 363, "image_name": "000000039914.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039914.jpg", "image_width": 487, "image_height": 640, "image_resolution": "487x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1930.026, "latencies_ms": [1930.026], "images_per_second": 0.518, "prompt_tokens": 1444, "response_tokens_est": 18, "n_tiles": 1, "output_text": " A woman and a child are flying a kite in a park with trees in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27676.8, "ram_available_mb": 98095.4, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 27690.6, "ram_available_mb": 98081.6, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.14, "peak": 40.16, "min": 16.56}, "VIN": {"avg": 76.99, "peak": 124.25, "min": 29.33}, "VIN_SYS_5V0": {"avg": 15.25, "peak": 16.56, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 16.45, "peak": 17.71, "min": 13.39}}, "power_watts_avg": 32.14, "energy_joules_est": 62.05, "sample_count": 19, "duration_seconds": 1.93}, "timestamp": "2026-01-19T15:16:24.772561"}
{"image_index": 363, "image_name": "000000039914.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039914.jpg", "image_width": 487, "image_height": 640, "image_resolution": "487x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3271.989, "latencies_ms": [3271.989], "images_per_second": 0.306, "prompt_tokens": 1442, "response_tokens_est": 68, "n_tiles": 1, "output_text": " The image features a vibrant kite with a mix of colors, including pink, blue, and yellow, soaring in the sky. The kite is being flown by a person wearing a black jacket and blue jeans, while a child stands nearby, also wearing a black shirt and pink pants. The sky is clear and blue, indicating a sunny day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27690.6, "ram_available_mb": 98081.6, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 27697.6, "ram_available_mb": 98074.6, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.36, "peak": 40.97, "min": 18.91}, "VIN": {"avg": 71.23, "peak": 124.16, "min": 29.79}, "VIN_SYS_5V0": {"avg": 15.2, "peak": 16.66, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 16.29, "peak": 17.71, "min": 13.79}}, "power_watts_avg": 28.36, "energy_joules_est": 92.81, "sample_count": 32, "duration_seconds": 3.272}, "timestamp": "2026-01-19T15:16:28.093691"}
{"image_index": 364, "image_name": "000000039951.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039951.jpg", "image_width": 640, "image_height": 445, "image_resolution": "640x445", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1530.855, "latencies_ms": [1530.855], "images_per_second": 0.653, "prompt_tokens": 1099, "response_tokens_est": 17, "n_tiles": 1, "output_text": " A young boy is playing tennis on a court with a green fence in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27697.6, "ram_available_mb": 98074.6, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 6.6, "ram_used_mb": 27575.2, "ram_available_mb": 98197.0, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.15, "peak": 40.16, "min": 16.16}, "VIN": {"avg": 81.81, "peak": 126.67, "min": 29.08}, "VIN_SYS_5V0": {"avg": 15.01, "peak": 16.15, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 16.27, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 31.15, "energy_joules_est": 47.7, "sample_count": 15, "duration_seconds": 1.531}, "timestamp": "2026-01-19T15:16:29.655482"}
{"image_index": 364, "image_name": "000000039951.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039951.jpg", "image_width": 640, "image_height": 445, "image_resolution": "640x445", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2598.164, "latencies_ms": [2598.164], "images_per_second": 0.385, "prompt_tokens": 1113, "response_tokens_est": 59, "n_tiles": 1, "output_text": " 1. person: 1\n2. tennis racket: 1\n3. ball: 1\n4. tennis court: 1\n5. banner: 1\n6. fence: 1\n7. net: 1\n8. text: 2", "error": null, "sys_before": {"cpu_percent": 50.0, "ram_used_mb": 27575.2, "ram_available_mb": 98197.0, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 27591.7, "ram_available_mb": 98180.5, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.81, "peak": 40.95, "min": 17.35}, "VIN": {"avg": 69.18, "peak": 97.3, "min": 28.62}, "VIN_SYS_5V0": {"avg": 15.04, "peak": 16.15, "min": 12.96}, "VDD_CPU_SOC_MSS": {"avg": 16.22, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 27.81, "energy_joules_est": 72.26, "sample_count": 26, "duration_seconds": 2.598}, "timestamp": "2026-01-19T15:16:32.358966"}
{"image_index": 364, "image_name": "000000039951.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039951.jpg", "image_width": 640, "image_height": 445, "image_resolution": "640x445", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2110.396, "latencies_ms": [2110.396], "images_per_second": 0.474, "prompt_tokens": 1117, "response_tokens_est": 39, "n_tiles": 1, "output_text": " The tennis player is in the foreground, holding a yellow tennis racket and preparing to hit the ball. The green fence is in the background, and the blue court is in the middle ground.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 27591.7, "ram_available_mb": 98180.5, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 27596.6, "ram_available_mb": 98175.5, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.17, "peak": 39.77, "min": 14.98}, "VIN": {"avg": 62.75, "peak": 86.01, "min": 28.99}, "VIN_SYS_5V0": {"avg": 14.71, "peak": 15.95, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 15.86, "peak": 17.32, "min": 12.61}}, "power_watts_avg": 28.17, "energy_joules_est": 59.46, "sample_count": 21, "duration_seconds": 2.111}, "timestamp": "2026-01-19T15:16:34.534644"}
{"image_index": 364, "image_name": "000000039951.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039951.jpg", "image_width": 640, "image_height": 445, "image_resolution": "640x445", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1554.31, "latencies_ms": [1554.31], "images_per_second": 0.643, "prompt_tokens": 1111, "response_tokens_est": 17, "n_tiles": 1, "output_text": " A young boy is playing tennis on a court with a green fence in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27596.6, "ram_available_mb": 98175.5, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 27602.3, "ram_available_mb": 98169.8, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.15, "peak": 40.18, "min": 16.55}, "VIN": {"avg": 72.57, "peak": 121.51, "min": 32.79}, "VIN_SYS_5V0": {"avg": 14.89, "peak": 16.05, "min": 12.25}, "VDD_CPU_SOC_MSS": {"avg": 16.01, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 31.15, "energy_joules_est": 48.44, "sample_count": 15, "duration_seconds": 1.555}, "timestamp": "2026-01-19T15:16:36.098105"}
{"image_index": 364, "image_name": "000000039951.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039951.jpg", "image_width": 640, "image_height": 445, "image_resolution": "640x445", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2798.737, "latencies_ms": [2798.737], "images_per_second": 0.357, "prompt_tokens": 1109, "response_tokens_est": 67, "n_tiles": 1, "output_text": " The image features a young boy playing tennis on a green court with a blue surface. The boy is wearing a white shirt and black shorts, and he is holding a yellow tennis racket. The background of the image is a green curtain, and there is a sign on the left side of the image that reads \"Are you next?\"", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27602.3, "ram_available_mb": 98169.8, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 27611.3, "ram_available_mb": 98160.9, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.48, "peak": 40.95, "min": 19.31}, "VIN": {"avg": 70.5, "peak": 123.65, "min": 29.28}, "VIN_SYS_5V0": {"avg": 15.12, "peak": 16.15, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 16.14, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 27.48, "energy_joules_est": 76.92, "sample_count": 27, "duration_seconds": 2.799}, "timestamp": "2026-01-19T15:16:38.911988"}
{"image_index": 365, "image_name": "000000039956.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039956.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1997.554, "latencies_ms": [1997.554], "images_per_second": 0.501, "prompt_tokens": 1099, "response_tokens_est": 36, "n_tiles": 1, "output_text": " The image depicts a cluttered bedroom with a bed covered in a brown blanket, a chair, and a desk with various items on it, including a lamp and a computer monitor.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 27611.3, "ram_available_mb": 98160.9, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 27628.1, "ram_available_mb": 98144.0, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.05, "peak": 40.57, "min": 16.55}, "VIN": {"avg": 71.57, "peak": 104.57, "min": 28.7}, "VIN_SYS_5V0": {"avg": 15.11, "peak": 16.15, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 16.22, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 29.05, "energy_joules_est": 58.04, "sample_count": 20, "duration_seconds": 1.998}, "timestamp": "2026-01-19T15:16:40.993903"}
{"image_index": 365, "image_name": "000000039956.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039956.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1984.928, "latencies_ms": [1984.928], "images_per_second": 0.504, "prompt_tokens": 1113, "response_tokens_est": 34, "n_tiles": 1, "output_text": " bed: 1, chair: 1, desk: 1, door: 1, box: 1, blanket: 1, pillow: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27628.1, "ram_available_mb": 98144.0, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 27631.8, "ram_available_mb": 98140.4, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.82, "peak": 40.18, "min": 16.56}, "VIN": {"avg": 72.26, "peak": 120.05, "min": 30.02}, "VIN_SYS_5V0": {"avg": 14.82, "peak": 16.05, "min": 12.15}, "VDD_CPU_SOC_MSS": {"avg": 15.93, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 28.82, "energy_joules_est": 57.22, "sample_count": 20, "duration_seconds": 1.985}, "timestamp": "2026-01-19T15:16:43.080283"}
{"image_index": 365, "image_name": "000000039956.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039956.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2183.923, "latencies_ms": [2183.923], "images_per_second": 0.458, "prompt_tokens": 1117, "response_tokens_est": 42, "n_tiles": 1, "output_text": " The bed is located on the left side of the room, with the desk and chair situated on the right side. The desk is positioned in the foreground, while the chair is located further back in the room.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27631.8, "ram_available_mb": 98140.4, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 27633.8, "ram_available_mb": 98138.4, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.52, "peak": 40.18, "min": 16.16}, "VIN": {"avg": 70.83, "peak": 131.93, "min": 29.47}, "VIN_SYS_5V0": {"avg": 14.83, "peak": 16.05, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.95, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 28.52, "energy_joules_est": 62.29, "sample_count": 21, "duration_seconds": 2.184}, "timestamp": "2026-01-19T15:16:45.274643"}
{"image_index": 365, "image_name": "000000039956.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039956.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1737.356, "latencies_ms": [1737.356], "images_per_second": 0.576, "prompt_tokens": 1111, "response_tokens_est": 26, "n_tiles": 1, "output_text": " The image depicts a bedroom with a bed covered in a brown blanket, a chair, and a desk with a computer on it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27633.8, "ram_available_mb": 98138.4, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 27642.5, "ram_available_mb": 98129.6, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.68, "peak": 40.57, "min": 17.73}, "VIN": {"avg": 76.5, "peak": 131.34, "min": 30.39}, "VIN_SYS_5V0": {"avg": 15.19, "peak": 16.15, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 16.42, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 30.68, "energy_joules_est": 53.32, "sample_count": 17, "duration_seconds": 1.738}, "timestamp": "2026-01-19T15:16:47.057593"}
{"image_index": 365, "image_name": "000000039956.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039956.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1546.612, "latencies_ms": [1546.612], "images_per_second": 0.647, "prompt_tokens": 1109, "response_tokens_est": 18, "n_tiles": 1, "output_text": " The room is lit by a single light bulb, and the walls are made of brick.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27642.5, "ram_available_mb": 98129.6, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 27644.8, "ram_available_mb": 98127.4, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.96, "peak": 40.56, "min": 18.52}, "VIN": {"avg": 69.26, "peak": 84.97, "min": 29.19}, "VIN_SYS_5V0": {"avg": 15.05, "peak": 16.15, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 16.22, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 31.96, "energy_joules_est": 49.44, "sample_count": 15, "duration_seconds": 1.547}, "timestamp": "2026-01-19T15:16:48.624317"}
{"image_index": 366, "image_name": "000000040036.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000040036.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1746.066, "latencies_ms": [1746.066], "images_per_second": 0.573, "prompt_tokens": 1099, "response_tokens_est": 26, "n_tiles": 1, "output_text": " A woman in a red and green outfit is riding a brown horse over a jump obstacle in a field with trees in the background.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 27644.8, "ram_available_mb": 98127.4, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 27670.2, "ram_available_mb": 98102.0, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.57, "peak": 40.97, "min": 20.89}, "VIN": {"avg": 72.08, "peak": 104.69, "min": 29.61}, "VIN_SYS_5V0": {"avg": 15.17, "peak": 16.15, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 16.3, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 31.57, "energy_joules_est": 55.14, "sample_count": 17, "duration_seconds": 1.747}, "timestamp": "2026-01-19T15:16:50.396891"}
{"image_index": 366, "image_name": "000000040036.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000040036.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2180.801, "latencies_ms": [2180.801], "images_per_second": 0.459, "prompt_tokens": 1113, "response_tokens_est": 42, "n_tiles": 1, "output_text": " horse: 1, rider: 1, saddle: 1, bridle: 1, leg protection: 2, leg band: 1, helmet: 1, number: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27670.2, "ram_available_mb": 98102.0, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 27669.3, "ram_available_mb": 98102.9, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.15, "peak": 40.56, "min": 18.92}, "VIN": {"avg": 60.78, "peak": 104.19, "min": 30.04}, "VIN_SYS_5V0": {"avg": 15.11, "peak": 16.15, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 16.16, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 29.15, "energy_joules_est": 63.58, "sample_count": 21, "duration_seconds": 2.181}, "timestamp": "2026-01-19T15:16:52.587205"}
{"image_index": 366, "image_name": "000000040036.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000040036.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2328.596, "latencies_ms": [2328.596], "images_per_second": 0.429, "prompt_tokens": 1117, "response_tokens_est": 48, "n_tiles": 1, "output_text": " The horse is in the foreground, jumping over the obstacle, while the rider is in the background, wearing a red and green jacket. The obstacle is located in the middle ground, and the rider is positioned to the left of the horse.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27669.3, "ram_available_mb": 98102.9, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 27683.9, "ram_available_mb": 98088.2, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.16, "peak": 40.57, "min": 17.35}, "VIN": {"avg": 67.33, "peak": 107.38, "min": 28.83}, "VIN_SYS_5V0": {"avg": 15.14, "peak": 16.15, "min": 13.29}, "VDD_CPU_SOC_MSS": {"avg": 16.26, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 28.16, "energy_joules_est": 65.58, "sample_count": 23, "duration_seconds": 2.329}, "timestamp": "2026-01-19T15:16:54.966583"}
{"image_index": 366, "image_name": "000000040036.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000040036.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1628.336, "latencies_ms": [1628.336], "images_per_second": 0.614, "prompt_tokens": 1111, "response_tokens_est": 21, "n_tiles": 1, "output_text": " A woman in a red and green outfit is riding a brown horse over a jump obstacle in a field.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27683.9, "ram_available_mb": 98088.2, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 27688.3, "ram_available_mb": 98083.9, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.8, "peak": 40.18, "min": 16.16}, "VIN": {"avg": 76.87, "peak": 122.01, "min": 29.6}, "VIN_SYS_5V0": {"avg": 14.99, "peak": 16.05, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 16.14, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 30.8, "energy_joules_est": 50.18, "sample_count": 16, "duration_seconds": 1.629}, "timestamp": "2026-01-19T15:16:56.626812"}
{"image_index": 366, "image_name": "000000040036.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000040036.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2697.988, "latencies_ms": [2697.988], "images_per_second": 0.371, "prompt_tokens": 1109, "response_tokens_est": 63, "n_tiles": 1, "output_text": " The image features a horse and rider in a dynamic pose, with the horse's coat a rich brown and the rider's attire a vibrant mix of red, green, and white. The scene is bathed in natural light, casting soft shadows and highlighting the textures of the horse's coat and the rider's gear.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27688.3, "ram_available_mb": 98083.9, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 27699.3, "ram_available_mb": 98072.8, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.59, "peak": 40.57, "min": 19.7}, "VIN": {"avg": 70.51, "peak": 94.82, "min": 34.3}, "VIN_SYS_5V0": {"avg": 15.12, "peak": 16.15, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 16.25, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 27.59, "energy_joules_est": 74.46, "sample_count": 26, "duration_seconds": 2.699}, "timestamp": "2026-01-19T15:16:59.330802"}
{"image_index": 367, "image_name": "000000040083.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000040083.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1708.274, "latencies_ms": [1708.274], "images_per_second": 0.585, "prompt_tokens": 1099, "response_tokens_est": 24, "n_tiles": 1, "output_text": " Three men are sitting under a large umbrella with Chinese writing on it, with a car and a bicycle parked behind them.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27699.3, "ram_available_mb": 98072.8, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 27716.5, "ram_available_mb": 98055.6, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.52, "peak": 40.56, "min": 17.34}, "VIN": {"avg": 79.78, "peak": 125.02, "min": 29.11}, "VIN_SYS_5V0": {"avg": 15.13, "peak": 16.15, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 16.35, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 30.52, "energy_joules_est": 52.15, "sample_count": 17, "duration_seconds": 1.709}, "timestamp": "2026-01-19T15:17:01.103209"}
{"image_index": 367, "image_name": "000000040083.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000040083.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2538.197, "latencies_ms": [2538.197], "images_per_second": 0.394, "prompt_tokens": 1113, "response_tokens_est": 56, "n_tiles": 1, "output_text": " 1. man: 3\n2. umbrella: 2\n3. chair: 1\n4. car: 2\n5. bicycle: 1\n6. table: 1\n7. box: 1\n8. sign: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27716.5, "ram_available_mb": 98055.6, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 27729.1, "ram_available_mb": 98043.0, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.5, "peak": 40.56, "min": 18.13}, "VIN": {"avg": 68.98, "peak": 121.71, "min": 28.79}, "VIN_SYS_5V0": {"avg": 14.94, "peak": 16.15, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.97, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 27.5, "energy_joules_est": 69.81, "sample_count": 25, "duration_seconds": 2.539}, "timestamp": "2026-01-19T15:17:03.707252"}
{"image_index": 367, "image_name": "000000040083.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000040083.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2483.605, "latencies_ms": [2483.605], "images_per_second": 0.403, "prompt_tokens": 1117, "response_tokens_est": 54, "n_tiles": 1, "output_text": " The man sitting under the umbrella is positioned to the left of the man sitting on the chair, with the man on the chair being closer to the camera. The man sitting on the chair is in the foreground, while the man sitting under the umbrella is in the background.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 27729.1, "ram_available_mb": 98043.0, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 27568.5, "ram_available_mb": 98203.7, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.44, "peak": 40.16, "min": 15.77}, "VIN": {"avg": 65.26, "peak": 125.71, "min": 30.29}, "VIN_SYS_5V0": {"avg": 14.98, "peak": 16.05, "min": 12.15}, "VDD_CPU_SOC_MSS": {"avg": 16.24, "peak": 17.71, "min": 13.39}}, "power_watts_avg": 27.44, "energy_joules_est": 68.17, "sample_count": 24, "duration_seconds": 2.484}, "timestamp": "2026-01-19T15:17:06.202386"}
{"image_index": 367, "image_name": "000000040083.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000040083.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1686.736, "latencies_ms": [1686.736], "images_per_second": 0.593, "prompt_tokens": 1111, "response_tokens_est": 24, "n_tiles": 1, "output_text": " A man sits under a large umbrella on a sidewalk, while two other men sit on the ground in front of him.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27568.5, "ram_available_mb": 98203.7, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 27578.5, "ram_available_mb": 98193.7, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.66, "peak": 40.56, "min": 16.95}, "VIN": {"avg": 76.92, "peak": 117.65, "min": 29.93}, "VIN_SYS_5V0": {"avg": 15.2, "peak": 16.15, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 16.35, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 30.66, "energy_joules_est": 51.74, "sample_count": 17, "duration_seconds": 1.687}, "timestamp": "2026-01-19T15:17:07.966044"}
{"image_index": 367, "image_name": "000000040083.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000040083.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1975.456, "latencies_ms": [1975.456], "images_per_second": 0.506, "prompt_tokens": 1109, "response_tokens_est": 34, "n_tiles": 1, "output_text": " The image is in black and white, with a clear contrast between the light and dark areas. The scene is well-lit, suggesting it was taken during the day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27578.5, "ram_available_mb": 98193.7, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 27581.3, "ram_available_mb": 98190.9, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.77, "peak": 40.57, "min": 17.74}, "VIN": {"avg": 75.88, "peak": 125.37, "min": 35.01}, "VIN_SYS_5V0": {"avg": 14.9, "peak": 16.05, "min": 12.15}, "VDD_CPU_SOC_MSS": {"avg": 16.06, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 29.77, "energy_joules_est": 58.82, "sample_count": 19, "duration_seconds": 1.976}, "timestamp": "2026-01-19T15:17:09.946670"}
{"image_index": 368, "image_name": "000000040471.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000040471.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1726.511, "latencies_ms": [1726.511], "images_per_second": 0.579, "prompt_tokens": 1100, "response_tokens_est": 26, "n_tiles": 1, "output_text": " The image depicts a kitchen with white cabinets, a stove, and a refrigerator, and a ceiling fan with lights hanging from it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27581.3, "ram_available_mb": 98190.9, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 27589.1, "ram_available_mb": 98183.1, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.77, "peak": 40.16, "min": 18.52}, "VIN": {"avg": 71.14, "peak": 105.98, "min": 29.15}, "VIN_SYS_5V0": {"avg": 15.24, "peak": 16.15, "min": 13.29}, "VDD_CPU_SOC_MSS": {"avg": 16.44, "peak": 17.71, "min": 14.57}}, "power_watts_avg": 30.77, "energy_joules_est": 53.14, "sample_count": 17, "duration_seconds": 1.727}, "timestamp": "2026-01-19T15:17:11.718683"}
{"image_index": 368, "image_name": "000000040471.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000040471.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2132.921, "latencies_ms": [2132.921], "images_per_second": 0.469, "prompt_tokens": 1114, "response_tokens_est": 41, "n_tiles": 1, "output_text": " 1. white cabinets\n2. white stove\n3. white refrigerator\n4. white oven\n5. white sink\n6. white dishwasher\n7. white microwave\n8. white toaster", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27589.1, "ram_available_mb": 98183.1, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 27604.5, "ram_available_mb": 98167.6, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.06, "peak": 40.56, "min": 18.53}, "VIN": {"avg": 73.63, "peak": 129.94, "min": 29.62}, "VIN_SYS_5V0": {"avg": 15.06, "peak": 16.15, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 16.2, "peak": 17.71, "min": 13.79}}, "power_watts_avg": 29.06, "energy_joules_est": 62.01, "sample_count": 21, "duration_seconds": 2.134}, "timestamp": "2026-01-19T15:17:13.908343"}
{"image_index": 368, "image_name": "000000040471.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000040471.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2146.885, "latencies_ms": [2146.885], "images_per_second": 0.466, "prompt_tokens": 1118, "response_tokens_est": 41, "n_tiles": 1, "output_text": " The stove is located to the left of the sink, and the refrigerator is positioned to the right of the sink. The ceiling fan is located above the sink, and the window is located above the sink.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 27604.5, "ram_available_mb": 98167.6, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 27612.6, "ram_available_mb": 98159.6, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.53, "peak": 40.18, "min": 16.56}, "VIN": {"avg": 70.78, "peak": 123.15, "min": 29.63}, "VIN_SYS_5V0": {"avg": 14.95, "peak": 16.15, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 16.08, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 28.53, "energy_joules_est": 61.26, "sample_count": 21, "duration_seconds": 2.147}, "timestamp": "2026-01-19T15:17:16.089408"}
{"image_index": 368, "image_name": "000000040471.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000040471.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2153.853, "latencies_ms": [2153.853], "images_per_second": 0.464, "prompt_tokens": 1112, "response_tokens_est": 42, "n_tiles": 1, "output_text": " The image captures a quaint kitchen bathed in soft light, with white cabinets that gleam under the ceiling fan's light. The kitchen is adorned with various utensils and appliances, creating a warm and inviting atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27612.6, "ram_available_mb": 98159.6, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 27625.7, "ram_available_mb": 98146.5, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.44, "peak": 39.39, "min": 16.95}, "VIN": {"avg": 58.88, "peak": 106.37, "min": 28.85}, "VIN_SYS_5V0": {"avg": 15.06, "peak": 16.26, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 16.2, "peak": 17.71, "min": 14.18}}, "power_watts_avg": 28.44, "energy_joules_est": 61.27, "sample_count": 21, "duration_seconds": 2.154}, "timestamp": "2026-01-19T15:17:18.281109"}
{"image_index": 368, "image_name": "000000040471.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000040471.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1618.935, "latencies_ms": [1618.935], "images_per_second": 0.618, "prompt_tokens": 1110, "response_tokens_est": 21, "n_tiles": 1, "output_text": " The kitchen is well-lit with natural light coming through the windows, and the walls are painted white.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27625.7, "ram_available_mb": 98146.5, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 27631.4, "ram_available_mb": 98140.8, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.85, "peak": 40.57, "min": 16.95}, "VIN": {"avg": 73.87, "peak": 108.88, "min": 30.54}, "VIN_SYS_5V0": {"avg": 15.03, "peak": 16.15, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 16.19, "peak": 17.71, "min": 13.79}}, "power_watts_avg": 30.85, "energy_joules_est": 49.96, "sample_count": 16, "duration_seconds": 1.619}, "timestamp": "2026-01-19T15:17:19.952870"}
{"image_index": 369, "image_name": "000000040757.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000040757.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1973.765, "latencies_ms": [1973.765], "images_per_second": 0.507, "prompt_tokens": 1100, "response_tokens_est": 35, "n_tiles": 1, "output_text": " A child is lying in bed with a pillow under their head, wearing a white tank top and blue pajama pants with a pattern of daisies and other shapes.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27631.4, "ram_available_mb": 98140.8, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 27639.4, "ram_available_mb": 98132.8, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.09, "peak": 40.57, "min": 19.32}, "VIN": {"avg": 76.06, "peak": 123.06, "min": 34.35}, "VIN_SYS_5V0": {"avg": 15.05, "peak": 16.26, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 16.14, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 30.09, "energy_joules_est": 59.41, "sample_count": 19, "duration_seconds": 1.974}, "timestamp": "2026-01-19T15:17:21.934732"}
{"image_index": 369, "image_name": "000000040757.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000040757.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2524.234, "latencies_ms": [2524.234], "images_per_second": 0.396, "prompt_tokens": 1114, "response_tokens_est": 57, "n_tiles": 1, "output_text": " 1. child: 1\n2. pillow: 1\n3. blanket: 1\n4. bed: 1\n5. wall: 1\n6. nightstand: 1\n7. lamp: 1\n8. curtain: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27639.4, "ram_available_mb": 98132.8, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 27651.4, "ram_available_mb": 98120.7, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.55, "peak": 40.18, "min": 18.13}, "VIN": {"avg": 73.52, "peak": 121.51, "min": 29.09}, "VIN_SYS_5V0": {"avg": 15.17, "peak": 16.26, "min": 13.39}, "VDD_CPU_SOC_MSS": {"avg": 16.24, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 27.55, "energy_joules_est": 69.56, "sample_count": 25, "duration_seconds": 2.525}, "timestamp": "2026-01-19T15:17:24.527368"}
{"image_index": 369, "image_name": "000000040757.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000040757.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2093.541, "latencies_ms": [2093.541], "images_per_second": 0.478, "prompt_tokens": 1118, "response_tokens_est": 39, "n_tiles": 1, "output_text": " The child is lying on the bed, which is positioned in the foreground of the image. The bed is situated in the middle of the room, with the wall and a lamp in the background.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 27651.4, "ram_available_mb": 98120.7, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 27671.3, "ram_available_mb": 98100.9, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.25, "peak": 39.78, "min": 15.77}, "VIN": {"avg": 69.82, "peak": 131.66, "min": 29.43}, "VIN_SYS_5V0": {"avg": 14.83, "peak": 16.05, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 15.97, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 28.25, "energy_joules_est": 59.16, "sample_count": 21, "duration_seconds": 2.094}, "timestamp": "2026-01-19T15:17:26.710956"}
{"image_index": 369, "image_name": "000000040757.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000040757.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1449.543, "latencies_ms": [1449.543], "images_per_second": 0.69, "prompt_tokens": 1112, "response_tokens_est": 14, "n_tiles": 1, "output_text": " A child is sleeping in a room with a lamp on the wall.", "error": null, "sys_before": {"cpu_percent": 50.0, "ram_used_mb": 27671.3, "ram_available_mb": 98100.9, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 27675.6, "ram_available_mb": 98096.6, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.1, "peak": 39.78, "min": 16.16}, "VIN": {"avg": 70.3, "peak": 97.86, "min": 34.5}, "VIN_SYS_5V0": {"avg": 14.81, "peak": 16.15, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.97, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 31.1, "energy_joules_est": 45.11, "sample_count": 14, "duration_seconds": 1.45}, "timestamp": "2026-01-19T15:17:28.166562"}
{"image_index": 369, "image_name": "000000040757.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000040757.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2754.24, "latencies_ms": [2754.24], "images_per_second": 0.363, "prompt_tokens": 1110, "response_tokens_est": 66, "n_tiles": 1, "output_text": " The image is a nighttime photograph of a child in a bedroom, with the room dimly lit by a lamp on the wall. The child is lying on a bed with a blue and white checkered comforter, wearing a white tank top and blue pajama pants adorned with a pattern of white daisies.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27675.6, "ram_available_mb": 98096.6, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 27687.4, "ram_available_mb": 98084.8, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.71, "peak": 40.57, "min": 18.52}, "VIN": {"avg": 75.33, "peak": 125.99, "min": 30.76}, "VIN_SYS_5V0": {"avg": 15.2, "peak": 16.26, "min": 13.29}, "VDD_CPU_SOC_MSS": {"avg": 16.33, "peak": 17.71, "min": 14.57}}, "power_watts_avg": 27.71, "energy_joules_est": 76.33, "sample_count": 27, "duration_seconds": 2.755}, "timestamp": "2026-01-19T15:17:30.970372"}
{"image_index": 370, "image_name": "000000041488.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041488.jpg", "image_width": 640, "image_height": 369, "image_resolution": "640x369", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2087.385, "latencies_ms": [2087.385], "images_per_second": 0.479, "prompt_tokens": 1099, "response_tokens_est": 39, "n_tiles": 1, "output_text": " The image features a green highway sign with the text \"EAST Interstate 78 Queens Bronx\" and a \"NO TRUCKS\" sign above it, indicating a restricted area for trucks.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27687.4, "ram_available_mb": 98084.8, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 27699.8, "ram_available_mb": 98072.4, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.44, "peak": 40.57, "min": 15.77}, "VIN": {"avg": 75.16, "peak": 123.11, "min": 29.63}, "VIN_SYS_5V0": {"avg": 14.94, "peak": 16.05, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.97, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 28.44, "energy_joules_est": 59.39, "sample_count": 21, "duration_seconds": 2.088}, "timestamp": "2026-01-19T15:17:33.164366"}
{"image_index": 370, "image_name": "000000041488.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041488.jpg", "image_width": 640, "image_height": 369, "image_resolution": "640x369", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2598.698, "latencies_ms": [2598.698], "images_per_second": 0.385, "prompt_tokens": 1113, "response_tokens_est": 58, "n_tiles": 1, "output_text": " 1. sign: 1\n2. graffiti: 1\n3. road sign: 1\n4. metal structure: 1\n5. highway: 1\n6. bridge: 1\n7. sky: 1\n8. clouds: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27699.8, "ram_available_mb": 98072.4, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 27711.3, "ram_available_mb": 98060.9, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.14, "peak": 40.18, "min": 15.77}, "VIN": {"avg": 69.68, "peak": 119.0, "min": 33.53}, "VIN_SYS_5V0": {"avg": 14.87, "peak": 16.05, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.92, "peak": 17.32, "min": 12.61}}, "power_watts_avg": 27.14, "energy_joules_est": 70.54, "sample_count": 25, "duration_seconds": 2.599}, "timestamp": "2026-01-19T15:17:35.769121"}
{"image_index": 370, "image_name": "000000041488.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041488.jpg", "image_width": 640, "image_height": 369, "image_resolution": "640x369", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2080.343, "latencies_ms": [2080.343], "images_per_second": 0.481, "prompt_tokens": 1117, "response_tokens_est": 39, "n_tiles": 1, "output_text": " The green highway sign is positioned in the foreground, with the metal truss structure in the background. The sign is to the left of the truss structure, and the sky is visible in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27711.3, "ram_available_mb": 98060.9, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 27719.4, "ram_available_mb": 98052.8, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.84, "peak": 40.57, "min": 17.34}, "VIN": {"avg": 79.26, "peak": 129.24, "min": 28.07}, "VIN_SYS_5V0": {"avg": 15.15, "peak": 16.26, "min": 13.06}, "VDD_CPU_SOC_MSS": {"avg": 16.27, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 28.84, "energy_joules_est": 60.0, "sample_count": 21, "duration_seconds": 2.081}, "timestamp": "2026-01-19T15:17:37.955192"}
{"image_index": 370, "image_name": "000000041488.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041488.jpg", "image_width": 640, "image_height": 369, "image_resolution": "640x369", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2392.716, "latencies_ms": [2392.716], "images_per_second": 0.418, "prompt_tokens": 1111, "response_tokens_est": 50, "n_tiles": 1, "output_text": " The image captures a green highway sign with the words \"Queens Bronx\" written on it, indicating a route for travelers. The sign is situated on a metal structure, possibly part of a highway or bridge, and is surrounded by a clear sky.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27719.4, "ram_available_mb": 98052.8, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 27727.4, "ram_available_mb": 98044.8, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.3, "peak": 40.16, "min": 15.77}, "VIN": {"avg": 68.64, "peak": 102.39, "min": 29.47}, "VIN_SYS_5V0": {"avg": 14.72, "peak": 15.95, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 15.85, "peak": 17.32, "min": 12.61}}, "power_watts_avg": 27.3, "energy_joules_est": 65.34, "sample_count": 24, "duration_seconds": 2.394}, "timestamp": "2026-01-19T15:17:40.450609"}
{"image_index": 370, "image_name": "000000041488.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041488.jpg", "image_width": 640, "image_height": 369, "image_resolution": "640x369", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1942.47, "latencies_ms": [1942.47], "images_per_second": 0.515, "prompt_tokens": 1109, "response_tokens_est": 33, "n_tiles": 1, "output_text": " The sign is green with white lettering, and it is attached to a metal frame. The sky is overcast, and the sign is illuminated by artificial lighting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27727.4, "ram_available_mb": 98044.8, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 27738.4, "ram_available_mb": 98033.8, "ram_percent": 22.1}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.03, "peak": 40.18, "min": 15.38}, "VIN": {"avg": 69.84, "peak": 123.4, "min": 27.71}, "VIN_SYS_5V0": {"avg": 14.8, "peak": 15.95, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.83, "peak": 17.32, "min": 12.61}}, "power_watts_avg": 29.03, "energy_joules_est": 56.41, "sample_count": 19, "duration_seconds": 1.943}, "timestamp": "2026-01-19T15:17:42.435401"}
{"image_index": 371, "image_name": "000000041633.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041633.jpg", "image_width": 640, "image_height": 429, "image_resolution": "640x429", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1622.484, "latencies_ms": [1622.484], "images_per_second": 0.616, "prompt_tokens": 1099, "response_tokens_est": 21, "n_tiles": 1, "output_text": " A red vintage Chevrolet pickup truck is parked in a lot with a cloudy sky in the background.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 27738.4, "ram_available_mb": 98033.8, "ram_percent": 22.1}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 27746.1, "ram_available_mb": 98026.0, "ram_percent": 22.1}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.05, "peak": 40.56, "min": 17.34}, "VIN": {"avg": 71.88, "peak": 103.66, "min": 27.77}, "VIN_SYS_5V0": {"avg": 15.01, "peak": 16.15, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 16.19, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 31.05, "energy_joules_est": 50.39, "sample_count": 16, "duration_seconds": 1.623}, "timestamp": "2026-01-19T15:17:44.108578"}
{"image_index": 371, "image_name": "000000041633.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041633.jpg", "image_width": 640, "image_height": 429, "image_resolution": "640x429", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2617.869, "latencies_ms": [2617.869], "images_per_second": 0.382, "prompt_tokens": 1113, "response_tokens_est": 59, "n_tiles": 1, "output_text": " 1. truck: 1\n2. wheels: 2\n3. tires: 2\n4. tail lights: 2\n5. headlights: 2\n6. license plate: 1\n7. body: 1\n8. roof: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27746.1, "ram_available_mb": 98026.0, "ram_percent": 22.1}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 27753.4, "ram_available_mb": 98018.8, "ram_percent": 22.1}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.35, "peak": 40.57, "min": 17.74}, "VIN": {"avg": 63.93, "peak": 119.32, "min": 27.61}, "VIN_SYS_5V0": {"avg": 14.9, "peak": 16.05, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.97, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 27.35, "energy_joules_est": 71.61, "sample_count": 26, "duration_seconds": 2.618}, "timestamp": "2026-01-19T15:17:46.817324"}
{"image_index": 371, "image_name": "000000041633.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041633.jpg", "image_width": 640, "image_height": 429, "image_resolution": "640x429", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2269.337, "latencies_ms": [2269.337], "images_per_second": 0.441, "prompt_tokens": 1117, "response_tokens_est": 45, "n_tiles": 1, "output_text": " The red pickup truck is positioned in the foreground, with the blue tent and street lamp in the background. The truck is facing towards the left side of the image, while the tent and lamp are located towards the right side.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27753.4, "ram_available_mb": 98018.8, "ram_percent": 22.1}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 27759.6, "ram_available_mb": 98012.6, "ram_percent": 22.1}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.9, "peak": 40.18, "min": 15.38}, "VIN": {"avg": 69.67, "peak": 107.57, "min": 29.36}, "VIN_SYS_5V0": {"avg": 14.83, "peak": 16.05, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 16.03, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 27.9, "energy_joules_est": 63.33, "sample_count": 22, "duration_seconds": 2.27}, "timestamp": "2026-01-19T15:17:49.104036"}
{"image_index": 371, "image_name": "000000041633.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041633.jpg", "image_width": 640, "image_height": 429, "image_resolution": "640x429", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1521.649, "latencies_ms": [1521.649], "images_per_second": 0.657, "prompt_tokens": 1111, "response_tokens_est": 17, "n_tiles": 1, "output_text": " A red vintage truck is parked in a lot with a cloudy sky in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27759.6, "ram_available_mb": 98012.6, "ram_percent": 22.1}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 27771.7, "ram_available_mb": 98000.5, "ram_percent": 22.1}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.54, "peak": 40.57, "min": 17.35}, "VIN": {"avg": 70.02, "peak": 105.47, "min": 30.23}, "VIN_SYS_5V0": {"avg": 15.13, "peak": 16.15, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 16.32, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 31.54, "energy_joules_est": 48.0, "sample_count": 15, "duration_seconds": 1.522}, "timestamp": "2026-01-19T15:17:50.669514"}
{"image_index": 371, "image_name": "000000041633.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041633.jpg", "image_width": 640, "image_height": 429, "image_resolution": "640x429", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1634.346, "latencies_ms": [1634.346], "images_per_second": 0.612, "prompt_tokens": 1109, "response_tokens_est": 21, "n_tiles": 1, "output_text": " The truck is red with chrome wheels and a shiny finish. The sky is blue with white clouds.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27771.7, "ram_available_mb": 98000.5, "ram_percent": 22.1}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 27787.5, "ram_available_mb": 97984.7, "ram_percent": 22.1}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.98, "peak": 40.95, "min": 20.49}, "VIN": {"avg": 75.24, "peak": 122.81, "min": 30.18}, "VIN_SYS_5V0": {"avg": 15.03, "peak": 16.15, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 16.29, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 31.98, "energy_joules_est": 52.28, "sample_count": 16, "duration_seconds": 1.635}, "timestamp": "2026-01-19T15:17:52.336676"}
{"image_index": 372, "image_name": "000000041635.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041635.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1505.451, "latencies_ms": [1505.451], "images_per_second": 0.664, "prompt_tokens": 1099, "response_tokens_est": 17, "n_tiles": 1, "output_text": " A group of cows are standing behind a barbed wire fence, looking at the camera.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27787.5, "ram_available_mb": 97984.7, "ram_percent": 22.1}, "sys_after": {"cpu_percent": 6.5, "ram_used_mb": 27581.4, "ram_available_mb": 98190.7, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.3, "peak": 40.56, "min": 20.1}, "VIN": {"avg": 74.19, "peak": 124.23, "min": 30.32}, "VIN_SYS_5V0": {"avg": 15.2, "peak": 16.15, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 16.51, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 32.3, "energy_joules_est": 48.65, "sample_count": 15, "duration_seconds": 1.506}, "timestamp": "2026-01-19T15:17:53.909818"}
{"image_index": 372, "image_name": "000000041635.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041635.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2583.438, "latencies_ms": [2583.438], "images_per_second": 0.387, "prompt_tokens": 1113, "response_tokens_est": 58, "n_tiles": 1, "output_text": " 1. barbed wire: 1\n2. cows: 4\n3. grass: 1\n4. trees: 1\n5. fence: 1\n6. sky: 1\n7. clouds: 1\n8. fence post: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27581.4, "ram_available_mb": 98190.7, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 27567.8, "ram_available_mb": 98204.4, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.96, "peak": 40.95, "min": 19.7}, "VIN": {"avg": 65.83, "peak": 129.18, "min": 29.5}, "VIN_SYS_5V0": {"avg": 15.03, "peak": 16.15, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 16.14, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 27.96, "energy_joules_est": 72.24, "sample_count": 25, "duration_seconds": 2.584}, "timestamp": "2026-01-19T15:17:56.506985"}
{"image_index": 372, "image_name": "000000041635.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041635.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1758.067, "latencies_ms": [1758.067], "images_per_second": 0.569, "prompt_tokens": 1117, "response_tokens_est": 27, "n_tiles": 1, "output_text": " The cows are positioned in the foreground, with the barbed wire fence in the middle ground, and the field and trees in the background.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 27567.8, "ram_available_mb": 98204.4, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 27583.2, "ram_available_mb": 98188.9, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.79, "peak": 40.56, "min": 17.34}, "VIN": {"avg": 75.19, "peak": 125.73, "min": 32.63}, "VIN_SYS_5V0": {"avg": 15.33, "peak": 16.26, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 16.49, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 30.79, "energy_joules_est": 54.14, "sample_count": 17, "duration_seconds": 1.758}, "timestamp": "2026-01-19T15:17:58.271682"}
{"image_index": 372, "image_name": "000000041635.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041635.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2379.178, "latencies_ms": [2379.178], "images_per_second": 0.42, "prompt_tokens": 1111, "response_tokens_est": 46, "n_tiles": 1, "output_text": " In this black and white photo, we see a group of cows standing behind a barbed wire fence, looking directly at the camera. The cows are of different colors, with one having a white face and the others having black faces.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27583.2, "ram_available_mb": 98188.9, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 27592.9, "ram_available_mb": 98179.3, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.28, "peak": 40.18, "min": 18.91}, "VIN": {"avg": 69.95, "peak": 95.9, "min": 35.32}, "VIN_SYS_5V0": {"avg": 15.08, "peak": 16.26, "min": 13.59}, "VDD_CPU_SOC_MSS": {"avg": 16.14, "peak": 17.71, "min": 14.57}}, "power_watts_avg": 28.28, "energy_joules_est": 67.3, "sample_count": 23, "duration_seconds": 2.38}, "timestamp": "2026-01-19T15:18:00.658528"}
{"image_index": 372, "image_name": "000000041635.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041635.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1710.55, "latencies_ms": [1710.55], "images_per_second": 0.585, "prompt_tokens": 1109, "response_tokens_est": 24, "n_tiles": 1, "output_text": " The image is in black and white, with a barbed wire fence in the foreground and a cloudy sky in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27592.9, "ram_available_mb": 98179.3, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 27603.7, "ram_available_mb": 98168.5, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.33, "peak": 39.78, "min": 17.34}, "VIN": {"avg": 72.07, "peak": 106.9, "min": 28.93}, "VIN_SYS_5V0": {"avg": 15.07, "peak": 16.15, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 16.26, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 30.33, "energy_joules_est": 51.89, "sample_count": 17, "duration_seconds": 1.711}, "timestamp": "2026-01-19T15:18:02.437251"}
{"image_index": 373, "image_name": "000000041872.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041872.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2326.09, "latencies_ms": [2326.09], "images_per_second": 0.43, "prompt_tokens": 1099, "response_tokens_est": 46, "n_tiles": 1, "output_text": " The image depicts a cozy bedroom with a large bed, a comfortable chair, a fireplace, and a television mounted on the wall, all set against a warm and inviting backdrop of wooden furniture and a ceiling with a glowing light strip.", "error": null, "sys_before": {"cpu_percent": 14.3, "ram_used_mb": 27603.7, "ram_available_mb": 98168.5, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 27611.5, "ram_available_mb": 98160.6, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.94, "peak": 40.18, "min": 17.35}, "VIN": {"avg": 68.41, "peak": 116.22, "min": 29.0}, "VIN_SYS_5V0": {"avg": 14.86, "peak": 16.05, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.9, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 27.94, "energy_joules_est": 65.0, "sample_count": 23, "duration_seconds": 2.327}, "timestamp": "2026-01-19T15:18:04.820101"}
{"image_index": 373, "image_name": "000000041872.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041872.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2595.246, "latencies_ms": [2595.246], "images_per_second": 0.385, "prompt_tokens": 1113, "response_tokens_est": 58, "n_tiles": 1, "output_text": " 1. bed: 1\n2. clock: 1\n3. television: 1\n4. chair: 2\n5. lamp: 1\n6. fireplace: 1\n7. window blinds: 1\n8. rug: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 27611.5, "ram_available_mb": 98160.6, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 27623.4, "ram_available_mb": 98148.8, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.14, "peak": 40.18, "min": 16.16}, "VIN": {"avg": 66.15, "peak": 123.94, "min": 29.48}, "VIN_SYS_5V0": {"avg": 14.95, "peak": 16.05, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.94, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 27.14, "energy_joules_est": 70.44, "sample_count": 25, "duration_seconds": 2.596}, "timestamp": "2026-01-19T15:18:07.425038"}
{"image_index": 373, "image_name": "000000041872.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041872.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2542.882, "latencies_ms": [2542.882], "images_per_second": 0.393, "prompt_tokens": 1117, "response_tokens_est": 57, "n_tiles": 1, "output_text": " The bed is located on the left side of the room, with the clock on the wall above it. The fireplace is situated on the right side of the room, with the television above it. The armchair is placed in the middle of the room, with the window behind it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27623.4, "ram_available_mb": 98148.8, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 27629.6, "ram_available_mb": 98142.5, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.37, "peak": 40.57, "min": 16.95}, "VIN": {"avg": 70.38, "peak": 116.27, "min": 29.92}, "VIN_SYS_5V0": {"avg": 15.16, "peak": 16.15, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 16.2, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 27.37, "energy_joules_est": 69.61, "sample_count": 25, "duration_seconds": 2.543}, "timestamp": "2026-01-19T15:18:10.023520"}
{"image_index": 373, "image_name": "000000041872.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041872.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1509.47, "latencies_ms": [1509.47], "images_per_second": 0.662, "prompt_tokens": 1111, "response_tokens_est": 16, "n_tiles": 1, "output_text": " The room is a cozy bedroom with a bed, chairs, and a fireplace.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27629.6, "ram_available_mb": 98142.5, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 27638.3, "ram_available_mb": 98133.9, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.07, "peak": 40.16, "min": 16.16}, "VIN": {"avg": 78.24, "peak": 123.53, "min": 28.84}, "VIN_SYS_5V0": {"avg": 14.89, "peak": 16.05, "min": 12.36}, "VDD_CPU_SOC_MSS": {"avg": 16.04, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 31.07, "energy_joules_est": 46.91, "sample_count": 15, "duration_seconds": 1.51}, "timestamp": "2026-01-19T15:18:11.588504"}
{"image_index": 373, "image_name": "000000041872.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041872.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1863.862, "latencies_ms": [1863.862], "images_per_second": 0.537, "prompt_tokens": 1109, "response_tokens_est": 30, "n_tiles": 1, "output_text": " The room is illuminated by warm lighting, with a wooden ceiling and walls. The furniture is made of wood, and the room has a cozy atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27638.3, "ram_available_mb": 98133.9, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 27647.1, "ram_available_mb": 98125.0, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.79, "peak": 40.56, "min": 20.49}, "VIN": {"avg": 70.41, "peak": 120.99, "min": 28.92}, "VIN_SYS_5V0": {"avg": 14.97, "peak": 16.05, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 16.03, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 30.79, "energy_joules_est": 57.4, "sample_count": 18, "duration_seconds": 1.864}, "timestamp": "2026-01-19T15:18:13.467661"}
{"image_index": 374, "image_name": "000000041888.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041888.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1978.438, "latencies_ms": [1978.438], "images_per_second": 0.505, "prompt_tokens": 1099, "response_tokens_est": 36, "n_tiles": 1, "output_text": " In the image, there are three black birds with white spots on their bodies and blue and red head feathers, standing on a dry grassy field with a few bushes in the background.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 27647.1, "ram_available_mb": 98125.0, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 27657.9, "ram_available_mb": 98114.3, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.11, "peak": 40.16, "min": 18.91}, "VIN": {"avg": 73.75, "peak": 124.95, "min": 45.56}, "VIN_SYS_5V0": {"avg": 15.25, "peak": 16.15, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 16.29, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 30.11, "energy_joules_est": 59.6, "sample_count": 19, "duration_seconds": 1.979}, "timestamp": "2026-01-19T15:18:15.457476"}
{"image_index": 374, "image_name": "000000041888.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041888.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2493.566, "latencies_ms": [2493.566], "images_per_second": 0.401, "prompt_tokens": 1113, "response_tokens_est": 56, "n_tiles": 1, "output_text": " 1. bird: 4\n2. grass: 1\n3. bush: 1\n4. tree: 1\n5. sky: 1\n6. ground: 1\n7. dirt: 1\n8. water: 0", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27657.9, "ram_available_mb": 98114.3, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 27666.4, "ram_available_mb": 98105.8, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.99, "peak": 40.18, "min": 18.91}, "VIN": {"avg": 66.63, "peak": 99.19, "min": 29.1}, "VIN_SYS_5V0": {"avg": 15.23, "peak": 16.26, "min": 13.39}, "VDD_CPU_SOC_MSS": {"avg": 16.27, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 27.99, "energy_joules_est": 69.8, "sample_count": 24, "duration_seconds": 2.494}, "timestamp": "2026-01-19T15:18:17.965053"}
{"image_index": 374, "image_name": "000000041888.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041888.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2324.873, "latencies_ms": [2324.873], "images_per_second": 0.43, "prompt_tokens": 1117, "response_tokens_est": 49, "n_tiles": 1, "output_text": " The main objects are located in the foreground of the image, with the two larger birds in the center and the smaller bird to the right. The larger birds are positioned closer to the camera than the smaller bird, which is situated in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27666.4, "ram_available_mb": 98105.8, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 27677.5, "ram_available_mb": 98094.7, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.19, "peak": 40.56, "min": 17.35}, "VIN": {"avg": 71.39, "peak": 121.51, "min": 29.69}, "VIN_SYS_5V0": {"avg": 15.2, "peak": 16.15, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 16.19, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 28.19, "energy_joules_est": 65.55, "sample_count": 23, "duration_seconds": 2.325}, "timestamp": "2026-01-19T15:18:20.344184"}
{"image_index": 374, "image_name": "000000041888.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041888.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1572.404, "latencies_ms": [1572.404], "images_per_second": 0.636, "prompt_tokens": 1111, "response_tokens_est": 19, "n_tiles": 1, "output_text": " A group of four birds with black feathers and blue accents are walking on a dry grassy field.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 27677.5, "ram_available_mb": 98094.7, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 27682.7, "ram_available_mb": 98089.5, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.7, "peak": 40.16, "min": 16.56}, "VIN": {"avg": 70.24, "peak": 106.79, "min": 28.74}, "VIN_SYS_5V0": {"avg": 14.95, "peak": 16.15, "min": 12.46}, "VDD_CPU_SOC_MSS": {"avg": 15.99, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 30.7, "energy_joules_est": 48.29, "sample_count": 16, "duration_seconds": 1.573}, "timestamp": "2026-01-19T15:18:22.008323"}
{"image_index": 374, "image_name": "000000041888.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041888.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2298.775, "latencies_ms": [2298.775], "images_per_second": 0.435, "prompt_tokens": 1109, "response_tokens_est": 47, "n_tiles": 1, "output_text": " The image features a group of four birds with black feathers and blue accents, standing on a dry, grassy field. The sky is overcast, and the birds appear to be in a natural habitat, possibly a savannah or grassland.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27682.7, "ram_available_mb": 98089.5, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 27706.6, "ram_available_mb": 98065.6, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.21, "peak": 40.56, "min": 17.73}, "VIN": {"avg": 71.32, "peak": 126.59, "min": 27.79}, "VIN_SYS_5V0": {"avg": 14.86, "peak": 16.05, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.92, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 28.21, "energy_joules_est": 64.86, "sample_count": 23, "duration_seconds": 2.299}, "timestamp": "2026-01-19T15:18:24.412277"}
{"image_index": 375, "image_name": "000000041990.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041990.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1626.482, "latencies_ms": [1626.482], "images_per_second": 0.615, "prompt_tokens": 1099, "response_tokens_est": 21, "n_tiles": 1, "output_text": " Three people are standing in the snow, wearing winter clothing and holding ski poles, with a forested background.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 27706.6, "ram_available_mb": 98065.6, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 27717.7, "ram_available_mb": 98054.4, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.53, "peak": 40.18, "min": 15.38}, "VIN": {"avg": 76.06, "peak": 121.97, "min": 28.84}, "VIN_SYS_5V0": {"avg": 14.76, "peak": 15.95, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.9, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 30.53, "energy_joules_est": 49.67, "sample_count": 16, "duration_seconds": 1.627}, "timestamp": "2026-01-19T15:18:26.082093"}
{"image_index": 375, "image_name": "000000041990.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041990.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2566.372, "latencies_ms": [2566.372], "images_per_second": 0.39, "prompt_tokens": 1113, "response_tokens_est": 58, "n_tiles": 1, "output_text": " 1. skis: 3\n2. ski poles: 6\n3. snow: 1\n4. trees: 1\n5. people: 3\n6. clothing: 3\n7. backpack: 1\n8. hat: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27717.7, "ram_available_mb": 98054.4, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 27725.0, "ram_available_mb": 98047.2, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.97, "peak": 40.95, "min": 19.31}, "VIN": {"avg": 71.52, "peak": 116.04, "min": 28.89}, "VIN_SYS_5V0": {"avg": 15.13, "peak": 16.15, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 16.16, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 27.97, "energy_joules_est": 71.79, "sample_count": 25, "duration_seconds": 2.567}, "timestamp": "2026-01-19T15:18:28.676621"}
{"image_index": 375, "image_name": "000000041990.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041990.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3002.865, "latencies_ms": [3002.865], "images_per_second": 0.333, "prompt_tokens": 1117, "response_tokens_est": 75, "n_tiles": 1, "output_text": " The person on the left is standing closer to the camera than the person in the middle, who is standing slightly farther away. The person on the right is standing in the background, with the person in the middle being the closest to the camera. The person on the left is also standing closer to the camera than the person in the middle, who is standing slightly farther away.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27725.0, "ram_available_mb": 98047.2, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 27741.0, "ram_available_mb": 98031.1, "ram_percent": 22.1}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.35, "peak": 40.56, "min": 16.56}, "VIN": {"avg": 69.74, "peak": 126.19, "min": 29.31}, "VIN_SYS_5V0": {"avg": 15.09, "peak": 16.15, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 16.17, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 26.35, "energy_joules_est": 79.14, "sample_count": 29, "duration_seconds": 3.003}, "timestamp": "2026-01-19T15:18:31.697291"}
{"image_index": 375, "image_name": "000000041990.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041990.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1456.117, "latencies_ms": [1456.117], "images_per_second": 0.687, "prompt_tokens": 1111, "response_tokens_est": 15, "n_tiles": 1, "output_text": " Three people are standing in the snow wearing winter clothing and holding ski poles.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27741.0, "ram_available_mb": 98031.1, "ram_percent": 22.1}, "sys_after": {"cpu_percent": 6.8, "ram_used_mb": 27570.4, "ram_available_mb": 98201.8, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.02, "peak": 40.18, "min": 16.95}, "VIN": {"avg": 80.48, "peak": 123.7, "min": 39.86}, "VIN_SYS_5V0": {"avg": 15.31, "peak": 16.15, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 16.62, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 32.02, "energy_joules_est": 46.64, "sample_count": 14, "duration_seconds": 1.457}, "timestamp": "2026-01-19T15:18:33.159636"}
{"image_index": 375, "image_name": "000000041990.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041990.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2107.679, "latencies_ms": [2107.679], "images_per_second": 0.474, "prompt_tokens": 1109, "response_tokens_est": 41, "n_tiles": 1, "output_text": " The image features three people standing in a snowy landscape, wearing winter clothing and holding ski poles. The sky is overcast, and the snow is pristine white, covering the ground and trees in the background.", "error": null, "sys_before": {"cpu_percent": 50.0, "ram_used_mb": 27570.4, "ram_available_mb": 98201.8, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 27568.0, "ram_available_mb": 98204.1, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.03, "peak": 40.56, "min": 18.91}, "VIN": {"avg": 72.12, "peak": 102.64, "min": 28.43}, "VIN_SYS_5V0": {"avg": 15.33, "peak": 16.36, "min": 13.59}, "VDD_CPU_SOC_MSS": {"avg": 16.5, "peak": 17.71, "min": 14.57}}, "power_watts_avg": 30.03, "energy_joules_est": 63.3, "sample_count": 21, "duration_seconds": 2.108}, "timestamp": "2026-01-19T15:18:35.347780"}
{"image_index": 376, "image_name": "000000042070.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042070.jpg", "image_width": 640, "image_height": 512, "image_resolution": "640x512", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2299.641, "latencies_ms": [2299.641], "images_per_second": 0.435, "prompt_tokens": 1432, "response_tokens_est": 31, "n_tiles": 1, "output_text": " A white and blue bus with the number 61 and the word \"Crosstown\" on the front is parked on the side of the road.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 27568.0, "ram_available_mb": 98204.1, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 27586.1, "ram_available_mb": 98186.1, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.2, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.08, "peak": 39.78, "min": 16.16}, "VIN": {"avg": 73.7, "peak": 117.41, "min": 28.11}, "VIN_SYS_5V0": {"avg": 15.22, "peak": 16.76, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 16.36, "peak": 18.11, "min": 13.0}}, "power_watts_avg": 30.08, "energy_joules_est": 69.19, "sample_count": 23, "duration_seconds": 2.3}, "timestamp": "2026-01-19T15:18:37.747353"}
{"image_index": 376, "image_name": "000000042070.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042070.jpg", "image_width": 640, "image_height": 512, "image_resolution": "640x512", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3158.242, "latencies_ms": [3158.242], "images_per_second": 0.317, "prompt_tokens": 1446, "response_tokens_est": 64, "n_tiles": 1, "output_text": " 1. Bus: 1\n2. License plate: 1\n3. Bike rack: 1\n4. Side mirror: 1\n5. Headlights: 2\n6. Windshield: 1\n7. Side mirror: 1\n8. License plate: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27586.1, "ram_available_mb": 98186.1, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 27593.9, "ram_available_mb": 98178.2, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.06, "peak": 40.18, "min": 16.95}, "VIN": {"avg": 73.06, "peak": 120.22, "min": 30.08}, "VIN_SYS_5V0": {"avg": 15.09, "peak": 16.56, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 16.15, "peak": 17.71, "min": 13.0}}, "power_watts_avg": 28.06, "energy_joules_est": 88.63, "sample_count": 31, "duration_seconds": 3.159}, "timestamp": "2026-01-19T15:18:40.971855"}
{"image_index": 376, "image_name": "000000042070.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042070.jpg", "image_width": 640, "image_height": 512, "image_resolution": "640x512", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2612.419, "latencies_ms": [2612.419], "images_per_second": 0.383, "prompt_tokens": 1450, "response_tokens_est": 43, "n_tiles": 1, "output_text": " The bus is positioned on the left side of the image, with the driver's side facing the camera. The background features a brick building and a clear blue sky, while the foreground shows a sidewalk and a street.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 27593.9, "ram_available_mb": 98178.2, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 27600.2, "ram_available_mb": 98172.0, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.38, "peak": 40.18, "min": 16.16}, "VIN": {"avg": 73.16, "peak": 113.42, "min": 30.05}, "VIN_SYS_5V0": {"avg": 15.11, "peak": 16.56, "min": 12.25}, "VDD_CPU_SOC_MSS": {"avg": 16.28, "peak": 17.71, "min": 13.39}}, "power_watts_avg": 29.38, "energy_joules_est": 76.77, "sample_count": 26, "duration_seconds": 2.613}, "timestamp": "2026-01-19T15:18:43.676167"}
{"image_index": 376, "image_name": "000000042070.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042070.jpg", "image_width": 640, "image_height": 512, "image_resolution": "640x512", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2266.207, "latencies_ms": [2266.207], "images_per_second": 0.441, "prompt_tokens": 1444, "response_tokens_est": 30, "n_tiles": 1, "output_text": " A white and blue bus is parked on the side of the road, with a sign on the front that says \"51 CROSSTOWN\".", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27600.2, "ram_available_mb": 98172.0, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 27618.0, "ram_available_mb": 98154.1, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.69, "peak": 39.78, "min": 16.16}, "VIN": {"avg": 75.6, "peak": 118.44, "min": 29.11}, "VIN_SYS_5V0": {"avg": 15.1, "peak": 16.56, "min": 11.77}, "VDD_CPU_SOC_MSS": {"avg": 16.27, "peak": 17.71, "min": 13.0}}, "power_watts_avg": 30.69, "energy_joules_est": 69.56, "sample_count": 22, "duration_seconds": 2.267}, "timestamp": "2026-01-19T15:18:45.971887"}
{"image_index": 376, "image_name": "000000042070.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042070.jpg", "image_width": 640, "image_height": 512, "image_resolution": "640x512", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1951.647, "latencies_ms": [1951.647], "images_per_second": 0.512, "prompt_tokens": 1442, "response_tokens_est": 19, "n_tiles": 1, "output_text": " The bus is white with blue and green graphics, and the sky is blue with white clouds.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27618.0, "ram_available_mb": 98154.1, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 27623.8, "ram_available_mb": 98148.3, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.84, "peak": 40.56, "min": 18.52}, "VIN": {"avg": 78.57, "peak": 118.94, "min": 30.62}, "VIN_SYS_5V0": {"avg": 15.44, "peak": 16.66, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 16.64, "peak": 17.71, "min": 14.18}}, "power_watts_avg": 32.84, "energy_joules_est": 64.1, "sample_count": 19, "duration_seconds": 1.952}, "timestamp": "2026-01-19T15:18:47.945024"}
{"image_index": 377, "image_name": "000000042102.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042102.jpg", "image_width": 246, "image_height": 640, "image_resolution": "246x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1720.733, "latencies_ms": [1720.733], "images_per_second": 0.581, "prompt_tokens": 768, "response_tokens_est": 41, "n_tiles": 1, "output_text": " A man is dressed in a school uniform, consisting of a blue blazer, a white shirt, a red and blue striped tie, a gray pleated skirt, black tights, and black shoes.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27623.8, "ram_available_mb": 98148.3, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 27635.8, "ram_available_mb": 98136.3, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5180.1, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.42, "peak": 37.42, "min": 18.91}, "VIN": {"avg": 72.11, "peak": 123.95, "min": 28.88}, "VIN_SYS_5V0": {"avg": 14.96, "peak": 15.54, "min": 13.49}, "VDD_CPU_SOC_MSS": {"avg": 16.05, "peak": 16.92, "min": 14.57}}, "power_watts_avg": 27.42, "energy_joules_est": 47.2, "sample_count": 17, "duration_seconds": 1.721}, "timestamp": "2026-01-19T15:18:49.721987"}
{"image_index": 377, "image_name": "000000042102.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042102.jpg", "image_width": 246, "image_height": 640, "image_resolution": "246x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2227.863, "latencies_ms": [2227.863], "images_per_second": 0.449, "prompt_tokens": 782, "response_tokens_est": 61, "n_tiles": 1, "output_text": " 1. person: 1\n2. wall: 1\n3. red carpet: 1\n4. handbag: 1\n5. tie: 1\n6. tie clip: 1\n7. tie pin: 1\n8. tie knot: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27635.8, "ram_available_mb": 98136.3, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 27643.0, "ram_available_mb": 98129.2, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5180.1, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 24.71, "peak": 36.63, "min": 17.73}, "VIN": {"avg": 66.3, "peak": 99.45, "min": 29.78}, "VIN_SYS_5V0": {"avg": 14.78, "peak": 15.44, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 15.87, "peak": 16.54, "min": 13.79}}, "power_watts_avg": 24.71, "energy_joules_est": 55.06, "sample_count": 22, "duration_seconds": 2.228}, "timestamp": "2026-01-19T15:18:52.012604"}
{"image_index": 377, "image_name": "000000042102.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042102.jpg", "image_width": 246, "image_height": 640, "image_resolution": "246x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1767.094, "latencies_ms": [1767.094], "images_per_second": 0.566, "prompt_tokens": 786, "response_tokens_est": 42, "n_tiles": 1, "output_text": " The man is standing in the foreground of the image, with the red carpet and white wall in the background. The black bag is held in his left hand, while his right hand is resting on his hip.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27643.0, "ram_available_mb": 98129.2, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 27658.4, "ram_available_mb": 98113.8, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5180.1, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.1, "peak": 35.85, "min": 16.56}, "VIN": {"avg": 69.96, "peak": 87.36, "min": 58.39}, "VIN_SYS_5V0": {"avg": 14.75, "peak": 15.34, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.73, "peak": 16.54, "min": 13.79}}, "power_watts_avg": 26.1, "energy_joules_est": 46.13, "sample_count": 17, "duration_seconds": 1.767}, "timestamp": "2026-01-19T15:18:53.783937"}
{"image_index": 377, "image_name": "000000042102.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042102.jpg", "image_width": 246, "image_height": 640, "image_resolution": "246x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1155.178, "latencies_ms": [1155.178], "images_per_second": 0.866, "prompt_tokens": 780, "response_tokens_est": 18, "n_tiles": 1, "output_text": " A man dressed in a school uniform is standing in front of a red and white wall.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 27658.4, "ram_available_mb": 98113.8, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 27663.9, "ram_available_mb": 98108.2, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5180.1, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.99, "peak": 37.03, "min": 19.7}, "VIN": {"avg": 66.74, "peak": 89.77, "min": 28.46}, "VIN_SYS_5V0": {"avg": 14.97, "peak": 15.75, "min": 13.29}, "VDD_CPU_SOC_MSS": {"avg": 16.04, "peak": 16.92, "min": 14.18}}, "power_watts_avg": 28.99, "energy_joules_est": 33.49, "sample_count": 12, "duration_seconds": 1.155}, "timestamp": "2026-01-19T15:18:55.036308"}
{"image_index": 377, "image_name": "000000042102.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042102.jpg", "image_width": 246, "image_height": 640, "image_resolution": "246x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1342.483, "latencies_ms": [1342.483], "images_per_second": 0.745, "prompt_tokens": 778, "response_tokens_est": 25, "n_tiles": 1, "output_text": " The image is a photograph with a white background and a red carpet. The lighting is natural, and the colors are vibrant.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 27663.9, "ram_available_mb": 98108.2, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 27671.1, "ram_available_mb": 98101.0, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5180.1, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.64, "peak": 36.63, "min": 18.92}, "VIN": {"avg": 66.85, "peak": 99.47, "min": 27.83}, "VIN_SYS_5V0": {"avg": 14.52, "peak": 15.24, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.6, "peak": 16.53, "min": 13.39}}, "power_watts_avg": 28.64, "energy_joules_est": 38.46, "sample_count": 13, "duration_seconds": 1.343}, "timestamp": "2026-01-19T15:18:56.395773"}
{"image_index": 378, "image_name": "000000042178.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042178.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2059.348, "latencies_ms": [2059.348], "images_per_second": 0.486, "prompt_tokens": 1099, "response_tokens_est": 39, "n_tiles": 1, "output_text": " The image depicts a series of metal structures, possibly part of a railway or industrial facility, with numerous cables and wires crisscrossing above them, set against a hazy, yellowish backdrop.", "error": null, "sys_before": {"cpu_percent": 18.2, "ram_used_mb": 27671.1, "ram_available_mb": 98101.0, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 27686.2, "ram_available_mb": 98086.0, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.64, "peak": 40.16, "min": 19.31}, "VIN": {"avg": 70.33, "peak": 100.07, "min": 29.38}, "VIN_SYS_5V0": {"avg": 15.21, "peak": 16.15, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 16.4, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 29.64, "energy_joules_est": 61.05, "sample_count": 20, "duration_seconds": 2.06}, "timestamp": "2026-01-19T15:18:58.486714"}
{"image_index": 378, "image_name": "000000042178.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042178.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2554.795, "latencies_ms": [2554.795], "images_per_second": 0.391, "prompt_tokens": 1113, "response_tokens_est": 56, "n_tiles": 1, "output_text": " 1. train: 1\n2. train: 1\n3. train: 1\n4. train: 1\n5. train: 1\n6. train: 1\n7. train: 1\n8. train: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27686.2, "ram_available_mb": 98086.0, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 27694.2, "ram_available_mb": 98077.9, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.45, "peak": 40.16, "min": 17.74}, "VIN": {"avg": 68.98, "peak": 123.62, "min": 28.8}, "VIN_SYS_5V0": {"avg": 15.07, "peak": 16.15, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 16.14, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 27.45, "energy_joules_est": 70.14, "sample_count": 25, "duration_seconds": 2.555}, "timestamp": "2026-01-19T15:19:01.092508"}
{"image_index": 378, "image_name": "000000042178.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042178.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2551.284, "latencies_ms": [2551.284], "images_per_second": 0.392, "prompt_tokens": 1117, "response_tokens_est": 57, "n_tiles": 1, "output_text": " The main objects are arranged in a linear fashion, with the foreground featuring a series of metal poles and wires, while the background showcases a collection of train cars. The poles and wires are positioned on the left side of the image, while the train cars are located on the right side.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 27694.2, "ram_available_mb": 98077.9, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 27706.2, "ram_available_mb": 98066.0, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.17, "peak": 40.16, "min": 15.77}, "VIN": {"avg": 69.32, "peak": 107.53, "min": 28.6}, "VIN_SYS_5V0": {"avg": 14.97, "peak": 16.05, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.92, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 27.17, "energy_joules_est": 69.33, "sample_count": 25, "duration_seconds": 2.552}, "timestamp": "2026-01-19T15:19:03.692963"}
{"image_index": 378, "image_name": "000000042178.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042178.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1791.244, "latencies_ms": [1791.244], "images_per_second": 0.558, "prompt_tokens": 1111, "response_tokens_est": 28, "n_tiles": 1, "output_text": " The image depicts a series of train tracks and overhead power lines in a foggy environment, with the tracks appearing to be submerged in water.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27706.2, "ram_available_mb": 98066.0, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 6.3, "ram_used_mb": 27582.8, "ram_available_mb": 98189.4, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.85, "peak": 40.57, "min": 16.16}, "VIN": {"avg": 76.49, "peak": 121.7, "min": 27.22}, "VIN_SYS_5V0": {"avg": 15.11, "peak": 16.15, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 16.34, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 29.85, "energy_joules_est": 53.48, "sample_count": 18, "duration_seconds": 1.792}, "timestamp": "2026-01-19T15:19:05.572948"}
{"image_index": 378, "image_name": "000000042178.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042178.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2457.391, "latencies_ms": [2457.391], "images_per_second": 0.407, "prompt_tokens": 1109, "response_tokens_est": 53, "n_tiles": 1, "output_text": " The image features a series of metal structures with wires attached to them, set against a foggy backdrop. The colors are muted, with the metal structures appearing in shades of silver and gray, while the fog gives the scene a hazy, dreamlike quality.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27582.8, "ram_available_mb": 98189.4, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 27594.4, "ram_available_mb": 98177.8, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.56, "peak": 40.57, "min": 16.95}, "VIN": {"avg": 69.45, "peak": 104.74, "min": 28.68}, "VIN_SYS_5V0": {"avg": 14.91, "peak": 16.05, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.93, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 27.56, "energy_joules_est": 67.73, "sample_count": 24, "duration_seconds": 2.458}, "timestamp": "2026-01-19T15:19:08.072882"}
{"image_index": 379, "image_name": "000000042276.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042276.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1923.297, "latencies_ms": [1923.297], "images_per_second": 0.52, "prompt_tokens": 1100, "response_tokens_est": 33, "n_tiles": 1, "output_text": " The image depicts a bathroom with a white toilet, a green stool with a potted plant on it, and a pile of clothes and shoes scattered on the floor.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27594.4, "ram_available_mb": 98177.8, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 27603.0, "ram_available_mb": 98169.2, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.26, "peak": 39.77, "min": 16.16}, "VIN": {"avg": 70.24, "peak": 101.78, "min": 29.17}, "VIN_SYS_5V0": {"avg": 15.06, "peak": 16.26, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 16.2, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 29.26, "energy_joules_est": 56.29, "sample_count": 19, "duration_seconds": 1.924}, "timestamp": "2026-01-19T15:19:10.050607"}
{"image_index": 379, "image_name": "000000042276.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042276.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1973.346, "latencies_ms": [1973.346], "images_per_second": 0.507, "prompt_tokens": 1114, "response_tokens_est": 35, "n_tiles": 1, "output_text": " toilet: 1, shoes: 10, socks: 1, bag: 1, boots: 1, jacket: 1, towel: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27603.0, "ram_available_mb": 98169.2, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 27607.8, "ram_available_mb": 98164.4, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.52, "peak": 40.16, "min": 17.35}, "VIN": {"avg": 65.62, "peak": 111.48, "min": 33.1}, "VIN_SYS_5V0": {"avg": 15.09, "peak": 16.15, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 16.18, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 29.52, "energy_joules_est": 58.26, "sample_count": 19, "duration_seconds": 1.974}, "timestamp": "2026-01-19T15:19:12.031838"}
{"image_index": 379, "image_name": "000000042276.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042276.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2569.344, "latencies_ms": [2569.344], "images_per_second": 0.389, "prompt_tokens": 1118, "response_tokens_est": 59, "n_tiles": 1, "output_text": " The toilet is located on the left side of the image, with the sink and mirror situated to its right. The shoes are scattered throughout the room, with some near the toilet and others closer to the sink. The clothes are primarily located near the sink, with a few scattered around the room.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 27607.8, "ram_available_mb": 98164.4, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 27620.9, "ram_available_mb": 98151.3, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.62, "peak": 39.77, "min": 18.91}, "VIN": {"avg": 69.97, "peak": 102.47, "min": 29.1}, "VIN_SYS_5V0": {"avg": 15.24, "peak": 16.26, "min": 13.29}, "VDD_CPU_SOC_MSS": {"avg": 16.3, "peak": 17.71, "min": 14.57}}, "power_watts_avg": 27.62, "energy_joules_est": 70.97, "sample_count": 25, "duration_seconds": 2.57}, "timestamp": "2026-01-19T15:19:14.634442"}
{"image_index": 379, "image_name": "000000042276.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042276.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1556.92, "latencies_ms": [1556.92], "images_per_second": 0.642, "prompt_tokens": 1112, "response_tokens_est": 19, "n_tiles": 1, "output_text": " A messy bathroom with a toilet, a plant, and a bunch of clothes on the floor.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27620.9, "ram_available_mb": 98151.3, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 27622.4, "ram_available_mb": 98149.8, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.06, "peak": 39.77, "min": 16.55}, "VIN": {"avg": 71.34, "peak": 100.63, "min": 36.61}, "VIN_SYS_5V0": {"avg": 15.19, "peak": 16.36, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 16.35, "peak": 17.71, "min": 13.79}}, "power_watts_avg": 31.06, "energy_joules_est": 48.38, "sample_count": 15, "duration_seconds": 1.558}, "timestamp": "2026-01-19T15:19:16.194846"}
{"image_index": 379, "image_name": "000000042276.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042276.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1537.561, "latencies_ms": [1537.561], "images_per_second": 0.65, "prompt_tokens": 1110, "response_tokens_est": 19, "n_tiles": 1, "output_text": " The room is lit by natural light coming from a window, and the walls are painted white.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27622.4, "ram_available_mb": 98149.8, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 27626.2, "ram_available_mb": 98145.9, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.93, "peak": 40.56, "min": 21.66}, "VIN": {"avg": 80.09, "peak": 122.14, "min": 29.81}, "VIN_SYS_5V0": {"avg": 15.42, "peak": 16.26, "min": 13.49}, "VDD_CPU_SOC_MSS": {"avg": 16.56, "peak": 17.71, "min": 14.57}}, "power_watts_avg": 32.93, "energy_joules_est": 50.65, "sample_count": 15, "duration_seconds": 1.538}, "timestamp": "2026-01-19T15:19:17.752232"}
{"image_index": 380, "image_name": "000000042296.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042296.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1429.208, "latencies_ms": [1429.208], "images_per_second": 0.7, "prompt_tokens": 1099, "response_tokens_est": 14, "n_tiles": 1, "output_text": " A polar bear is playing with two balls in a pool of water.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27626.2, "ram_available_mb": 98145.9, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 27646.2, "ram_available_mb": 98125.9, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 33.15, "peak": 40.95, "min": 20.89}, "VIN": {"avg": 74.49, "peak": 109.71, "min": 29.06}, "VIN_SYS_5V0": {"avg": 15.22, "peak": 16.15, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 16.39, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 33.15, "energy_joules_est": 47.41, "sample_count": 14, "duration_seconds": 1.43}, "timestamp": "2026-01-19T15:19:19.216386"}
{"image_index": 380, "image_name": "000000042296.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042296.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2631.692, "latencies_ms": [2631.692], "images_per_second": 0.38, "prompt_tokens": 1113, "response_tokens_est": 60, "n_tiles": 1, "output_text": " 1. polar bear: 1\n2. green ball: 2\n3. yellow ball: 1\n4. water: 1\n5. rock: 1\n6. concrete: 1\n7. sand: 1\n8. concrete wall: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27646.2, "ram_available_mb": 98125.9, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 27650.3, "ram_available_mb": 98121.9, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.94, "peak": 40.56, "min": 17.74}, "VIN": {"avg": 69.79, "peak": 133.34, "min": 28.58}, "VIN_SYS_5V0": {"avg": 15.0, "peak": 16.15, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 16.19, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 27.94, "energy_joules_est": 73.54, "sample_count": 26, "duration_seconds": 2.632}, "timestamp": "2026-01-19T15:19:21.928163"}
{"image_index": 380, "image_name": "000000042296.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042296.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2082.257, "latencies_ms": [2082.257], "images_per_second": 0.48, "prompt_tokens": 1117, "response_tokens_est": 38, "n_tiles": 1, "output_text": " The polar bear is in the foreground of the image, with the green and yellow balls in the middle ground. The bear is reaching towards the balls with its mouth, indicating a playful interaction.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27650.3, "ram_available_mb": 98121.9, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 27664.2, "ram_available_mb": 98108.0, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.8, "peak": 40.18, "min": 15.38}, "VIN": {"avg": 72.69, "peak": 118.14, "min": 36.87}, "VIN_SYS_5V0": {"avg": 14.89, "peak": 16.05, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 16.01, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 28.8, "energy_joules_est": 59.98, "sample_count": 20, "duration_seconds": 2.083}, "timestamp": "2026-01-19T15:19:24.017326"}
{"image_index": 380, "image_name": "000000042296.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042296.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1640.881, "latencies_ms": [1640.881], "images_per_second": 0.609, "prompt_tokens": 1111, "response_tokens_est": 22, "n_tiles": 1, "output_text": " In a zoo enclosure, a polar bear is playing with two green and yellow balls in a pool of water.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27664.2, "ram_available_mb": 98108.0, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 27674.5, "ram_available_mb": 98097.6, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.29, "peak": 40.57, "min": 18.13}, "VIN": {"avg": 80.58, "peak": 119.47, "min": 29.69}, "VIN_SYS_5V0": {"avg": 15.23, "peak": 16.15, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 16.44, "peak": 17.71, "min": 14.57}}, "power_watts_avg": 31.29, "energy_joules_est": 51.36, "sample_count": 16, "duration_seconds": 1.641}, "timestamp": "2026-01-19T15:19:25.690051"}
{"image_index": 380, "image_name": "000000042296.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042296.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2005.008, "latencies_ms": [2005.008], "images_per_second": 0.499, "prompt_tokens": 1109, "response_tokens_est": 36, "n_tiles": 1, "output_text": " The polar bear is in a pool of water, with a green and yellow ball in its mouth. The lighting is natural, and the bear's fur is wet and matted.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27674.5, "ram_available_mb": 98097.6, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 27690.1, "ram_available_mb": 98082.1, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.76, "peak": 40.97, "min": 19.31}, "VIN": {"avg": 72.8, "peak": 104.67, "min": 28.29}, "VIN_SYS_5V0": {"avg": 15.08, "peak": 16.15, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 16.22, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 29.76, "energy_joules_est": 59.68, "sample_count": 20, "duration_seconds": 2.005}, "timestamp": "2026-01-19T15:19:27.774187"}
{"image_index": 381, "image_name": "000000042528.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042528.jpg", "image_width": 640, "image_height": 495, "image_resolution": "640x495", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2633.472, "latencies_ms": [2633.472], "images_per_second": 0.38, "prompt_tokens": 1432, "response_tokens_est": 43, "n_tiles": 1, "output_text": " The image is a split-screen photo showing a person sitting on a chair with their legs crossed, wearing blue socks and jeans, and holding a flip phone, with the phone's screen displaying the time and date.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 27690.1, "ram_available_mb": 98082.1, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 27699.3, "ram_available_mb": 98072.8, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.2, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.12, "peak": 39.78, "min": 16.16}, "VIN": {"avg": 70.16, "peak": 106.78, "min": 29.77}, "VIN_SYS_5V0": {"avg": 15.18, "peak": 16.76, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 16.29, "peak": 18.11, "min": 13.0}}, "power_watts_avg": 29.12, "energy_joules_est": 76.71, "sample_count": 26, "duration_seconds": 2.634}, "timestamp": "2026-01-19T15:19:30.482767"}
{"image_index": 381, "image_name": "000000042528.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042528.jpg", "image_width": 640, "image_height": 495, "image_resolution": "640x495", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2432.366, "latencies_ms": [2432.366], "images_per_second": 0.411, "prompt_tokens": 1446, "response_tokens_est": 35, "n_tiles": 1, "output_text": " chair: 1, person: 1, cell phone: 1, window: 1, floor: 1, legs: 1, socks: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27699.3, "ram_available_mb": 98072.8, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 27713.1, "ram_available_mb": 98059.0, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.1, "peak": 40.18, "min": 16.56}, "VIN": {"avg": 73.48, "peak": 125.62, "min": 29.44}, "VIN_SYS_5V0": {"avg": 15.11, "peak": 16.56, "min": 12.15}, "VDD_CPU_SOC_MSS": {"avg": 16.19, "peak": 17.71, "min": 13.39}}, "power_watts_avg": 30.1, "energy_joules_est": 73.24, "sample_count": 24, "duration_seconds": 2.433}, "timestamp": "2026-01-19T15:19:32.977105"}
{"image_index": 381, "image_name": "000000042528.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042528.jpg", "image_width": 640, "image_height": 495, "image_resolution": "640x495", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2985.53, "latencies_ms": [2985.53], "images_per_second": 0.335, "prompt_tokens": 1450, "response_tokens_est": 57, "n_tiles": 1, "output_text": " The left side of the image shows a person's legs and feet, with the phone held in the right side of the image. The phone is positioned closer to the camera than the person's legs. The person's legs are in the foreground, while the phone is in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27713.1, "ram_available_mb": 98059.0, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 27721.4, "ram_available_mb": 98050.8, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.61, "peak": 40.18, "min": 16.95}, "VIN": {"avg": 71.81, "peak": 116.75, "min": 29.04}, "VIN_SYS_5V0": {"avg": 15.12, "peak": 16.56, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 16.17, "peak": 17.71, "min": 13.0}}, "power_watts_avg": 28.61, "energy_joules_est": 85.43, "sample_count": 29, "duration_seconds": 2.986}, "timestamp": "2026-01-19T15:19:35.995138"}
{"image_index": 381, "image_name": "000000042528.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042528.jpg", "image_width": 640, "image_height": 495, "image_resolution": "640x495", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2062.561, "latencies_ms": [2062.561], "images_per_second": 0.485, "prompt_tokens": 1444, "response_tokens_est": 23, "n_tiles": 1, "output_text": " A person is sitting on a chair with their feet up on the floor, and they are holding a cell phone.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27721.4, "ram_available_mb": 98050.8, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 27730.7, "ram_available_mb": 98041.5, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.91, "peak": 40.16, "min": 16.95}, "VIN": {"avg": 77.54, "peak": 123.27, "min": 28.98}, "VIN_SYS_5V0": {"avg": 15.39, "peak": 16.66, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 16.53, "peak": 17.71, "min": 13.79}}, "power_watts_avg": 31.91, "energy_joules_est": 65.84, "sample_count": 20, "duration_seconds": 2.063}, "timestamp": "2026-01-19T15:19:38.076179"}
{"image_index": 381, "image_name": "000000042528.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042528.jpg", "image_width": 640, "image_height": 495, "image_resolution": "640x495", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2414.696, "latencies_ms": [2414.696], "images_per_second": 0.414, "prompt_tokens": 1442, "response_tokens_est": 36, "n_tiles": 1, "output_text": " The image features a wooden floor with a blue towel on it, and a person is holding a cell phone. The lighting is natural, coming from a window with a white frame.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 27730.7, "ram_available_mb": 98041.5, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 27733.0, "ram_available_mb": 98039.2, "ram_percent": 22.1}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.76, "peak": 40.56, "min": 19.31}, "VIN": {"avg": 73.15, "peak": 118.04, "min": 28.17}, "VIN_SYS_5V0": {"avg": 15.36, "peak": 16.66, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 16.47, "peak": 17.71, "min": 14.18}}, "power_watts_avg": 30.76, "energy_joules_est": 74.29, "sample_count": 24, "duration_seconds": 2.415}, "timestamp": "2026-01-19T15:19:40.570760"}
{"image_index": 382, "image_name": "000000042563.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042563.jpg", "image_width": 624, "image_height": 640, "image_resolution": "624x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1904.449, "latencies_ms": [1904.449], "images_per_second": 0.525, "prompt_tokens": 1432, "response_tokens_est": 15, "n_tiles": 1, "output_text": " A yellow train is traveling down a snowy track with trees on either side.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 27733.0, "ram_available_mb": 98039.2, "ram_percent": 22.1}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 27743.2, "ram_available_mb": 98029.0, "ram_percent": 22.1}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.2, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.68, "peak": 40.18, "min": 16.56}, "VIN": {"avg": 78.64, "peak": 126.36, "min": 27.68}, "VIN_SYS_5V0": {"avg": 15.13, "peak": 16.56, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 16.29, "peak": 17.71, "min": 13.0}}, "power_watts_avg": 31.68, "energy_joules_est": 60.35, "sample_count": 19, "duration_seconds": 1.905}, "timestamp": "2026-01-19T15:19:42.565560"}
{"image_index": 382, "image_name": "000000042563.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042563.jpg", "image_width": 624, "image_height": 640, "image_resolution": "624x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2446.331, "latencies_ms": [2446.331], "images_per_second": 0.409, "prompt_tokens": 1446, "response_tokens_est": 35, "n_tiles": 1, "output_text": " train: 1, trees: 10, snow: 1, sky: 1, birds: 1, signal: 1, wires: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27743.2, "ram_available_mb": 98029.0, "ram_percent": 22.1}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 27759.5, "ram_available_mb": 98012.7, "ram_percent": 22.1}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.45, "peak": 40.57, "min": 18.92}, "VIN": {"avg": 74.49, "peak": 127.53, "min": 29.73}, "VIN_SYS_5V0": {"avg": 15.1, "peak": 16.56, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 16.19, "peak": 17.71, "min": 13.0}}, "power_watts_avg": 30.45, "energy_joules_est": 74.5, "sample_count": 24, "duration_seconds": 2.447}, "timestamp": "2026-01-19T15:19:45.060678"}
{"image_index": 382, "image_name": "000000042563.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042563.jpg", "image_width": 624, "image_height": 640, "image_resolution": "624x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2756.927, "latencies_ms": [2756.927], "images_per_second": 0.363, "prompt_tokens": 1450, "response_tokens_est": 48, "n_tiles": 1, "output_text": " The train is moving from the left to the right side of the image, with the tracks running parallel to the viewer's perspective. The trees and power lines are located in the background, while the snow-covered ground is in the foreground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27759.5, "ram_available_mb": 98012.7, "ram_percent": 22.1}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 27563.1, "ram_available_mb": 98209.1, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.23, "peak": 40.18, "min": 16.95}, "VIN": {"avg": 74.93, "peak": 123.26, "min": 30.35}, "VIN_SYS_5V0": {"avg": 15.18, "peak": 16.66, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 16.51, "peak": 18.89, "min": 13.39}}, "power_watts_avg": 29.23, "energy_joules_est": 80.6, "sample_count": 27, "duration_seconds": 2.757}, "timestamp": "2026-01-19T15:19:47.867058"}
{"image_index": 382, "image_name": "000000042563.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042563.jpg", "image_width": 624, "image_height": 640, "image_resolution": "624x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1937.483, "latencies_ms": [1937.483], "images_per_second": 0.516, "prompt_tokens": 1444, "response_tokens_est": 18, "n_tiles": 1, "output_text": " A yellow train is traveling down a snowy track, surrounded by trees and a cloudy sky.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27563.1, "ram_available_mb": 98209.1, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 27561.4, "ram_available_mb": 98210.8, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.1, "peak": 40.18, "min": 16.56}, "VIN": {"avg": 76.62, "peak": 119.24, "min": 29.5}, "VIN_SYS_5V0": {"avg": 15.3, "peak": 16.66, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 16.47, "peak": 17.71, "min": 13.79}}, "power_watts_avg": 32.1, "energy_joules_est": 62.21, "sample_count": 19, "duration_seconds": 1.938}, "timestamp": "2026-01-19T15:19:49.851266"}
{"image_index": 382, "image_name": "000000042563.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042563.jpg", "image_width": 624, "image_height": 640, "image_resolution": "624x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2157.934, "latencies_ms": [2157.934], "images_per_second": 0.463, "prompt_tokens": 1442, "response_tokens_est": 26, "n_tiles": 1, "output_text": " The image features a yellow train traveling down a snowy track, with a gray sky overhead and trees lining the sides of the track.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27561.4, "ram_available_mb": 98210.8, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 27581.3, "ram_available_mb": 98190.8, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.08, "peak": 40.57, "min": 20.11}, "VIN": {"avg": 76.99, "peak": 122.07, "min": 29.82}, "VIN_SYS_5V0": {"avg": 15.31, "peak": 16.66, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 16.46, "peak": 18.11, "min": 13.79}}, "power_watts_avg": 32.08, "energy_joules_est": 69.24, "sample_count": 21, "duration_seconds": 2.158}, "timestamp": "2026-01-19T15:19:52.044423"}
{"image_index": 383, "image_name": "000000042628.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042628.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2029.177, "latencies_ms": [2029.177], "images_per_second": 0.493, "prompt_tokens": 1099, "response_tokens_est": 37, "n_tiles": 1, "output_text": " In the image, a pile of snow has accumulated on the side of a road, with a few people walking in the background, and a black pipe is partially buried in the snow.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 27581.3, "ram_available_mb": 98190.8, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 27594.3, "ram_available_mb": 98177.9, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.63, "peak": 40.97, "min": 18.92}, "VIN": {"avg": 71.79, "peak": 106.48, "min": 29.87}, "VIN_SYS_5V0": {"avg": 15.06, "peak": 16.15, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 16.2, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 29.63, "energy_joules_est": 60.14, "sample_count": 20, "duration_seconds": 2.03}, "timestamp": "2026-01-19T15:19:54.132389"}
{"image_index": 383, "image_name": "000000042628.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042628.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2674.205, "latencies_ms": [2674.205], "images_per_second": 0.374, "prompt_tokens": 1113, "response_tokens_est": 61, "n_tiles": 1, "output_text": " 1. snow: 3\n2. people: 4\n3. snowbank: 1\n4. pole: 1\n5. building: 1\n6. road: 1\n7. snowplow: 1\n8. snowdrift: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27594.3, "ram_available_mb": 98177.9, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 27601.3, "ram_available_mb": 98170.8, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.05, "peak": 40.57, "min": 16.95}, "VIN": {"avg": 73.39, "peak": 126.22, "min": 30.39}, "VIN_SYS_5V0": {"avg": 14.96, "peak": 16.05, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 16.16, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 27.05, "energy_joules_est": 72.35, "sample_count": 26, "duration_seconds": 2.675}, "timestamp": "2026-01-19T15:19:56.836758"}
{"image_index": 383, "image_name": "000000042628.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042628.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2612.591, "latencies_ms": [2612.591], "images_per_second": 0.383, "prompt_tokens": 1117, "response_tokens_est": 59, "n_tiles": 1, "output_text": " The main objects are located in the foreground of the image, with the snow-covered ground and the two metal poles being the closest to the viewer. The people are positioned in the background, walking away from the camera, and the wooden building is situated further back, behind the pile of snow.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 27601.3, "ram_available_mb": 98170.8, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 27614.6, "ram_available_mb": 98157.6, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.87, "peak": 40.18, "min": 16.16}, "VIN": {"avg": 68.52, "peak": 88.84, "min": 27.71}, "VIN_SYS_5V0": {"avg": 15.01, "peak": 16.15, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 16.05, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 26.87, "energy_joules_est": 70.21, "sample_count": 26, "duration_seconds": 2.613}, "timestamp": "2026-01-19T15:19:59.544357"}
{"image_index": 383, "image_name": "000000042628.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042628.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1788.292, "latencies_ms": [1788.292], "images_per_second": 0.559, "prompt_tokens": 1111, "response_tokens_est": 26, "n_tiles": 1, "output_text": " In a snowy urban setting, a pile of snow has accumulated on the side of a road, with several people walking past it.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 27614.6, "ram_available_mb": 98157.6, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 27621.8, "ram_available_mb": 98150.3, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.46, "peak": 40.18, "min": 14.98}, "VIN": {"avg": 72.36, "peak": 125.09, "min": 27.24}, "VIN_SYS_5V0": {"avg": 14.72, "peak": 15.85, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.75, "peak": 16.92, "min": 13.0}}, "power_watts_avg": 29.46, "energy_joules_est": 52.7, "sample_count": 18, "duration_seconds": 1.789}, "timestamp": "2026-01-19T15:20:01.415663"}
{"image_index": 383, "image_name": "000000042628.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042628.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2141.32, "latencies_ms": [2141.32], "images_per_second": 0.467, "prompt_tokens": 1109, "response_tokens_est": 40, "n_tiles": 1, "output_text": " The image depicts a snowy scene with a pile of snow in the foreground, and people walking in the background. The snow is white and fluffy, and the lighting is natural, suggesting it is daytime.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27621.8, "ram_available_mb": 98150.3, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 27633.8, "ram_available_mb": 98138.4, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.57, "peak": 40.57, "min": 16.95}, "VIN": {"avg": 72.75, "peak": 120.75, "min": 29.46}, "VIN_SYS_5V0": {"avg": 14.86, "peak": 16.05, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.94, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 28.57, "energy_joules_est": 61.19, "sample_count": 21, "duration_seconds": 2.142}, "timestamp": "2026-01-19T15:20:03.606419"}
{"image_index": 384, "image_name": "000000042888.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042888.jpg", "image_width": 500, "image_height": 374, "image_resolution": "500x374", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1415.245, "latencies_ms": [1415.245], "images_per_second": 0.707, "prompt_tokens": 1099, "response_tokens_est": 13, "n_tiles": 1, "output_text": " A no parking sign is on a pole in a wooded area.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 27633.8, "ram_available_mb": 98138.4, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 27649.0, "ram_available_mb": 98123.1, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.88, "peak": 40.56, "min": 16.55}, "VIN": {"avg": 72.32, "peak": 103.69, "min": 28.81}, "VIN_SYS_5V0": {"avg": 15.01, "peak": 16.05, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 16.11, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 31.88, "energy_joules_est": 45.13, "sample_count": 14, "duration_seconds": 1.416}, "timestamp": "2026-01-19T15:20:05.065159"}
{"image_index": 384, "image_name": "000000042888.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042888.jpg", "image_width": 500, "image_height": 374, "image_resolution": "500x374", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2582.076, "latencies_ms": [2582.076], "images_per_second": 0.387, "prompt_tokens": 1113, "response_tokens_est": 58, "n_tiles": 1, "output_text": " 1. sign: 2\n2. tree: 2\n3. pole: 1\n4. building: 1\n5. street light: 1\n6. fence: 1\n7. tree branch: 1\n8. leaves: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27649.0, "ram_available_mb": 98123.1, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 6.3, "ram_used_mb": 27580.4, "ram_available_mb": 98191.8, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.41, "peak": 40.97, "min": 19.7}, "VIN": {"avg": 67.25, "peak": 108.88, "min": 30.43}, "VIN_SYS_5V0": {"avg": 15.11, "peak": 16.15, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 16.33, "peak": 17.71, "min": 13.79}}, "power_watts_avg": 28.41, "energy_joules_est": 73.36, "sample_count": 25, "duration_seconds": 2.582}, "timestamp": "2026-01-19T15:20:07.663416"}
{"image_index": 384, "image_name": "000000042888.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042888.jpg", "image_width": 500, "image_height": 374, "image_resolution": "500x374", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2169.644, "latencies_ms": [2169.644], "images_per_second": 0.461, "prompt_tokens": 1117, "response_tokens_est": 42, "n_tiles": 1, "output_text": " The sign is located on the left side of the image, with the tree on the left and the building on the right. The sign is in the foreground, while the tree and building are in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27580.4, "ram_available_mb": 98191.8, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 27590.6, "ram_available_mb": 98181.6, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.8, "peak": 40.57, "min": 16.56}, "VIN": {"avg": 66.86, "peak": 94.77, "min": 30.11}, "VIN_SYS_5V0": {"avg": 15.11, "peak": 16.15, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 16.22, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 28.8, "energy_joules_est": 62.49, "sample_count": 21, "duration_seconds": 2.17}, "timestamp": "2026-01-19T15:20:09.853850"}
{"image_index": 384, "image_name": "000000042888.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042888.jpg", "image_width": 500, "image_height": 374, "image_resolution": "500x374", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1441.216, "latencies_ms": [1441.216], "images_per_second": 0.694, "prompt_tokens": 1111, "response_tokens_est": 14, "n_tiles": 1, "output_text": " A no parking sign is posted on a pole in a residential area.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27590.6, "ram_available_mb": 98181.6, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 27598.4, "ram_available_mb": 98173.8, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.11, "peak": 40.57, "min": 17.34}, "VIN": {"avg": 71.85, "peak": 103.46, "min": 30.3}, "VIN_SYS_5V0": {"avg": 15.18, "peak": 16.15, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 16.37, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 32.11, "energy_joules_est": 46.29, "sample_count": 14, "duration_seconds": 1.442}, "timestamp": "2026-01-19T15:20:11.316391"}
{"image_index": 384, "image_name": "000000042888.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042888.jpg", "image_width": 500, "image_height": 374, "image_resolution": "500x374", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1718.862, "latencies_ms": [1718.862], "images_per_second": 0.582, "prompt_tokens": 1109, "response_tokens_est": 25, "n_tiles": 1, "output_text": " The image features a street scene with a no parking sign, a yellow and black umbrella, and a tree with green leaves.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 27598.1, "ram_available_mb": 98174.1, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 27603.5, "ram_available_mb": 98168.7, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.86, "peak": 40.95, "min": 21.67}, "VIN": {"avg": 76.07, "peak": 121.42, "min": 29.67}, "VIN_SYS_5V0": {"avg": 15.2, "peak": 16.15, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 16.28, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 31.86, "energy_joules_est": 54.78, "sample_count": 17, "duration_seconds": 1.719}, "timestamp": "2026-01-19T15:20:13.090453"}
{"image_index": 385, "image_name": "000000042889.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042889.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1811.124, "latencies_ms": [1811.124], "images_per_second": 0.552, "prompt_tokens": 1100, "response_tokens_est": 29, "n_tiles": 1, "output_text": " A teddy bear wearing glasses and headphones is sitting on a red desk with a keyboard, a microphone, and a tape recorder in front of it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27603.5, "ram_available_mb": 98168.7, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 27603.3, "ram_available_mb": 98168.9, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.27, "peak": 40.57, "min": 18.52}, "VIN": {"avg": 63.6, "peak": 102.86, "min": 29.96}, "VIN_SYS_5V0": {"avg": 14.98, "peak": 16.26, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 16.08, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 30.27, "energy_joules_est": 54.84, "sample_count": 18, "duration_seconds": 1.812}, "timestamp": "2026-01-19T15:20:14.970563"}
{"image_index": 385, "image_name": "000000042889.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042889.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2060.205, "latencies_ms": [2060.205], "images_per_second": 0.485, "prompt_tokens": 1114, "response_tokens_est": 38, "n_tiles": 1, "output_text": " teddy bear: 1\nglasses: 1\nkeyboard: 1\ncamera: 1\nmic: 1\nremote: 1\ncomputer mouse: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27603.3, "ram_available_mb": 98168.9, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 27622.9, "ram_available_mb": 98149.2, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.31, "peak": 40.57, "min": 17.74}, "VIN": {"avg": 71.14, "peak": 131.03, "min": 29.38}, "VIN_SYS_5V0": {"avg": 14.94, "peak": 16.15, "min": 12.25}, "VDD_CPU_SOC_MSS": {"avg": 16.06, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 29.31, "energy_joules_est": 60.4, "sample_count": 20, "duration_seconds": 2.061}, "timestamp": "2026-01-19T15:20:17.041580"}
{"image_index": 385, "image_name": "000000042889.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042889.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2082.934, "latencies_ms": [2082.934], "images_per_second": 0.48, "prompt_tokens": 1118, "response_tokens_est": 40, "n_tiles": 1, "output_text": " The teddy bear is positioned in the foreground, with the keyboard and other objects arranged behind it. The teddy bear is located to the left of the keyboard, and the background is a dark blue wall.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27622.9, "ram_available_mb": 98149.2, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 27625.7, "ram_available_mb": 98146.5, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.0, "peak": 39.78, "min": 18.52}, "VIN": {"avg": 70.82, "peak": 106.29, "min": 29.02}, "VIN_SYS_5V0": {"avg": 15.16, "peak": 16.26, "min": 13.17}, "VDD_CPU_SOC_MSS": {"avg": 16.35, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 29.0, "energy_joules_est": 60.42, "sample_count": 21, "duration_seconds": 2.083}, "timestamp": "2026-01-19T15:20:19.225639"}
{"image_index": 385, "image_name": "000000042889.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042889.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1595.793, "latencies_ms": [1595.793], "images_per_second": 0.627, "prompt_tokens": 1112, "response_tokens_est": 20, "n_tiles": 1, "output_text": " A teddy bear wearing glasses and headphones is sitting on a desk with a keyboard in front of it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27625.7, "ram_available_mb": 98146.5, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 27645.6, "ram_available_mb": 98126.5, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.19, "peak": 39.78, "min": 15.77}, "VIN": {"avg": 79.88, "peak": 123.18, "min": 29.35}, "VIN_SYS_5V0": {"avg": 14.72, "peak": 16.15, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.92, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 30.19, "energy_joules_est": 48.19, "sample_count": 16, "duration_seconds": 1.596}, "timestamp": "2026-01-19T15:20:20.899172"}
{"image_index": 385, "image_name": "000000042889.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042889.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1536.31, "latencies_ms": [1536.31], "images_per_second": 0.651, "prompt_tokens": 1110, "response_tokens_est": 18, "n_tiles": 1, "output_text": " The teddy bear is beige, the keyboard is black, and the background is blue.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27645.6, "ram_available_mb": 98126.5, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 27653.0, "ram_available_mb": 98119.1, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.8, "peak": 40.56, "min": 18.53}, "VIN": {"avg": 73.69, "peak": 105.35, "min": 29.92}, "VIN_SYS_5V0": {"avg": 14.96, "peak": 16.15, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 16.12, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 31.8, "energy_joules_est": 48.87, "sample_count": 15, "duration_seconds": 1.537}, "timestamp": "2026-01-19T15:20:22.460484"}
{"image_index": 386, "image_name": "000000043314.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000043314.jpg", "image_width": 500, "image_height": 376, "image_resolution": "500x376", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2357.153, "latencies_ms": [2357.153], "images_per_second": 0.424, "prompt_tokens": 1432, "response_tokens_est": 33, "n_tiles": 1, "output_text": " The image captures a skier in mid-air, performing a backflip on a snowy mountain, with a clear blue sky and a mountain range in the background.", "error": null, "sys_before": {"cpu_percent": 28.6, "ram_used_mb": 27653.0, "ram_available_mb": 98119.1, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 27664.3, "ram_available_mb": 98107.9, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.2, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.17, "peak": 40.56, "min": 20.49}, "VIN": {"avg": 72.47, "peak": 132.1, "min": 29.3}, "VIN_SYS_5V0": {"avg": 15.46, "peak": 16.76, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 16.59, "peak": 18.11, "min": 14.18}}, "power_watts_avg": 31.17, "energy_joules_est": 73.49, "sample_count": 23, "duration_seconds": 2.358}, "timestamp": "2026-01-19T15:20:24.861887"}
{"image_index": 386, "image_name": "000000043314.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000043314.jpg", "image_width": 500, "image_height": 376, "image_resolution": "500x376", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2792.744, "latencies_ms": [2792.744], "images_per_second": 0.358, "prompt_tokens": 1446, "response_tokens_est": 50, "n_tiles": 1, "output_text": " 1. skis: 2\n2. person: 1\n3. snow: 1\n4. mountain: 1\n5. trees: 1\n6. text: 1\n7. logo: 4", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27664.3, "ram_available_mb": 98107.9, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 27672.6, "ram_available_mb": 98099.5, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.37, "peak": 40.18, "min": 17.74}, "VIN": {"avg": 72.55, "peak": 119.84, "min": 30.45}, "VIN_SYS_5V0": {"avg": 15.27, "peak": 16.66, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 16.32, "peak": 17.71, "min": 13.79}}, "power_watts_avg": 29.37, "energy_joules_est": 82.03, "sample_count": 27, "duration_seconds": 2.793}, "timestamp": "2026-01-19T15:20:27.670202"}
{"image_index": 386, "image_name": "000000043314.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000043314.jpg", "image_width": 500, "image_height": 376, "image_resolution": "500x376", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2443.441, "latencies_ms": [2443.441], "images_per_second": 0.409, "prompt_tokens": 1450, "response_tokens_est": 37, "n_tiles": 1, "output_text": " The skier is in the foreground, with the mountain and trees in the background. The skier is to the left of the mountain, and the skis are to the right.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27672.6, "ram_available_mb": 98099.5, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 27688.4, "ram_available_mb": 98083.8, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.4, "peak": 40.18, "min": 17.35}, "VIN": {"avg": 69.09, "peak": 123.6, "min": 29.65}, "VIN_SYS_5V0": {"avg": 15.36, "peak": 16.76, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 16.45, "peak": 18.11, "min": 14.18}}, "power_watts_avg": 30.4, "energy_joules_est": 74.29, "sample_count": 24, "duration_seconds": 2.444}, "timestamp": "2026-01-19T15:20:30.169616"}
{"image_index": 386, "image_name": "000000043314.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000043314.jpg", "image_width": 500, "image_height": 376, "image_resolution": "500x376", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1944.595, "latencies_ms": [1944.595], "images_per_second": 0.514, "prompt_tokens": 1444, "response_tokens_est": 18, "n_tiles": 1, "output_text": " A skier is performing a trick in the snow, wearing a white and orange outfit.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27688.4, "ram_available_mb": 98083.8, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 27694.1, "ram_available_mb": 98078.1, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.39, "peak": 40.57, "min": 16.95}, "VIN": {"avg": 76.54, "peak": 122.46, "min": 29.23}, "VIN_SYS_5V0": {"avg": 15.27, "peak": 16.66, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 16.41, "peak": 17.71, "min": 13.39}}, "power_watts_avg": 32.39, "energy_joules_est": 63.0, "sample_count": 19, "duration_seconds": 1.945}, "timestamp": "2026-01-19T15:20:32.148809"}
{"image_index": 386, "image_name": "000000043314.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000043314.jpg", "image_width": 500, "image_height": 376, "image_resolution": "500x376", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3004.941, "latencies_ms": [3004.941], "images_per_second": 0.333, "prompt_tokens": 1442, "response_tokens_est": 58, "n_tiles": 1, "output_text": " The image features a skier in an orange and white outfit, with skis in the air, against a backdrop of a snowy mountain and clear blue sky. The lighting is bright and natural, suggesting daytime, and the snow appears to be freshly fallen, giving it a pristine white appearance.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27694.1, "ram_available_mb": 98078.1, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 27706.7, "ram_available_mb": 98065.4, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.21, "peak": 40.57, "min": 20.1}, "VIN": {"avg": 72.1, "peak": 118.74, "min": 33.29}, "VIN_SYS_5V0": {"avg": 15.31, "peak": 16.66, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 16.4, "peak": 17.71, "min": 14.18}}, "power_watts_avg": 29.21, "energy_joules_est": 87.79, "sample_count": 29, "duration_seconds": 3.005}, "timestamp": "2026-01-19T15:20:35.160084"}
{"image_index": 387, "image_name": "000000043435.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000043435.jpg", "image_width": 638, "image_height": 640, "image_resolution": "638x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2484.732, "latencies_ms": [2484.732], "images_per_second": 0.402, "prompt_tokens": 1432, "response_tokens_est": 38, "n_tiles": 1, "output_text": " In the image, there are two surfers riding the waves in the ocean, with one surfer positioned closer to the camera and the other further away, both enjoying the thrill of surfing.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27706.7, "ram_available_mb": 98065.4, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 27721.2, "ram_available_mb": 98051.0, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.2, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.09, "peak": 40.18, "min": 16.95}, "VIN": {"avg": 71.71, "peak": 125.67, "min": 30.48}, "VIN_SYS_5V0": {"avg": 15.49, "peak": 16.76, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 16.68, "peak": 18.11, "min": 14.18}}, "power_watts_avg": 30.09, "energy_joules_est": 74.78, "sample_count": 24, "duration_seconds": 2.485}, "timestamp": "2026-01-19T15:20:37.673635"}
{"image_index": 387, "image_name": "000000043435.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000043435.jpg", "image_width": 638, "image_height": 640, "image_resolution": "638x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3017.741, "latencies_ms": [3017.741], "images_per_second": 0.331, "prompt_tokens": 1446, "response_tokens_est": 58, "n_tiles": 1, "output_text": " 1. surfboard: 2\n2. surfer: 2\n3. wave: 2\n4. ocean: 2\n5. sky: 1\n6. sun: 1\n7. water: 2\n8. sand: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27721.2, "ram_available_mb": 98051.0, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 27741.2, "ram_available_mb": 98031.0, "ram_percent": 22.1}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.38, "peak": 40.18, "min": 17.73}, "VIN": {"avg": 70.68, "peak": 124.04, "min": 27.17}, "VIN_SYS_5V0": {"avg": 15.24, "peak": 16.66, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 16.27, "peak": 17.71, "min": 14.18}}, "power_watts_avg": 28.38, "energy_joules_est": 85.65, "sample_count": 30, "duration_seconds": 3.018}, "timestamp": "2026-01-19T15:20:40.792700"}
{"image_index": 387, "image_name": "000000043435.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000043435.jpg", "image_width": 638, "image_height": 640, "image_resolution": "638x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2867.322, "latencies_ms": [2867.322], "images_per_second": 0.349, "prompt_tokens": 1450, "response_tokens_est": 52, "n_tiles": 1, "output_text": " The surfer on the left is closer to the camera than the surfer on the right, who is positioned further away. The surfer on the left is also closer to the viewer than the surfer on the right, who is positioned in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27741.2, "ram_available_mb": 98031.0, "ram_percent": 22.1}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 27749.7, "ram_available_mb": 98022.5, "ram_percent": 22.1}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.52, "peak": 40.18, "min": 15.38}, "VIN": {"avg": 70.81, "peak": 111.3, "min": 28.89}, "VIN_SYS_5V0": {"avg": 15.05, "peak": 16.56, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 16.14, "peak": 17.71, "min": 13.0}}, "power_watts_avg": 28.52, "energy_joules_est": 81.79, "sample_count": 28, "duration_seconds": 2.868}, "timestamp": "2026-01-19T15:20:43.718970"}
{"image_index": 387, "image_name": "000000043435.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000043435.jpg", "image_width": 638, "image_height": 640, "image_resolution": "638x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1978.883, "latencies_ms": [1978.883], "images_per_second": 0.505, "prompt_tokens": 1444, "response_tokens_est": 19, "n_tiles": 1, "output_text": " Two surfers are riding the waves on the ocean, with the sun setting in the background.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 27749.7, "ram_available_mb": 98022.5, "ram_percent": 22.1}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 27759.6, "ram_available_mb": 98012.6, "ram_percent": 22.1}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.05, "peak": 39.78, "min": 16.16}, "VIN": {"avg": 75.75, "peak": 122.68, "min": 33.12}, "VIN_SYS_5V0": {"avg": 15.23, "peak": 16.56, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 16.41, "peak": 17.71, "min": 13.39}}, "power_watts_avg": 32.05, "energy_joules_est": 63.45, "sample_count": 19, "duration_seconds": 1.98}, "timestamp": "2026-01-19T15:20:45.701341"}
{"image_index": 387, "image_name": "000000043435.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000043435.jpg", "image_width": 638, "image_height": 640, "image_resolution": "638x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3253.248, "latencies_ms": [3253.248], "images_per_second": 0.307, "prompt_tokens": 1442, "response_tokens_est": 67, "n_tiles": 1, "output_text": " The image captures the dynamic interaction between the surfer and the ocean, with the surfer riding a wave and the ocean's vastness and power. The lighting is natural, with the sun casting a warm glow on the scene, and the colors are vibrant, with the blue of the ocean and the white of the waves standing out.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27759.6, "ram_available_mb": 98012.6, "ram_percent": 22.1}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 27605.6, "ram_available_mb": 98166.5, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.3, "peak": 40.97, "min": 17.73}, "VIN": {"avg": 72.18, "peak": 124.43, "min": 29.35}, "VIN_SYS_5V0": {"avg": 15.27, "peak": 16.86, "min": 13.17}, "VDD_CPU_SOC_MSS": {"avg": 16.48, "peak": 18.5, "min": 14.18}}, "power_watts_avg": 28.3, "energy_joules_est": 92.08, "sample_count": 32, "duration_seconds": 3.254}, "timestamp": "2026-01-19T15:20:49.049463"}
{"image_index": 388, "image_name": "000000043581.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000043581.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2252.17, "latencies_ms": [2252.17], "images_per_second": 0.444, "prompt_tokens": 1099, "response_tokens_est": 39, "n_tiles": 1, "output_text": " A pizza with a variety of toppings is placed on a metal tray, accompanied by a jar of pepperoni and a bottle of sauce, all set on a table with a blue tablecloth.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 27605.6, "ram_available_mb": 98166.5, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 27580.0, "ram_available_mb": 98192.2, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.24, "peak": 40.18, "min": 15.38}, "VIN": {"avg": 68.74, "peak": 121.96, "min": 28.36}, "VIN_SYS_5V0": {"avg": 14.49, "peak": 16.05, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.59, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 27.24, "energy_joules_est": 61.37, "sample_count": 22, "duration_seconds": 2.253}, "timestamp": "2026-01-19T15:20:51.347261"}
{"image_index": 388, "image_name": "000000043581.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000043581.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2792.105, "latencies_ms": [2792.105], "images_per_second": 0.358, "prompt_tokens": 1113, "response_tokens_est": 63, "n_tiles": 1, "output_text": " 1. Pizza: 1\n2. Plate: 1\n3. Sauce jar: 1\n4. Napkin: 1\n5. Table: 1\n6. Red cushion: 1\n7. Blue tablecloth: 1\n8. Glass: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27580.0, "ram_available_mb": 98192.2, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 27591.7, "ram_available_mb": 98180.5, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.41, "peak": 40.18, "min": 15.77}, "VIN": {"avg": 69.79, "peak": 125.59, "min": 29.25}, "VIN_SYS_5V0": {"avg": 14.91, "peak": 16.05, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.79, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 26.41, "energy_joules_est": 73.76, "sample_count": 27, "duration_seconds": 2.793}, "timestamp": "2026-01-19T15:20:54.169683"}
{"image_index": 388, "image_name": "000000043581.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000043581.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2550.103, "latencies_ms": [2550.103], "images_per_second": 0.392, "prompt_tokens": 1117, "response_tokens_est": 57, "n_tiles": 1, "output_text": " The pizza is in the foreground, with the pepperoni and cheese visible. The pepperoni and cheese are on the pizza, while the pepperoni jar is in the background. The pizza is on the left side of the table, while the pepperoni jar is on the right side.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27591.7, "ram_available_mb": 98180.5, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 27598.9, "ram_available_mb": 98173.3, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.22, "peak": 40.18, "min": 16.16}, "VIN": {"avg": 69.79, "peak": 105.2, "min": 29.71}, "VIN_SYS_5V0": {"avg": 15.04, "peak": 16.15, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 16.11, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 27.22, "energy_joules_est": 69.42, "sample_count": 25, "duration_seconds": 2.55}, "timestamp": "2026-01-19T15:20:56.775809"}
{"image_index": 388, "image_name": "000000043581.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000043581.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2465.218, "latencies_ms": [2465.218], "images_per_second": 0.406, "prompt_tokens": 1111, "response_tokens_est": 53, "n_tiles": 1, "output_text": " In a dimly lit restaurant, a delicious pizza sits on a metal tray, accompanied by a jar of pepperoni sauce and a napkin. The pizza boasts a golden crust, topped with melted cheese, and a variety of toppings that add to its appeal.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27598.9, "ram_available_mb": 98173.3, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 27570.1, "ram_available_mb": 98202.1, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.44, "peak": 40.18, "min": 15.77}, "VIN": {"avg": 73.16, "peak": 124.59, "min": 29.82}, "VIN_SYS_5V0": {"avg": 15.01, "peak": 16.15, "min": 12.36}, "VDD_CPU_SOC_MSS": {"avg": 16.17, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 27.44, "energy_joules_est": 67.66, "sample_count": 24, "duration_seconds": 2.466}, "timestamp": "2026-01-19T15:20:59.268891"}
{"image_index": 388, "image_name": "000000043581.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000043581.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2785.439, "latencies_ms": [2785.439], "images_per_second": 0.359, "prompt_tokens": 1109, "response_tokens_est": 63, "n_tiles": 1, "output_text": " The image features a pizza with a golden crust, red sauce, and toppings, placed on a metal tray. The pizza is set on a blue tablecloth, and there is a glass jar of pepperoni sauce and a napkin beside it. The lighting in the image is dim, creating a cozy atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27570.1, "ram_available_mb": 98202.1, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 27566.8, "ram_available_mb": 98205.4, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.58, "peak": 40.16, "min": 16.55}, "VIN": {"avg": 67.95, "peak": 126.67, "min": 30.29}, "VIN_SYS_5V0": {"avg": 14.97, "peak": 16.05, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 15.89, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 26.58, "energy_joules_est": 74.05, "sample_count": 27, "duration_seconds": 2.786}, "timestamp": "2026-01-19T15:21:02.079707"}
{"image_index": 389, "image_name": "000000043737.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000043737.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1522.092, "latencies_ms": [1522.092], "images_per_second": 0.657, "prompt_tokens": 1100, "response_tokens_est": 18, "n_tiles": 1, "output_text": " A clock with Roman numerals is on a pole, and the street is covered in snow.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27566.8, "ram_available_mb": 98205.4, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 27567.3, "ram_available_mb": 98204.9, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.1, "peak": 39.78, "min": 16.16}, "VIN": {"avg": 75.11, "peak": 120.35, "min": 30.09}, "VIN_SYS_5V0": {"avg": 15.1, "peak": 16.15, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 16.19, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 31.1, "energy_joules_est": 47.36, "sample_count": 15, "duration_seconds": 1.523}, "timestamp": "2026-01-19T15:21:03.648666"}
{"image_index": 389, "image_name": "000000043737.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000043737.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2567.93, "latencies_ms": [2567.93], "images_per_second": 0.389, "prompt_tokens": 1114, "response_tokens_est": 58, "n_tiles": 1, "output_text": " 1. clock: 1\n2. street: 1\n3. snow: 1\n4. building: 1\n5. sidewalk: 1\n6. pole: 1\n7. streetlamp: 1\n8. tree: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27567.3, "ram_available_mb": 98204.9, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 27589.4, "ram_available_mb": 98182.8, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.83, "peak": 40.57, "min": 18.91}, "VIN": {"avg": 60.94, "peak": 107.58, "min": 29.93}, "VIN_SYS_5V0": {"avg": 15.03, "peak": 16.15, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 16.1, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 27.83, "energy_joules_est": 71.48, "sample_count": 25, "duration_seconds": 2.568}, "timestamp": "2026-01-19T15:21:06.253282"}
{"image_index": 389, "image_name": "000000043737.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000043737.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2090.202, "latencies_ms": [2090.202], "images_per_second": 0.478, "prompt_tokens": 1118, "response_tokens_est": 40, "n_tiles": 1, "output_text": " The clock is positioned on the left side of the image, with the street and buildings extending into the background. The clock is in the foreground, with the street and buildings receding into the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27589.4, "ram_available_mb": 98182.8, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 27589.3, "ram_available_mb": 98182.8, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.51, "peak": 39.77, "min": 16.16}, "VIN": {"avg": 73.71, "peak": 125.45, "min": 28.2}, "VIN_SYS_5V0": {"avg": 15.06, "peak": 16.15, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 16.12, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 28.51, "energy_joules_est": 59.6, "sample_count": 21, "duration_seconds": 2.091}, "timestamp": "2026-01-19T15:21:08.441927"}
{"image_index": 389, "image_name": "000000043737.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000043737.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2554.112, "latencies_ms": [2554.112], "images_per_second": 0.392, "prompt_tokens": 1112, "response_tokens_est": 57, "n_tiles": 1, "output_text": " The image captures a serene winter scene on a street corner, where a black clock tower stands tall against the backdrop of a clear blue sky. The clock, adorned with a gold face and black numbers, silently marks the passage of time amidst the quietude of the snow-covered sidewalk.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27589.3, "ram_available_mb": 98182.8, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 27604.6, "ram_available_mb": 98167.6, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.79, "peak": 39.38, "min": 15.77}, "VIN": {"avg": 71.08, "peak": 120.06, "min": 29.03}, "VIN_SYS_5V0": {"avg": 14.87, "peak": 16.15, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.97, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 26.79, "energy_joules_est": 68.44, "sample_count": 25, "duration_seconds": 2.555}, "timestamp": "2026-01-19T15:21:11.056103"}
{"image_index": 389, "image_name": "000000043737.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000043737.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1618.683, "latencies_ms": [1618.683], "images_per_second": 0.618, "prompt_tokens": 1110, "response_tokens_est": 21, "n_tiles": 1, "output_text": " The clock is black with white numbers and hands, and the sky is blue with a hint of orange.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27604.6, "ram_available_mb": 98167.6, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 27604.6, "ram_available_mb": 98167.5, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.53, "peak": 40.18, "min": 16.16}, "VIN": {"avg": 74.01, "peak": 122.31, "min": 30.29}, "VIN_SYS_5V0": {"avg": 14.96, "peak": 16.15, "min": 12.36}, "VDD_CPU_SOC_MSS": {"avg": 15.99, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 30.53, "energy_joules_est": 49.43, "sample_count": 16, "duration_seconds": 1.619}, "timestamp": "2026-01-19T15:21:12.730457"}
{"image_index": 390, "image_name": "000000043816.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000043816.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1701.264, "latencies_ms": [1701.264], "images_per_second": 0.588, "prompt_tokens": 1099, "response_tokens_est": 24, "n_tiles": 1, "output_text": " A baseball player wearing a blue helmet and white uniform is swinging a bat at a baseball that is in mid-air.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27604.6, "ram_available_mb": 98167.5, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 27616.0, "ram_available_mb": 98156.2, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.96, "peak": 40.97, "min": 18.92}, "VIN": {"avg": 78.74, "peak": 125.42, "min": 29.48}, "VIN_SYS_5V0": {"avg": 14.95, "peak": 16.05, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.98, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 30.96, "energy_joules_est": 52.7, "sample_count": 17, "duration_seconds": 1.702}, "timestamp": "2026-01-19T15:21:14.508318"}
{"image_index": 390, "image_name": "000000043816.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000043816.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3106.143, "latencies_ms": [3106.143], "images_per_second": 0.322, "prompt_tokens": 1113, "response_tokens_est": 74, "n_tiles": 1, "output_text": " 1. baseball bat: 1\n2. baseball: 1\n3. catcher's mitt: 1\n4. baseball player: 1\n5. baseball player's uniform: 1\n6. baseball player's helmet: 1\n7. baseball player's glove: 1\n8. baseball player's pants: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27616.0, "ram_available_mb": 98156.2, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 27626.1, "ram_available_mb": 98146.0, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 25.91, "peak": 40.57, "min": 18.13}, "VIN": {"avg": 65.87, "peak": 120.1, "min": 30.37}, "VIN_SYS_5V0": {"avg": 14.8, "peak": 16.05, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.76, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 25.91, "energy_joules_est": 80.49, "sample_count": 30, "duration_seconds": 3.107}, "timestamp": "2026-01-19T15:21:17.663115"}
{"image_index": 390, "image_name": "000000043816.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000043816.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2369.816, "latencies_ms": [2369.816], "images_per_second": 0.422, "prompt_tokens": 1117, "response_tokens_est": 49, "n_tiles": 1, "output_text": " The batter is in the foreground, the catcher is in the background, and the ball is in the middle ground. The batter is closer to the camera than the catcher, and the ball is closer to the batter than the catcher.", "error": null, "sys_before": {"cpu_percent": 40.0, "ram_used_mb": 27626.1, "ram_available_mb": 98146.0, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 27626.0, "ram_available_mb": 98146.2, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.71, "peak": 40.18, "min": 15.38}, "VIN": {"avg": 71.97, "peak": 124.95, "min": 30.35}, "VIN_SYS_5V0": {"avg": 14.94, "peak": 16.05, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 16.02, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 27.71, "energy_joules_est": 65.68, "sample_count": 23, "duration_seconds": 2.37}, "timestamp": "2026-01-19T15:21:20.060757"}
{"image_index": 390, "image_name": "000000043816.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000043816.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2066.539, "latencies_ms": [2066.539], "images_per_second": 0.484, "prompt_tokens": 1111, "response_tokens_est": 38, "n_tiles": 1, "output_text": " A baseball player wearing a blue helmet and white uniform is swinging a bat at a baseball. A catcher in a red uniform is crouched behind him, ready to catch the ball.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27626.0, "ram_available_mb": 98146.2, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 27633.3, "ram_available_mb": 98138.9, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.02, "peak": 40.57, "min": 16.55}, "VIN": {"avg": 72.85, "peak": 118.11, "min": 31.52}, "VIN_SYS_5V0": {"avg": 15.1, "peak": 16.15, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 16.2, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 29.02, "energy_joules_est": 59.98, "sample_count": 20, "duration_seconds": 2.067}, "timestamp": "2026-01-19T15:21:22.155390"}
{"image_index": 390, "image_name": "000000043816.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000043816.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2904.112, "latencies_ms": [2904.112], "images_per_second": 0.344, "prompt_tokens": 1109, "response_tokens_est": 70, "n_tiles": 1, "output_text": " The image captures a dynamic moment in a baseball game, with the batter in full swing, the catcher poised to catch the ball, and the field bathed in the warm glow of sunlight. The colors are vibrant, with the blue of the batter's uniform standing out against the green of the field and the red of the catcher's uniform.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 27633.3, "ram_available_mb": 98138.9, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 27640.7, "ram_available_mb": 98131.4, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.58, "peak": 40.57, "min": 17.35}, "VIN": {"avg": 70.3, "peak": 106.06, "min": 29.65}, "VIN_SYS_5V0": {"avg": 15.07, "peak": 16.15, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 16.06, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 26.58, "energy_joules_est": 77.2, "sample_count": 28, "duration_seconds": 2.904}, "timestamp": "2026-01-19T15:21:25.071031"}
{"image_index": 391, "image_name": "000000044068.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044068.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1542.85, "latencies_ms": [1542.85], "images_per_second": 0.648, "prompt_tokens": 1100, "response_tokens_est": 19, "n_tiles": 1, "output_text": " A brown teddy bear with a red bow tie is sitting on a chair with a red cushion.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 27640.7, "ram_available_mb": 98131.4, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 27641.3, "ram_available_mb": 98130.9, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.46, "peak": 39.78, "min": 17.35}, "VIN": {"avg": 72.13, "peak": 107.31, "min": 28.97}, "VIN_SYS_5V0": {"avg": 15.31, "peak": 16.26, "min": 13.29}, "VDD_CPU_SOC_MSS": {"avg": 16.4, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 31.46, "energy_joules_est": 48.55, "sample_count": 15, "duration_seconds": 1.543}, "timestamp": "2026-01-19T15:21:26.642272"}
{"image_index": 391, "image_name": "000000044068.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044068.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2361.346, "latencies_ms": [2361.346], "images_per_second": 0.423, "prompt_tokens": 1114, "response_tokens_est": 50, "n_tiles": 1, "output_text": " chair: 1, teddy bear: 1, red cushion: 1, brown teddy bear: 1, brown teddy bear's bow: 1, brown teddy bear's paws: 1, brown teddy bear's head: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27641.3, "ram_available_mb": 98130.9, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 27645.2, "ram_available_mb": 98127.0, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.71, "peak": 40.97, "min": 19.7}, "VIN": {"avg": 69.98, "peak": 115.81, "min": 31.25}, "VIN_SYS_5V0": {"avg": 15.2, "peak": 16.26, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 16.24, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 28.71, "energy_joules_est": 67.81, "sample_count": 23, "duration_seconds": 2.362}, "timestamp": "2026-01-19T15:21:29.021862"}
{"image_index": 391, "image_name": "000000044068.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044068.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2154.306, "latencies_ms": [2154.306], "images_per_second": 0.464, "prompt_tokens": 1118, "response_tokens_est": 42, "n_tiles": 1, "output_text": " The teddy bear is positioned in the center of the image, with the chair and the striped curtain in the background. The teddy bear is sitting on the chair, which is placed in the foreground of the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27645.2, "ram_available_mb": 98127.0, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 6.4, "ram_used_mb": 27575.9, "ram_available_mb": 98196.3, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.74, "peak": 39.78, "min": 17.34}, "VIN": {"avg": 67.46, "peak": 126.27, "min": 29.13}, "VIN_SYS_5V0": {"avg": 15.23, "peak": 16.26, "min": 13.29}, "VDD_CPU_SOC_MSS": {"avg": 16.53, "peak": 17.71, "min": 14.97}}, "power_watts_avg": 28.74, "energy_joules_est": 61.93, "sample_count": 21, "duration_seconds": 2.155}, "timestamp": "2026-01-19T15:21:31.210538"}
{"image_index": 391, "image_name": "000000044068.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044068.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1393.968, "latencies_ms": [1393.968], "images_per_second": 0.717, "prompt_tokens": 1112, "response_tokens_est": 13, "n_tiles": 1, "output_text": " A teddy bear is sitting on a chair with a red cushion.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27575.9, "ram_available_mb": 98196.3, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 27581.2, "ram_available_mb": 98190.9, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.57, "peak": 39.38, "min": 16.95}, "VIN": {"avg": 73.84, "peak": 115.19, "min": 30.71}, "VIN_SYS_5V0": {"avg": 15.14, "peak": 16.15, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 16.28, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 31.57, "energy_joules_est": 44.03, "sample_count": 14, "duration_seconds": 1.395}, "timestamp": "2026-01-19T15:21:32.672856"}
{"image_index": 391, "image_name": "000000044068.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044068.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1696.494, "latencies_ms": [1696.494], "images_per_second": 0.589, "prompt_tokens": 1110, "response_tokens_est": 24, "n_tiles": 1, "output_text": " The teddy bear is brown and has a red bow tie. The chair is made of wood and has a red cushion.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27581.2, "ram_available_mb": 98190.9, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 27599.9, "ram_available_mb": 98172.3, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.31, "peak": 40.56, "min": 20.89}, "VIN": {"avg": 77.14, "peak": 125.36, "min": 28.82}, "VIN_SYS_5V0": {"avg": 14.94, "peak": 16.15, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 16.07, "peak": 17.32, "min": 13.78}}, "power_watts_avg": 31.31, "energy_joules_est": 53.13, "sample_count": 17, "duration_seconds": 1.697}, "timestamp": "2026-01-19T15:21:34.451709"}
{"image_index": 392, "image_name": "000000044195.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044195.jpg", "image_width": 361, "image_height": 500, "image_resolution": "361x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1551.952, "latencies_ms": [1551.952], "images_per_second": 0.644, "prompt_tokens": 1100, "response_tokens_est": 17, "n_tiles": 1, "output_text": " Two people are standing on a snowy mountain with a bright sun shining in the background.", "error": null, "sys_before": {"cpu_percent": 14.3, "ram_used_mb": 27599.9, "ram_available_mb": 98172.3, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 27617.8, "ram_available_mb": 98154.3, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.28, "peak": 40.18, "min": 17.74}, "VIN": {"avg": 69.9, "peak": 104.58, "min": 33.04}, "VIN_SYS_5V0": {"avg": 14.84, "peak": 16.05, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.93, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 31.28, "energy_joules_est": 48.56, "sample_count": 15, "duration_seconds": 1.552}, "timestamp": "2026-01-19T15:21:36.013445"}
{"image_index": 392, "image_name": "000000044195.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044195.jpg", "image_width": 361, "image_height": 500, "image_resolution": "361x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2584.023, "latencies_ms": [2584.023], "images_per_second": 0.387, "prompt_tokens": 1114, "response_tokens_est": 57, "n_tiles": 1, "output_text": " 1. snowboard: 1\n2. person: 2\n3. helmet: 1\n4. goggles: 1\n5. sun: 1\n6. rock: 1\n7. snow: 1\n8. mountain: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27617.8, "ram_available_mb": 98154.3, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 27617.7, "ram_available_mb": 98154.5, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.97, "peak": 40.56, "min": 19.31}, "VIN": {"avg": 68.04, "peak": 120.46, "min": 29.11}, "VIN_SYS_5V0": {"avg": 15.13, "peak": 16.15, "min": 13.29}, "VDD_CPU_SOC_MSS": {"avg": 16.13, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 27.97, "energy_joules_est": 72.29, "sample_count": 25, "duration_seconds": 2.584}, "timestamp": "2026-01-19T15:21:38.609928"}
{"image_index": 392, "image_name": "000000044195.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044195.jpg", "image_width": 361, "image_height": 500, "image_resolution": "361x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2678.325, "latencies_ms": [2678.325], "images_per_second": 0.373, "prompt_tokens": 1118, "response_tokens_est": 63, "n_tiles": 1, "output_text": " The person on the left is standing closer to the camera than the person on the right. The person on the right is standing further away from the camera than the person on the left. The person on the right is standing in the foreground of the image, while the person on the left is standing in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27617.7, "ram_available_mb": 98154.5, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 27617.4, "ram_available_mb": 98154.7, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.02, "peak": 40.18, "min": 16.56}, "VIN": {"avg": 71.96, "peak": 128.56, "min": 28.82}, "VIN_SYS_5V0": {"avg": 15.17, "peak": 16.26, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 16.19, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 27.02, "energy_joules_est": 72.38, "sample_count": 26, "duration_seconds": 2.679}, "timestamp": "2026-01-19T15:21:41.318036"}
{"image_index": 392, "image_name": "000000044195.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044195.jpg", "image_width": 361, "image_height": 500, "image_resolution": "361x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1788.539, "latencies_ms": [1788.539], "images_per_second": 0.559, "prompt_tokens": 1112, "response_tokens_est": 28, "n_tiles": 1, "output_text": " Two snowboarders stand on a snowy mountain, one holding a snowboard and the other a helmet, ready to ride down the slope.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27617.4, "ram_available_mb": 98154.7, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 27618.5, "ram_available_mb": 98153.7, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.63, "peak": 39.77, "min": 16.56}, "VIN": {"avg": 70.98, "peak": 103.35, "min": 30.46}, "VIN_SYS_5V0": {"avg": 15.11, "peak": 16.26, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 16.21, "peak": 17.71, "min": 13.79}}, "power_watts_avg": 29.63, "energy_joules_est": 53.01, "sample_count": 18, "duration_seconds": 1.789}, "timestamp": "2026-01-19T15:21:43.185605"}
{"image_index": 392, "image_name": "000000044195.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044195.jpg", "image_width": 361, "image_height": 500, "image_resolution": "361x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1998.807, "latencies_ms": [1998.807], "images_per_second": 0.5, "prompt_tokens": 1110, "response_tokens_est": 35, "n_tiles": 1, "output_text": " The image features two individuals standing on a snowy mountain, with the sun shining brightly in the background. The sky is clear and blue, and the snow is white and pristine.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27618.5, "ram_available_mb": 98153.7, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 27618.4, "ram_available_mb": 98153.7, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.86, "peak": 40.18, "min": 17.34}, "VIN": {"avg": 67.98, "peak": 119.16, "min": 29.63}, "VIN_SYS_5V0": {"avg": 14.81, "peak": 16.05, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.94, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 28.86, "energy_joules_est": 57.69, "sample_count": 20, "duration_seconds": 1.999}, "timestamp": "2026-01-19T15:21:45.275813"}
{"image_index": 393, "image_name": "000000044260.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044260.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2298.032, "latencies_ms": [2298.032], "images_per_second": 0.435, "prompt_tokens": 1099, "response_tokens_est": 47, "n_tiles": 1, "output_text": " The image captures a serene autumn scene, where a tree laden with ripe red apples stands tall, its branches adorned with a few remaining leaves, while the background is a blur of trees and foliage, creating a sense of depth and tranquility.", "error": null, "sys_before": {"cpu_percent": 16.7, "ram_used_mb": 27618.4, "ram_available_mb": 98153.7, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 27568.0, "ram_available_mb": 98204.2, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.8, "peak": 40.57, "min": 16.16}, "VIN": {"avg": 65.44, "peak": 115.63, "min": 27.28}, "VIN_SYS_5V0": {"avg": 14.79, "peak": 16.05, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 16.16, "peak": 17.71, "min": 13.0}}, "power_watts_avg": 27.8, "energy_joules_est": 63.9, "sample_count": 23, "duration_seconds": 2.299}, "timestamp": "2026-01-19T15:21:47.667527"}
{"image_index": 393, "image_name": "000000044260.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044260.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1990.381, "latencies_ms": [1990.381], "images_per_second": 0.502, "prompt_tokens": 1113, "response_tokens_est": 35, "n_tiles": 1, "output_text": " apple: 6, tree: 1, branch: 1, leaf: 1, hole: 1, tree trunk: 1, background: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27568.0, "ram_available_mb": 98204.2, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 27568.2, "ram_available_mb": 98203.9, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.82, "peak": 40.57, "min": 15.77}, "VIN": {"avg": 69.78, "peak": 116.47, "min": 29.78}, "VIN_SYS_5V0": {"avg": 14.79, "peak": 16.05, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.81, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 28.82, "energy_joules_est": 57.38, "sample_count": 20, "duration_seconds": 1.991}, "timestamp": "2026-01-19T15:21:49.749783"}
{"image_index": 393, "image_name": "000000044260.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044260.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2095.682, "latencies_ms": [2095.682], "images_per_second": 0.477, "prompt_tokens": 1117, "response_tokens_est": 39, "n_tiles": 1, "output_text": " The apples are hanging from the tree branches, which are positioned in the foreground of the image. The tree trunk is located in the middle ground, while the background consists of a blurred forest scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27568.2, "ram_available_mb": 98203.9, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 27568.0, "ram_available_mb": 98204.2, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.61, "peak": 40.57, "min": 16.56}, "VIN": {"avg": 68.96, "peak": 125.75, "min": 28.89}, "VIN_SYS_5V0": {"avg": 14.78, "peak": 16.05, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.9, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 28.61, "energy_joules_est": 59.97, "sample_count": 21, "duration_seconds": 2.096}, "timestamp": "2026-01-19T15:21:51.935054"}
{"image_index": 393, "image_name": "000000044260.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044260.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2136.018, "latencies_ms": [2136.018], "images_per_second": 0.468, "prompt_tokens": 1111, "response_tokens_est": 40, "n_tiles": 1, "output_text": " The image captures a serene autumn scene in a forest where a tree stands tall with its branches laden with ripe red apples. The warm sunlight filters through the leaves, casting a soft glow on the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27568.0, "ram_available_mb": 98204.2, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 7.0, "ram_used_mb": 27565.9, "ram_available_mb": 98206.3, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.51, "peak": 40.16, "min": 16.16}, "VIN": {"avg": 72.47, "peak": 124.11, "min": 30.22}, "VIN_SYS_5V0": {"avg": 14.84, "peak": 16.05, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 16.1, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 28.51, "energy_joules_est": 60.91, "sample_count": 21, "duration_seconds": 2.137}, "timestamp": "2026-01-19T15:21:54.114814"}
{"image_index": 393, "image_name": "000000044260.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044260.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2184.289, "latencies_ms": [2184.289], "images_per_second": 0.458, "prompt_tokens": 1109, "response_tokens_est": 43, "n_tiles": 1, "output_text": " The image features a tree with a hole in its trunk, surrounded by a warm, golden light. The apples on the tree are a vibrant red color, and the leaves are a mix of green and brown hues.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27565.9, "ram_available_mb": 98206.3, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 27560.3, "ram_available_mb": 98211.8, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.8, "peak": 40.56, "min": 16.95}, "VIN": {"avg": 71.73, "peak": 105.24, "min": 33.23}, "VIN_SYS_5V0": {"avg": 15.06, "peak": 16.15, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 16.38, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 28.8, "energy_joules_est": 62.92, "sample_count": 21, "duration_seconds": 2.185}, "timestamp": "2026-01-19T15:21:56.303186"}
{"image_index": 394, "image_name": "000000044279.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044279.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1624.746, "latencies_ms": [1624.746], "images_per_second": 0.615, "prompt_tokens": 1099, "response_tokens_est": 22, "n_tiles": 1, "output_text": " Two men in white shirts are working in a kitchen, one of them is preparing food in a frying pan.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 27560.3, "ram_available_mb": 98211.8, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 27558.7, "ram_available_mb": 98213.5, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.37, "peak": 40.57, "min": 18.14}, "VIN": {"avg": 80.56, "peak": 124.04, "min": 30.61}, "VIN_SYS_5V0": {"avg": 15.25, "peak": 16.15, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 16.44, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 31.37, "energy_joules_est": 50.99, "sample_count": 16, "duration_seconds": 1.625}, "timestamp": "2026-01-19T15:21:57.970091"}
{"image_index": 394, "image_name": "000000044279.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044279.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2132.13, "latencies_ms": [2132.13], "images_per_second": 0.469, "prompt_tokens": 1113, "response_tokens_est": 40, "n_tiles": 1, "output_text": " 1. Kitchen counter\n2. Food containers\n3. Pots\n4. Pans\n5. Food trays\n6. Food containers\n7. Food trays\n8. Food containers", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27558.7, "ram_available_mb": 98213.5, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 27589.9, "ram_available_mb": 98182.3, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.15, "peak": 40.57, "min": 19.31}, "VIN": {"avg": 68.86, "peak": 127.08, "min": 29.6}, "VIN_SYS_5V0": {"avg": 14.98, "peak": 16.15, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 16.14, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 29.15, "energy_joules_est": 62.16, "sample_count": 21, "duration_seconds": 2.133}, "timestamp": "2026-01-19T15:22:00.160092"}
{"image_index": 394, "image_name": "000000044279.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044279.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2643.064, "latencies_ms": [2643.064], "images_per_second": 0.378, "prompt_tokens": 1117, "response_tokens_est": 60, "n_tiles": 1, "output_text": " The man in the foreground is closer to the camera than the man in the background. The man in the foreground is on the left side of the image, while the man in the background is on the right side. The man in the foreground is closer to the camera than the man in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27589.9, "ram_available_mb": 98182.3, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 27593.3, "ram_available_mb": 98178.9, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.94, "peak": 40.56, "min": 16.56}, "VIN": {"avg": 69.34, "peak": 114.75, "min": 29.14}, "VIN_SYS_5V0": {"avg": 14.93, "peak": 16.05, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 16.07, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 26.94, "energy_joules_est": 71.21, "sample_count": 26, "duration_seconds": 2.643}, "timestamp": "2026-01-19T15:22:02.866479"}
{"image_index": 394, "image_name": "000000044279.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044279.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1774.587, "latencies_ms": [1774.587], "images_per_second": 0.564, "prompt_tokens": 1111, "response_tokens_est": 26, "n_tiles": 1, "output_text": " Two men are working in a kitchen, one is cooking food in a fryer and the other is preparing food in a pan.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27593.3, "ram_available_mb": 98178.9, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 6.9, "ram_used_mb": 27582.8, "ram_available_mb": 98189.4, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.55, "peak": 40.18, "min": 15.77}, "VIN": {"avg": 77.96, "peak": 123.09, "min": 29.12}, "VIN_SYS_5V0": {"avg": 14.88, "peak": 16.05, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 16.21, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 29.55, "energy_joules_est": 52.45, "sample_count": 18, "duration_seconds": 1.775}, "timestamp": "2026-01-19T15:22:04.743547"}
{"image_index": 394, "image_name": "000000044279.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044279.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1651.807, "latencies_ms": [1651.807], "images_per_second": 0.605, "prompt_tokens": 1109, "response_tokens_est": 22, "n_tiles": 1, "output_text": " The kitchen is well-lit with fluorescent lights, and the stainless steel appliances gleam under the bright lights.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27582.8, "ram_available_mb": 98189.4, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 27577.9, "ram_available_mb": 98194.2, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.02, "peak": 40.57, "min": 16.95}, "VIN": {"avg": 77.0, "peak": 122.81, "min": 29.68}, "VIN_SYS_5V0": {"avg": 14.82, "peak": 16.05, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 16.07, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 31.02, "energy_joules_est": 51.25, "sample_count": 16, "duration_seconds": 1.652}, "timestamp": "2026-01-19T15:22:06.410758"}
{"image_index": 395, "image_name": "000000044590.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044590.jpg", "image_width": 640, "image_height": 246, "image_resolution": "640x246", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1347.022, "latencies_ms": [1347.022], "images_per_second": 0.742, "prompt_tokens": 766, "response_tokens_est": 26, "n_tiles": 1, "output_text": " A group of motorcyclists are gathered on a roadside, with their bikes parked alongside the road, and a cloudy sky overhead.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27577.9, "ram_available_mb": 98194.2, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 27575.7, "ram_available_mb": 98196.5, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5179.1, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.46, "peak": 37.82, "min": 22.06}, "VIN": {"avg": 69.22, "peak": 97.72, "min": 29.75}, "VIN_SYS_5V0": {"avg": 14.89, "peak": 15.54, "min": 13.59}, "VDD_CPU_SOC_MSS": {"avg": 16.17, "peak": 16.93, "min": 14.96}}, "power_watts_avg": 29.46, "energy_joules_est": 39.7, "sample_count": 13, "duration_seconds": 1.347}, "timestamp": "2026-01-19T15:22:07.772156"}
{"image_index": 395, "image_name": "000000044590.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044590.jpg", "image_width": 640, "image_height": 246, "image_resolution": "640x246", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1104.79, "latencies_ms": [1104.79], "images_per_second": 0.905, "prompt_tokens": 780, "response_tokens_est": 16, "n_tiles": 1, "output_text": " motorcycle: 10\nperson: 10\nroad: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27575.7, "ram_available_mb": 98196.5, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 27575.7, "ram_available_mb": 98196.5, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5180.1, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.23, "peak": 37.82, "min": 21.28}, "VIN": {"avg": 70.35, "peak": 90.9, "min": 28.69}, "VIN_SYS_5V0": {"avg": 14.91, "peak": 15.65, "min": 13.69}, "VDD_CPU_SOC_MSS": {"avg": 16.07, "peak": 16.92, "min": 14.96}}, "power_watts_avg": 30.23, "energy_joules_est": 33.41, "sample_count": 11, "duration_seconds": 1.105}, "timestamp": "2026-01-19T15:22:08.927377"}
{"image_index": 395, "image_name": "000000044590.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044590.jpg", "image_width": 640, "image_height": 246, "image_resolution": "640x246", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1970.283, "latencies_ms": [1970.283], "images_per_second": 0.508, "prompt_tokens": 784, "response_tokens_est": 51, "n_tiles": 1, "output_text": " The motorcycles are positioned on the left side of the road, with the nearest motorcycle being a red and black model. The background of the image features a cloudy sky and a body of water, while the foreground shows a group of people standing near the motorcycles.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27575.7, "ram_available_mb": 98196.5, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 27571.3, "ram_available_mb": 98200.9, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5180.1, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.48, "peak": 37.42, "min": 19.7}, "VIN": {"avg": 66.16, "peak": 95.27, "min": 37.04}, "VIN_SYS_5V0": {"avg": 14.9, "peak": 15.44, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 15.98, "peak": 16.54, "min": 13.79}}, "power_watts_avg": 26.48, "energy_joules_est": 52.18, "sample_count": 19, "duration_seconds": 1.971}, "timestamp": "2026-01-19T15:22:10.905273"}
{"image_index": 395, "image_name": "000000044590.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044590.jpg", "image_width": 640, "image_height": 246, "image_resolution": "640x246", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1221.227, "latencies_ms": [1221.227], "images_per_second": 0.819, "prompt_tokens": 778, "response_tokens_est": 21, "n_tiles": 1, "output_text": " A group of motorcyclists are gathered on a roadside overlooking the ocean, with a cloudy sky overhead.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27571.3, "ram_available_mb": 98200.9, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 6.4, "ram_used_mb": 27569.8, "ram_available_mb": 98202.4, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5180.1, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.99, "peak": 37.03, "min": 19.31}, "VIN": {"avg": 72.7, "peak": 96.97, "min": 30.63}, "VIN_SYS_5V0": {"avg": 15.07, "peak": 15.75, "min": 14.0}, "VDD_CPU_SOC_MSS": {"avg": 16.47, "peak": 17.32, "min": 15.36}}, "power_watts_avg": 28.99, "energy_joules_est": 35.41, "sample_count": 12, "duration_seconds": 1.222}, "timestamp": "2026-01-19T15:22:12.163325"}
{"image_index": 395, "image_name": "000000044590.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044590.jpg", "image_width": 640, "image_height": 246, "image_resolution": "640x246", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2313.199, "latencies_ms": [2313.199], "images_per_second": 0.432, "prompt_tokens": 776, "response_tokens_est": 65, "n_tiles": 1, "output_text": " The image features a group of motorcyclists gathered on a roadside, with their motorcycles parked in a line. The sky is filled with clouds, and the lighting suggests it is either early morning or late afternoon. The colors in the image are predominantly blue and gray, with the motorcycles adding a splash of color to the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27569.8, "ram_available_mb": 98202.4, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 27569.0, "ram_available_mb": 98203.2, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5180.1, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 24.93, "peak": 37.01, "min": 17.35}, "VIN": {"avg": 63.87, "peak": 102.56, "min": 28.73}, "VIN_SYS_5V0": {"avg": 14.84, "peak": 15.54, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 15.94, "peak": 16.93, "min": 14.57}}, "power_watts_avg": 24.93, "energy_joules_est": 57.68, "sample_count": 23, "duration_seconds": 2.313}, "timestamp": "2026-01-19T15:22:14.556141"}
{"image_index": 396, "image_name": "000000044652.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044652.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1759.9, "latencies_ms": [1759.9], "images_per_second": 0.568, "prompt_tokens": 1099, "response_tokens_est": 25, "n_tiles": 1, "output_text": " A small airplane with the letters GVRZ on the tail flies through the sky, leaving a trail of smoke behind it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27569.0, "ram_available_mb": 98203.2, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 27569.0, "ram_available_mb": 98203.2, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.83, "peak": 40.18, "min": 14.98}, "VIN": {"avg": 77.18, "peak": 126.25, "min": 28.62}, "VIN_SYS_5V0": {"avg": 14.87, "peak": 16.05, "min": 12.15}, "VDD_CPU_SOC_MSS": {"avg": 15.89, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 29.83, "energy_joules_est": 52.51, "sample_count": 17, "duration_seconds": 1.76}, "timestamp": "2026-01-19T15:22:16.328540"}
{"image_index": 396, "image_name": "000000044652.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044652.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2522.735, "latencies_ms": [2522.735], "images_per_second": 0.396, "prompt_tokens": 1113, "response_tokens_est": 56, "n_tiles": 1, "output_text": " 1. airplane: 1\n2. smoke: 1\n3. clouds: 2\n4. sky: 1\n5. tail: 1\n6. propeller: 1\n7. wing: 1\n8. fuselage: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27569.0, "ram_available_mb": 98203.2, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 27563.4, "ram_available_mb": 98208.7, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.62, "peak": 40.56, "min": 17.73}, "VIN": {"avg": 68.16, "peak": 120.62, "min": 29.83}, "VIN_SYS_5V0": {"avg": 15.1, "peak": 16.15, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 16.24, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 27.62, "energy_joules_est": 69.69, "sample_count": 25, "duration_seconds": 2.523}, "timestamp": "2026-01-19T15:22:18.933739"}
{"image_index": 396, "image_name": "000000044652.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044652.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1865.606, "latencies_ms": [1865.606], "images_per_second": 0.536, "prompt_tokens": 1117, "response_tokens_est": 30, "n_tiles": 1, "output_text": " The airplane is in the foreground, flying from left to right, while the clouds are in the background. The airplane is flying higher than the clouds.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27563.4, "ram_available_mb": 98208.7, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 27563.1, "ram_available_mb": 98209.1, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.59, "peak": 40.18, "min": 15.38}, "VIN": {"avg": 71.62, "peak": 128.26, "min": 31.14}, "VIN_SYS_5V0": {"avg": 14.92, "peak": 16.05, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 15.99, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 29.59, "energy_joules_est": 55.22, "sample_count": 18, "duration_seconds": 1.866}, "timestamp": "2026-01-19T15:22:20.808946"}
{"image_index": 396, "image_name": "000000044652.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044652.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1505.564, "latencies_ms": [1505.564], "images_per_second": 0.664, "prompt_tokens": 1111, "response_tokens_est": 17, "n_tiles": 1, "output_text": " A small airplane is flying in the sky, leaving a trail of smoke behind it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27563.1, "ram_available_mb": 98209.1, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 27563.1, "ram_available_mb": 98209.1, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.93, "peak": 40.56, "min": 18.92}, "VIN": {"avg": 73.89, "peak": 122.38, "min": 30.3}, "VIN_SYS_5V0": {"avg": 15.24, "peak": 16.26, "min": 13.29}, "VDD_CPU_SOC_MSS": {"avg": 16.32, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 31.93, "energy_joules_est": 48.08, "sample_count": 15, "duration_seconds": 1.506}, "timestamp": "2026-01-19T15:22:22.377350"}
{"image_index": 396, "image_name": "000000044652.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044652.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2044.367, "latencies_ms": [2044.367], "images_per_second": 0.489, "prompt_tokens": 1109, "response_tokens_est": 37, "n_tiles": 1, "output_text": " The image is in black and white, with a clear contrast between the dark clouds and the lighter sky. The airplane is flying through the air, leaving a trail of smoke behind it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27563.1, "ram_available_mb": 98209.1, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 27563.1, "ram_available_mb": 98209.1, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.84, "peak": 40.56, "min": 20.1}, "VIN": {"avg": 70.91, "peak": 122.8, "min": 31.22}, "VIN_SYS_5V0": {"avg": 14.98, "peak": 16.05, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.91, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 29.84, "energy_joules_est": 61.02, "sample_count": 20, "duration_seconds": 2.045}, "timestamp": "2026-01-19T15:22:24.458470"}
{"image_index": 397, "image_name": "000000044699.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044699.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1695.372, "latencies_ms": [1695.372], "images_per_second": 0.59, "prompt_tokens": 1099, "response_tokens_est": 24, "n_tiles": 1, "output_text": " In the image, a group of sheep are standing on a grassy hill, with a lake and mountains in the background.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 27563.1, "ram_available_mb": 98209.1, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 27563.1, "ram_available_mb": 98209.1, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.54, "peak": 40.57, "min": 17.35}, "VIN": {"avg": 72.98, "peak": 103.46, "min": 28.79}, "VIN_SYS_5V0": {"avg": 15.01, "peak": 16.15, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 16.09, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 30.54, "energy_joules_est": 51.8, "sample_count": 17, "duration_seconds": 1.696}, "timestamp": "2026-01-19T15:22:26.234305"}
{"image_index": 397, "image_name": "000000044699.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044699.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2538.634, "latencies_ms": [2538.634], "images_per_second": 0.394, "prompt_tokens": 1113, "response_tokens_est": 56, "n_tiles": 1, "output_text": " 1. sheep: 5\n2. grass: 1\n3. rocks: 1\n4. water: 1\n5. mountains: 2\n6. sky: 1\n7. clouds: 1\n8. lake: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27563.1, "ram_available_mb": 98209.1, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 27563.1, "ram_available_mb": 98209.1, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.47, "peak": 40.57, "min": 17.74}, "VIN": {"avg": 65.48, "peak": 122.93, "min": 28.8}, "VIN_SYS_5V0": {"avg": 14.85, "peak": 16.05, "min": 12.15}, "VDD_CPU_SOC_MSS": {"avg": 15.89, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 27.47, "energy_joules_est": 69.76, "sample_count": 25, "duration_seconds": 2.539}, "timestamp": "2026-01-19T15:22:28.839729"}
{"image_index": 397, "image_name": "000000044699.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044699.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2074.094, "latencies_ms": [2074.094], "images_per_second": 0.482, "prompt_tokens": 1117, "response_tokens_est": 38, "n_tiles": 1, "output_text": " The sheep are positioned in the foreground of the image, with the mountains in the background. The sheep are standing on a grassy hill, with the lake visible to the right of the image.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 27563.1, "ram_available_mb": 98209.1, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 27563.1, "ram_available_mb": 98209.1, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.84, "peak": 40.16, "min": 15.77}, "VIN": {"avg": 63.88, "peak": 123.66, "min": 29.51}, "VIN_SYS_5V0": {"avg": 14.88, "peak": 16.05, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 16.0, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 28.84, "energy_joules_est": 59.83, "sample_count": 20, "duration_seconds": 2.074}, "timestamp": "2026-01-19T15:22:30.928467"}
{"image_index": 397, "image_name": "000000044699.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044699.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1774.259, "latencies_ms": [1774.259], "images_per_second": 0.564, "prompt_tokens": 1111, "response_tokens_est": 27, "n_tiles": 1, "output_text": " In this picturesque landscape, a group of sheep are grazing on a grassy hillside, with a serene lake and majestic mountains in the background.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 27563.1, "ram_available_mb": 98209.1, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 27563.1, "ram_available_mb": 98209.1, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.16, "peak": 40.57, "min": 17.74}, "VIN": {"avg": 70.55, "peak": 97.69, "min": 27.76}, "VIN_SYS_5V0": {"avg": 15.09, "peak": 16.15, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 16.21, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 30.16, "energy_joules_est": 53.53, "sample_count": 18, "duration_seconds": 1.775}, "timestamp": "2026-01-19T15:22:32.807458"}
{"image_index": 397, "image_name": "000000044699.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044699.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2391.033, "latencies_ms": [2391.033], "images_per_second": 0.418, "prompt_tokens": 1109, "response_tokens_est": 50, "n_tiles": 1, "output_text": " The image features a group of sheep standing on a grassy hill, with a clear blue sky above and a body of water in the background. The sheep are white, and the grass is green, with the mountains in the background providing a natural backdrop.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 27563.1, "ram_available_mb": 98209.1, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 27563.1, "ram_available_mb": 98209.1, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.9, "peak": 40.18, "min": 16.56}, "VIN": {"avg": 70.8, "peak": 105.26, "min": 31.36}, "VIN_SYS_5V0": {"avg": 14.78, "peak": 15.95, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.94, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 27.9, "energy_joules_est": 66.72, "sample_count": 23, "duration_seconds": 2.392}, "timestamp": "2026-01-19T15:22:35.208493"}
{"image_index": 398, "image_name": "000000044877.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044877.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1507.051, "latencies_ms": [1507.051], "images_per_second": 0.664, "prompt_tokens": 1099, "response_tokens_est": 17, "n_tiles": 1, "output_text": " A woman in a wheelchair is holding a tennis racket and looking at the camera.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27563.1, "ram_available_mb": 98209.1, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 27563.1, "ram_available_mb": 98209.1, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.49, "peak": 40.57, "min": 17.34}, "VIN": {"avg": 69.6, "peak": 116.85, "min": 29.16}, "VIN_SYS_5V0": {"avg": 15.17, "peak": 16.15, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 16.38, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 31.49, "energy_joules_est": 47.48, "sample_count": 15, "duration_seconds": 1.508}, "timestamp": "2026-01-19T15:22:36.780200"}
{"image_index": 398, "image_name": "000000044877.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044877.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2845.411, "latencies_ms": [2845.411], "images_per_second": 0.351, "prompt_tokens": 1113, "response_tokens_est": 68, "n_tiles": 1, "output_text": " 1. woman: 1\n2. wheelchair: 1\n3. tennis racket: 1\n4. woman's hand: 1\n5. woman's arm: 1\n6. woman's leg: 1\n7. woman's foot: 1\n8. woman's knee: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27563.1, "ram_available_mb": 98209.1, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 27564.1, "ram_available_mb": 98208.1, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.96, "peak": 40.57, "min": 17.73}, "VIN": {"avg": 67.66, "peak": 124.67, "min": 28.87}, "VIN_SYS_5V0": {"avg": 14.96, "peak": 16.05, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.93, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 26.96, "energy_joules_est": 76.72, "sample_count": 28, "duration_seconds": 2.846}, "timestamp": "2026-01-19T15:22:39.700464"}
{"image_index": 398, "image_name": "000000044877.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044877.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2231.183, "latencies_ms": [2231.183], "images_per_second": 0.448, "prompt_tokens": 1117, "response_tokens_est": 44, "n_tiles": 1, "output_text": " The woman in the wheelchair is in the foreground, holding a tennis racket. The other woman is in the background, sitting in a wheelchair. The woman in the wheelchair is closer to the camera than the other woman.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 27564.1, "ram_available_mb": 98208.1, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 27564.1, "ram_available_mb": 98208.1, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.9, "peak": 40.16, "min": 15.38}, "VIN": {"avg": 70.19, "peak": 121.01, "min": 29.68}, "VIN_SYS_5V0": {"avg": 14.86, "peak": 16.05, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 15.87, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 27.9, "energy_joules_est": 62.26, "sample_count": 22, "duration_seconds": 2.232}, "timestamp": "2026-01-19T15:22:42.001075"}
{"image_index": 398, "image_name": "000000044877.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044877.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1872.307, "latencies_ms": [1872.307], "images_per_second": 0.534, "prompt_tokens": 1111, "response_tokens_est": 30, "n_tiles": 1, "output_text": " A woman in a wheelchair is holding a tennis racket and looking at the camera. There are other people in the background, also in wheelchairs.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27564.1, "ram_available_mb": 98208.1, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 27564.1, "ram_available_mb": 98208.1, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.72, "peak": 40.18, "min": 16.16}, "VIN": {"avg": 76.61, "peak": 120.85, "min": 31.56}, "VIN_SYS_5V0": {"avg": 14.93, "peak": 16.05, "min": 12.15}, "VDD_CPU_SOC_MSS": {"avg": 15.97, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 29.72, "energy_joules_est": 55.65, "sample_count": 18, "duration_seconds": 1.873}, "timestamp": "2026-01-19T15:22:43.878327"}
{"image_index": 398, "image_name": "000000044877.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044877.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1945.044, "latencies_ms": [1945.044], "images_per_second": 0.514, "prompt_tokens": 1109, "response_tokens_est": 34, "n_tiles": 1, "output_text": " The image is taken in a gymnasium with a white wall in the background. The lighting is natural, coming from the windows on the left side of the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27564.1, "ram_available_mb": 98208.1, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 27564.1, "ram_available_mb": 98208.1, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.23, "peak": 40.95, "min": 18.92}, "VIN": {"avg": 74.13, "peak": 119.99, "min": 29.56}, "VIN_SYS_5V0": {"avg": 15.26, "peak": 16.26, "min": 13.39}, "VDD_CPU_SOC_MSS": {"avg": 16.35, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 30.23, "energy_joules_est": 58.81, "sample_count": 19, "duration_seconds": 1.945}, "timestamp": "2026-01-19T15:22:45.845030"}
{"image_index": 399, "image_name": "000000045070.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045070.jpg", "image_width": 640, "image_height": 596, "image_resolution": "640x596", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2154.135, "latencies_ms": [2154.135], "images_per_second": 0.464, "prompt_tokens": 1432, "response_tokens_est": 26, "n_tiles": 1, "output_text": " A young child wearing a black helmet and a pink plaid shirt is sitting on a brown leather saddle with a purple blanket underneath.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27564.1, "ram_available_mb": 98208.1, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 27564.8, "ram_available_mb": 98207.4, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.2, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.44, "peak": 40.16, "min": 17.73}, "VIN": {"avg": 74.48, "peak": 118.14, "min": 31.0}, "VIN_SYS_5V0": {"avg": 15.49, "peak": 16.76, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 16.72, "peak": 18.11, "min": 14.18}}, "power_watts_avg": 31.44, "energy_joules_est": 67.75, "sample_count": 21, "duration_seconds": 2.155}, "timestamp": "2026-01-19T15:22:48.038382"}
{"image_index": 399, "image_name": "000000045070.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045070.jpg", "image_width": 640, "image_height": 596, "image_resolution": "640x596", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3061.716, "latencies_ms": [3061.716], "images_per_second": 0.327, "prompt_tokens": 1446, "response_tokens_est": 60, "n_tiles": 1, "output_text": " 1. child: 1\n2. helmet: 1\n3. plaid shirt: 1\n4. jeans: 1\n5. brown saddle: 1\n6. purple blanket: 1\n7. tree: 1\n8. horse: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27564.8, "ram_available_mb": 98207.4, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 27564.8, "ram_available_mb": 98207.4, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.69, "peak": 40.57, "min": 18.92}, "VIN": {"avg": 68.91, "peak": 116.03, "min": 29.57}, "VIN_SYS_5V0": {"avg": 15.23, "peak": 16.66, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 16.25, "peak": 17.71, "min": 14.18}}, "power_watts_avg": 28.69, "energy_joules_est": 87.85, "sample_count": 30, "duration_seconds": 3.062}, "timestamp": "2026-01-19T15:22:51.140829"}
{"image_index": 399, "image_name": "000000045070.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045070.jpg", "image_width": 640, "image_height": 596, "image_resolution": "640x596", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3002.434, "latencies_ms": [3002.434], "images_per_second": 0.333, "prompt_tokens": 1450, "response_tokens_est": 58, "n_tiles": 1, "output_text": " The child is sitting on the saddle of the horse, which is positioned in the foreground of the image. The child is wearing a black helmet, which is positioned on the child's head. The background of the image features a green forest, which is positioned behind the child and the horse.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27564.8, "ram_available_mb": 98207.4, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 27564.8, "ram_available_mb": 98207.4, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.61, "peak": 40.18, "min": 16.56}, "VIN": {"avg": 69.76, "peak": 118.9, "min": 29.12}, "VIN_SYS_5V0": {"avg": 15.22, "peak": 16.66, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 16.39, "peak": 17.71, "min": 13.79}}, "power_watts_avg": 28.61, "energy_joules_est": 85.91, "sample_count": 29, "duration_seconds": 3.003}, "timestamp": "2026-01-19T15:22:54.155442"}
{"image_index": 399, "image_name": "000000045070.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045070.jpg", "image_width": 640, "image_height": 596, "image_resolution": "640x596", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2187.504, "latencies_ms": [2187.504], "images_per_second": 0.457, "prompt_tokens": 1444, "response_tokens_est": 28, "n_tiles": 1, "output_text": " A young girl wearing a black helmet and a pink plaid shirt is sitting on a brown saddle in a field with trees in the background.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 27564.8, "ram_available_mb": 98207.4, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 27568.8, "ram_available_mb": 98203.4, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.07, "peak": 40.18, "min": 17.35}, "VIN": {"avg": 72.85, "peak": 131.84, "min": 27.39}, "VIN_SYS_5V0": {"avg": 15.36, "peak": 16.66, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 16.5, "peak": 17.71, "min": 14.18}}, "power_watts_avg": 31.07, "energy_joules_est": 67.98, "sample_count": 22, "duration_seconds": 2.188}, "timestamp": "2026-01-19T15:22:56.448783"}
{"image_index": 399, "image_name": "000000045070.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045070.jpg", "image_width": 640, "image_height": 596, "image_resolution": "640x596", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2452.268, "latencies_ms": [2452.268], "images_per_second": 0.408, "prompt_tokens": 1442, "response_tokens_est": 35, "n_tiles": 1, "output_text": " The child is wearing a pink and white plaid shirt, blue jeans, and a black helmet. The background is a lush green forest with trees and a clear blue sky.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27568.8, "ram_available_mb": 98203.4, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 27568.8, "ram_available_mb": 98203.4, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.99, "peak": 40.18, "min": 16.56}, "VIN": {"avg": 74.01, "peak": 127.15, "min": 31.13}, "VIN_SYS_5V0": {"avg": 14.95, "peak": 16.46, "min": 11.44}, "VDD_CPU_SOC_MSS": {"avg": 16.04, "peak": 17.71, "min": 12.61}}, "power_watts_avg": 29.99, "energy_joules_est": 73.56, "sample_count": 24, "duration_seconds": 2.453}, "timestamp": "2026-01-19T15:22:58.960929"}
{"image_index": 400, "image_name": "000000045090.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045090.jpg", "image_width": 500, "image_height": 259, "image_resolution": "500x259", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2006.044, "latencies_ms": [2006.044], "images_per_second": 0.498, "prompt_tokens": 1099, "response_tokens_est": 34, "n_tiles": 1, "output_text": " In the image, two surfers are riding the waves on their surfboards, with one of them being a woman, and the location is Raglan, New Zealand.", "error": null, "sys_before": {"cpu_percent": 16.7, "ram_used_mb": 27568.8, "ram_available_mb": 98203.4, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 27568.8, "ram_available_mb": 98203.4, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.86, "peak": 40.18, "min": 16.95}, "VIN": {"avg": 70.28, "peak": 124.04, "min": 29.1}, "VIN_SYS_5V0": {"avg": 14.8, "peak": 16.05, "min": 12.15}, "VDD_CPU_SOC_MSS": {"avg": 15.81, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 28.86, "energy_joules_est": 57.92, "sample_count": 20, "duration_seconds": 2.007}, "timestamp": "2026-01-19T15:23:01.057500"}
{"image_index": 400, "image_name": "000000045090.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045090.jpg", "image_width": 500, "image_height": 259, "image_resolution": "500x259", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2667.821, "latencies_ms": [2667.821], "images_per_second": 0.375, "prompt_tokens": 1113, "response_tokens_est": 61, "n_tiles": 1, "output_text": " 1. Surfer: 2\n2. Surfboard: 1\n3. Ocean: 1\n4. Wave: 1\n5. Bird: 1\n6. Photographer: 1\n7. Photograph: 1\n8. Frame: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27568.8, "ram_available_mb": 98203.4, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 27568.8, "ram_available_mb": 98203.4, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.87, "peak": 40.18, "min": 16.16}, "VIN": {"avg": 64.34, "peak": 103.11, "min": 29.19}, "VIN_SYS_5V0": {"avg": 14.86, "peak": 16.05, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.82, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 26.87, "energy_joules_est": 71.7, "sample_count": 26, "duration_seconds": 2.668}, "timestamp": "2026-01-19T15:23:03.763734"}
{"image_index": 400, "image_name": "000000045090.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045090.jpg", "image_width": 500, "image_height": 259, "image_resolution": "500x259", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2910.416, "latencies_ms": [2910.416], "images_per_second": 0.344, "prompt_tokens": 1117, "response_tokens_est": 71, "n_tiles": 1, "output_text": " The surfer on the left is closer to the camera than the surfer on the right. The surfer on the left is in the foreground, while the surfer on the right is in the background. The surfer on the left is riding a wave that is closer to the camera than the wave that the surfer on the right is riding.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 27568.8, "ram_available_mb": 98203.4, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 27568.8, "ram_available_mb": 98203.4, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.5, "peak": 40.56, "min": 16.16}, "VIN": {"avg": 71.01, "peak": 105.42, "min": 35.69}, "VIN_SYS_5V0": {"avg": 15.05, "peak": 16.15, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 16.03, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 26.5, "energy_joules_est": 77.14, "sample_count": 28, "duration_seconds": 2.911}, "timestamp": "2026-01-19T15:23:06.677134"}
{"image_index": 400, "image_name": "000000045090.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045090.jpg", "image_width": 500, "image_height": 259, "image_resolution": "500x259", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1547.77, "latencies_ms": [1547.77], "images_per_second": 0.646, "prompt_tokens": 1111, "response_tokens_est": 19, "n_tiles": 1, "output_text": " Two surfers are riding the waves in the ocean, with a bird flying in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27568.8, "ram_available_mb": 98203.4, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 27569.0, "ram_available_mb": 98203.1, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.72, "peak": 40.18, "min": 17.34}, "VIN": {"avg": 77.31, "peak": 121.8, "min": 29.61}, "VIN_SYS_5V0": {"avg": 15.3, "peak": 16.15, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 16.43, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 31.72, "energy_joules_est": 49.11, "sample_count": 15, "duration_seconds": 1.548}, "timestamp": "2026-01-19T15:23:08.243408"}
{"image_index": 400, "image_name": "000000045090.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045090.jpg", "image_width": 500, "image_height": 259, "image_resolution": "500x259", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2308.106, "latencies_ms": [2308.106], "images_per_second": 0.433, "prompt_tokens": 1109, "response_tokens_est": 48, "n_tiles": 1, "output_text": " The image captures a dynamic scene of two surfers riding a wave in the ocean, with the water appearing deep blue and the sky clear. The surfers are clad in black wetsuits, contrasting with the white foam of the wave.", "error": null, "sys_before": {"cpu_percent": 50.0, "ram_used_mb": 27569.0, "ram_available_mb": 98203.1, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 27569.3, "ram_available_mb": 98202.9, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.8, "peak": 40.97, "min": 18.13}, "VIN": {"avg": 66.87, "peak": 125.75, "min": 27.94}, "VIN_SYS_5V0": {"avg": 15.12, "peak": 16.15, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 16.29, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 28.8, "energy_joules_est": 66.49, "sample_count": 23, "duration_seconds": 2.309}, "timestamp": "2026-01-19T15:23:10.644712"}
{"image_index": 401, "image_name": "000000045229.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045229.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1866.635, "latencies_ms": [1866.635], "images_per_second": 0.536, "prompt_tokens": 1099, "response_tokens_est": 31, "n_tiles": 1, "output_text": " The image depicts a cozy kitchen with a window that has frosted glass, a wooden cabinet, and a small table with a potted plant on it.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 27569.3, "ram_available_mb": 98202.9, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 27569.3, "ram_available_mb": 98202.9, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.68, "peak": 40.57, "min": 15.38}, "VIN": {"avg": 63.66, "peak": 87.87, "min": 29.07}, "VIN_SYS_5V0": {"avg": 14.9, "peak": 16.05, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.92, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 29.68, "energy_joules_est": 55.41, "sample_count": 18, "duration_seconds": 1.867}, "timestamp": "2026-01-19T15:23:12.528319"}
{"image_index": 401, "image_name": "000000045229.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045229.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2511.422, "latencies_ms": [2511.422], "images_per_second": 0.398, "prompt_tokens": 1113, "response_tokens_est": 57, "n_tiles": 1, "output_text": " 1. window: 1\n2. shelf: 2\n3. cabinet: 1\n4. stove: 1\n5. pot: 1\n6. plant: 1\n7. shelf: 1\n8. cupboard: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 27569.3, "ram_available_mb": 98202.9, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 27569.3, "ram_available_mb": 98202.9, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.77, "peak": 40.18, "min": 17.73}, "VIN": {"avg": 68.6, "peak": 106.35, "min": 28.57}, "VIN_SYS_5V0": {"avg": 15.17, "peak": 16.26, "min": 13.27}, "VDD_CPU_SOC_MSS": {"avg": 16.24, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 27.77, "energy_joules_est": 69.75, "sample_count": 25, "duration_seconds": 2.512}, "timestamp": "2026-01-19T15:23:15.135441"}
{"image_index": 401, "image_name": "000000045229.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045229.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2187.476, "latencies_ms": [2187.476], "images_per_second": 0.457, "prompt_tokens": 1117, "response_tokens_est": 43, "n_tiles": 1, "output_text": " The window is located in the center of the image, with the stove on the left and the cabinet on the right. The plants are placed near the window, while the books are on the shelf in the background.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 27569.3, "ram_available_mb": 98202.9, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 27569.5, "ram_available_mb": 98202.7, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.96, "peak": 40.16, "min": 15.38}, "VIN": {"avg": 71.42, "peak": 125.71, "min": 30.47}, "VIN_SYS_5V0": {"avg": 14.8, "peak": 16.05, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.8, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 27.96, "energy_joules_est": 61.17, "sample_count": 22, "duration_seconds": 2.188}, "timestamp": "2026-01-19T15:23:17.419514"}
{"image_index": 401, "image_name": "000000045229.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045229.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3583.549, "latencies_ms": [3583.549], "images_per_second": 0.279, "prompt_tokens": 1111, "response_tokens_est": 97, "n_tiles": 1, "output_text": " The image captures a cozy kitchen scene, bathed in soft light filtering through a window adorned with a colorful paper decoration. The room is filled with an array of objects, each telling a story of its own. Shelves brimming with jars and containers, a stove waiting for its next culinary adventure, and a wooden cabinet that seems to hold secrets of its own. The overall setting is one of warmth and homeliness, inviting one to step in and explore the many stories hidden within.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 27569.5, "ram_available_mb": 98202.7, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 27569.5, "ram_available_mb": 98202.7, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 24.96, "peak": 40.18, "min": 15.77}, "VIN": {"avg": 63.55, "peak": 119.17, "min": 29.56}, "VIN_SYS_5V0": {"avg": 14.88, "peak": 16.05, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.82, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 24.96, "energy_joules_est": 89.45, "sample_count": 35, "duration_seconds": 3.584}, "timestamp": "2026-01-19T15:23:21.065687"}
{"image_index": 401, "image_name": "000000045229.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045229.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1591.168, "latencies_ms": [1591.168], "images_per_second": 0.628, "prompt_tokens": 1109, "response_tokens_est": 20, "n_tiles": 1, "output_text": " The room is dimly lit with a warm yellow light, and the window is covered with frost.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27569.5, "ram_available_mb": 98202.7, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 27569.5, "ram_available_mb": 98202.7, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.48, "peak": 40.16, "min": 15.38}, "VIN": {"avg": 75.06, "peak": 111.62, "min": 28.78}, "VIN_SYS_5V0": {"avg": 14.95, "peak": 16.05, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.97, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 30.48, "energy_joules_est": 48.51, "sample_count": 16, "duration_seconds": 1.592}, "timestamp": "2026-01-19T15:23:22.723997"}
{"image_index": 402, "image_name": "000000045472.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045472.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 3567.451, "latencies_ms": [3567.451], "images_per_second": 0.28, "prompt_tokens": 1099, "response_tokens_est": 91, "n_tiles": 1, "output_text": " In the image, a vibrant red background serves as a backdrop for a collection of objects, including a pineapple, a potted plant, a plate of oranges, and a small container with red incense sticks. The objects are arranged in a visually appealing manner, with the pineapple and plant on the left, the plate of oranges in the center, and the incense container on the right. The red background enhances the colors of the objects, creating a striking contrast.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 27569.5, "ram_available_mb": 98202.7, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 27569.5, "ram_available_mb": 98202.7, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 24.98, "peak": 40.57, "min": 16.95}, "VIN": {"avg": 69.34, "peak": 120.91, "min": 29.01}, "VIN_SYS_5V0": {"avg": 14.78, "peak": 16.05, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.79, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 24.98, "energy_joules_est": 89.13, "sample_count": 35, "duration_seconds": 3.568}, "timestamp": "2026-01-19T15:23:26.368673"}
{"image_index": 402, "image_name": "000000045472.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045472.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1952.059, "latencies_ms": [1952.059], "images_per_second": 0.512, "prompt_tokens": 1113, "response_tokens_est": 33, "n_tiles": 1, "output_text": " pineapple: 1, red cups: 4, oranges: 5, red pot: 1, incense sticks: 1, red plate: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27569.5, "ram_available_mb": 98202.7, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 27569.5, "ram_available_mb": 98202.7, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.97, "peak": 40.18, "min": 14.98}, "VIN": {"avg": 73.04, "peak": 121.65, "min": 31.4}, "VIN_SYS_5V0": {"avg": 14.81, "peak": 16.05, "min": 11.87}, "VDD_CPU_SOC_MSS": {"avg": 15.91, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 28.97, "energy_joules_est": 56.56, "sample_count": 19, "duration_seconds": 1.953}, "timestamp": "2026-01-19T15:23:28.355581"}
{"image_index": 402, "image_name": "000000045472.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045472.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2347.692, "latencies_ms": [2347.692], "images_per_second": 0.426, "prompt_tokens": 1117, "response_tokens_est": 49, "n_tiles": 1, "output_text": " The pineapple is positioned to the left of the incense holder, with the oranges placed on a white plate to the right. The red cups are arranged in a row in the foreground, with the incense holder and plate of oranges positioned in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27569.5, "ram_available_mb": 98202.7, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 27569.5, "ram_available_mb": 98202.7, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.21, "peak": 40.57, "min": 17.74}, "VIN": {"avg": 69.07, "peak": 90.67, "min": 31.34}, "VIN_SYS_5V0": {"avg": 15.05, "peak": 16.15, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 16.14, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 28.21, "energy_joules_est": 66.24, "sample_count": 23, "duration_seconds": 2.348}, "timestamp": "2026-01-19T15:23:30.758682"}
{"image_index": 402, "image_name": "000000045472.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045472.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2224.738, "latencies_ms": [2224.738], "images_per_second": 0.449, "prompt_tokens": 1111, "response_tokens_est": 44, "n_tiles": 1, "output_text": " In this image, we can see a table with some objects like a pineapple, a plate with oranges, some cups and a pot. In the background, we can see a wall with some text and some other objects.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27569.5, "ram_available_mb": 98202.7, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 27569.5, "ram_available_mb": 98202.7, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.1, "peak": 40.18, "min": 16.16}, "VIN": {"avg": 69.38, "peak": 121.61, "min": 29.64}, "VIN_SYS_5V0": {"avg": 14.92, "peak": 16.05, "min": 12.36}, "VDD_CPU_SOC_MSS": {"avg": 15.98, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 28.1, "energy_joules_est": 62.53, "sample_count": 22, "duration_seconds": 2.225}, "timestamp": "2026-01-19T15:23:33.053361"}
{"image_index": 402, "image_name": "000000045472.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045472.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2851.885, "latencies_ms": [2851.885], "images_per_second": 0.351, "prompt_tokens": 1109, "response_tokens_est": 68, "n_tiles": 1, "output_text": " The image features a vibrant display of red and orange fruits, with a pineapple and a plate of oranges prominently displayed. The lighting is warm and inviting, casting a soft glow on the objects. The materials used in the image are a mix of natural and man-made elements, with the fruits being fresh and the table being made of wood.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27569.5, "ram_available_mb": 98202.7, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 27569.5, "ram_available_mb": 98202.7, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.3, "peak": 40.18, "min": 16.16}, "VIN": {"avg": 66.74, "peak": 111.54, "min": 29.29}, "VIN_SYS_5V0": {"avg": 14.88, "peak": 16.05, "min": 12.15}, "VDD_CPU_SOC_MSS": {"avg": 15.86, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 26.3, "energy_joules_est": 75.02, "sample_count": 28, "duration_seconds": 2.852}, "timestamp": "2026-01-19T15:23:35.973695"}
{"image_index": 403, "image_name": "000000045550.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045550.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1684.0, "latencies_ms": [1684.0], "images_per_second": 0.594, "prompt_tokens": 1099, "response_tokens_est": 24, "n_tiles": 1, "output_text": " A man with a beard and glasses is eating a large piece of fried food with fries and dipping sauce on a plate.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 27569.5, "ram_available_mb": 98202.7, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 27569.7, "ram_available_mb": 98202.4, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.08, "peak": 40.57, "min": 15.38}, "VIN": {"avg": 72.37, "peak": 114.11, "min": 28.99}, "VIN_SYS_5V0": {"avg": 14.99, "peak": 16.05, "min": 12.25}, "VDD_CPU_SOC_MSS": {"avg": 15.91, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 30.08, "energy_joules_est": 50.67, "sample_count": 17, "duration_seconds": 1.684}, "timestamp": "2026-01-19T15:23:37.741478"}
{"image_index": 403, "image_name": "000000045550.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045550.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2543.974, "latencies_ms": [2543.974], "images_per_second": 0.393, "prompt_tokens": 1113, "response_tokens_est": 56, "n_tiles": 1, "output_text": " 1. plate: 1\n2. fries: 1\n3. sandwich: 1\n4. cup: 1\n5. man: 1\n6. clock: 1\n7. wall: 1\n8. table: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27569.7, "ram_available_mb": 98202.4, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 27570.0, "ram_available_mb": 98202.2, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.55, "peak": 40.57, "min": 17.74}, "VIN": {"avg": 69.87, "peak": 126.04, "min": 29.17}, "VIN_SYS_5V0": {"avg": 14.91, "peak": 16.05, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.94, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 27.55, "energy_joules_est": 70.1, "sample_count": 25, "duration_seconds": 2.544}, "timestamp": "2026-01-19T15:23:40.335714"}
{"image_index": 403, "image_name": "000000045550.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045550.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1881.296, "latencies_ms": [1881.296], "images_per_second": 0.532, "prompt_tokens": 1117, "response_tokens_est": 31, "n_tiles": 1, "output_text": " The man is in the foreground, holding a plate of food. The fries are in the middle ground, and the background shows a restaurant with other people.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 27570.0, "ram_available_mb": 98202.2, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 27570.0, "ram_available_mb": 98202.2, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.28, "peak": 40.18, "min": 16.16}, "VIN": {"avg": 72.8, "peak": 114.49, "min": 27.37}, "VIN_SYS_5V0": {"avg": 14.95, "peak": 16.15, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 16.1, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 29.28, "energy_joules_est": 55.1, "sample_count": 19, "duration_seconds": 1.882}, "timestamp": "2026-01-19T15:23:42.319986"}
{"image_index": 403, "image_name": "000000045550.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045550.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1821.669, "latencies_ms": [1821.669], "images_per_second": 0.549, "prompt_tokens": 1111, "response_tokens_est": 28, "n_tiles": 1, "output_text": " A man is eating a large piece of fried food with fries and dipping sauce. He is in a restaurant with other people in the background.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 27570.0, "ram_available_mb": 98202.2, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 27570.0, "ram_available_mb": 98202.2, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.68, "peak": 40.18, "min": 16.16}, "VIN": {"avg": 69.8, "peak": 121.67, "min": 29.91}, "VIN_SYS_5V0": {"avg": 14.77, "peak": 16.05, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.77, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 29.68, "energy_joules_est": 54.08, "sample_count": 18, "duration_seconds": 1.822}, "timestamp": "2026-01-19T15:23:44.201887"}
{"image_index": 403, "image_name": "000000045550.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045550.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2043.837, "latencies_ms": [2043.837], "images_per_second": 0.489, "prompt_tokens": 1109, "response_tokens_est": 37, "n_tiles": 1, "output_text": " The image is taken in a restaurant with a warm and inviting atmosphere. The lighting is bright and natural, illuminating the scene and highlighting the colors of the food and the man's attire.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27570.0, "ram_available_mb": 98202.2, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 27570.0, "ram_available_mb": 98202.2, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.25, "peak": 40.57, "min": 17.74}, "VIN": {"avg": 67.73, "peak": 123.81, "min": 28.35}, "VIN_SYS_5V0": {"avg": 14.94, "peak": 16.05, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.93, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 29.25, "energy_joules_est": 59.8, "sample_count": 20, "duration_seconds": 2.044}, "timestamp": "2026-01-19T15:23:46.284469"}
{"image_index": 404, "image_name": "000000045596.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045596.jpg", "image_width": 408, "image_height": 640, "image_resolution": "408x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1905.024, "latencies_ms": [1905.024], "images_per_second": 0.525, "prompt_tokens": 1100, "response_tokens_est": 32, "n_tiles": 1, "output_text": " The image captures a rainy day scene from inside a building, where a person is seen walking with an umbrella, and a bike rack is filled with bicycles outside.", "error": null, "sys_before": {"cpu_percent": 14.3, "ram_used_mb": 27570.0, "ram_available_mb": 98202.2, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 27570.0, "ram_available_mb": 98202.2, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.36, "peak": 40.16, "min": 16.95}, "VIN": {"avg": 74.8, "peak": 121.06, "min": 28.42}, "VIN_SYS_5V0": {"avg": 14.98, "peak": 16.05, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 16.08, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 29.36, "energy_joules_est": 55.95, "sample_count": 19, "duration_seconds": 1.906}, "timestamp": "2026-01-19T15:23:48.276608"}
{"image_index": 404, "image_name": "000000045596.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045596.jpg", "image_width": 408, "image_height": 640, "image_resolution": "408x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2751.27, "latencies_ms": [2751.27], "images_per_second": 0.363, "prompt_tokens": 1114, "response_tokens_est": 64, "n_tiles": 1, "output_text": " 1. Bike: 5\n2. Bike: 2\n3. Bike: 1\n4. Bike: 1\n5. Bike: 1\n6. Bike: 1\n7. Bike: 1\n8. Bike: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27570.0, "ram_available_mb": 98202.2, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 27570.0, "ram_available_mb": 98202.2, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.63, "peak": 40.18, "min": 16.56}, "VIN": {"avg": 70.6, "peak": 117.21, "min": 29.4}, "VIN_SYS_5V0": {"avg": 14.84, "peak": 16.05, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.92, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 26.63, "energy_joules_est": 73.28, "sample_count": 27, "duration_seconds": 2.752}, "timestamp": "2026-01-19T15:23:51.076406"}
{"image_index": 404, "image_name": "000000045596.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045596.jpg", "image_width": 408, "image_height": 640, "image_resolution": "408x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2035.367, "latencies_ms": [2035.367], "images_per_second": 0.491, "prompt_tokens": 1118, "response_tokens_est": 37, "n_tiles": 1, "output_text": " The bicycle is positioned in the foreground, while the person is in the background. The building is located on the left side of the image, and the sky is visible in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27570.0, "ram_available_mb": 98202.2, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 27570.0, "ram_available_mb": 98202.2, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.82, "peak": 40.18, "min": 16.16}, "VIN": {"avg": 72.78, "peak": 126.69, "min": 31.03}, "VIN_SYS_5V0": {"avg": 14.98, "peak": 16.15, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 16.12, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 28.82, "energy_joules_est": 58.67, "sample_count": 20, "duration_seconds": 2.036}, "timestamp": "2026-01-19T15:23:53.166114"}
{"image_index": 404, "image_name": "000000045596.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045596.jpg", "image_width": 408, "image_height": 640, "image_resolution": "408x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1763.856, "latencies_ms": [1763.856], "images_per_second": 0.567, "prompt_tokens": 1112, "response_tokens_est": 27, "n_tiles": 1, "output_text": " The image captures a rainy day at a modern building complex, where people are seen walking with umbrellas and bicycles are parked outside.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27570.0, "ram_available_mb": 98202.2, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 27570.0, "ram_available_mb": 98202.2, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.15, "peak": 39.78, "min": 16.95}, "VIN": {"avg": 65.18, "peak": 115.35, "min": 30.59}, "VIN_SYS_5V0": {"avg": 15.03, "peak": 16.26, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 16.09, "peak": 17.71, "min": 13.79}}, "power_watts_avg": 30.15, "energy_joules_est": 53.19, "sample_count": 17, "duration_seconds": 1.764}, "timestamp": "2026-01-19T15:23:54.944326"}
{"image_index": 404, "image_name": "000000045596.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045596.jpg", "image_width": 408, "image_height": 640, "image_resolution": "408x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1898.129, "latencies_ms": [1898.129], "images_per_second": 0.527, "prompt_tokens": 1110, "response_tokens_est": 33, "n_tiles": 1, "output_text": " The image features a rainy day with a gray sky and a wet ground. The buildings are made of glass and metal, and the bicycles are parked in a row.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 27570.0, "ram_available_mb": 98202.2, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 27570.0, "ram_available_mb": 98202.2, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.05, "peak": 40.18, "min": 19.32}, "VIN": {"avg": 78.53, "peak": 128.45, "min": 28.5}, "VIN_SYS_5V0": {"avg": 15.19, "peak": 16.26, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 16.29, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 30.05, "energy_joules_est": 57.05, "sample_count": 19, "duration_seconds": 1.898}, "timestamp": "2026-01-19T15:23:56.927715"}
{"image_index": 405, "image_name": "000000045728.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045728.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2045.858, "latencies_ms": [2045.858], "images_per_second": 0.489, "prompt_tokens": 1099, "response_tokens_est": 38, "n_tiles": 1, "output_text": " The image shows a close-up of a plate with a creamy, yellow-colored substance, possibly cheese, on top of a piece of bread, with a fork and knife placed nearby.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 27570.0, "ram_available_mb": 98202.2, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 27570.0, "ram_available_mb": 98202.2, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.06, "peak": 40.57, "min": 16.56}, "VIN": {"avg": 64.6, "peak": 115.7, "min": 30.03}, "VIN_SYS_5V0": {"avg": 14.93, "peak": 16.05, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 15.93, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 29.06, "energy_joules_est": 59.48, "sample_count": 20, "duration_seconds": 2.047}, "timestamp": "2026-01-19T15:23:59.022563"}
{"image_index": 405, "image_name": "000000045728.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045728.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1578.235, "latencies_ms": [1578.235], "images_per_second": 0.634, "prompt_tokens": 1113, "response_tokens_est": 19, "n_tiles": 1, "output_text": " fork: 1, plate: 1, cheese: 1, table: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 27570.0, "ram_available_mb": 98202.2, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 27570.0, "ram_available_mb": 98202.2, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.95, "peak": 40.56, "min": 16.95}, "VIN": {"avg": 77.79, "peak": 124.67, "min": 28.54}, "VIN_SYS_5V0": {"avg": 15.07, "peak": 16.15, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 16.07, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 30.95, "energy_joules_est": 48.86, "sample_count": 16, "duration_seconds": 1.579}, "timestamp": "2026-01-19T15:24:00.681639"}
{"image_index": 405, "image_name": "000000045728.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045728.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1863.792, "latencies_ms": [1863.792], "images_per_second": 0.537, "prompt_tokens": 1117, "response_tokens_est": 30, "n_tiles": 1, "output_text": " The fork is located in the background, behind the plate of food. The plate is in the foreground, and the food is on top of it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27570.0, "ram_available_mb": 98202.2, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 27570.0, "ram_available_mb": 98202.2, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.42, "peak": 40.57, "min": 18.53}, "VIN": {"avg": 64.58, "peak": 113.65, "min": 29.56}, "VIN_SYS_5V0": {"avg": 14.92, "peak": 16.05, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.92, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 30.42, "energy_joules_est": 56.71, "sample_count": 18, "duration_seconds": 1.864}, "timestamp": "2026-01-19T15:24:02.558345"}
{"image_index": 405, "image_name": "000000045728.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045728.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3025.629, "latencies_ms": [3025.629], "images_per_second": 0.331, "prompt_tokens": 1111, "response_tokens_est": 76, "n_tiles": 1, "output_text": " In the image, a close-up view of a plate of food is presented, with a fork and knife placed on the plate. The food appears to be a type of pasta, possibly spaghetti, with a creamy sauce coating it. The pasta is arranged in a wavy pattern, and the sauce has a glossy appearance, suggesting it might be a rich and creamy sauce.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27570.0, "ram_available_mb": 98202.2, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 27570.0, "ram_available_mb": 98202.2, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.38, "peak": 40.18, "min": 17.34}, "VIN": {"avg": 70.95, "peak": 120.63, "min": 30.32}, "VIN_SYS_5V0": {"avg": 15.1, "peak": 16.15, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 16.08, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 26.38, "energy_joules_est": 79.83, "sample_count": 30, "duration_seconds": 3.026}, "timestamp": "2026-01-19T15:24:05.680120"}
{"image_index": 405, "image_name": "000000045728.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045728.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2295.458, "latencies_ms": [2295.458], "images_per_second": 0.436, "prompt_tokens": 1109, "response_tokens_est": 47, "n_tiles": 1, "output_text": " The image features a close-up of a plate of food with a creamy yellow sauce on top, and a fork and knife on the side. The lighting is dim, and the food appears to be in a warm, inviting setting.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 27570.0, "ram_available_mb": 98202.2, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 27570.0, "ram_available_mb": 98202.2, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.48, "peak": 40.18, "min": 14.98}, "VIN": {"avg": 74.38, "peak": 131.47, "min": 28.66}, "VIN_SYS_5V0": {"avg": 14.76, "peak": 16.05, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.77, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 27.48, "energy_joules_est": 63.09, "sample_count": 23, "duration_seconds": 2.296}, "timestamp": "2026-01-19T15:24:08.080810"}
{"image_index": 406, "image_name": "000000046031.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046031.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1696.919, "latencies_ms": [1696.919], "images_per_second": 0.589, "prompt_tokens": 1099, "response_tokens_est": 24, "n_tiles": 1, "output_text": " A silver laptop sits open on a desk with a green leaf wallpaper, a black computer mouse, and a black keyboard.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 27570.0, "ram_available_mb": 98202.2, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 27570.2, "ram_available_mb": 98202.0, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.92, "peak": 40.18, "min": 15.38}, "VIN": {"avg": 73.6, "peak": 120.45, "min": 29.41}, "VIN_SYS_5V0": {"avg": 14.73, "peak": 15.95, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 15.8, "peak": 17.32, "min": 12.61}}, "power_watts_avg": 29.92, "energy_joules_est": 50.79, "sample_count": 17, "duration_seconds": 1.698}, "timestamp": "2026-01-19T15:24:09.858645"}
{"image_index": 406, "image_name": "000000046031.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046031.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1208.644, "latencies_ms": [1208.644], "images_per_second": 0.827, "prompt_tokens": 1113, "response_tokens_est": 5, "n_tiles": 1, "output_text": " computer monitor: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27570.2, "ram_available_mb": 98202.0, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 2.4, "ram_used_mb": 27570.5, "ram_available_mb": 98201.7, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.83, "peak": 40.57, "min": 18.13}, "VIN": {"avg": 76.14, "peak": 103.16, "min": 28.88}, "VIN_SYS_5V0": {"avg": 14.84, "peak": 16.05, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.95, "peak": 17.32, "min": 13.4}}, "power_watts_avg": 32.83, "energy_joules_est": 39.7, "sample_count": 12, "duration_seconds": 1.209}, "timestamp": "2026-01-19T15:24:11.117936"}
{"image_index": 406, "image_name": "000000046031.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046031.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2186.302, "latencies_ms": [2186.302], "images_per_second": 0.457, "prompt_tokens": 1117, "response_tokens_est": 41, "n_tiles": 1, "output_text": " The laptop is in the foreground, to the right of the mouse, and in front of the computer monitor. The mouse is near the laptop, and the computer monitor is to the left of the laptop.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27570.5, "ram_available_mb": 98201.7, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 27570.5, "ram_available_mb": 98201.7, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.54, "peak": 41.34, "min": 20.49}, "VIN": {"avg": 68.28, "peak": 103.98, "min": 34.63}, "VIN_SYS_5V0": {"avg": 15.04, "peak": 16.05, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 16.07, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 30.54, "energy_joules_est": 66.78, "sample_count": 21, "duration_seconds": 2.187}, "timestamp": "2026-01-19T15:24:13.309131"}
{"image_index": 406, "image_name": "000000046031.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046031.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1414.523, "latencies_ms": [1414.523], "images_per_second": 0.707, "prompt_tokens": 1111, "response_tokens_est": 14, "n_tiles": 1, "output_text": " A silver laptop sits on a desk with a computer monitor and keyboard.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27570.5, "ram_available_mb": 98201.7, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 27570.5, "ram_available_mb": 98201.7, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.3, "peak": 39.78, "min": 18.13}, "VIN": {"avg": 68.13, "peak": 83.59, "min": 28.64}, "VIN_SYS_5V0": {"avg": 15.28, "peak": 16.15, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 16.42, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 32.3, "energy_joules_est": 45.71, "sample_count": 14, "duration_seconds": 1.415}, "timestamp": "2026-01-19T15:24:14.776588"}
{"image_index": 406, "image_name": "000000046031.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046031.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1603.213, "latencies_ms": [1603.213], "images_per_second": 0.624, "prompt_tokens": 1109, "response_tokens_est": 21, "n_tiles": 1, "output_text": " The laptop is silver and the mouse is black. The room is well lit and the desk is white.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27570.5, "ram_available_mb": 98201.7, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 27558.5, "ram_available_mb": 98213.7, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.4, "peak": 41.34, "min": 21.28}, "VIN": {"avg": 74.07, "peak": 121.79, "min": 28.76}, "VIN_SYS_5V0": {"avg": 15.08, "peak": 16.15, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 16.17, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 32.4, "energy_joules_est": 51.95, "sample_count": 16, "duration_seconds": 1.604}, "timestamp": "2026-01-19T15:24:16.445885"}
{"image_index": 407, "image_name": "000000046048.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046048.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1719.54, "latencies_ms": [1719.54], "images_per_second": 0.582, "prompt_tokens": 1099, "response_tokens_est": 25, "n_tiles": 1, "output_text": " A young girl in a green shirt is sitting on a bed with a brown blanket, playing with a toy on the floor.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 27558.5, "ram_available_mb": 98213.7, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 27558.8, "ram_available_mb": 98213.4, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.05, "peak": 40.95, "min": 18.91}, "VIN": {"avg": 73.51, "peak": 106.05, "min": 28.6}, "VIN_SYS_5V0": {"avg": 14.93, "peak": 16.05, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 16.0, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 31.05, "energy_joules_est": 53.41, "sample_count": 17, "duration_seconds": 1.72}, "timestamp": "2026-01-19T15:24:18.223577"}
{"image_index": 407, "image_name": "000000046048.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046048.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2515.989, "latencies_ms": [2515.989], "images_per_second": 0.397, "prompt_tokens": 1113, "response_tokens_est": 56, "n_tiles": 1, "output_text": " 1. bed: 1\n2. girl: 1\n3. lamp: 1\n4. books: 1\n5. table: 1\n6. floor: 1\n7. wall: 1\n8. curtain: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27558.8, "ram_available_mb": 98213.4, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 27558.8, "ram_available_mb": 98213.4, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.52, "peak": 40.56, "min": 17.74}, "VIN": {"avg": 68.47, "peak": 106.93, "min": 30.21}, "VIN_SYS_5V0": {"avg": 14.96, "peak": 16.15, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.89, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 27.52, "energy_joules_est": 69.25, "sample_count": 25, "duration_seconds": 2.516}, "timestamp": "2026-01-19T15:24:20.835119"}
{"image_index": 407, "image_name": "000000046048.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046048.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2221.144, "latencies_ms": [2221.144], "images_per_second": 0.45, "prompt_tokens": 1117, "response_tokens_est": 44, "n_tiles": 1, "output_text": " The bed is positioned to the left of the room, with the girl sitting on it. The lamp is placed on the right side of the bed, and the books are scattered on the floor in front of the bed.", "error": null, "sys_before": {"cpu_percent": 40.0, "ram_used_mb": 27558.8, "ram_available_mb": 98213.4, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 27559.0, "ram_available_mb": 98213.2, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.99, "peak": 40.18, "min": 15.38}, "VIN": {"avg": 72.46, "peak": 124.46, "min": 29.08}, "VIN_SYS_5V0": {"avg": 14.8, "peak": 16.05, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.8, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 27.99, "energy_joules_est": 62.18, "sample_count": 22, "duration_seconds": 2.221}, "timestamp": "2026-01-19T15:24:23.136286"}
{"image_index": 407, "image_name": "000000046048.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046048.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1464.881, "latencies_ms": [1464.881], "images_per_second": 0.683, "prompt_tokens": 1111, "response_tokens_est": 15, "n_tiles": 1, "output_text": " A young girl is sitting on a bed in a bedroom with orange walls.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27559.0, "ram_available_mb": 98213.2, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 27559.0, "ram_available_mb": 98213.2, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.02, "peak": 40.57, "min": 15.77}, "VIN": {"avg": 75.8, "peak": 109.11, "min": 29.74}, "VIN_SYS_5V0": {"avg": 14.78, "peak": 16.05, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 15.85, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 31.02, "energy_joules_est": 45.45, "sample_count": 15, "duration_seconds": 1.465}, "timestamp": "2026-01-19T15:24:24.707166"}
{"image_index": 407, "image_name": "000000046048.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046048.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1574.059, "latencies_ms": [1574.059], "images_per_second": 0.635, "prompt_tokens": 1109, "response_tokens_est": 19, "n_tiles": 1, "output_text": " The room is painted in a warm orange color, and the floor is made of stone tiles.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27559.0, "ram_available_mb": 98213.2, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 27559.0, "ram_available_mb": 98213.2, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.52, "peak": 40.57, "min": 19.32}, "VIN": {"avg": 72.5, "peak": 123.63, "min": 29.27}, "VIN_SYS_5V0": {"avg": 14.7, "peak": 16.05, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.77, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 31.52, "energy_joules_est": 49.64, "sample_count": 16, "duration_seconds": 1.575}, "timestamp": "2026-01-19T15:24:26.373458"}
{"image_index": 408, "image_name": "000000046252.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046252.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2307.621, "latencies_ms": [2307.621], "images_per_second": 0.433, "prompt_tokens": 1099, "response_tokens_est": 48, "n_tiles": 1, "output_text": " In the image, a baseball game is in progress with a batter in a red uniform, a catcher in a gray uniform, and an umpire in a blue uniform, all positioned on a dirt field with a green grass background.", "error": null, "sys_before": {"cpu_percent": 7.1, "ram_used_mb": 27559.0, "ram_available_mb": 98213.2, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 27559.0, "ram_available_mb": 98213.2, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.27, "peak": 40.57, "min": 18.14}, "VIN": {"avg": 68.55, "peak": 105.37, "min": 27.9}, "VIN_SYS_5V0": {"avg": 14.84, "peak": 16.05, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.82, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 28.27, "energy_joules_est": 65.26, "sample_count": 23, "duration_seconds": 2.308}, "timestamp": "2026-01-19T15:24:28.775992"}
{"image_index": 408, "image_name": "000000046252.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046252.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2626.753, "latencies_ms": [2626.753], "images_per_second": 0.381, "prompt_tokens": 1113, "response_tokens_est": 60, "n_tiles": 1, "output_text": " 1. batter: 1\n2. catcher: 1\n3. umpire: 1\n4. pitcher: 1\n5. baseball: 1\n6. baseball field: 1\n7. grass: 1\n8. dirt: 1", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 27559.0, "ram_available_mb": 98213.2, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 27559.2, "ram_available_mb": 98212.9, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.81, "peak": 40.18, "min": 15.38}, "VIN": {"avg": 70.06, "peak": 126.29, "min": 27.85}, "VIN_SYS_5V0": {"avg": 14.88, "peak": 16.05, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 15.8, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 26.81, "energy_joules_est": 70.44, "sample_count": 26, "duration_seconds": 2.627}, "timestamp": "2026-01-19T15:24:31.484492"}
{"image_index": 408, "image_name": "000000046252.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046252.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2055.371, "latencies_ms": [2055.371], "images_per_second": 0.487, "prompt_tokens": 1117, "response_tokens_est": 36, "n_tiles": 1, "output_text": " The batter is positioned in the foreground, with the catcher and umpire in the background. The batter is closer to the camera than the catcher and umpire.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27559.2, "ram_available_mb": 98212.9, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 27559.2, "ram_available_mb": 98212.9, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.72, "peak": 40.18, "min": 15.38}, "VIN": {"avg": 68.0, "peak": 119.91, "min": 30.21}, "VIN_SYS_5V0": {"avg": 14.84, "peak": 16.05, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.83, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 28.72, "energy_joules_est": 59.04, "sample_count": 20, "duration_seconds": 2.056}, "timestamp": "2026-01-19T15:24:33.570892"}
{"image_index": 408, "image_name": "000000046252.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046252.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2710.586, "latencies_ms": [2710.586], "images_per_second": 0.369, "prompt_tokens": 1111, "response_tokens_est": 63, "n_tiles": 1, "output_text": " The image captures a dynamic moment in a baseball game, with the batter poised to swing at the incoming pitch, the catcher crouched behind him, and the umpire attentively observing the play. The lush green field stretches out behind them, providing a stark contrast to the brown dirt of the infield.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27559.2, "ram_available_mb": 98212.9, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 27559.2, "ram_available_mb": 98212.9, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.11, "peak": 40.57, "min": 17.35}, "VIN": {"avg": 69.88, "peak": 107.17, "min": 34.08}, "VIN_SYS_5V0": {"avg": 15.07, "peak": 16.15, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 16.07, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 27.11, "energy_joules_est": 73.49, "sample_count": 26, "duration_seconds": 2.711}, "timestamp": "2026-01-19T15:24:36.286017"}
{"image_index": 408, "image_name": "000000046252.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046252.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2353.786, "latencies_ms": [2353.786], "images_per_second": 0.425, "prompt_tokens": 1109, "response_tokens_est": 50, "n_tiles": 1, "output_text": " The image captures a vibrant scene of a baseball game, with the lush green grass of the field contrasting against the brown dirt of the infield. The sun casts a warm glow on the scene, illuminating the players and their actions with a soft light.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27559.2, "ram_available_mb": 98212.9, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 27559.2, "ram_available_mb": 98212.9, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.09, "peak": 40.18, "min": 17.34}, "VIN": {"avg": 70.3, "peak": 119.86, "min": 28.94}, "VIN_SYS_5V0": {"avg": 15.14, "peak": 16.15, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 16.26, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 28.09, "energy_joules_est": 66.12, "sample_count": 23, "duration_seconds": 2.354}, "timestamp": "2026-01-19T15:24:38.688547"}
{"image_index": 409, "image_name": "000000046378.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046378.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1375.517, "latencies_ms": [1375.517], "images_per_second": 0.727, "prompt_tokens": 1099, "response_tokens_est": 11, "n_tiles": 1, "output_text": " A cat is eating a bird on a concrete surface.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 27559.2, "ram_available_mb": 98212.9, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 27559.2, "ram_available_mb": 98212.9, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.35, "peak": 40.18, "min": 16.16}, "VIN": {"avg": 74.9, "peak": 126.67, "min": 27.48}, "VIN_SYS_5V0": {"avg": 14.86, "peak": 16.05, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 16.03, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 31.35, "energy_joules_est": 43.13, "sample_count": 14, "duration_seconds": 1.376}, "timestamp": "2026-01-19T15:24:40.155719"}
{"image_index": 409, "image_name": "000000046378.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046378.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2075.898, "latencies_ms": [2075.898], "images_per_second": 0.482, "prompt_tokens": 1113, "response_tokens_est": 38, "n_tiles": 1, "output_text": " cat: 1, bird: 1, mouse: 1, mouse head: 1, mouse body: 1, mouse tail: 1, mouse legs: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27559.2, "ram_available_mb": 98212.9, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 27559.2, "ram_available_mb": 98212.9, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.12, "peak": 40.97, "min": 20.89}, "VIN": {"avg": 69.62, "peak": 121.87, "min": 29.03}, "VIN_SYS_5V0": {"avg": 14.85, "peak": 16.05, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.96, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 30.12, "energy_joules_est": 62.53, "sample_count": 20, "duration_seconds": 2.076}, "timestamp": "2026-01-19T15:24:42.243760"}
{"image_index": 409, "image_name": "000000046378.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046378.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2113.262, "latencies_ms": [2113.262], "images_per_second": 0.473, "prompt_tokens": 1117, "response_tokens_est": 40, "n_tiles": 1, "output_text": " The bird is in the foreground, with the cat's head in the middle ground. The cat's body is partially obscured by the bird, and the bird is positioned to the right of the cat.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27559.2, "ram_available_mb": 98212.9, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 27559.2, "ram_available_mb": 98212.9, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.91, "peak": 40.57, "min": 17.74}, "VIN": {"avg": 78.96, "peak": 124.64, "min": 29.96}, "VIN_SYS_5V0": {"avg": 15.11, "peak": 16.15, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 16.25, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 28.91, "energy_joules_est": 61.1, "sample_count": 21, "duration_seconds": 2.114}, "timestamp": "2026-01-19T15:24:44.429965"}
{"image_index": 409, "image_name": "000000046378.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046378.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1379.051, "latencies_ms": [1379.051], "images_per_second": 0.725, "prompt_tokens": 1111, "response_tokens_est": 11, "n_tiles": 1, "output_text": " A cat is eating a bird on a concrete surface.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27559.2, "ram_available_mb": 98212.9, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 27559.2, "ram_available_mb": 98212.9, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.4, "peak": 40.18, "min": 16.16}, "VIN": {"avg": 75.23, "peak": 131.1, "min": 28.29}, "VIN_SYS_5V0": {"avg": 14.77, "peak": 16.05, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.89, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 31.4, "energy_joules_est": 43.31, "sample_count": 14, "duration_seconds": 1.379}, "timestamp": "2026-01-19T15:24:45.896277"}
{"image_index": 409, "image_name": "000000046378.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046378.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2291.298, "latencies_ms": [2291.298], "images_per_second": 0.436, "prompt_tokens": 1109, "response_tokens_est": 47, "n_tiles": 1, "output_text": " The image features a close-up of a cat with a bird in its mouth, the bird is red and the cat is white. The cat is on a concrete surface, and the lighting is natural, casting shadows on the ground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27559.2, "ram_available_mb": 98212.9, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 27559.2, "ram_available_mb": 98212.9, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.23, "peak": 40.97, "min": 20.49}, "VIN": {"avg": 71.84, "peak": 122.62, "min": 32.61}, "VIN_SYS_5V0": {"avg": 14.92, "peak": 16.05, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 16.02, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 29.23, "energy_joules_est": 66.98, "sample_count": 22, "duration_seconds": 2.292}, "timestamp": "2026-01-19T15:24:48.194771"}
{"image_index": 410, "image_name": "000000046463.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046463.jpg", "image_width": 500, "image_height": 400, "image_resolution": "500x400", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1904.977, "latencies_ms": [1904.977], "images_per_second": 0.525, "prompt_tokens": 1432, "response_tokens_est": 17, "n_tiles": 1, "output_text": " A person is holding a sandwich with lettuce, tomato, and cheese in their hand.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27559.2, "ram_available_mb": 98212.9, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 2.4, "ram_used_mb": 27559.2, "ram_available_mb": 98212.9, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.2, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.16, "peak": 40.57, "min": 17.35}, "VIN": {"avg": 79.72, "peak": 133.19, "min": 28.73}, "VIN_SYS_5V0": {"avg": 15.57, "peak": 16.76, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 16.76, "peak": 18.11, "min": 14.57}}, "power_watts_avg": 32.16, "energy_joules_est": 61.28, "sample_count": 19, "duration_seconds": 1.905}, "timestamp": "2026-01-19T15:24:50.185714"}
{"image_index": 410, "image_name": "000000046463.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046463.jpg", "image_width": 500, "image_height": 400, "image_resolution": "500x400", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2943.086, "latencies_ms": [2943.086], "images_per_second": 0.34, "prompt_tokens": 1446, "response_tokens_est": 56, "n_tiles": 1, "output_text": " 1. hand: 1\n2. sandwich: 1\n3. bread: 1\n4. tomato: 1\n5. lettuce: 1\n6. cheese: 1\n7. sauce: 1\n8. stove: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27559.2, "ram_available_mb": 98212.9, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 27559.2, "ram_available_mb": 98212.9, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.98, "peak": 40.56, "min": 18.91}, "VIN": {"avg": 71.46, "peak": 116.23, "min": 28.61}, "VIN_SYS_5V0": {"avg": 15.12, "peak": 16.56, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 16.2, "peak": 17.71, "min": 13.39}}, "power_watts_avg": 28.98, "energy_joules_est": 85.3, "sample_count": 29, "duration_seconds": 2.943}, "timestamp": "2026-01-19T15:24:53.197603"}
{"image_index": 410, "image_name": "000000046463.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046463.jpg", "image_width": 500, "image_height": 400, "image_resolution": "500x400", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2354.724, "latencies_ms": [2354.724], "images_per_second": 0.425, "prompt_tokens": 1450, "response_tokens_est": 34, "n_tiles": 1, "output_text": " The sandwich is held in the left hand, with the right hand holding the plate. The sandwich is in the foreground, with the stove and countertop in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27559.2, "ram_available_mb": 98212.9, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 27559.2, "ram_available_mb": 98212.9, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.49, "peak": 40.18, "min": 16.16}, "VIN": {"avg": 70.95, "peak": 117.55, "min": 29.02}, "VIN_SYS_5V0": {"avg": 15.19, "peak": 16.56, "min": 12.15}, "VDD_CPU_SOC_MSS": {"avg": 16.31, "peak": 17.71, "min": 13.39}}, "power_watts_avg": 30.49, "energy_joules_est": 71.8, "sample_count": 23, "duration_seconds": 2.355}, "timestamp": "2026-01-19T15:24:55.600068"}
{"image_index": 410, "image_name": "000000046463.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046463.jpg", "image_width": 500, "image_height": 400, "image_resolution": "500x400", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2004.077, "latencies_ms": [2004.077], "images_per_second": 0.499, "prompt_tokens": 1444, "response_tokens_est": 21, "n_tiles": 1, "output_text": " A person is holding a sandwich with lettuce and tomato on it. The sandwich is on a white plate.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 27559.2, "ram_available_mb": 98212.9, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 27559.2, "ram_available_mb": 98212.9, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.1, "peak": 40.57, "min": 17.74}, "VIN": {"avg": 77.93, "peak": 130.87, "min": 28.72}, "VIN_SYS_5V0": {"avg": 15.27, "peak": 16.66, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 16.42, "peak": 17.71, "min": 13.79}}, "power_watts_avg": 32.1, "energy_joules_est": 64.34, "sample_count": 20, "duration_seconds": 2.004}, "timestamp": "2026-01-19T15:24:57.683045"}
{"image_index": 410, "image_name": "000000046463.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046463.jpg", "image_width": 500, "image_height": 400, "image_resolution": "500x400", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1885.8, "latencies_ms": [1885.8], "images_per_second": 0.53, "prompt_tokens": 1442, "response_tokens_est": 16, "n_tiles": 1, "output_text": " The sandwich is on a white plate and the background is a white countertop.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27559.2, "ram_available_mb": 98212.9, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 2.5, "ram_used_mb": 27559.2, "ram_available_mb": 98212.9, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.51, "peak": 40.57, "min": 18.53}, "VIN": {"avg": 74.65, "peak": 126.64, "min": 27.74}, "VIN_SYS_5V0": {"avg": 15.12, "peak": 16.56, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 16.26, "peak": 17.71, "min": 13.39}}, "power_watts_avg": 32.51, "energy_joules_est": 61.31, "sample_count": 19, "duration_seconds": 1.886}, "timestamp": "2026-01-19T15:24:59.668480"}
{"image_index": 411, "image_name": "000000046497.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046497.jpg", "image_width": 500, "image_height": 332, "image_resolution": "500x332", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1783.873, "latencies_ms": [1783.873], "images_per_second": 0.561, "prompt_tokens": 1099, "response_tokens_est": 26, "n_tiles": 1, "output_text": " Two girls are sitting on the back of a boat, one of them is wearing a hat and they are looking at the water.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27559.2, "ram_available_mb": 98212.9, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 27559.2, "ram_available_mb": 98213.0, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.31, "peak": 40.56, "min": 18.92}, "VIN": {"avg": 75.39, "peak": 124.28, "min": 29.81}, "VIN_SYS_5V0": {"avg": 14.68, "peak": 15.85, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.73, "peak": 16.92, "min": 13.0}}, "power_watts_avg": 30.31, "energy_joules_est": 54.09, "sample_count": 18, "duration_seconds": 1.784}, "timestamp": "2026-01-19T15:25:01.547980"}
{"image_index": 411, "image_name": "000000046497.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046497.jpg", "image_width": 500, "image_height": 332, "image_resolution": "500x332", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2603.553, "latencies_ms": [2603.553], "images_per_second": 0.384, "prompt_tokens": 1113, "response_tokens_est": 56, "n_tiles": 1, "output_text": " 1. girl: 2\n2. hat: 1\n3. girl: 1\n4. girl: 1\n5. girl: 1\n6. girl: 1\n7. girl: 1\n8. girl: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 27559.2, "ram_available_mb": 98213.0, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 27559.2, "ram_available_mb": 98213.0, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.79, "peak": 40.18, "min": 16.95}, "VIN": {"avg": 66.41, "peak": 119.93, "min": 28.81}, "VIN_SYS_5V0": {"avg": 14.72, "peak": 15.95, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.63, "peak": 17.32, "min": 12.61}}, "power_watts_avg": 26.79, "energy_joules_est": 69.76, "sample_count": 26, "duration_seconds": 2.604}, "timestamp": "2026-01-19T15:25:04.255375"}
{"image_index": 411, "image_name": "000000046497.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046497.jpg", "image_width": 500, "image_height": 332, "image_resolution": "500x332", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2352.454, "latencies_ms": [2352.454], "images_per_second": 0.425, "prompt_tokens": 1117, "response_tokens_est": 49, "n_tiles": 1, "output_text": " The girl in the white shirt is sitting on the left side of the boat, while the girl in the pink shirt is sitting on the right side. The girl in the white shirt is closer to the camera than the girl in the pink shirt.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 27559.2, "ram_available_mb": 98213.0, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 27559.2, "ram_available_mb": 98213.0, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.58, "peak": 40.18, "min": 14.59}, "VIN": {"avg": 67.87, "peak": 116.54, "min": 29.25}, "VIN_SYS_5V0": {"avg": 14.78, "peak": 16.05, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 15.77, "peak": 17.32, "min": 12.61}}, "power_watts_avg": 27.58, "energy_joules_est": 64.89, "sample_count": 23, "duration_seconds": 2.353}, "timestamp": "2026-01-19T15:25:06.650218"}
{"image_index": 411, "image_name": "000000046497.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046497.jpg", "image_width": 500, "image_height": 332, "image_resolution": "500x332", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1517.31, "latencies_ms": [1517.31], "images_per_second": 0.659, "prompt_tokens": 1111, "response_tokens_est": 17, "n_tiles": 1, "output_text": " Two girls are sitting on the back of a boat, looking out at the ocean.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 27559.2, "ram_available_mb": 98213.0, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 27559.2, "ram_available_mb": 98213.0, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.28, "peak": 40.16, "min": 16.56}, "VIN": {"avg": 70.49, "peak": 103.53, "min": 28.76}, "VIN_SYS_5V0": {"avg": 15.02, "peak": 16.15, "min": 12.56}, "VDD_CPU_SOC_MSS": {"avg": 16.11, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 31.28, "energy_joules_est": 47.48, "sample_count": 15, "duration_seconds": 1.518}, "timestamp": "2026-01-19T15:25:08.216106"}
{"image_index": 411, "image_name": "000000046497.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046497.jpg", "image_width": 500, "image_height": 332, "image_resolution": "500x332", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1876.792, "latencies_ms": [1876.792], "images_per_second": 0.533, "prompt_tokens": 1109, "response_tokens_est": 31, "n_tiles": 1, "output_text": " The image features a boat with a blue ocean in the background, and the sky is clear. The boat is white and has a gray cushion on it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27559.2, "ram_available_mb": 98213.0, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 27559.2, "ram_available_mb": 98213.0, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.44, "peak": 40.95, "min": 19.31}, "VIN": {"avg": 74.13, "peak": 107.9, "min": 28.59}, "VIN_SYS_5V0": {"avg": 14.99, "peak": 16.05, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 16.04, "peak": 17.32, "min": 13.78}}, "power_watts_avg": 30.44, "energy_joules_est": 57.14, "sample_count": 19, "duration_seconds": 1.877}, "timestamp": "2026-01-19T15:25:10.195001"}
{"image_index": 412, "image_name": "000000046804.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046804.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1876.183, "latencies_ms": [1876.183], "images_per_second": 0.533, "prompt_tokens": 1099, "response_tokens_est": 31, "n_tiles": 1, "output_text": " A sheep with white wool stands in a grassy field, its head turned towards the camera, with a stone wall and yellow-green moss in the background.", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 27559.2, "ram_available_mb": 98213.0, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 27559.2, "ram_available_mb": 98213.0, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.88, "peak": 40.56, "min": 16.16}, "VIN": {"avg": 74.79, "peak": 108.32, "min": 36.78}, "VIN_SYS_5V0": {"avg": 14.84, "peak": 15.95, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.81, "peak": 17.32, "min": 12.61}}, "power_watts_avg": 29.88, "energy_joules_est": 56.08, "sample_count": 18, "duration_seconds": 1.877}, "timestamp": "2026-01-19T15:25:12.084472"}
{"image_index": 412, "image_name": "000000046804.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046804.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1151.52, "latencies_ms": [1151.52], "images_per_second": 0.868, "prompt_tokens": 1113, "response_tokens_est": 4, "n_tiles": 1, "output_text": " sheep: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27559.2, "ram_available_mb": 98213.0, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 1.8, "ram_used_mb": 27559.2, "ram_available_mb": 98213.0, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 33.52, "peak": 40.18, "min": 19.7}, "VIN": {"avg": 74.82, "peak": 104.18, "min": 30.25}, "VIN_SYS_5V0": {"avg": 15.26, "peak": 16.26, "min": 13.39}, "VDD_CPU_SOC_MSS": {"avg": 16.44, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 33.52, "energy_joules_est": 38.61, "sample_count": 12, "duration_seconds": 1.152}, "timestamp": "2026-01-19T15:25:13.339801"}
{"image_index": 412, "image_name": "000000046804.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046804.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1866.044, "latencies_ms": [1866.044], "images_per_second": 0.536, "prompt_tokens": 1117, "response_tokens_est": 30, "n_tiles": 1, "output_text": " The sheep is in the foreground, standing in front of the stone wall. The wall is behind the sheep, and the grass is in the foreground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27559.2, "ram_available_mb": 98213.0, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 27559.2, "ram_available_mb": 98213.0, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.91, "peak": 41.34, "min": 22.07}, "VIN": {"avg": 74.98, "peak": 125.19, "min": 30.22}, "VIN_SYS_5V0": {"avg": 14.88, "peak": 16.05, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.97, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 31.91, "energy_joules_est": 59.56, "sample_count": 18, "duration_seconds": 1.867}, "timestamp": "2026-01-19T15:25:15.225041"}
{"image_index": 412, "image_name": "000000046804.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046804.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1444.513, "latencies_ms": [1444.513], "images_per_second": 0.692, "prompt_tokens": 1111, "response_tokens_est": 15, "n_tiles": 1, "output_text": " A sheep stands in a grassy field with a stone wall in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27559.2, "ram_available_mb": 98213.0, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 27559.5, "ram_available_mb": 98212.7, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.76, "peak": 40.56, "min": 18.91}, "VIN": {"avg": 76.58, "peak": 126.84, "min": 30.56}, "VIN_SYS_5V0": {"avg": 15.27, "peak": 16.15, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 16.37, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 32.76, "energy_joules_est": 47.33, "sample_count": 14, "duration_seconds": 1.445}, "timestamp": "2026-01-19T15:25:16.684055"}
{"image_index": 412, "image_name": "000000046804.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046804.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1464.912, "latencies_ms": [1464.912], "images_per_second": 0.683, "prompt_tokens": 1109, "response_tokens_est": 16, "n_tiles": 1, "output_text": " The sheep is white, the grass is green, and the wall is gray.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 27559.5, "ram_available_mb": 98212.7, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 27559.7, "ram_available_mb": 98212.5, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 33.93, "peak": 40.57, "min": 23.25}, "VIN": {"avg": 76.1, "peak": 103.66, "min": 55.66}, "VIN_SYS_5V0": {"avg": 15.42, "peak": 16.26, "min": 13.39}, "VDD_CPU_SOC_MSS": {"avg": 16.51, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 33.93, "energy_joules_est": 49.72, "sample_count": 14, "duration_seconds": 1.465}, "timestamp": "2026-01-19T15:25:18.153865"}
{"image_index": 413, "image_name": "000000046872.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046872.jpg", "image_width": 640, "image_height": 391, "image_resolution": "640x391", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2103.753, "latencies_ms": [2103.753], "images_per_second": 0.475, "prompt_tokens": 1099, "response_tokens_est": 39, "n_tiles": 1, "output_text": " A man in a white shirt and plaid shorts is standing next to a truck with a large cylindrical object on it, while another man in a blue shirt is standing on top of the truck.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 27559.7, "ram_available_mb": 98212.5, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 27559.7, "ram_available_mb": 98212.5, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.9, "peak": 40.56, "min": 18.52}, "VIN": {"avg": 70.59, "peak": 126.01, "min": 28.76}, "VIN_SYS_5V0": {"avg": 15.13, "peak": 16.26, "min": 13.29}, "VDD_CPU_SOC_MSS": {"avg": 16.16, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 29.9, "energy_joules_est": 62.93, "sample_count": 21, "duration_seconds": 2.105}, "timestamp": "2026-01-19T15:25:20.344331"}
{"image_index": 413, "image_name": "000000046872.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046872.jpg", "image_width": 640, "image_height": 391, "image_resolution": "640x391", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2747.152, "latencies_ms": [2747.152], "images_per_second": 0.364, "prompt_tokens": 1113, "response_tokens_est": 64, "n_tiles": 1, "output_text": " 1. truck: 1\n2. man: 2\n3. man on truck: 1\n4. man on ground: 1\n5. container: 1\n6. pipe: 1\n7. container on truck: 1\n8. container on ground: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27559.7, "ram_available_mb": 98212.5, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 27559.7, "ram_available_mb": 98212.5, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.62, "peak": 40.18, "min": 16.16}, "VIN": {"avg": 70.6, "peak": 116.58, "min": 29.24}, "VIN_SYS_5V0": {"avg": 14.82, "peak": 16.05, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.76, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 26.62, "energy_joules_est": 73.14, "sample_count": 27, "duration_seconds": 2.748}, "timestamp": "2026-01-19T15:25:23.143654"}
{"image_index": 413, "image_name": "000000046872.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046872.jpg", "image_width": 640, "image_height": 391, "image_resolution": "640x391", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2300.235, "latencies_ms": [2300.235], "images_per_second": 0.435, "prompt_tokens": 1117, "response_tokens_est": 47, "n_tiles": 1, "output_text": " The man in the white shirt is standing in the foreground, while the man in the blue shirt is standing on the truck in the background. The man in the white shirt is closer to the camera than the man in the blue shirt.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27559.7, "ram_available_mb": 98212.5, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 27559.7, "ram_available_mb": 98212.5, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.71, "peak": 40.16, "min": 16.16}, "VIN": {"avg": 71.63, "peak": 129.16, "min": 28.1}, "VIN_SYS_5V0": {"avg": 14.89, "peak": 16.05, "min": 12.36}, "VDD_CPU_SOC_MSS": {"avg": 15.87, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 27.71, "energy_joules_est": 63.75, "sample_count": 23, "duration_seconds": 2.301}, "timestamp": "2026-01-19T15:25:25.542332"}
{"image_index": 413, "image_name": "000000046872.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046872.jpg", "image_width": 640, "image_height": 391, "image_resolution": "640x391", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2125.627, "latencies_ms": [2125.627], "images_per_second": 0.47, "prompt_tokens": 1111, "response_tokens_est": 40, "n_tiles": 1, "output_text": " A man in a white shirt and plaid shorts is standing next to a truck with a large cylindrical object on it. The truck is parked in a parking lot, surrounded by trees and other buildings.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27559.7, "ram_available_mb": 98212.5, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 27559.7, "ram_available_mb": 98212.5, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.22, "peak": 40.16, "min": 15.38}, "VIN": {"avg": 70.87, "peak": 120.7, "min": 28.59}, "VIN_SYS_5V0": {"avg": 14.76, "peak": 15.95, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.73, "peak": 17.32, "min": 12.61}}, "power_watts_avg": 28.22, "energy_joules_est": 60.01, "sample_count": 21, "duration_seconds": 2.126}, "timestamp": "2026-01-19T15:25:27.739617"}
{"image_index": 413, "image_name": "000000046872.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046872.jpg", "image_width": 640, "image_height": 391, "image_resolution": "640x391", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1781.742, "latencies_ms": [1781.742], "images_per_second": 0.561, "prompt_tokens": 1109, "response_tokens_est": 27, "n_tiles": 1, "output_text": " The image is taken in a cloudy day with a man standing on a truck. The man is wearing a white shirt and blue shorts.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27559.7, "ram_available_mb": 98212.5, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 27559.7, "ram_available_mb": 98212.5, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.77, "peak": 40.56, "min": 16.56}, "VIN": {"avg": 71.27, "peak": 125.62, "min": 27.31}, "VIN_SYS_5V0": {"avg": 14.82, "peak": 16.05, "min": 12.15}, "VDD_CPU_SOC_MSS": {"avg": 15.92, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 29.77, "energy_joules_est": 53.05, "sample_count": 18, "duration_seconds": 1.782}, "timestamp": "2026-01-19T15:25:29.610017"}
{"image_index": 414, "image_name": "000000047010.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047010.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1669.936, "latencies_ms": [1669.936], "images_per_second": 0.599, "prompt_tokens": 1099, "response_tokens_est": 23, "n_tiles": 1, "output_text": " A group of giraffes is walking in a line across a dirt path, with a pond and trees in the background.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 27559.7, "ram_available_mb": 98212.5, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 27559.7, "ram_available_mb": 98212.5, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.41, "peak": 40.57, "min": 16.95}, "VIN": {"avg": 70.82, "peak": 104.5, "min": 27.14}, "VIN_SYS_5V0": {"avg": 14.77, "peak": 16.05, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.84, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 30.41, "energy_joules_est": 50.81, "sample_count": 17, "duration_seconds": 1.671}, "timestamp": "2026-01-19T15:25:31.389886"}
{"image_index": 414, "image_name": "000000047010.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047010.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2594.256, "latencies_ms": [2594.256], "images_per_second": 0.385, "prompt_tokens": 1113, "response_tokens_est": 56, "n_tiles": 1, "output_text": " 1. giraffe: 4\n2. giraffe: 1\n3. giraffe: 1\n4. giraffe: 1\n5. giraffe: 1\n6. giraffe: 1\n7. giraffe: 1\n8. giraffe: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27559.7, "ram_available_mb": 98212.5, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 27559.7, "ram_available_mb": 98212.5, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.31, "peak": 40.18, "min": 17.35}, "VIN": {"avg": 71.28, "peak": 130.38, "min": 33.44}, "VIN_SYS_5V0": {"avg": 14.82, "peak": 15.95, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.67, "peak": 16.92, "min": 12.61}}, "power_watts_avg": 27.31, "energy_joules_est": 70.86, "sample_count": 25, "duration_seconds": 2.595}, "timestamp": "2026-01-19T15:25:33.992903"}
{"image_index": 414, "image_name": "000000047010.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047010.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2032.262, "latencies_ms": [2032.262], "images_per_second": 0.492, "prompt_tokens": 1117, "response_tokens_est": 38, "n_tiles": 1, "output_text": " The giraffes are positioned in the foreground of the image, with the pond and trees in the background. The giraffes are walking towards the pond, which is located to the left of the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27559.7, "ram_available_mb": 98212.5, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 27559.7, "ram_available_mb": 98212.5, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.33, "peak": 40.16, "min": 17.73}, "VIN": {"avg": 73.08, "peak": 108.77, "min": 28.61}, "VIN_SYS_5V0": {"avg": 15.23, "peak": 16.15, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 16.14, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 29.33, "energy_joules_est": 59.61, "sample_count": 20, "duration_seconds": 2.033}, "timestamp": "2026-01-19T15:25:36.074739"}
{"image_index": 414, "image_name": "000000047010.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047010.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1717.704, "latencies_ms": [1717.704], "images_per_second": 0.582, "prompt_tokens": 1111, "response_tokens_est": 25, "n_tiles": 1, "output_text": " A group of giraffes are walking in a line through a dirt path near a pond, with trees and bushes in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27559.7, "ram_available_mb": 98212.5, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 27559.7, "ram_available_mb": 98212.5, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.5, "peak": 40.56, "min": 16.95}, "VIN": {"avg": 74.95, "peak": 113.36, "min": 29.99}, "VIN_SYS_5V0": {"avg": 15.02, "peak": 16.15, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 16.16, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 30.5, "energy_joules_est": 52.41, "sample_count": 17, "duration_seconds": 1.718}, "timestamp": "2026-01-19T15:25:37.847444"}
{"image_index": 414, "image_name": "000000047010.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047010.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1538.083, "latencies_ms": [1538.083], "images_per_second": 0.65, "prompt_tokens": 1109, "response_tokens_est": 18, "n_tiles": 1, "output_text": " The giraffes are brown and white, the trees are green, and the sky is cloudy.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 27559.7, "ram_available_mb": 98212.5, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 27559.7, "ram_available_mb": 98212.5, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.91, "peak": 40.56, "min": 18.53}, "VIN": {"avg": 67.74, "peak": 93.65, "min": 29.36}, "VIN_SYS_5V0": {"avg": 15.05, "peak": 16.15, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 16.09, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 31.91, "energy_joules_est": 49.1, "sample_count": 15, "duration_seconds": 1.539}, "timestamp": "2026-01-19T15:25:39.416650"}
{"image_index": 415, "image_name": "000000047112.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047112.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1647.164, "latencies_ms": [1647.164], "images_per_second": 0.607, "prompt_tokens": 1099, "response_tokens_est": 23, "n_tiles": 1, "output_text": " A pizza with various toppings is on a plate, with a glass of beer and a glass of water nearby.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27559.7, "ram_available_mb": 98212.5, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 27559.7, "ram_available_mb": 98212.5, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.21, "peak": 40.97, "min": 20.89}, "VIN": {"avg": 81.55, "peak": 130.48, "min": 29.91}, "VIN_SYS_5V0": {"avg": 15.19, "peak": 16.15, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 16.27, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 32.21, "energy_joules_est": 53.08, "sample_count": 16, "duration_seconds": 1.648}, "timestamp": "2026-01-19T15:25:41.084558"}
{"image_index": 415, "image_name": "000000047112.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047112.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1828.555, "latencies_ms": [1828.555], "images_per_second": 0.547, "prompt_tokens": 1113, "response_tokens_est": 30, "n_tiles": 1, "output_text": " pizza: 1, wine: 2, glasses: 2, tablecloth: 1, chair: 1, person: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27559.7, "ram_available_mb": 98212.5, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 27559.7, "ram_available_mb": 98212.5, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.03, "peak": 40.57, "min": 20.49}, "VIN": {"avg": 74.9, "peak": 126.65, "min": 30.32}, "VIN_SYS_5V0": {"avg": 15.29, "peak": 16.15, "min": 13.29}, "VDD_CPU_SOC_MSS": {"avg": 16.29, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 31.03, "energy_joules_est": 56.75, "sample_count": 18, "duration_seconds": 1.829}, "timestamp": "2026-01-19T15:25:42.960238"}
{"image_index": 415, "image_name": "000000047112.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047112.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2018.93, "latencies_ms": [2018.93], "images_per_second": 0.495, "prompt_tokens": 1117, "response_tokens_est": 37, "n_tiles": 1, "output_text": " The pizza is positioned in the foreground, with the glasses of beer placed behind it. The glasses of beer are located in the middle ground, with the table and chairs in the background.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 27559.7, "ram_available_mb": 98212.5, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 27559.7, "ram_available_mb": 98212.5, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.43, "peak": 40.56, "min": 18.13}, "VIN": {"avg": 69.39, "peak": 121.72, "min": 29.27}, "VIN_SYS_5V0": {"avg": 15.03, "peak": 16.15, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 16.12, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 29.43, "energy_joules_est": 59.43, "sample_count": 20, "duration_seconds": 2.019}, "timestamp": "2026-01-19T15:25:45.046197"}
{"image_index": 415, "image_name": "000000047112.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047112.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1743.248, "latencies_ms": [1743.248], "images_per_second": 0.574, "prompt_tokens": 1111, "response_tokens_est": 26, "n_tiles": 1, "output_text": " In a cozy restaurant, a delicious pizza with various toppings is served on a white plate, accompanied by two glasses of beer.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27559.7, "ram_available_mb": 98212.5, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 27559.9, "ram_available_mb": 98212.2, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.54, "peak": 40.57, "min": 16.95}, "VIN": {"avg": 72.01, "peak": 105.83, "min": 29.51}, "VIN_SYS_5V0": {"avg": 14.96, "peak": 16.05, "min": 12.25}, "VDD_CPU_SOC_MSS": {"avg": 16.02, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 30.54, "energy_joules_est": 53.25, "sample_count": 17, "duration_seconds": 1.744}, "timestamp": "2026-01-19T15:25:46.817010"}
{"image_index": 415, "image_name": "000000047112.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047112.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2831.026, "latencies_ms": [2831.026], "images_per_second": 0.353, "prompt_tokens": 1109, "response_tokens_est": 69, "n_tiles": 1, "output_text": " The image features a pizza with a variety of toppings, including mushrooms, pineapple, and ham, placed on a white plate. The pizza is placed on a table covered with a blue tablecloth, and there are two glasses of beer in the background. The lighting in the room is warm, and the overall atmosphere suggests a casual dining experience.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27559.9, "ram_available_mb": 98212.2, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 27559.9, "ram_available_mb": 98212.2, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.93, "peak": 40.18, "min": 17.73}, "VIN": {"avg": 69.9, "peak": 130.29, "min": 30.32}, "VIN_SYS_5V0": {"avg": 15.06, "peak": 16.15, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 16.05, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 26.93, "energy_joules_est": 76.25, "sample_count": 28, "duration_seconds": 2.831}, "timestamp": "2026-01-19T15:25:49.735013"}
{"image_index": 416, "image_name": "000000047121.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047121.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1495.302, "latencies_ms": [1495.302], "images_per_second": 0.669, "prompt_tokens": 1099, "response_tokens_est": 16, "n_tiles": 1, "output_text": " A black and white cat is drinking water from a faucet in a bathroom sink.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27559.9, "ram_available_mb": 98212.2, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 27559.9, "ram_available_mb": 98212.2, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.81, "peak": 40.18, "min": 14.98}, "VIN": {"avg": 76.25, "peak": 128.95, "min": 29.12}, "VIN_SYS_5V0": {"avg": 14.75, "peak": 16.05, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.85, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 30.81, "energy_joules_est": 46.09, "sample_count": 15, "duration_seconds": 1.496}, "timestamp": "2026-01-19T15:25:51.308350"}
{"image_index": 416, "image_name": "000000047121.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047121.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2672.072, "latencies_ms": [2672.072], "images_per_second": 0.374, "prompt_tokens": 1113, "response_tokens_est": 62, "n_tiles": 1, "output_text": " 1. black cat: 1\n2. sink: 1\n3. faucet: 1\n4. soap bottle: 1\n5. water: 1\n6. white plate: 1\n7. white wall: 1\n8. white baseboard: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27559.9, "ram_available_mb": 98212.2, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 27559.9, "ram_available_mb": 98212.2, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.58, "peak": 40.97, "min": 18.91}, "VIN": {"avg": 65.65, "peak": 106.38, "min": 29.37}, "VIN_SYS_5V0": {"avg": 14.95, "peak": 16.05, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.93, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 27.58, "energy_joules_est": 73.71, "sample_count": 26, "duration_seconds": 2.673}, "timestamp": "2026-01-19T15:25:54.015639"}
{"image_index": 416, "image_name": "000000047121.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047121.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2281.672, "latencies_ms": [2281.672], "images_per_second": 0.438, "prompt_tokens": 1117, "response_tokens_est": 47, "n_tiles": 1, "output_text": " The cat is positioned to the right of the faucet, with its head bent down towards the water. The sink is located in the foreground, while the bottle of soap is situated in the background, slightly to the left of the faucet.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27559.9, "ram_available_mb": 98212.2, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 27559.9, "ram_available_mb": 98212.3, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.28, "peak": 40.16, "min": 16.16}, "VIN": {"avg": 75.52, "peak": 124.31, "min": 44.82}, "VIN_SYS_5V0": {"avg": 15.12, "peak": 16.15, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 16.09, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 28.28, "energy_joules_est": 64.53, "sample_count": 22, "duration_seconds": 2.282}, "timestamp": "2026-01-19T15:25:56.302484"}
{"image_index": 416, "image_name": "000000047121.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047121.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1454.781, "latencies_ms": [1454.781], "images_per_second": 0.687, "prompt_tokens": 1111, "response_tokens_est": 16, "n_tiles": 1, "output_text": " A black and white cat is drinking water from a faucet in a bathroom sink.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27559.9, "ram_available_mb": 98212.3, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 27559.9, "ram_available_mb": 98212.3, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.44, "peak": 39.77, "min": 18.91}, "VIN": {"avg": 70.26, "peak": 84.76, "min": 38.96}, "VIN_SYS_5V0": {"avg": 15.45, "peak": 16.26, "min": 13.49}, "VDD_CPU_SOC_MSS": {"avg": 16.53, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 32.44, "energy_joules_est": 47.22, "sample_count": 14, "duration_seconds": 1.455}, "timestamp": "2026-01-19T15:25:57.767967"}
{"image_index": 416, "image_name": "000000047121.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047121.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1752.941, "latencies_ms": [1752.941], "images_per_second": 0.57, "prompt_tokens": 1109, "response_tokens_est": 27, "n_tiles": 1, "output_text": " The image features a black and white cat drinking water from a faucet in a bathroom. The faucet is silver and the sink is white.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27559.9, "ram_available_mb": 98212.3, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 27559.9, "ram_available_mb": 98212.3, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.19, "peak": 40.97, "min": 22.46}, "VIN": {"avg": 71.51, "peak": 119.76, "min": 29.57}, "VIN_SYS_5V0": {"avg": 15.35, "peak": 16.26, "min": 13.49}, "VDD_CPU_SOC_MSS": {"avg": 16.42, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 32.19, "energy_joules_est": 56.44, "sample_count": 17, "duration_seconds": 1.753}, "timestamp": "2026-01-19T15:25:59.536699"}
{"image_index": 417, "image_name": "000000047571.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047571.jpg", "image_width": 639, "image_height": 640, "image_resolution": "639x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2709.541, "latencies_ms": [2709.541], "images_per_second": 0.369, "prompt_tokens": 1432, "response_tokens_est": 45, "n_tiles": 1, "output_text": " In the image, a man wearing a hat and a woman wearing a hat are riding in a horse-drawn carriage, with the horse walking in a muddy field, and the carriage is reflected in a puddle of water.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27559.9, "ram_available_mb": 98212.3, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 27559.9, "ram_available_mb": 98212.3, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.2, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.17, "peak": 40.18, "min": 17.74}, "VIN": {"avg": 69.66, "peak": 127.63, "min": 27.08}, "VIN_SYS_5V0": {"avg": 15.25, "peak": 16.66, "min": 12.96}, "VDD_CPU_SOC_MSS": {"avg": 16.24, "peak": 17.71, "min": 13.79}}, "power_watts_avg": 29.17, "energy_joules_est": 79.05, "sample_count": 27, "duration_seconds": 2.71}, "timestamp": "2026-01-19T15:26:02.359335"}
{"image_index": 417, "image_name": "000000047571.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047571.jpg", "image_width": 639, "image_height": 640, "image_resolution": "639x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3232.364, "latencies_ms": [3232.364], "images_per_second": 0.309, "prompt_tokens": 1446, "response_tokens_est": 66, "n_tiles": 1, "output_text": " 1. horse: 1\n2. carriage: 1\n3. man: 2\n4. man's hat: 1\n5. man's shirt: 1\n6. man's pants: 1\n7. man's boots: 1\n8. man's belt: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27559.9, "ram_available_mb": 98212.3, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 27559.9, "ram_available_mb": 98212.3, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.49, "peak": 39.78, "min": 15.38}, "VIN": {"avg": 69.83, "peak": 119.28, "min": 28.82}, "VIN_SYS_5V0": {"avg": 14.97, "peak": 16.46, "min": 11.46}, "VDD_CPU_SOC_MSS": {"avg": 15.98, "peak": 17.71, "min": 12.61}}, "power_watts_avg": 27.49, "energy_joules_est": 88.87, "sample_count": 32, "duration_seconds": 3.233}, "timestamp": "2026-01-19T15:26:05.690803"}
{"image_index": 417, "image_name": "000000047571.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047571.jpg", "image_width": 639, "image_height": 640, "image_resolution": "639x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3012.603, "latencies_ms": [3012.603], "images_per_second": 0.332, "prompt_tokens": 1450, "response_tokens_est": 58, "n_tiles": 1, "output_text": " The horse is in the foreground, pulling the carriage, while the man is in the background. The man is wearing a hat, and the horse is wearing a bridle. The carriage is in the middle of the image, and the man is sitting on the left side of the carriage.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27559.9, "ram_available_mb": 98212.3, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 27559.9, "ram_available_mb": 98212.3, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.08, "peak": 40.18, "min": 15.38}, "VIN": {"avg": 71.77, "peak": 132.18, "min": 27.72}, "VIN_SYS_5V0": {"avg": 15.01, "peak": 16.46, "min": 11.54}, "VDD_CPU_SOC_MSS": {"avg": 16.01, "peak": 17.71, "min": 12.61}}, "power_watts_avg": 28.08, "energy_joules_est": 84.61, "sample_count": 30, "duration_seconds": 3.013}, "timestamp": "2026-01-19T15:26:08.800457"}
{"image_index": 417, "image_name": "000000047571.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047571.jpg", "image_width": 639, "image_height": 640, "image_resolution": "639x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2051.112, "latencies_ms": [2051.112], "images_per_second": 0.488, "prompt_tokens": 1444, "response_tokens_est": 22, "n_tiles": 1, "output_text": " A man and woman are riding in a horse-drawn carriage, with a large puddle reflecting the scene.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 27559.9, "ram_available_mb": 98212.3, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 27559.9, "ram_available_mb": 98212.3, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.42, "peak": 39.78, "min": 15.38}, "VIN": {"avg": 75.12, "peak": 131.2, "min": 30.34}, "VIN_SYS_5V0": {"avg": 15.09, "peak": 16.46, "min": 11.66}, "VDD_CPU_SOC_MSS": {"avg": 16.26, "peak": 17.71, "min": 13.0}}, "power_watts_avg": 31.42, "energy_joules_est": 64.45, "sample_count": 20, "duration_seconds": 2.051}, "timestamp": "2026-01-19T15:26:10.882982"}
{"image_index": 417, "image_name": "000000047571.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047571.jpg", "image_width": 639, "image_height": 640, "image_resolution": "639x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2460.514, "latencies_ms": [2460.514], "images_per_second": 0.406, "prompt_tokens": 1442, "response_tokens_est": 38, "n_tiles": 1, "output_text": " The image features a horse-drawn carriage with two people riding in it, with the horse's reflection visible in the water. The sky is clear and blue, and the ground is muddy.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27559.9, "ram_available_mb": 98212.3, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 27559.9, "ram_available_mb": 98212.3, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.79, "peak": 40.56, "min": 19.7}, "VIN": {"avg": 72.91, "peak": 116.1, "min": 29.03}, "VIN_SYS_5V0": {"avg": 15.33, "peak": 16.66, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 16.57, "peak": 17.71, "min": 14.18}}, "power_watts_avg": 30.79, "energy_joules_est": 75.78, "sample_count": 24, "duration_seconds": 2.461}, "timestamp": "2026-01-19T15:26:13.381378"}
{"image_index": 418, "image_name": "000000047585.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047585.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1605.372, "latencies_ms": [1605.372], "images_per_second": 0.623, "prompt_tokens": 1100, "response_tokens_est": 21, "n_tiles": 1, "output_text": " A man and woman are standing together in a grassy area, with the man holding an umbrella over them.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27559.9, "ram_available_mb": 98212.3, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 27559.9, "ram_available_mb": 98212.3, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.87, "peak": 40.16, "min": 17.35}, "VIN": {"avg": 66.23, "peak": 100.14, "min": 29.92}, "VIN_SYS_5V0": {"avg": 15.04, "peak": 16.26, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 16.17, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 30.87, "energy_joules_est": 49.58, "sample_count": 16, "duration_seconds": 1.606}, "timestamp": "2026-01-19T15:26:15.057034"}
{"image_index": 418, "image_name": "000000047585.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047585.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2515.844, "latencies_ms": [2515.844], "images_per_second": 0.397, "prompt_tokens": 1114, "response_tokens_est": 56, "n_tiles": 1, "output_text": " 1. umbrella: 1\n2. bride: 1\n3. groom: 1\n4. woman: 1\n5. man: 1\n6. house: 1\n7. grass: 1\n8. flowers: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27559.9, "ram_available_mb": 98212.3, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 27559.9, "ram_available_mb": 98212.3, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.66, "peak": 40.57, "min": 17.73}, "VIN": {"avg": 68.73, "peak": 115.35, "min": 27.27}, "VIN_SYS_5V0": {"avg": 14.94, "peak": 16.15, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 16.06, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 27.66, "energy_joules_est": 69.6, "sample_count": 25, "duration_seconds": 2.516}, "timestamp": "2026-01-19T15:26:17.660766"}
{"image_index": 418, "image_name": "000000047585.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047585.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2185.133, "latencies_ms": [2185.133], "images_per_second": 0.458, "prompt_tokens": 1118, "response_tokens_est": 38, "n_tiles": 1, "output_text": " The bride is standing to the left of the groom, with the umbrella held by the groom. The bride is in the foreground, while the groom and the other guests are in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27559.9, "ram_available_mb": 98212.3, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 27559.9, "ram_available_mb": 98212.3, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.44, "peak": 40.16, "min": 15.38}, "VIN": {"avg": 70.66, "peak": 120.51, "min": 28.08}, "VIN_SYS_5V0": {"avg": 14.59, "peak": 16.05, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.66, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 27.44, "energy_joules_est": 59.97, "sample_count": 22, "duration_seconds": 2.185}, "timestamp": "2026-01-19T15:26:19.949890"}
{"image_index": 418, "image_name": "000000047585.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047585.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2339.359, "latencies_ms": [2339.359], "images_per_second": 0.427, "prompt_tokens": 1112, "response_tokens_est": 47, "n_tiles": 1, "output_text": " A newlywed couple is standing on a grassy lawn, holding hands and smiling at the camera. The bride is wearing a white wedding dress and holding a bouquet of flowers. The groom is wearing a black suit and holding an umbrella.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27559.9, "ram_available_mb": 98212.3, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 27559.9, "ram_available_mb": 98212.3, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.05, "peak": 39.39, "min": 14.59}, "VIN": {"avg": 71.83, "peak": 125.08, "min": 28.93}, "VIN_SYS_5V0": {"avg": 14.7, "peak": 15.95, "min": 11.34}, "VDD_CPU_SOC_MSS": {"avg": 15.72, "peak": 17.32, "min": 12.61}}, "power_watts_avg": 27.05, "energy_joules_est": 63.29, "sample_count": 23, "duration_seconds": 2.34}, "timestamp": "2026-01-19T15:26:22.344228"}
{"image_index": 418, "image_name": "000000047585.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047585.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2278.72, "latencies_ms": [2278.72], "images_per_second": 0.439, "prompt_tokens": 1110, "response_tokens_est": 47, "n_tiles": 1, "output_text": " The image features a couple standing under a black and white umbrella, with the bride wearing a white dress and the groom in a black suit. The weather appears to be rainy, as the umbrella is open to shield them from the rain.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27559.9, "ram_available_mb": 98212.3, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 27560.2, "ram_available_mb": 98212.0, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.2, "peak": 40.16, "min": 16.16}, "VIN": {"avg": 65.17, "peak": 104.93, "min": 29.39}, "VIN_SYS_5V0": {"avg": 15.01, "peak": 16.15, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.98, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 28.2, "energy_joules_est": 64.27, "sample_count": 22, "duration_seconds": 2.279}, "timestamp": "2026-01-19T15:26:24.641169"}
{"image_index": 419, "image_name": "000000047740.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047740.jpg", "image_width": 640, "image_height": 359, "image_resolution": "640x359", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1780.552, "latencies_ms": [1780.552], "images_per_second": 0.562, "prompt_tokens": 1099, "response_tokens_est": 28, "n_tiles": 1, "output_text": " A group of people are on a beach, with one person lying down and another person sitting up, and a kite flying in the air.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27560.2, "ram_available_mb": 98212.0, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 27560.1, "ram_available_mb": 98212.0, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.03, "peak": 40.56, "min": 16.95}, "VIN": {"avg": 75.78, "peak": 123.7, "min": 27.25}, "VIN_SYS_5V0": {"avg": 15.07, "peak": 16.15, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 16.21, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 30.03, "energy_joules_est": 53.49, "sample_count": 18, "duration_seconds": 1.781}, "timestamp": "2026-01-19T15:26:26.520673"}
{"image_index": 419, "image_name": "000000047740.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047740.jpg", "image_width": 640, "image_height": 359, "image_resolution": "640x359", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2626.439, "latencies_ms": [2626.439], "images_per_second": 0.381, "prompt_tokens": 1113, "response_tokens_est": 57, "n_tiles": 1, "output_text": " 1. sand: 3\n2. legs: 4\n3. shorts: 2\n4. kite: 1\n5. kite string: 1\n6. person: 2\n7. beach: 1\n8. ocean: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27560.1, "ram_available_mb": 98212.0, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 27560.1, "ram_available_mb": 98212.0, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.78, "peak": 40.57, "min": 16.95}, "VIN": {"avg": 71.64, "peak": 122.7, "min": 27.24}, "VIN_SYS_5V0": {"avg": 14.73, "peak": 16.05, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.75, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 26.78, "energy_joules_est": 70.34, "sample_count": 26, "duration_seconds": 2.627}, "timestamp": "2026-01-19T15:26:29.231984"}
{"image_index": 419, "image_name": "000000047740.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047740.jpg", "image_width": 640, "image_height": 359, "image_resolution": "640x359", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2171.664, "latencies_ms": [2171.664], "images_per_second": 0.46, "prompt_tokens": 1117, "response_tokens_est": 42, "n_tiles": 1, "output_text": " The kite is in the foreground, with the people lying on the beach in the background. The kite is positioned to the right of the people, and the people are lying on the left side of the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27560.1, "ram_available_mb": 98212.0, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 27557.7, "ram_available_mb": 98214.4, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.33, "peak": 40.18, "min": 14.98}, "VIN": {"avg": 70.48, "peak": 120.78, "min": 29.67}, "VIN_SYS_5V0": {"avg": 14.81, "peak": 16.05, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.94, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 28.33, "energy_joules_est": 61.53, "sample_count": 21, "duration_seconds": 2.172}, "timestamp": "2026-01-19T15:26:31.422991"}
{"image_index": 419, "image_name": "000000047740.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047740.jpg", "image_width": 640, "image_height": 359, "image_resolution": "640x359", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1472.065, "latencies_ms": [1472.065], "images_per_second": 0.679, "prompt_tokens": 1111, "response_tokens_est": 16, "n_tiles": 1, "output_text": " A group of people are on a beach with a kite flying in the air.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 27557.7, "ram_available_mb": 98214.4, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 2.5, "ram_used_mb": 27557.7, "ram_available_mb": 98214.4, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.67, "peak": 40.57, "min": 17.74}, "VIN": {"avg": 83.52, "peak": 123.69, "min": 27.63}, "VIN_SYS_5V0": {"avg": 15.18, "peak": 16.15, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 16.24, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 31.67, "energy_joules_est": 46.64, "sample_count": 15, "duration_seconds": 1.473}, "timestamp": "2026-01-19T15:26:32.988321"}
{"image_index": 419, "image_name": "000000047740.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047740.jpg", "image_width": 640, "image_height": 359, "image_resolution": "640x359", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2189.783, "latencies_ms": [2189.783], "images_per_second": 0.457, "prompt_tokens": 1109, "response_tokens_est": 43, "n_tiles": 1, "output_text": " The image captures a sunny day at the beach with clear blue skies and the ocean in the background. The sand is light brown and the kite flying in the air is colorful with red, blue, and green stripes.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27557.7, "ram_available_mb": 98214.4, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 27557.7, "ram_available_mb": 98214.4, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.3, "peak": 40.56, "min": 19.32}, "VIN": {"avg": 73.65, "peak": 125.74, "min": 33.19}, "VIN_SYS_5V0": {"avg": 14.94, "peak": 16.05, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 16.01, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 29.3, "energy_joules_est": 64.17, "sample_count": 21, "duration_seconds": 2.19}, "timestamp": "2026-01-19T15:26:35.184328"}
{"image_index": 420, "image_name": "000000047769.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047769.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2067.395, "latencies_ms": [2067.395], "images_per_second": 0.484, "prompt_tokens": 1099, "response_tokens_est": 40, "n_tiles": 1, "output_text": " The image depicts a cozy living room with a brown couch, a red chair, a small table, and a television, all set against a backdrop of a window with wooden shutters and a lamp.", "error": null, "sys_before": {"cpu_percent": 14.3, "ram_used_mb": 27557.7, "ram_available_mb": 98214.4, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 27557.7, "ram_available_mb": 98214.4, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.45, "peak": 40.18, "min": 18.53}, "VIN": {"avg": 69.52, "peak": 116.46, "min": 30.27}, "VIN_SYS_5V0": {"avg": 15.26, "peak": 16.26, "min": 13.39}, "VDD_CPU_SOC_MSS": {"avg": 16.38, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 29.45, "energy_joules_est": 60.9, "sample_count": 20, "duration_seconds": 2.068}, "timestamp": "2026-01-19T15:26:37.275387"}
{"image_index": 420, "image_name": "000000047769.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047769.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2754.265, "latencies_ms": [2754.265], "images_per_second": 0.363, "prompt_tokens": 1113, "response_tokens_est": 66, "n_tiles": 1, "output_text": " 1. brown sofa: 1\n2. black chair: 1\n3. black stool: 1\n4. black table: 1\n5. black TV stand: 1\n6. black TV: 1\n7. black lamp: 1\n8. yellow lamp: 1", "error": null, "sys_before": {"cpu_percent": 50.0, "ram_used_mb": 27557.7, "ram_available_mb": 98214.4, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 27557.7, "ram_available_mb": 98214.4, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.01, "peak": 40.16, "min": 18.13}, "VIN": {"avg": 71.98, "peak": 126.78, "min": 29.0}, "VIN_SYS_5V0": {"avg": 15.11, "peak": 16.15, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 16.17, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 27.01, "energy_joules_est": 74.41, "sample_count": 27, "duration_seconds": 2.755}, "timestamp": "2026-01-19T15:26:40.085181"}
{"image_index": 420, "image_name": "000000047769.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047769.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2306.993, "latencies_ms": [2306.993], "images_per_second": 0.433, "prompt_tokens": 1117, "response_tokens_est": 48, "n_tiles": 1, "output_text": " The sofa is located to the left of the coffee table, which is in the foreground of the room. The television is situated in the background, near the windows, while the lamp is positioned in the foreground, closer to the camera.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27557.7, "ram_available_mb": 98214.4, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 27559.3, "ram_available_mb": 98212.8, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.77, "peak": 40.56, "min": 15.77}, "VIN": {"avg": 67.24, "peak": 113.9, "min": 28.81}, "VIN_SYS_5V0": {"avg": 14.95, "peak": 16.05, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.95, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 27.77, "energy_joules_est": 64.07, "sample_count": 23, "duration_seconds": 2.307}, "timestamp": "2026-01-19T15:26:42.480324"}
{"image_index": 420, "image_name": "000000047769.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047769.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1598.178, "latencies_ms": [1598.178], "images_per_second": 0.626, "prompt_tokens": 1111, "response_tokens_est": 20, "n_tiles": 1, "output_text": " A living room with a brown couch, a red chair, a black table, and a TV.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 27559.3, "ram_available_mb": 98212.8, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 27559.3, "ram_available_mb": 98212.8, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.51, "peak": 40.18, "min": 15.38}, "VIN": {"avg": 79.3, "peak": 128.66, "min": 27.51}, "VIN_SYS_5V0": {"avg": 14.77, "peak": 16.05, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.87, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 30.51, "energy_joules_est": 48.77, "sample_count": 16, "duration_seconds": 1.598}, "timestamp": "2026-01-19T15:26:44.147930"}
{"image_index": 420, "image_name": "000000047769.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047769.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1754.99, "latencies_ms": [1754.99], "images_per_second": 0.57, "prompt_tokens": 1109, "response_tokens_est": 25, "n_tiles": 1, "output_text": " The room is well lit with natural light coming through the windows, and the walls are painted in a light beige color.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 27559.3, "ram_available_mb": 98212.8, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 27559.3, "ram_available_mb": 98212.8, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.0, "peak": 40.57, "min": 18.92}, "VIN": {"avg": 74.18, "peak": 126.58, "min": 30.6}, "VIN_SYS_5V0": {"avg": 14.91, "peak": 16.05, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.93, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 31.0, "energy_joules_est": 54.42, "sample_count": 17, "duration_seconds": 1.755}, "timestamp": "2026-01-19T15:26:45.921052"}
{"image_index": 421, "image_name": "000000047801.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047801.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1920.26, "latencies_ms": [1920.26], "images_per_second": 0.521, "prompt_tokens": 1432, "response_tokens_est": 18, "n_tiles": 1, "output_text": " A bald man wearing glasses is eating a slice of cake with a spoon in his hand.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27559.3, "ram_available_mb": 98212.8, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 2.3, "ram_used_mb": 27559.3, "ram_available_mb": 98212.9, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.2, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.67, "peak": 40.97, "min": 18.91}, "VIN": {"avg": 75.42, "peak": 131.34, "min": 29.99}, "VIN_SYS_5V0": {"avg": 15.58, "peak": 16.76, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 16.68, "peak": 18.11, "min": 14.18}}, "power_watts_avg": 32.67, "energy_joules_est": 62.76, "sample_count": 19, "duration_seconds": 1.921}, "timestamp": "2026-01-19T15:26:47.902632"}
{"image_index": 421, "image_name": "000000047801.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047801.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2935.395, "latencies_ms": [2935.395], "images_per_second": 0.341, "prompt_tokens": 1446, "response_tokens_est": 56, "n_tiles": 1, "output_text": " 1. man: 1\n2. plate: 1\n3. spoon: 1\n4. cake: 1\n5. grass: 1\n6. tree: 1\n7. sky: 1\n8. person: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27559.3, "ram_available_mb": 98212.9, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 27558.5, "ram_available_mb": 98213.7, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.08, "peak": 40.56, "min": 18.52}, "VIN": {"avg": 69.47, "peak": 119.26, "min": 30.54}, "VIN_SYS_5V0": {"avg": 15.2, "peak": 16.66, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 16.25, "peak": 17.71, "min": 13.79}}, "power_watts_avg": 29.08, "energy_joules_est": 85.38, "sample_count": 29, "duration_seconds": 2.936}, "timestamp": "2026-01-19T15:26:50.917988"}
{"image_index": 421, "image_name": "000000047801.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047801.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2437.163, "latencies_ms": [2437.163], "images_per_second": 0.41, "prompt_tokens": 1450, "response_tokens_est": 37, "n_tiles": 1, "output_text": " The man is in the foreground, eating a slice of cake. The cake is on a plate, which is in front of him. The background is a park with trees and grass.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27558.5, "ram_available_mb": 98213.7, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 27558.4, "ram_available_mb": 98213.7, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.14, "peak": 40.18, "min": 15.77}, "VIN": {"avg": 73.84, "peak": 117.87, "min": 28.75}, "VIN_SYS_5V0": {"avg": 15.16, "peak": 16.56, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 16.22, "peak": 17.71, "min": 13.0}}, "power_watts_avg": 30.14, "energy_joules_est": 73.46, "sample_count": 24, "duration_seconds": 2.437}, "timestamp": "2026-01-19T15:26:53.415876"}
{"image_index": 421, "image_name": "000000047801.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047801.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1962.122, "latencies_ms": [1962.122], "images_per_second": 0.51, "prompt_tokens": 1444, "response_tokens_est": 20, "n_tiles": 1, "output_text": " A bald man wearing glasses is eating a slice of cake in a park with trees in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27558.4, "ram_available_mb": 98213.7, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 27558.4, "ram_available_mb": 98213.7, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.55, "peak": 40.18, "min": 17.35}, "VIN": {"avg": 77.95, "peak": 130.68, "min": 27.7}, "VIN_SYS_5V0": {"avg": 15.35, "peak": 16.66, "min": 12.36}, "VDD_CPU_SOC_MSS": {"avg": 16.43, "peak": 17.71, "min": 13.39}}, "power_watts_avg": 32.55, "energy_joules_est": 63.88, "sample_count": 19, "duration_seconds": 1.963}, "timestamp": "2026-01-19T15:26:55.401166"}
{"image_index": 421, "image_name": "000000047801.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047801.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2093.255, "latencies_ms": [2093.255], "images_per_second": 0.478, "prompt_tokens": 1442, "response_tokens_est": 25, "n_tiles": 1, "output_text": " The man is wearing a blue shirt and has a bald head. The sun is shining brightly, and the grass is green.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27558.4, "ram_available_mb": 98213.7, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 27558.4, "ram_available_mb": 98213.7, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.23, "peak": 40.56, "min": 20.49}, "VIN": {"avg": 75.7, "peak": 119.99, "min": 29.06}, "VIN_SYS_5V0": {"avg": 15.43, "peak": 16.76, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 16.51, "peak": 18.11, "min": 14.18}}, "power_watts_avg": 32.23, "energy_joules_est": 67.48, "sample_count": 21, "duration_seconds": 2.094}, "timestamp": "2026-01-19T15:26:57.592968"}
{"image_index": 422, "image_name": "000000047819.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047819.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1799.049, "latencies_ms": [1799.049], "images_per_second": 0.556, "prompt_tokens": 1099, "response_tokens_est": 27, "n_tiles": 1, "output_text": " A man wearing a purple and blue jacket is walking next to a brown horse that is carrying a large load of luggage on its back.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 27558.4, "ram_available_mb": 98213.7, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 27558.7, "ram_available_mb": 98213.5, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.92, "peak": 40.57, "min": 17.35}, "VIN": {"avg": 66.72, "peak": 88.84, "min": 28.23}, "VIN_SYS_5V0": {"avg": 14.75, "peak": 16.05, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.77, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 29.92, "energy_joules_est": 53.85, "sample_count": 18, "duration_seconds": 1.8}, "timestamp": "2026-01-19T15:26:59.487339"}
{"image_index": 422, "image_name": "000000047819.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047819.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2584.754, "latencies_ms": [2584.754], "images_per_second": 0.387, "prompt_tokens": 1113, "response_tokens_est": 56, "n_tiles": 1, "output_text": " 1. man: 1\n2. horse: 1\n3. luggage: 2\n4. backpack: 1\n5. blanket: 1\n6. rope: 1\n7. tree: 1\n8. ground: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27558.7, "ram_available_mb": 98213.5, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 27558.7, "ram_available_mb": 98213.5, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.3, "peak": 40.57, "min": 16.56}, "VIN": {"avg": 65.46, "peak": 102.89, "min": 31.18}, "VIN_SYS_5V0": {"avg": 14.82, "peak": 15.95, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.7, "peak": 16.93, "min": 13.0}}, "power_watts_avg": 27.3, "energy_joules_est": 70.59, "sample_count": 25, "duration_seconds": 2.586}, "timestamp": "2026-01-19T15:27:02.081756"}
{"image_index": 422, "image_name": "000000047819.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047819.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2298.997, "latencies_ms": [2298.997], "images_per_second": 0.435, "prompt_tokens": 1117, "response_tokens_est": 48, "n_tiles": 1, "output_text": " The man is standing to the left of the donkey, which is positioned in the foreground of the image. The donkey is facing the man, and the man is walking away from the donkey, which is positioned in the background of the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27558.7, "ram_available_mb": 98213.5, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 27558.7, "ram_available_mb": 98213.5, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.18, "peak": 40.18, "min": 17.73}, "VIN": {"avg": 64.97, "peak": 108.15, "min": 30.42}, "VIN_SYS_5V0": {"avg": 15.14, "peak": 16.15, "min": 13.29}, "VDD_CPU_SOC_MSS": {"avg": 16.19, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 28.18, "energy_joules_est": 64.8, "sample_count": 23, "duration_seconds": 2.299}, "timestamp": "2026-01-19T15:27:04.460391"}
{"image_index": 422, "image_name": "000000047819.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047819.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1758.242, "latencies_ms": [1758.242], "images_per_second": 0.569, "prompt_tokens": 1111, "response_tokens_est": 26, "n_tiles": 1, "output_text": " A man is walking with a donkey that is carrying a large load of luggage. The man is wearing a purple jacket and jeans.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27558.7, "ram_available_mb": 98213.5, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 27558.7, "ram_available_mb": 98213.5, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.24, "peak": 40.16, "min": 15.77}, "VIN": {"avg": 71.26, "peak": 129.21, "min": 33.07}, "VIN_SYS_5V0": {"avg": 14.88, "peak": 16.05, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 15.93, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 30.24, "energy_joules_est": 53.18, "sample_count": 17, "duration_seconds": 1.759}, "timestamp": "2026-01-19T15:27:06.228520"}
{"image_index": 422, "image_name": "000000047819.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047819.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2500.816, "latencies_ms": [2500.816], "images_per_second": 0.4, "prompt_tokens": 1109, "response_tokens_est": 56, "n_tiles": 1, "output_text": " The image features a man wearing a purple and blue jacket, standing next to a brown horse with a colorful blanket on its back. The horse is carrying a large red and gray bag on its back. The lighting in the image is natural, and the weather appears to be clear.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 27558.7, "ram_available_mb": 98213.5, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 27558.7, "ram_available_mb": 98213.5, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.85, "peak": 40.18, "min": 17.35}, "VIN": {"avg": 71.64, "peak": 102.13, "min": 29.37}, "VIN_SYS_5V0": {"avg": 15.15, "peak": 16.26, "min": 13.17}, "VDD_CPU_SOC_MSS": {"avg": 16.16, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 27.85, "energy_joules_est": 69.66, "sample_count": 25, "duration_seconds": 2.501}, "timestamp": "2026-01-19T15:27:08.829724"}
{"image_index": 423, "image_name": "000000047828.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047828.jpg", "image_width": 640, "image_height": 318, "image_resolution": "640x318", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2037.524, "latencies_ms": [2037.524], "images_per_second": 0.491, "prompt_tokens": 766, "response_tokens_est": 53, "n_tiles": 1, "output_text": " The image captures a vibrant night scene of a bridge adorned with blue lights, standing majestically over a river, with a boat docked on the river's edge, and people strolling along the riverbank, all under a dark sky speckled with stars.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27558.7, "ram_available_mb": 98213.5, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 27558.7, "ram_available_mb": 98213.5, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5179.1, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 24.86, "peak": 35.85, "min": 15.38}, "VIN": {"avg": 71.34, "peak": 119.22, "min": 29.53}, "VIN_SYS_5V0": {"avg": 14.55, "peak": 15.14, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.59, "peak": 16.15, "min": 13.0}}, "power_watts_avg": 24.86, "energy_joules_est": 50.66, "sample_count": 20, "duration_seconds": 2.038}, "timestamp": "2026-01-19T15:27:10.918449"}
{"image_index": 423, "image_name": "000000047828.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047828.jpg", "image_width": 640, "image_height": 318, "image_resolution": "640x318", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2112.67, "latencies_ms": [2112.67], "images_per_second": 0.473, "prompt_tokens": 780, "response_tokens_est": 56, "n_tiles": 1, "output_text": " 1. bridge: 1\n2. lights: 1\n3. people: 1\n4. boat: 1\n5. water: 1\n6. street: 1\n7. sky: 1\n8. clouds: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27558.7, "ram_available_mb": 98213.5, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 27558.9, "ram_available_mb": 98213.3, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5180.1, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 24.86, "peak": 36.24, "min": 16.95}, "VIN": {"avg": 65.0, "peak": 84.29, "min": 30.71}, "VIN_SYS_5V0": {"avg": 14.74, "peak": 15.44, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 15.73, "peak": 16.54, "min": 14.18}}, "power_watts_avg": 24.86, "energy_joules_est": 52.53, "sample_count": 21, "duration_seconds": 2.113}, "timestamp": "2026-01-19T15:27:13.102828"}
{"image_index": 423, "image_name": "000000047828.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047828.jpg", "image_width": 640, "image_height": 318, "image_resolution": "640x318", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1470.438, "latencies_ms": [1470.438], "images_per_second": 0.68, "prompt_tokens": 784, "response_tokens_est": 30, "n_tiles": 1, "output_text": " The bridge is located in the background, with the river in the foreground. The people are standing on the sidewalk, which is adjacent to the river.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27558.9, "ram_available_mb": 98213.3, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 27558.9, "ram_available_mb": 98213.3, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5180.1, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.64, "peak": 35.85, "min": 15.38}, "VIN": {"avg": 73.28, "peak": 123.66, "min": 27.51}, "VIN_SYS_5V0": {"avg": 14.49, "peak": 15.24, "min": 12.36}, "VDD_CPU_SOC_MSS": {"avg": 15.49, "peak": 16.53, "min": 13.39}}, "power_watts_avg": 26.64, "energy_joules_est": 39.18, "sample_count": 15, "duration_seconds": 1.471}, "timestamp": "2026-01-19T15:27:14.665321"}
{"image_index": 423, "image_name": "000000047828.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047828.jpg", "image_width": 640, "image_height": 318, "image_resolution": "640x318", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2397.932, "latencies_ms": [2397.932], "images_per_second": 0.417, "prompt_tokens": 778, "response_tokens_est": 67, "n_tiles": 1, "output_text": " The image captures a vibrant night scene of a bridge illuminated with blue lights, spanning across a river. The bridge is adorned with a series of lights that create a striking pattern against the dark sky. In the foreground, a boat is docked, adding to the lively atmosphere. The city lights in the background suggest a bustling urban environment.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27558.9, "ram_available_mb": 98213.3, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 27558.9, "ram_available_mb": 98213.3, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5180.1, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 24.12, "peak": 37.03, "min": 16.16}, "VIN": {"avg": 63.1, "peak": 109.03, "min": 29.75}, "VIN_SYS_5V0": {"avg": 14.56, "peak": 15.14, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.51, "peak": 16.14, "min": 13.0}}, "power_watts_avg": 24.12, "energy_joules_est": 57.84, "sample_count": 24, "duration_seconds": 2.398}, "timestamp": "2026-01-19T15:27:17.163937"}
{"image_index": 423, "image_name": "000000047828.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047828.jpg", "image_width": 640, "image_height": 318, "image_resolution": "640x318", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2022.711, "latencies_ms": [2022.711], "images_per_second": 0.494, "prompt_tokens": 776, "response_tokens_est": 52, "n_tiles": 1, "output_text": " The image captures a vibrant night scene featuring a brightly lit bridge with a unique blue and white design, reflecting off the calm waters of the river below. The sky is dark, and the city lights are visible in the distance, creating a picturesque and lively atmosphere.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 27558.9, "ram_available_mb": 98213.3, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 27558.9, "ram_available_mb": 98213.3, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5180.1, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 24.65, "peak": 36.24, "min": 14.59}, "VIN": {"avg": 67.24, "peak": 116.16, "min": 28.81}, "VIN_SYS_5V0": {"avg": 14.46, "peak": 15.14, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 15.53, "peak": 16.53, "min": 13.0}}, "power_watts_avg": 24.65, "energy_joules_est": 49.87, "sample_count": 20, "duration_seconds": 2.023}, "timestamp": "2026-01-19T15:27:19.251621"}
{"image_index": 424, "image_name": "000000048153.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048153.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1467.854, "latencies_ms": [1467.854], "images_per_second": 0.681, "prompt_tokens": 1099, "response_tokens_est": 15, "n_tiles": 1, "output_text": " A person is wearing blue jeans and pink shoes with a bow on them.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27558.9, "ram_available_mb": 98213.3, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 27558.9, "ram_available_mb": 98213.3, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.91, "peak": 40.18, "min": 15.38}, "VIN": {"avg": 76.11, "peak": 130.63, "min": 29.44}, "VIN_SYS_5V0": {"avg": 14.79, "peak": 16.05, "min": 12.15}, "VDD_CPU_SOC_MSS": {"avg": 15.96, "peak": 17.32, "min": 13.4}}, "power_watts_avg": 30.91, "energy_joules_est": 45.4, "sample_count": 15, "duration_seconds": 1.469}, "timestamp": "2026-01-19T15:27:20.822107"}
{"image_index": 424, "image_name": "000000048153.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048153.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2747.204, "latencies_ms": [2747.204], "images_per_second": 0.364, "prompt_tokens": 1113, "response_tokens_est": 64, "n_tiles": 1, "output_text": " 1. pink shoe: 1\n2. blue jeans: 1\n3. wooden floor: 1\n4. pink bow: 1\n5. blue paint: 1\n6. green paint: 1\n7. black paint: 1\n8. white paint: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27558.9, "ram_available_mb": 98213.3, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 27557.0, "ram_available_mb": 98215.2, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.11, "peak": 40.57, "min": 17.74}, "VIN": {"avg": 68.07, "peak": 126.94, "min": 30.41}, "VIN_SYS_5V0": {"avg": 14.83, "peak": 16.05, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.82, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 27.11, "energy_joules_est": 74.48, "sample_count": 27, "duration_seconds": 2.747}, "timestamp": "2026-01-19T15:27:23.647847"}
{"image_index": 424, "image_name": "000000048153.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048153.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2114.72, "latencies_ms": [2114.72], "images_per_second": 0.473, "prompt_tokens": 1117, "response_tokens_est": 40, "n_tiles": 1, "output_text": " The pink shoe is positioned on the left side of the image, with the person's leg extending towards the right side. The shoe is in the foreground, while the wooden bench is in the background.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 27557.0, "ram_available_mb": 98215.2, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 27557.0, "ram_available_mb": 98215.2, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.4, "peak": 40.18, "min": 15.38}, "VIN": {"avg": 69.22, "peak": 124.47, "min": 28.65}, "VIN_SYS_5V0": {"avg": 14.84, "peak": 16.05, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.81, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 28.4, "energy_joules_est": 60.07, "sample_count": 21, "duration_seconds": 2.115}, "timestamp": "2026-01-19T15:27:25.832590"}
{"image_index": 424, "image_name": "000000048153.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048153.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1549.96, "latencies_ms": [1549.96], "images_per_second": 0.645, "prompt_tokens": 1111, "response_tokens_est": 18, "n_tiles": 1, "output_text": " A person is wearing pink shoes and blue jeans. The shoes are on a wooden floor.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27557.0, "ram_available_mb": 98215.2, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 27557.0, "ram_available_mb": 98215.2, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.31, "peak": 40.57, "min": 16.56}, "VIN": {"avg": 77.67, "peak": 119.94, "min": 29.61}, "VIN_SYS_5V0": {"avg": 14.91, "peak": 16.05, "min": 12.15}, "VDD_CPU_SOC_MSS": {"avg": 15.93, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 31.31, "energy_joules_est": 48.54, "sample_count": 15, "duration_seconds": 1.55}, "timestamp": "2026-01-19T15:27:27.399718"}
{"image_index": 424, "image_name": "000000048153.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048153.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2338.102, "latencies_ms": [2338.102], "images_per_second": 0.428, "prompt_tokens": 1109, "response_tokens_est": 50, "n_tiles": 1, "output_text": " The image features a person wearing a pink shoe and blue jeans, with a wooden floor in the background. The lighting is bright and the colors are vibrant, with the pink of the shoe standing out against the blue of the jeans and the wooden floor.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27557.0, "ram_available_mb": 98215.2, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 27557.0, "ram_available_mb": 98215.2, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.81, "peak": 40.56, "min": 18.52}, "VIN": {"avg": 62.86, "peak": 102.63, "min": 28.96}, "VIN_SYS_5V0": {"avg": 15.14, "peak": 16.26, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 16.13, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 28.81, "energy_joules_est": 67.37, "sample_count": 23, "duration_seconds": 2.339}, "timestamp": "2026-01-19T15:27:29.803612"}
{"image_index": 425, "image_name": "000000048396.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048396.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1677.393, "latencies_ms": [1677.393], "images_per_second": 0.596, "prompt_tokens": 1099, "response_tokens_est": 23, "n_tiles": 1, "output_text": " A woman and a boy are standing in a room with a red wall, and the woman is holding a knife.", "error": null, "sys_before": {"cpu_percent": 27.3, "ram_used_mb": 27557.0, "ram_available_mb": 98215.2, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 27558.2, "ram_available_mb": 98214.0, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.08, "peak": 40.57, "min": 16.16}, "VIN": {"avg": 75.1, "peak": 124.97, "min": 28.62}, "VIN_SYS_5V0": {"avg": 14.81, "peak": 16.05, "min": 12.15}, "VDD_CPU_SOC_MSS": {"avg": 15.91, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 30.08, "energy_joules_est": 50.47, "sample_count": 17, "duration_seconds": 1.678}, "timestamp": "2026-01-19T15:27:31.584406"}
{"image_index": 425, "image_name": "000000048396.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048396.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2011.108, "latencies_ms": [2011.108], "images_per_second": 0.497, "prompt_tokens": 1113, "response_tokens_est": 34, "n_tiles": 1, "output_text": " woman: 1, man: 1, table: 1, cup: 1, plate: 1, knife: 1, wall: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 27558.2, "ram_available_mb": 98214.0, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 27557.8, "ram_available_mb": 98214.3, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.98, "peak": 40.56, "min": 17.35}, "VIN": {"avg": 70.95, "peak": 111.07, "min": 28.35}, "VIN_SYS_5V0": {"avg": 14.71, "peak": 15.95, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.79, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 28.98, "energy_joules_est": 58.29, "sample_count": 20, "duration_seconds": 2.012}, "timestamp": "2026-01-19T15:27:33.674880"}
{"image_index": 425, "image_name": "000000048396.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048396.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1908.79, "latencies_ms": [1908.79], "images_per_second": 0.524, "prompt_tokens": 1117, "response_tokens_est": 31, "n_tiles": 1, "output_text": " The woman is standing to the left of the boy, and the table is in the foreground. The boy is standing closer to the camera than the woman.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27557.8, "ram_available_mb": 98214.3, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 27557.8, "ram_available_mb": 98214.3, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.28, "peak": 40.16, "min": 16.16}, "VIN": {"avg": 69.76, "peak": 99.49, "min": 30.38}, "VIN_SYS_5V0": {"avg": 14.77, "peak": 16.05, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.85, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 29.28, "energy_joules_est": 55.9, "sample_count": 19, "duration_seconds": 1.909}, "timestamp": "2026-01-19T15:27:35.653719"}
{"image_index": 425, "image_name": "000000048396.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048396.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1845.277, "latencies_ms": [1845.277], "images_per_second": 0.542, "prompt_tokens": 1111, "response_tokens_est": 29, "n_tiles": 1, "output_text": " A woman and a boy are standing in a room with a red wall. The woman is holding a knife and the boy is holding a cup.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27557.8, "ram_available_mb": 98214.3, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 27557.8, "ram_available_mb": 98214.3, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.12, "peak": 40.57, "min": 17.35}, "VIN": {"avg": 72.82, "peak": 132.67, "min": 29.15}, "VIN_SYS_5V0": {"avg": 14.86, "peak": 16.05, "min": 12.15}, "VDD_CPU_SOC_MSS": {"avg": 16.01, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 30.12, "energy_joules_est": 55.59, "sample_count": 18, "duration_seconds": 1.846}, "timestamp": "2026-01-19T15:27:37.532794"}
{"image_index": 425, "image_name": "000000048396.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048396.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2099.558, "latencies_ms": [2099.558], "images_per_second": 0.476, "prompt_tokens": 1109, "response_tokens_est": 39, "n_tiles": 1, "output_text": " The image has a warm and inviting atmosphere, with soft lighting and a red wall in the background. The woman is wearing a green cardigan and the boy is wearing a blue plaid shirt.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27557.8, "ram_available_mb": 98214.3, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 27557.8, "ram_available_mb": 98214.3, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.91, "peak": 40.56, "min": 18.13}, "VIN": {"avg": 71.3, "peak": 123.56, "min": 28.14}, "VIN_SYS_5V0": {"avg": 14.96, "peak": 16.05, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 16.07, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 28.91, "energy_joules_est": 60.71, "sample_count": 21, "duration_seconds": 2.1}, "timestamp": "2026-01-19T15:27:39.713768"}
{"image_index": 426, "image_name": "000000048504.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048504.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 3584.999, "latencies_ms": [3584.999], "images_per_second": 0.279, "prompt_tokens": 1099, "response_tokens_est": 96, "n_tiles": 1, "output_text": " In the center of an indoor arena, two elephants are engaged in a performance, their trunks intertwined in a display of strength and coordination. The arena is adorned with a vibrant red and yellow striped border, adding a splash of color to the scene. In the background, a solitary figure can be seen, perhaps an audience member or a performer, observing the spectacle. The image captures a moment of harmony and spectacle, as the elephants showcase their unique bond and the skill of their handlers.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 27557.8, "ram_available_mb": 98214.3, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 27557.8, "ram_available_mb": 98214.3, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 24.82, "peak": 40.18, "min": 16.16}, "VIN": {"avg": 55.65, "peak": 125.84, "min": 29.03}, "VIN_SYS_5V0": {"avg": 14.85, "peak": 16.05, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.88, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 24.82, "energy_joules_est": 88.99, "sample_count": 35, "duration_seconds": 3.585}, "timestamp": "2026-01-19T15:27:43.358566"}
{"image_index": 426, "image_name": "000000048504.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048504.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2097.698, "latencies_ms": [2097.698], "images_per_second": 0.477, "prompt_tokens": 1113, "response_tokens_est": 39, "n_tiles": 1, "output_text": " elephant: 3, elephant: 1, elephant: 1, elephant: 1, elephant: 1, elephant: 1, elephant: 1, elephant: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27557.8, "ram_available_mb": 98214.3, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 27557.8, "ram_available_mb": 98214.3, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.4, "peak": 40.18, "min": 15.77}, "VIN": {"avg": 66.36, "peak": 121.21, "min": 27.8}, "VIN_SYS_5V0": {"avg": 14.87, "peak": 16.05, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.95, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 28.4, "energy_joules_est": 59.59, "sample_count": 21, "duration_seconds": 2.098}, "timestamp": "2026-01-19T15:27:45.552184"}
{"image_index": 426, "image_name": "000000048504.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048504.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2624.449, "latencies_ms": [2624.449], "images_per_second": 0.381, "prompt_tokens": 1117, "response_tokens_est": 59, "n_tiles": 1, "output_text": " The elephants are positioned in the foreground of the image, with the camera and the audience in the background. The elephants are facing each other, with the brown elephant on the left and the gray elephant on the right. The camera is positioned in the foreground, closer to the elephants than the audience.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27557.8, "ram_available_mb": 98214.3, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 27557.8, "ram_available_mb": 98214.3, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.8, "peak": 40.16, "min": 15.77}, "VIN": {"avg": 70.73, "peak": 113.47, "min": 28.59}, "VIN_SYS_5V0": {"avg": 14.77, "peak": 15.95, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.82, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 26.8, "energy_joules_est": 70.35, "sample_count": 26, "duration_seconds": 2.625}, "timestamp": "2026-01-19T15:27:48.256335"}
{"image_index": 426, "image_name": "000000048504.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048504.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2205.098, "latencies_ms": [2205.098], "images_per_second": 0.453, "prompt_tokens": 1111, "response_tokens_est": 43, "n_tiles": 1, "output_text": " In an indoor arena, two elephants are performing a trick for an audience. The elephants are standing on a circular platform with a red and yellow border, and they are being guided by a person in a red shirt.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 27557.8, "ram_available_mb": 98214.3, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 27557.8, "ram_available_mb": 98214.3, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.81, "peak": 40.18, "min": 15.38}, "VIN": {"avg": 66.84, "peak": 118.01, "min": 29.99}, "VIN_SYS_5V0": {"avg": 14.82, "peak": 16.05, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 15.8, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 27.81, "energy_joules_est": 61.34, "sample_count": 22, "duration_seconds": 2.206}, "timestamp": "2026-01-19T15:27:50.547637"}
{"image_index": 426, "image_name": "000000048504.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048504.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1820.42, "latencies_ms": [1820.42], "images_per_second": 0.549, "prompt_tokens": 1109, "response_tokens_est": 28, "n_tiles": 1, "output_text": " The elephants are brown and gray, and the circus ring is red and yellow. The lighting is bright and the elephants are well-lit.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27557.8, "ram_available_mb": 98214.3, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 27557.8, "ram_available_mb": 98214.3, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.57, "peak": 40.18, "min": 15.77}, "VIN": {"avg": 69.52, "peak": 114.94, "min": 28.02}, "VIN_SYS_5V0": {"avg": 14.77, "peak": 15.95, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.81, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 29.57, "energy_joules_est": 53.84, "sample_count": 18, "duration_seconds": 1.821}, "timestamp": "2026-01-19T15:27:52.427704"}
{"image_index": 427, "image_name": "000000048555.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048555.jpg", "image_width": 640, "image_height": 465, "image_resolution": "640x465", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2065.65, "latencies_ms": [2065.65], "images_per_second": 0.484, "prompt_tokens": 1099, "response_tokens_est": 38, "n_tiles": 1, "output_text": " In the sepia-toned photograph, a group of jockeys on horseback are galloping across a beach, their horses' hooves leaving distinct hoof prints in the wet sand.", "error": null, "sys_before": {"cpu_percent": 14.3, "ram_used_mb": 27557.8, "ram_available_mb": 98214.3, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 27557.8, "ram_available_mb": 98214.3, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.15, "peak": 40.56, "min": 17.35}, "VIN": {"avg": 72.47, "peak": 120.2, "min": 33.15}, "VIN_SYS_5V0": {"avg": 14.92, "peak": 16.05, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 16.04, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 29.15, "energy_joules_est": 60.23, "sample_count": 20, "duration_seconds": 2.066}, "timestamp": "2026-01-19T15:27:54.507387"}
{"image_index": 427, "image_name": "000000048555.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048555.jpg", "image_width": 640, "image_height": 465, "image_resolution": "640x465", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2514.855, "latencies_ms": [2514.855], "images_per_second": 0.398, "prompt_tokens": 1113, "response_tokens_est": 56, "n_tiles": 1, "output_text": " 1. horse: 2\n2. rider: 2\n3. horse: 1\n4. rider: 1\n5. horse: 1\n6. rider: 1\n7. horse: 1\n8. rider: 1", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 27557.8, "ram_available_mb": 98214.3, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 27557.8, "ram_available_mb": 98214.3, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.45, "peak": 40.57, "min": 17.73}, "VIN": {"avg": 69.61, "peak": 121.46, "min": 27.51}, "VIN_SYS_5V0": {"avg": 15.11, "peak": 16.15, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 16.19, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 27.45, "energy_joules_est": 69.05, "sample_count": 25, "duration_seconds": 2.515}, "timestamp": "2026-01-19T15:27:57.102845"}
{"image_index": 427, "image_name": "000000048555.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048555.jpg", "image_width": 640, "image_height": 465, "image_resolution": "640x465", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2199.863, "latencies_ms": [2199.863], "images_per_second": 0.455, "prompt_tokens": 1117, "response_tokens_est": 43, "n_tiles": 1, "output_text": " The horses are positioned on the left side of the image, with the riders appearing to be in the middle of the frame. The background of the image is the beach, which is located behind the horses and riders.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27557.8, "ram_available_mb": 98214.3, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 27557.8, "ram_available_mb": 98214.3, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.95, "peak": 40.18, "min": 15.38}, "VIN": {"avg": 68.94, "peak": 116.4, "min": 29.47}, "VIN_SYS_5V0": {"avg": 14.82, "peak": 16.05, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.87, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 27.95, "energy_joules_est": 61.51, "sample_count": 22, "duration_seconds": 2.201}, "timestamp": "2026-01-19T15:27:59.389939"}
{"image_index": 427, "image_name": "000000048555.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048555.jpg", "image_width": 640, "image_height": 465, "image_resolution": "640x465", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1585.948, "latencies_ms": [1585.948], "images_per_second": 0.631, "prompt_tokens": 1111, "response_tokens_est": 18, "n_tiles": 1, "output_text": " A group of people are riding horses on a beach, with the ocean in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27557.8, "ram_available_mb": 98214.3, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 27557.8, "ram_available_mb": 98214.3, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.19, "peak": 39.77, "min": 15.77}, "VIN": {"avg": 67.7, "peak": 110.4, "min": 27.92}, "VIN_SYS_5V0": {"avg": 14.65, "peak": 15.95, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.7, "peak": 16.92, "min": 13.0}}, "power_watts_avg": 30.19, "energy_joules_est": 47.89, "sample_count": 16, "duration_seconds": 1.586}, "timestamp": "2026-01-19T15:28:01.061927"}
{"image_index": 427, "image_name": "000000048555.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048555.jpg", "image_width": 640, "image_height": 465, "image_resolution": "640x465", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1731.665, "latencies_ms": [1731.665], "images_per_second": 0.577, "prompt_tokens": 1109, "response_tokens_est": 24, "n_tiles": 1, "output_text": " The image is in black and white, with a sepia tone, and the horses are running on a wet beach.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27557.8, "ram_available_mb": 98214.3, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 27557.8, "ram_available_mb": 98214.3, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.61, "peak": 40.18, "min": 18.53}, "VIN": {"avg": 68.4, "peak": 115.43, "min": 28.58}, "VIN_SYS_5V0": {"avg": 14.74, "peak": 15.95, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.79, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 30.61, "energy_joules_est": 53.01, "sample_count": 17, "duration_seconds": 1.732}, "timestamp": "2026-01-19T15:28:02.831528"}
{"image_index": 428, "image_name": "000000048564.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048564.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1660.057, "latencies_ms": [1660.057], "images_per_second": 0.602, "prompt_tokens": 1100, "response_tokens_est": 23, "n_tiles": 1, "output_text": " A young man wearing a black jacket and a blue and black helmet is talking on a cell phone in the snow.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27557.8, "ram_available_mb": 98214.3, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 27557.8, "ram_available_mb": 98214.3, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.29, "peak": 40.16, "min": 18.92}, "VIN": {"avg": 66.23, "peak": 102.17, "min": 29.33}, "VIN_SYS_5V0": {"avg": 15.05, "peak": 16.15, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 16.21, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 31.29, "energy_joules_est": 51.96, "sample_count": 16, "duration_seconds": 1.661}, "timestamp": "2026-01-19T15:28:04.507549"}
{"image_index": 428, "image_name": "000000048564.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048564.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2556.807, "latencies_ms": [2556.807], "images_per_second": 0.391, "prompt_tokens": 1114, "response_tokens_est": 58, "n_tiles": 1, "output_text": " 1. person: 1\n2. goggles: 1\n3. jacket: 1\n4. phone: 1\n5. tree: 1\n6. snow: 1\n7. tree trunk: 1\n8. snowboard: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27557.8, "ram_available_mb": 98214.3, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 27557.8, "ram_available_mb": 98214.4, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.78, "peak": 40.57, "min": 18.91}, "VIN": {"avg": 63.31, "peak": 118.09, "min": 28.62}, "VIN_SYS_5V0": {"avg": 15.15, "peak": 16.26, "min": 13.29}, "VDD_CPU_SOC_MSS": {"avg": 16.22, "peak": 17.71, "min": 14.57}}, "power_watts_avg": 27.78, "energy_joules_est": 71.04, "sample_count": 25, "duration_seconds": 2.557}, "timestamp": "2026-01-19T15:28:07.104367"}
{"image_index": 428, "image_name": "000000048564.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048564.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2107.409, "latencies_ms": [2107.409], "images_per_second": 0.475, "prompt_tokens": 1118, "response_tokens_est": 40, "n_tiles": 1, "output_text": " The person is standing in the foreground of the image, wearing a black jacket and a black helmet with blue goggles. The background of the image shows a snowy landscape with trees and a clear blue sky.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 27557.8, "ram_available_mb": 98214.4, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 27557.8, "ram_available_mb": 98214.4, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.46, "peak": 40.18, "min": 16.16}, "VIN": {"avg": 72.43, "peak": 126.78, "min": 29.65}, "VIN_SYS_5V0": {"avg": 15.05, "peak": 16.15, "min": 12.66}, "VDD_CPU_SOC_MSS": {"avg": 16.07, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 28.46, "energy_joules_est": 59.99, "sample_count": 21, "duration_seconds": 2.108}, "timestamp": "2026-01-19T15:28:09.292745"}
{"image_index": 428, "image_name": "000000048564.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048564.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1730.412, "latencies_ms": [1730.412], "images_per_second": 0.578, "prompt_tokens": 1112, "response_tokens_est": 25, "n_tiles": 1, "output_text": " A young man wearing a black jacket and a black helmet with blue goggles is talking on a cell phone in a snowy forest.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27557.8, "ram_available_mb": 98214.4, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 27557.8, "ram_available_mb": 98214.4, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.78, "peak": 39.39, "min": 16.16}, "VIN": {"avg": 65.1, "peak": 101.8, "min": 28.93}, "VIN_SYS_5V0": {"avg": 14.83, "peak": 16.15, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 15.91, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 29.78, "energy_joules_est": 51.54, "sample_count": 17, "duration_seconds": 1.731}, "timestamp": "2026-01-19T15:28:11.060393"}
{"image_index": 428, "image_name": "000000048564.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048564.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1621.157, "latencies_ms": [1621.157], "images_per_second": 0.617, "prompt_tokens": 1110, "response_tokens_est": 21, "n_tiles": 1, "output_text": " The person is wearing a black jacket and a blue and black helmet, and the sun is shining brightly.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27557.8, "ram_available_mb": 98214.4, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 27557.8, "ram_available_mb": 98214.4, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.32, "peak": 40.57, "min": 18.53}, "VIN": {"avg": 74.51, "peak": 125.63, "min": 29.89}, "VIN_SYS_5V0": {"avg": 15.02, "peak": 16.15, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 16.09, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 31.32, "energy_joules_est": 50.78, "sample_count": 16, "duration_seconds": 1.621}, "timestamp": "2026-01-19T15:28:12.732988"}
{"image_index": 429, "image_name": "000000048924.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048924.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1567.938, "latencies_ms": [1567.938], "images_per_second": 0.638, "prompt_tokens": 1099, "response_tokens_est": 19, "n_tiles": 1, "output_text": " A green tent and a white motorcycle are parked in a field with a sunset in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27557.8, "ram_available_mb": 98214.4, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 27557.8, "ram_available_mb": 98214.4, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.44, "peak": 40.56, "min": 18.92}, "VIN": {"avg": 72.93, "peak": 122.2, "min": 28.37}, "VIN_SYS_5V0": {"avg": 14.93, "peak": 16.05, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.99, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 31.44, "energy_joules_est": 49.32, "sample_count": 16, "duration_seconds": 1.569}, "timestamp": "2026-01-19T15:28:14.405935"}
{"image_index": 429, "image_name": "000000048924.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048924.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2129.082, "latencies_ms": [2129.082], "images_per_second": 0.47, "prompt_tokens": 1113, "response_tokens_est": 40, "n_tiles": 1, "output_text": " tent: 1, motorcycle: 1, backpack: 1, tent pole: 1, motorcycle seat: 1, motorcycle handlebars: 1, motorcycle front wheel: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27557.8, "ram_available_mb": 98214.4, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 27557.8, "ram_available_mb": 98214.4, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.99, "peak": 40.18, "min": 18.53}, "VIN": {"avg": 69.67, "peak": 106.54, "min": 29.73}, "VIN_SYS_5V0": {"avg": 14.83, "peak": 16.05, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.75, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 28.99, "energy_joules_est": 61.73, "sample_count": 21, "duration_seconds": 2.129}, "timestamp": "2026-01-19T15:28:16.585056"}
{"image_index": 429, "image_name": "000000048924.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048924.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2145.134, "latencies_ms": [2145.134], "images_per_second": 0.466, "prompt_tokens": 1117, "response_tokens_est": 41, "n_tiles": 1, "output_text": " The motorcycle is positioned to the right of the tent, with the motorcycle's front wheel slightly closer to the camera than the tent. The motorcycle is in the foreground, with the tent situated in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27557.8, "ram_available_mb": 98214.4, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 27558.1, "ram_available_mb": 98214.1, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.7, "peak": 40.16, "min": 16.95}, "VIN": {"avg": 67.62, "peak": 121.91, "min": 30.01}, "VIN_SYS_5V0": {"avg": 14.96, "peak": 16.05, "min": 12.46}, "VDD_CPU_SOC_MSS": {"avg": 16.03, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 28.7, "energy_joules_est": 61.57, "sample_count": 21, "duration_seconds": 2.145}, "timestamp": "2026-01-19T15:28:18.771357"}
{"image_index": 429, "image_name": "000000048924.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048924.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1708.519, "latencies_ms": [1708.519], "images_per_second": 0.585, "prompt_tokens": 1111, "response_tokens_est": 24, "n_tiles": 1, "output_text": " A green tent and a white motorcycle are parked in a field with tall grass. The sun is setting in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27558.1, "ram_available_mb": 98214.1, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 27558.1, "ram_available_mb": 98214.1, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.36, "peak": 40.16, "min": 16.95}, "VIN": {"avg": 71.57, "peak": 125.12, "min": 27.81}, "VIN_SYS_5V0": {"avg": 14.98, "peak": 16.15, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 16.07, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 30.36, "energy_joules_est": 51.89, "sample_count": 17, "duration_seconds": 1.709}, "timestamp": "2026-01-19T15:28:20.547712"}
{"image_index": 429, "image_name": "000000048924.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048924.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2115.355, "latencies_ms": [2115.355], "images_per_second": 0.473, "prompt_tokens": 1109, "response_tokens_est": 38, "n_tiles": 1, "output_text": " The image features a green tent and a white motorcycle parked in a field with dry grass. The sky is filled with clouds and the sun is setting, casting a warm glow on the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27558.1, "ram_available_mb": 98214.1, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 27558.1, "ram_available_mb": 98214.1, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.76, "peak": 40.57, "min": 18.14}, "VIN": {"avg": 69.72, "peak": 123.17, "min": 28.69}, "VIN_SYS_5V0": {"avg": 14.82, "peak": 16.05, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.8, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 28.76, "energy_joules_est": 60.85, "sample_count": 21, "duration_seconds": 2.116}, "timestamp": "2026-01-19T15:28:22.735375"}
{"image_index": 430, "image_name": "000000049060.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049060.jpg", "image_width": 640, "image_height": 411, "image_resolution": "640x411", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1990.692, "latencies_ms": [1990.692], "images_per_second": 0.502, "prompt_tokens": 1099, "response_tokens_est": 35, "n_tiles": 1, "output_text": " A black and white photo shows a steam locomotive with the number 67371 on it, pulling a train of passenger cars, with people standing on the platform.", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 27558.1, "ram_available_mb": 98214.1, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 27558.1, "ram_available_mb": 98214.1, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.78, "peak": 40.18, "min": 16.16}, "VIN": {"avg": 68.8, "peak": 123.49, "min": 28.97}, "VIN_SYS_5V0": {"avg": 14.77, "peak": 15.95, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.81, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 28.78, "energy_joules_est": 57.32, "sample_count": 20, "duration_seconds": 1.992}, "timestamp": "2026-01-19T15:28:24.828818"}
{"image_index": 430, "image_name": "000000049060.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049060.jpg", "image_width": 640, "image_height": 411, "image_resolution": "640x411", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2567.839, "latencies_ms": [2567.839], "images_per_second": 0.389, "prompt_tokens": 1113, "response_tokens_est": 57, "n_tiles": 1, "output_text": " 1. train: 1\n2. people: 4\n3. platform: 1\n4. sign: 1\n5. chimney: 1\n6. smoke: 1\n7. train car: 1\n8. building: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27558.1, "ram_available_mb": 98214.1, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 27558.1, "ram_available_mb": 98214.1, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.17, "peak": 40.18, "min": 16.16}, "VIN": {"avg": 67.77, "peak": 84.46, "min": 30.04}, "VIN_SYS_5V0": {"avg": 14.81, "peak": 15.95, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.94, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 27.17, "energy_joules_est": 69.78, "sample_count": 25, "duration_seconds": 2.568}, "timestamp": "2026-01-19T15:28:27.424350"}
{"image_index": 430, "image_name": "000000049060.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049060.jpg", "image_width": 640, "image_height": 411, "image_resolution": "640x411", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2287.596, "latencies_ms": [2287.596], "images_per_second": 0.437, "prompt_tokens": 1117, "response_tokens_est": 47, "n_tiles": 1, "output_text": " The train is on the left side of the image, with the platform on the right. The people are standing on the platform, near the train. The train is in the foreground, with the station building and trees in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27558.1, "ram_available_mb": 98214.1, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 27558.1, "ram_available_mb": 98214.1, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.31, "peak": 40.16, "min": 16.56}, "VIN": {"avg": 71.32, "peak": 106.14, "min": 34.04}, "VIN_SYS_5V0": {"avg": 15.07, "peak": 16.15, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 16.18, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 28.31, "energy_joules_est": 64.77, "sample_count": 22, "duration_seconds": 2.288}, "timestamp": "2026-01-19T15:28:29.718875"}
{"image_index": 430, "image_name": "000000049060.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049060.jpg", "image_width": 640, "image_height": 411, "image_resolution": "640x411", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1583.316, "latencies_ms": [1583.316], "images_per_second": 0.632, "prompt_tokens": 1111, "response_tokens_est": 20, "n_tiles": 1, "output_text": " A black and white photo of a train station with a steam locomotive and people waiting for the train.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27558.1, "ram_available_mb": 98214.1, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 27558.1, "ram_available_mb": 98214.1, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.07, "peak": 40.56, "min": 17.35}, "VIN": {"avg": 75.84, "peak": 119.83, "min": 28.35}, "VIN_SYS_5V0": {"avg": 15.16, "peak": 16.15, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 16.31, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 31.07, "energy_joules_est": 49.21, "sample_count": 16, "duration_seconds": 1.584}, "timestamp": "2026-01-19T15:28:31.393681"}
{"image_index": 430, "image_name": "000000049060.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049060.jpg", "image_width": 640, "image_height": 411, "image_resolution": "640x411", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1721.02, "latencies_ms": [1721.02], "images_per_second": 0.581, "prompt_tokens": 1109, "response_tokens_est": 24, "n_tiles": 1, "output_text": " The image is in black and white, with the train and people in the foreground and the background showing a clear sky.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27558.1, "ram_available_mb": 98214.1, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 27558.1, "ram_available_mb": 98214.1, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.75, "peak": 40.57, "min": 18.14}, "VIN": {"avg": 75.9, "peak": 127.67, "min": 30.18}, "VIN_SYS_5V0": {"avg": 14.78, "peak": 16.05, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.89, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 30.75, "energy_joules_est": 52.93, "sample_count": 17, "duration_seconds": 1.721}, "timestamp": "2026-01-19T15:28:33.170770"}
{"image_index": 431, "image_name": "000000049091.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049091.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1876.457, "latencies_ms": [1876.457], "images_per_second": 0.533, "prompt_tokens": 1099, "response_tokens_est": 31, "n_tiles": 1, "output_text": " The image captures a bustling street scene in a densely populated urban area, with numerous signs and advertisements hanging from the buildings, creating a vibrant and colorful atmosphere.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 27558.1, "ram_available_mb": 98214.1, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 27558.1, "ram_available_mb": 98214.1, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.86, "peak": 40.57, "min": 18.53}, "VIN": {"avg": 71.88, "peak": 123.28, "min": 29.19}, "VIN_SYS_5V0": {"avg": 14.89, "peak": 16.05, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 16.0, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 29.86, "energy_joules_est": 56.05, "sample_count": 19, "duration_seconds": 1.877}, "timestamp": "2026-01-19T15:28:35.156317"}
{"image_index": 431, "image_name": "000000049091.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049091.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2655.316, "latencies_ms": [2655.316], "images_per_second": 0.377, "prompt_tokens": 1113, "response_tokens_est": 60, "n_tiles": 1, "output_text": " 1. sign: 10\n2. building: 1\n3. window: 1\n4. air conditioner: 1\n5. power line: 1\n6. street sign: 1\n7. pole: 1\n8. wire: 1", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 27558.1, "ram_available_mb": 98214.1, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 27558.1, "ram_available_mb": 98214.1, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.9, "peak": 40.18, "min": 16.56}, "VIN": {"avg": 68.93, "peak": 105.83, "min": 28.6}, "VIN_SYS_5V0": {"avg": 14.76, "peak": 15.95, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.9, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 26.9, "energy_joules_est": 71.44, "sample_count": 26, "duration_seconds": 2.656}, "timestamp": "2026-01-19T15:28:37.864459"}
{"image_index": 431, "image_name": "000000049091.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049091.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2097.411, "latencies_ms": [2097.411], "images_per_second": 0.477, "prompt_tokens": 1117, "response_tokens_est": 39, "n_tiles": 1, "output_text": " The main objects are the street signs and the buildings. The street signs are in the foreground, while the buildings are in the background. The street signs are closer to the viewer than the buildings.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27558.1, "ram_available_mb": 98214.1, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 27558.0, "ram_available_mb": 98214.1, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.49, "peak": 40.57, "min": 16.16}, "VIN": {"avg": 73.13, "peak": 121.19, "min": 28.06}, "VIN_SYS_5V0": {"avg": 14.88, "peak": 16.05, "min": 12.46}, "VDD_CPU_SOC_MSS": {"avg": 15.94, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 28.49, "energy_joules_est": 59.77, "sample_count": 21, "duration_seconds": 2.098}, "timestamp": "2026-01-19T15:28:40.054625"}
{"image_index": 431, "image_name": "000000049091.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049091.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1618.538, "latencies_ms": [1618.538], "images_per_second": 0.618, "prompt_tokens": 1111, "response_tokens_est": 20, "n_tiles": 1, "output_text": " This is a black and white photo of a street in Hong Kong with lots of signs and buildings.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27558.0, "ram_available_mb": 98214.1, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 27558.0, "ram_available_mb": 98214.1, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.44, "peak": 40.18, "min": 15.77}, "VIN": {"avg": 71.71, "peak": 114.09, "min": 30.9}, "VIN_SYS_5V0": {"avg": 14.7, "peak": 15.95, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.87, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 30.44, "energy_joules_est": 49.28, "sample_count": 16, "duration_seconds": 1.619}, "timestamp": "2026-01-19T15:28:41.724803"}
{"image_index": 431, "image_name": "000000049091.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049091.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1990.601, "latencies_ms": [1990.601], "images_per_second": 0.502, "prompt_tokens": 1109, "response_tokens_est": 35, "n_tiles": 1, "output_text": " The image is in black and white, with a high contrast of light and dark. The buildings are covered in signs and advertisements, with a mix of metal and glass materials.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27558.0, "ram_available_mb": 98214.1, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 27558.0, "ram_available_mb": 98214.1, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.59, "peak": 40.57, "min": 18.92}, "VIN": {"avg": 73.96, "peak": 129.89, "min": 28.65}, "VIN_SYS_5V0": {"avg": 14.87, "peak": 16.05, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.98, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 29.59, "energy_joules_est": 58.92, "sample_count": 20, "duration_seconds": 1.991}, "timestamp": "2026-01-19T15:28:43.817234"}
{"image_index": 432, "image_name": "000000049259.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049259.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1935.264, "latencies_ms": [1935.264], "images_per_second": 0.517, "prompt_tokens": 1100, "response_tokens_est": 33, "n_tiles": 1, "output_text": " A man is lying on a concrete ledge by a body of water, while a sign reading \"OPEN\" is placed on the ground in front of him.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 27558.0, "ram_available_mb": 98214.1, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 27558.0, "ram_available_mb": 98214.1, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.95, "peak": 39.77, "min": 15.77}, "VIN": {"avg": 68.97, "peak": 104.32, "min": 28.67}, "VIN_SYS_5V0": {"avg": 14.75, "peak": 16.05, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 15.85, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 28.95, "energy_joules_est": 56.05, "sample_count": 19, "duration_seconds": 1.936}, "timestamp": "2026-01-19T15:28:45.805376"}
{"image_index": 432, "image_name": "000000049259.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049259.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2618.734, "latencies_ms": [2618.734], "images_per_second": 0.382, "prompt_tokens": 1114, "response_tokens_est": 60, "n_tiles": 1, "output_text": " 1. man: 1\n2. grass: 1\n3. lake: 1\n4. sign: 1\n5. wood: 1\n6. water: 1\n7. man's shorts: 1\n8. man's shirt: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27558.0, "ram_available_mb": 98214.1, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 27558.0, "ram_available_mb": 98214.1, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.08, "peak": 40.18, "min": 17.35}, "VIN": {"avg": 68.48, "peak": 125.27, "min": 29.35}, "VIN_SYS_5V0": {"avg": 14.98, "peak": 16.15, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 16.1, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 27.08, "energy_joules_est": 70.93, "sample_count": 26, "duration_seconds": 2.619}, "timestamp": "2026-01-19T15:28:48.510254"}
{"image_index": 432, "image_name": "000000049259.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049259.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2204.257, "latencies_ms": [2204.257], "images_per_second": 0.454, "prompt_tokens": 1118, "response_tokens_est": 43, "n_tiles": 1, "output_text": " The sign is in the foreground, close to the camera, and the man is in the background, far away from the camera. The man is sitting on the grass, and the sign is lying on the ground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27558.0, "ram_available_mb": 98214.1, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 27558.0, "ram_available_mb": 98214.1, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.78, "peak": 40.16, "min": 15.38}, "VIN": {"avg": 64.75, "peak": 129.32, "min": 27.89}, "VIN_SYS_5V0": {"avg": 14.77, "peak": 16.05, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.86, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 27.78, "energy_joules_est": 61.25, "sample_count": 22, "duration_seconds": 2.205}, "timestamp": "2026-01-19T15:28:50.803012"}
{"image_index": 432, "image_name": "000000049259.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049259.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2138.683, "latencies_ms": [2138.683], "images_per_second": 0.468, "prompt_tokens": 1112, "response_tokens_est": 41, "n_tiles": 1, "output_text": " A man is sunbathing on a concrete ledge overlooking a body of water. In the foreground, there is a sign that reads \"OPEN\" and \"CLOSED\" in red letters.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27558.0, "ram_available_mb": 98214.1, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 27558.0, "ram_available_mb": 98214.1, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.12, "peak": 39.39, "min": 15.77}, "VIN": {"avg": 70.85, "peak": 116.18, "min": 28.96}, "VIN_SYS_5V0": {"avg": 14.86, "peak": 16.15, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.99, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 28.12, "energy_joules_est": 60.15, "sample_count": 21, "duration_seconds": 2.139}, "timestamp": "2026-01-19T15:28:52.989454"}
{"image_index": 432, "image_name": "000000049259.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049259.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2532.242, "latencies_ms": [2532.242], "images_per_second": 0.395, "prompt_tokens": 1110, "response_tokens_est": 56, "n_tiles": 1, "output_text": " The image features a man lounging on a concrete ledge by a body of water, with a sign reading \"OPEN\" in the foreground. The scene is bathed in natural light, and the grass is a vibrant green, contrasting with the blue of the water.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27558.0, "ram_available_mb": 98214.1, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 27558.0, "ram_available_mb": 98214.1, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.17, "peak": 40.16, "min": 16.95}, "VIN": {"avg": 73.99, "peak": 127.62, "min": 28.82}, "VIN_SYS_5V0": {"avg": 14.95, "peak": 16.15, "min": 12.56}, "VDD_CPU_SOC_MSS": {"avg": 16.08, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 27.17, "energy_joules_est": 68.81, "sample_count": 25, "duration_seconds": 2.533}, "timestamp": "2026-01-19T15:28:55.596735"}
{"image_index": 433, "image_name": "000000049269.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049269.jpg", "image_width": 431, "image_height": 640, "image_resolution": "431x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1906.602, "latencies_ms": [1906.602], "images_per_second": 0.524, "prompt_tokens": 1100, "response_tokens_est": 32, "n_tiles": 1, "output_text": " In the image, there is a brown horse standing in a field of tall grass, while a white dog with a pink tongue is sitting in the grass nearby.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27558.0, "ram_available_mb": 98214.1, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 27558.0, "ram_available_mb": 98214.1, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.91, "peak": 40.18, "min": 15.38}, "VIN": {"avg": 75.11, "peak": 118.85, "min": 27.34}, "VIN_SYS_5V0": {"avg": 14.81, "peak": 16.05, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.89, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 28.91, "energy_joules_est": 55.13, "sample_count": 19, "duration_seconds": 1.907}, "timestamp": "2026-01-19T15:28:57.581077"}
{"image_index": 433, "image_name": "000000049269.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049269.jpg", "image_width": 431, "image_height": 640, "image_resolution": "431x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2345.042, "latencies_ms": [2345.042], "images_per_second": 0.426, "prompt_tokens": 1114, "response_tokens_est": 43, "n_tiles": 1, "output_text": " horse: 1, dog: 1, grass: 1, horse's tail: 1, horse's mane: 1, horse's ear: 1, horse's eye: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27558.0, "ram_available_mb": 98214.1, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 27558.0, "ram_available_mb": 98214.1, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.37, "peak": 40.18, "min": 16.56}, "VIN": {"avg": 69.03, "peak": 117.88, "min": 30.21}, "VIN_SYS_5V0": {"avg": 14.72, "peak": 16.15, "min": 12.15}, "VDD_CPU_SOC_MSS": {"avg": 15.68, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 27.37, "energy_joules_est": 64.19, "sample_count": 23, "duration_seconds": 2.345}, "timestamp": "2026-01-19T15:28:59.976675"}
{"image_index": 433, "image_name": "000000049269.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049269.jpg", "image_width": 431, "image_height": 640, "image_resolution": "431x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2287.98, "latencies_ms": [2287.98], "images_per_second": 0.437, "prompt_tokens": 1118, "response_tokens_est": 47, "n_tiles": 1, "output_text": " The brown horse is positioned to the left of the white dog, with the dog being in the foreground and the horse in the background. The dog is closer to the camera than the horse, and the horse is positioned behind the dog.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27558.0, "ram_available_mb": 98214.1, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 27558.0, "ram_available_mb": 98214.1, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.9, "peak": 40.18, "min": 14.98}, "VIN": {"avg": 73.6, "peak": 122.37, "min": 32.76}, "VIN_SYS_5V0": {"avg": 14.93, "peak": 16.15, "min": 11.87}, "VDD_CPU_SOC_MSS": {"avg": 15.95, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 27.9, "energy_joules_est": 63.85, "sample_count": 22, "duration_seconds": 2.288}, "timestamp": "2026-01-19T15:29:02.271360"}
{"image_index": 433, "image_name": "000000049269.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049269.jpg", "image_width": 431, "image_height": 640, "image_resolution": "431x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2077.69, "latencies_ms": [2077.69], "images_per_second": 0.481, "prompt_tokens": 1112, "response_tokens_est": 40, "n_tiles": 1, "output_text": " In a serene field, a brown horse and a golden retriever are enjoying a sunny day. The horse stands tall, while the dog sits comfortably in the grass, both looking content and happy.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 27558.0, "ram_available_mb": 98214.1, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 27558.0, "ram_available_mb": 98214.1, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.07, "peak": 39.39, "min": 17.34}, "VIN": {"avg": 79.28, "peak": 125.26, "min": 58.01}, "VIN_SYS_5V0": {"avg": 15.31, "peak": 16.26, "min": 13.29}, "VDD_CPU_SOC_MSS": {"avg": 16.48, "peak": 17.71, "min": 14.57}}, "power_watts_avg": 29.07, "energy_joules_est": 60.41, "sample_count": 20, "duration_seconds": 2.078}, "timestamp": "2026-01-19T15:29:04.352119"}
{"image_index": 433, "image_name": "000000049269.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049269.jpg", "image_width": 431, "image_height": 640, "image_resolution": "431x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2388.895, "latencies_ms": [2388.895], "images_per_second": 0.419, "prompt_tokens": 1110, "response_tokens_est": 52, "n_tiles": 1, "output_text": " The image features a brown horse and a white dog in a grassy field, with the horse wearing a bridle and the dog panting with its tongue out. The lighting is natural and bright, suggesting it is daytime, and the weather appears to be clear.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 27558.0, "ram_available_mb": 98214.1, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 27558.0, "ram_available_mb": 98214.1, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.24, "peak": 39.78, "min": 18.52}, "VIN": {"avg": 73.57, "peak": 124.26, "min": 54.31}, "VIN_SYS_5V0": {"avg": 15.26, "peak": 16.26, "min": 13.29}, "VDD_CPU_SOC_MSS": {"avg": 16.38, "peak": 17.71, "min": 14.57}}, "power_watts_avg": 28.24, "energy_joules_est": 67.47, "sample_count": 23, "duration_seconds": 2.389}, "timestamp": "2026-01-19T15:29:06.745052"}
{"image_index": 434, "image_name": "000000049759.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049759.jpg", "image_width": 640, "image_height": 457, "image_resolution": "640x457", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1885.182, "latencies_ms": [1885.182], "images_per_second": 0.53, "prompt_tokens": 1099, "response_tokens_est": 32, "n_tiles": 1, "output_text": " In the indoor volleyball court, a group of people are playing volleyball, with one player in the foreground wearing a blue shirt and the other players in green shirts.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27558.0, "ram_available_mb": 98214.1, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 27558.0, "ram_available_mb": 98214.1, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.59, "peak": 40.16, "min": 17.35}, "VIN": {"avg": 74.41, "peak": 122.34, "min": 27.67}, "VIN_SYS_5V0": {"avg": 15.13, "peak": 16.15, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 16.27, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 29.59, "energy_joules_est": 55.8, "sample_count": 19, "duration_seconds": 1.886}, "timestamp": "2026-01-19T15:29:08.732330"}
{"image_index": 434, "image_name": "000000049759.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049759.jpg", "image_width": 640, "image_height": 457, "image_resolution": "640x457", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2415.126, "latencies_ms": [2415.126], "images_per_second": 0.414, "prompt_tokens": 1113, "response_tokens_est": 51, "n_tiles": 1, "output_text": " volleyball: 1\nvolleyball: 1\ncourt: 1\nmen: 1\nmen's: 1\nmen's volleyball: 1\nmen's volleyball court: 1\nmen's volleyball court floor: 1", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 27558.0, "ram_available_mb": 98214.1, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 27558.0, "ram_available_mb": 98214.1, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.4, "peak": 40.18, "min": 16.16}, "VIN": {"avg": 64.43, "peak": 126.93, "min": 27.34}, "VIN_SYS_5V0": {"avg": 14.79, "peak": 15.95, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.81, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 27.4, "energy_joules_est": 66.18, "sample_count": 24, "duration_seconds": 2.415}, "timestamp": "2026-01-19T15:29:11.237684"}
{"image_index": 434, "image_name": "000000049759.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049759.jpg", "image_width": 640, "image_height": 457, "image_resolution": "640x457", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2444.175, "latencies_ms": [2444.175], "images_per_second": 0.409, "prompt_tokens": 1117, "response_tokens_est": 52, "n_tiles": 1, "output_text": " The volleyball court is located in the center of the image, with the players positioned on both sides of the net. The players are standing close to each other, indicating a team formation. The players are wearing uniforms, which suggests they are part of a team.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27558.0, "ram_available_mb": 98214.1, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 27558.0, "ram_available_mb": 98214.2, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.3, "peak": 40.18, "min": 15.38}, "VIN": {"avg": 69.91, "peak": 121.15, "min": 29.25}, "VIN_SYS_5V0": {"avg": 14.84, "peak": 16.05, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.85, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 27.3, "energy_joules_est": 66.74, "sample_count": 24, "duration_seconds": 2.445}, "timestamp": "2026-01-19T15:29:13.736427"}
{"image_index": 434, "image_name": "000000049759.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049759.jpg", "image_width": 640, "image_height": 457, "image_resolution": "640x457", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2941.82, "latencies_ms": [2941.82], "images_per_second": 0.34, "prompt_tokens": 1111, "response_tokens_est": 72, "n_tiles": 1, "output_text": " The image captures a lively volleyball game in an indoor gymnasium. The players, clad in blue uniforms, are actively engaged in the game, their movements and interactions highlighting the dynamic nature of the sport. The gymnasium itself is well-equipped with a high ceiling, large windows, and a basketball hoop, suggesting a versatile space for various sports activities.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27558.0, "ram_available_mb": 98214.2, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 27558.0, "ram_available_mb": 98214.2, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.15, "peak": 40.18, "min": 16.16}, "VIN": {"avg": 67.18, "peak": 125.11, "min": 28.47}, "VIN_SYS_5V0": {"avg": 14.92, "peak": 16.05, "min": 12.36}, "VDD_CPU_SOC_MSS": {"avg": 15.87, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 26.15, "energy_joules_est": 76.94, "sample_count": 29, "duration_seconds": 2.942}, "timestamp": "2026-01-19T15:29:16.745856"}
{"image_index": 434, "image_name": "000000049759.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049759.jpg", "image_width": 640, "image_height": 457, "image_resolution": "640x457", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1564.407, "latencies_ms": [1564.407], "images_per_second": 0.639, "prompt_tokens": 1109, "response_tokens_est": 18, "n_tiles": 1, "output_text": " The indoor volleyball court is blue with white lines, and the players are wearing blue uniforms.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27558.0, "ram_available_mb": 98214.2, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 27558.0, "ram_available_mb": 98214.2, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.86, "peak": 40.18, "min": 15.38}, "VIN": {"avg": 75.36, "peak": 118.49, "min": 33.35}, "VIN_SYS_5V0": {"avg": 14.87, "peak": 16.05, "min": 12.15}, "VDD_CPU_SOC_MSS": {"avg": 15.91, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 30.86, "energy_joules_est": 48.29, "sample_count": 15, "duration_seconds": 1.565}, "timestamp": "2026-01-19T15:29:18.313844"}
{"image_index": 435, "image_name": "000000049761.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049761.jpg", "image_width": 640, "image_height": 479, "image_resolution": "640x479", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2215.215, "latencies_ms": [2215.215], "images_per_second": 0.451, "prompt_tokens": 1099, "response_tokens_est": 43, "n_tiles": 1, "output_text": " In the image, a herd of zebras and wildebeests are grazing in a grassy field, with a flock of pink flamingos standing in the water behind them, and a mountain range in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27558.0, "ram_available_mb": 98214.2, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 27558.0, "ram_available_mb": 98214.2, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.06, "peak": 40.97, "min": 18.52}, "VIN": {"avg": 69.74, "peak": 121.5, "min": 29.67}, "VIN_SYS_5V0": {"avg": 15.13, "peak": 16.15, "min": 13.29}, "VDD_CPU_SOC_MSS": {"avg": 16.09, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 29.06, "energy_joules_est": 64.39, "sample_count": 22, "duration_seconds": 2.216}, "timestamp": "2026-01-19T15:29:20.609967"}
{"image_index": 435, "image_name": "000000049761.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049761.jpg", "image_width": 640, "image_height": 479, "image_resolution": "640x479", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2875.674, "latencies_ms": [2875.674], "images_per_second": 0.348, "prompt_tokens": 1113, "response_tokens_est": 69, "n_tiles": 1, "output_text": " 1. zebras: 4\n2. flamingos: 100\n3. wildebeest: 2\n4. grass: 100\n5. water: 100\n6. mountains: 1\n7. trees: 10\n8. water body: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27558.0, "ram_available_mb": 98214.2, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 27558.0, "ram_available_mb": 98214.2, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.24, "peak": 40.18, "min": 15.77}, "VIN": {"avg": 70.19, "peak": 125.55, "min": 30.02}, "VIN_SYS_5V0": {"avg": 14.83, "peak": 16.05, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 15.78, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 26.24, "energy_joules_est": 75.47, "sample_count": 28, "duration_seconds": 2.876}, "timestamp": "2026-01-19T15:29:23.529394"}
{"image_index": 435, "image_name": "000000049761.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049761.jpg", "image_width": 640, "image_height": 479, "image_resolution": "640x479", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2196.091, "latencies_ms": [2196.091], "images_per_second": 0.455, "prompt_tokens": 1117, "response_tokens_est": 43, "n_tiles": 1, "output_text": " The zebras are positioned in the foreground of the image, with the flamingos in the background. The zebras are closer to the camera than the flamingos, which are situated in the middle of the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27558.0, "ram_available_mb": 98214.2, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 27558.0, "ram_available_mb": 98214.2, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.1, "peak": 40.18, "min": 16.16}, "VIN": {"avg": 64.42, "peak": 120.12, "min": 27.18}, "VIN_SYS_5V0": {"avg": 14.91, "peak": 16.05, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.93, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 28.1, "energy_joules_est": 61.72, "sample_count": 22, "duration_seconds": 2.197}, "timestamp": "2026-01-19T15:29:25.819439"}
{"image_index": 435, "image_name": "000000049761.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049761.jpg", "image_width": 640, "image_height": 479, "image_resolution": "640x479", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2389.698, "latencies_ms": [2389.698], "images_per_second": 0.418, "prompt_tokens": 1111, "response_tokens_est": 50, "n_tiles": 1, "output_text": " In the vast savannah, a herd of zebras and wildebeests graze peacefully, while a flock of pink flamingos wades in the nearby lake. The tranquil scene is set against the backdrop of rolling hills and a clear blue sky.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27558.0, "ram_available_mb": 98214.2, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 27558.0, "ram_available_mb": 98214.2, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.72, "peak": 40.18, "min": 15.77}, "VIN": {"avg": 69.19, "peak": 93.63, "min": 32.43}, "VIN_SYS_5V0": {"avg": 14.84, "peak": 15.95, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.85, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 27.72, "energy_joules_est": 66.26, "sample_count": 23, "duration_seconds": 2.39}, "timestamp": "2026-01-19T15:29:28.215355"}
{"image_index": 435, "image_name": "000000049761.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049761.jpg", "image_width": 640, "image_height": 479, "image_resolution": "640x479", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2379.255, "latencies_ms": [2379.255], "images_per_second": 0.42, "prompt_tokens": 1109, "response_tokens_est": 51, "n_tiles": 1, "output_text": " The image features a vibrant scene with a mix of green grass, brown dirt, and pink flamingos in the background. The lighting is natural and bright, suggesting a sunny day, and the colors are vivid and clear, indicating a healthy and thriving environment.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27558.0, "ram_available_mb": 98214.2, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 27558.0, "ram_available_mb": 98214.2, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.03, "peak": 40.57, "min": 17.35}, "VIN": {"avg": 72.91, "peak": 127.48, "min": 29.16}, "VIN_SYS_5V0": {"avg": 15.18, "peak": 16.15, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 16.11, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 28.03, "energy_joules_est": 66.7, "sample_count": 23, "duration_seconds": 2.38}, "timestamp": "2026-01-19T15:29:30.621390"}
{"image_index": 436, "image_name": "000000049810.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049810.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1598.423, "latencies_ms": [1598.423], "images_per_second": 0.626, "prompt_tokens": 1099, "response_tokens_est": 20, "n_tiles": 1, "output_text": " A ginger and white cat is sitting on a wooden deck and looking at its reflection in a window.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 27558.0, "ram_available_mb": 98214.2, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 27558.0, "ram_available_mb": 98214.2, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.78, "peak": 40.57, "min": 16.56}, "VIN": {"avg": 71.16, "peak": 104.96, "min": 30.64}, "VIN_SYS_5V0": {"avg": 14.99, "peak": 16.15, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 16.19, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 30.78, "energy_joules_est": 49.22, "sample_count": 16, "duration_seconds": 1.599}, "timestamp": "2026-01-19T15:29:32.297895"}
{"image_index": 436, "image_name": "000000049810.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049810.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2058.404, "latencies_ms": [2058.404], "images_per_second": 0.486, "prompt_tokens": 1113, "response_tokens_est": 37, "n_tiles": 1, "output_text": " cat: 1, wooden floor: 1, glass: 1, mirror: 1, wall: 1, window: 1, cat's tail: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27558.0, "ram_available_mb": 98214.2, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 27559.2, "ram_available_mb": 98212.9, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.49, "peak": 40.57, "min": 18.92}, "VIN": {"avg": 70.64, "peak": 105.41, "min": 29.3}, "VIN_SYS_5V0": {"avg": 14.91, "peak": 16.05, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.88, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 29.49, "energy_joules_est": 60.71, "sample_count": 20, "duration_seconds": 2.059}, "timestamp": "2026-01-19T15:29:34.387526"}
{"image_index": 436, "image_name": "000000049810.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049810.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1777.33, "latencies_ms": [1777.33], "images_per_second": 0.563, "prompt_tokens": 1117, "response_tokens_est": 27, "n_tiles": 1, "output_text": " The cat is in the foreground, sitting on the wooden deck. The glass window is in the background, reflecting the cat's image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27559.2, "ram_available_mb": 98212.9, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 27558.9, "ram_available_mb": 98213.3, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.61, "peak": 40.56, "min": 17.35}, "VIN": {"avg": 76.5, "peak": 121.76, "min": 35.57}, "VIN_SYS_5V0": {"avg": 15.11, "peak": 16.15, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 16.23, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 30.61, "energy_joules_est": 54.41, "sample_count": 17, "duration_seconds": 1.778}, "timestamp": "2026-01-19T15:29:36.167824"}
{"image_index": 436, "image_name": "000000049810.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049810.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1576.126, "latencies_ms": [1576.126], "images_per_second": 0.634, "prompt_tokens": 1111, "response_tokens_est": 20, "n_tiles": 1, "output_text": " A ginger and white cat is sitting on a wooden deck, looking at its reflection in a window.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 27558.9, "ram_available_mb": 98213.3, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 27558.9, "ram_available_mb": 98213.3, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.79, "peak": 40.57, "min": 19.7}, "VIN": {"avg": 72.81, "peak": 126.8, "min": 27.37}, "VIN_SYS_5V0": {"avg": 15.22, "peak": 16.26, "min": 13.29}, "VDD_CPU_SOC_MSS": {"avg": 16.39, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 31.79, "energy_joules_est": 50.12, "sample_count": 16, "duration_seconds": 1.577}, "timestamp": "2026-01-19T15:29:37.836428"}
{"image_index": 436, "image_name": "000000049810.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049810.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1755.747, "latencies_ms": [1755.747], "images_per_second": 0.57, "prompt_tokens": 1109, "response_tokens_est": 25, "n_tiles": 1, "output_text": " The cat is orange and white, and the wooden floor is green. The cat is looking at its reflection in the window.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 27558.9, "ram_available_mb": 98213.3, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 27558.9, "ram_available_mb": 98213.3, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.66, "peak": 40.16, "min": 18.53}, "VIN": {"avg": 70.38, "peak": 113.88, "min": 30.54}, "VIN_SYS_5V0": {"avg": 14.74, "peak": 15.95, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.93, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 30.66, "energy_joules_est": 53.85, "sample_count": 17, "duration_seconds": 1.756}, "timestamp": "2026-01-19T15:29:39.614039"}
{"image_index": 437, "image_name": "000000050006.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050006.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2040.582, "latencies_ms": [2040.582], "images_per_second": 0.49, "prompt_tokens": 1099, "response_tokens_est": 38, "n_tiles": 1, "output_text": " The image captures a serene lakeside scene with boats docked at a pier, a quaint town nestled on the shore, and majestic mountains in the background, all under a hazy sky.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 27558.9, "ram_available_mb": 98213.3, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 27558.9, "ram_available_mb": 98213.3, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.71, "peak": 40.18, "min": 19.7}, "VIN": {"avg": 76.54, "peak": 127.22, "min": 29.09}, "VIN_SYS_5V0": {"avg": 15.14, "peak": 16.15, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 16.26, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 29.71, "energy_joules_est": 60.64, "sample_count": 20, "duration_seconds": 2.041}, "timestamp": "2026-01-19T15:29:41.703662"}
{"image_index": 437, "image_name": "000000050006.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050006.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2533.937, "latencies_ms": [2533.937], "images_per_second": 0.395, "prompt_tokens": 1113, "response_tokens_est": 56, "n_tiles": 1, "output_text": " 1. Boat: 3\n2. Boat: 2\n3. Boat: 1\n4. Boat: 1\n5. Boat: 1\n6. Boat: 1\n7. Boat: 1\n8. Boat: 1", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 27558.9, "ram_available_mb": 98213.3, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 27558.9, "ram_available_mb": 98213.3, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.34, "peak": 40.57, "min": 16.95}, "VIN": {"avg": 71.44, "peak": 110.78, "min": 29.79}, "VIN_SYS_5V0": {"avg": 14.98, "peak": 16.05, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 16.03, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 27.34, "energy_joules_est": 69.29, "sample_count": 25, "duration_seconds": 2.534}, "timestamp": "2026-01-19T15:29:44.305434"}
{"image_index": 437, "image_name": "000000050006.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050006.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2036.377, "latencies_ms": [2036.377], "images_per_second": 0.491, "prompt_tokens": 1117, "response_tokens_est": 37, "n_tiles": 1, "output_text": " The boats are positioned in the foreground, with the shoreline and buildings in the background. The skyline of mountains is visible in the far background, creating a sense of depth and distance.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27558.9, "ram_available_mb": 98213.3, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 27560.5, "ram_available_mb": 98211.7, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.8, "peak": 40.18, "min": 15.77}, "VIN": {"avg": 69.97, "peak": 88.36, "min": 30.16}, "VIN_SYS_5V0": {"avg": 14.96, "peak": 16.05, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 16.0, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 28.8, "energy_joules_est": 58.66, "sample_count": 20, "duration_seconds": 2.037}, "timestamp": "2026-01-19T15:29:46.392494"}
{"image_index": 437, "image_name": "000000050006.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050006.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1946.025, "latencies_ms": [1946.025], "images_per_second": 0.514, "prompt_tokens": 1111, "response_tokens_est": 34, "n_tiles": 1, "output_text": " The image captures a serene lakeside scene with boats docked at a pier, a building with a red roof in the background, and a mountainous landscape in the distance.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 27560.5, "ram_available_mb": 98211.7, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 27560.5, "ram_available_mb": 98211.7, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.61, "peak": 40.57, "min": 16.95}, "VIN": {"avg": 71.32, "peak": 118.78, "min": 29.72}, "VIN_SYS_5V0": {"avg": 15.06, "peak": 16.05, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 16.16, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 29.61, "energy_joules_est": 57.63, "sample_count": 19, "duration_seconds": 1.946}, "timestamp": "2026-01-19T15:29:48.368456"}
{"image_index": 437, "image_name": "000000050006.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050006.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1556.707, "latencies_ms": [1556.707], "images_per_second": 0.642, "prompt_tokens": 1109, "response_tokens_est": 19, "n_tiles": 1, "output_text": " The sky is overcast, the water is a deep blue, and the boats are white.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 27560.5, "ram_available_mb": 98211.7, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 27560.5, "ram_available_mb": 98211.7, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.75, "peak": 40.16, "min": 17.73}, "VIN": {"avg": 72.32, "peak": 121.86, "min": 29.44}, "VIN_SYS_5V0": {"avg": 15.16, "peak": 16.15, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 16.3, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 31.75, "energy_joules_est": 49.44, "sample_count": 15, "duration_seconds": 1.557}, "timestamp": "2026-01-19T15:29:49.938494"}
{"image_index": 438, "image_name": "000000050145.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050145.jpg", "image_width": 480, "image_height": 320, "image_resolution": "480x320", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1537.624, "latencies_ms": [1537.624], "images_per_second": 0.65, "prompt_tokens": 1099, "response_tokens_est": 18, "n_tiles": 1, "output_text": " A man is standing next to a bicycle in a street with a building in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27560.5, "ram_available_mb": 98211.7, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 27560.5, "ram_available_mb": 98211.7, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.62, "peak": 40.97, "min": 20.89}, "VIN": {"avg": 69.67, "peak": 86.7, "min": 30.23}, "VIN_SYS_5V0": {"avg": 15.21, "peak": 16.15, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 16.35, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 32.62, "energy_joules_est": 50.18, "sample_count": 15, "duration_seconds": 1.538}, "timestamp": "2026-01-19T15:29:51.507673"}
{"image_index": 438, "image_name": "000000050145.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050145.jpg", "image_width": 480, "image_height": 320, "image_resolution": "480x320", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2527.412, "latencies_ms": [2527.412], "images_per_second": 0.396, "prompt_tokens": 1113, "response_tokens_est": 56, "n_tiles": 1, "output_text": " 1. man: 1\n2. bicycle: 1\n3. umbrella: 1\n4. building: 1\n5. sign: 2\n6. person: 1\n7. pole: 1\n8. street: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27560.5, "ram_available_mb": 98211.7, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 27560.5, "ram_available_mb": 98211.7, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.06, "peak": 40.57, "min": 18.13}, "VIN": {"avg": 68.02, "peak": 126.03, "min": 29.27}, "VIN_SYS_5V0": {"avg": 15.04, "peak": 16.15, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 16.13, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 28.06, "energy_joules_est": 70.93, "sample_count": 25, "duration_seconds": 2.528}, "timestamp": "2026-01-19T15:29:54.115164"}
{"image_index": 438, "image_name": "000000050145.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050145.jpg", "image_width": 480, "image_height": 320, "image_resolution": "480x320", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2698.491, "latencies_ms": [2698.491], "images_per_second": 0.371, "prompt_tokens": 1117, "response_tokens_est": 62, "n_tiles": 1, "output_text": " The man is standing next to a bicycle, which is in the foreground of the image. The bicycle is positioned to the left of the man, and the man is standing on the right side of the bicycle. The man is also standing in front of a building, which is in the background of the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27560.5, "ram_available_mb": 98211.7, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 27560.5, "ram_available_mb": 98211.7, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.81, "peak": 40.18, "min": 15.38}, "VIN": {"avg": 67.63, "peak": 123.14, "min": 34.4}, "VIN_SYS_5V0": {"avg": 14.88, "peak": 16.05, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 15.87, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 26.81, "energy_joules_est": 72.36, "sample_count": 26, "duration_seconds": 2.699}, "timestamp": "2026-01-19T15:29:56.822184"}
{"image_index": 438, "image_name": "000000050145.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050145.jpg", "image_width": 480, "image_height": 320, "image_resolution": "480x320", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1537.422, "latencies_ms": [1537.422], "images_per_second": 0.65, "prompt_tokens": 1111, "response_tokens_est": 18, "n_tiles": 1, "output_text": " A man is standing next to a bicycle in a street with a building in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27560.5, "ram_available_mb": 98211.7, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 27560.5, "ram_available_mb": 98211.7, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.54, "peak": 40.56, "min": 16.95}, "VIN": {"avg": 80.13, "peak": 122.05, "min": 29.51}, "VIN_SYS_5V0": {"avg": 15.25, "peak": 16.15, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 16.4, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 31.54, "energy_joules_est": 48.5, "sample_count": 15, "duration_seconds": 1.538}, "timestamp": "2026-01-19T15:29:58.391430"}
{"image_index": 438, "image_name": "000000050145.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050145.jpg", "image_width": 480, "image_height": 320, "image_resolution": "480x320", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1858.002, "latencies_ms": [1858.002], "images_per_second": 0.538, "prompt_tokens": 1109, "response_tokens_est": 30, "n_tiles": 1, "output_text": " The image is in black and white, with a street scene in the background. The lighting is natural, and the weather appears to be overcast.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 27560.5, "ram_available_mb": 98211.7, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 27560.5, "ram_available_mb": 98211.7, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.97, "peak": 40.95, "min": 20.49}, "VIN": {"avg": 72.48, "peak": 99.1, "min": 29.21}, "VIN_SYS_5V0": {"avg": 15.15, "peak": 16.15, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 16.21, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 30.97, "energy_joules_est": 57.55, "sample_count": 18, "duration_seconds": 1.858}, "timestamp": "2026-01-19T15:30:00.270051"}
{"image_index": 439, "image_name": "000000050149.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050149.jpg", "image_width": 500, "image_height": 376, "image_resolution": "500x376", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2092.045, "latencies_ms": [2092.045], "images_per_second": 0.478, "prompt_tokens": 1432, "response_tokens_est": 24, "n_tiles": 1, "output_text": " A bunch of bananas are hanging from the ceiling of a store, and there is a black shirt hanging on the wall.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27560.5, "ram_available_mb": 98211.7, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 27560.7, "ram_available_mb": 98211.4, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.2, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.5, "peak": 40.18, "min": 18.14}, "VIN": {"avg": 74.85, "peak": 118.66, "min": 30.05}, "VIN_SYS_5V0": {"avg": 15.47, "peak": 16.76, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 16.59, "peak": 18.11, "min": 14.18}}, "power_watts_avg": 31.5, "energy_joules_est": 65.92, "sample_count": 21, "duration_seconds": 2.093}, "timestamp": "2026-01-19T15:30:02.462151"}
{"image_index": 439, "image_name": "000000050149.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050149.jpg", "image_width": 500, "image_height": 376, "image_resolution": "500x376", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2732.253, "latencies_ms": [2732.253], "images_per_second": 0.366, "prompt_tokens": 1446, "response_tokens_est": 45, "n_tiles": 1, "output_text": " banana: 100, bunches: 100, bananas: 100, black cloth: 1, blue door: 1, white wall: 1, wooden stick: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27560.7, "ram_available_mb": 98211.4, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 27560.1, "ram_available_mb": 98212.1, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.06, "peak": 40.16, "min": 17.35}, "VIN": {"avg": 72.08, "peak": 120.48, "min": 30.03}, "VIN_SYS_5V0": {"avg": 14.99, "peak": 16.56, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.98, "peak": 17.71, "min": 13.0}}, "power_watts_avg": 29.06, "energy_joules_est": 79.41, "sample_count": 27, "duration_seconds": 2.733}, "timestamp": "2026-01-19T15:30:05.273070"}
{"image_index": 439, "image_name": "000000050149.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050149.jpg", "image_width": 500, "image_height": 376, "image_resolution": "500x376", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2546.028, "latencies_ms": [2546.028], "images_per_second": 0.393, "prompt_tokens": 1450, "response_tokens_est": 41, "n_tiles": 1, "output_text": " The bananas are hanging from the ceiling, with the clothes hanging below them. The bananas are in the foreground, while the clothes are in the background. The bananas are closer to the viewer than the clothes.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27560.1, "ram_available_mb": 98212.1, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 27560.1, "ram_available_mb": 98212.1, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.75, "peak": 40.18, "min": 15.77}, "VIN": {"avg": 72.95, "peak": 124.15, "min": 29.01}, "VIN_SYS_5V0": {"avg": 15.11, "peak": 16.56, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 16.22, "peak": 17.71, "min": 13.0}}, "power_watts_avg": 29.75, "energy_joules_est": 75.75, "sample_count": 25, "duration_seconds": 2.546}, "timestamp": "2026-01-19T15:30:07.874895"}
{"image_index": 439, "image_name": "000000050149.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050149.jpg", "image_width": 500, "image_height": 376, "image_resolution": "500x376", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1794.832, "latencies_ms": [1794.832], "images_per_second": 0.557, "prompt_tokens": 1444, "response_tokens_est": 13, "n_tiles": 1, "output_text": " A bunch of bananas are hanging from the ceiling of a store.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 27560.1, "ram_available_mb": 98212.1, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 2.2, "ram_used_mb": 27560.1, "ram_available_mb": 98212.1, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.76, "peak": 40.57, "min": 16.95}, "VIN": {"avg": 79.78, "peak": 122.35, "min": 27.03}, "VIN_SYS_5V0": {"avg": 15.25, "peak": 16.56, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 16.45, "peak": 17.71, "min": 13.39}}, "power_watts_avg": 32.76, "energy_joules_est": 58.82, "sample_count": 18, "duration_seconds": 1.796}, "timestamp": "2026-01-19T15:30:09.752491"}
{"image_index": 439, "image_name": "000000050149.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050149.jpg", "image_width": 500, "image_height": 376, "image_resolution": "500x376", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2075.496, "latencies_ms": [2075.496], "images_per_second": 0.482, "prompt_tokens": 1442, "response_tokens_est": 23, "n_tiles": 1, "output_text": " The bananas are yellow and hanging from the ceiling, the lighting is bright and natural, and the weather is sunny.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 27560.1, "ram_available_mb": 98212.1, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 27560.1, "ram_available_mb": 98212.1, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.7, "peak": 40.97, "min": 20.49}, "VIN": {"avg": 75.53, "peak": 117.28, "min": 30.15}, "VIN_SYS_5V0": {"avg": 15.22, "peak": 16.56, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 16.34, "peak": 17.71, "min": 13.39}}, "power_watts_avg": 32.7, "energy_joules_est": 67.88, "sample_count": 20, "duration_seconds": 2.076}, "timestamp": "2026-01-19T15:30:11.848710"}
{"image_index": 440, "image_name": "000000050165.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050165.jpg", "image_width": 640, "image_height": 431, "image_resolution": "640x431", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1743.097, "latencies_ms": [1743.097], "images_per_second": 0.574, "prompt_tokens": 1099, "response_tokens_est": 26, "n_tiles": 1, "output_text": " A green and white electric train with red and green cargo cars is traveling on a track through a green field with a mountainous backdrop.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27560.1, "ram_available_mb": 98212.1, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 27560.1, "ram_available_mb": 98212.1, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.26, "peak": 40.97, "min": 19.7}, "VIN": {"avg": 70.96, "peak": 116.49, "min": 28.83}, "VIN_SYS_5V0": {"avg": 15.2, "peak": 16.15, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 16.28, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 31.26, "energy_joules_est": 54.5, "sample_count": 17, "duration_seconds": 1.744}, "timestamp": "2026-01-19T15:30:13.624795"}
{"image_index": 440, "image_name": "000000050165.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050165.jpg", "image_width": 640, "image_height": 431, "image_resolution": "640x431", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2598.995, "latencies_ms": [2598.995], "images_per_second": 0.385, "prompt_tokens": 1113, "response_tokens_est": 59, "n_tiles": 1, "output_text": " 1. train: 1\n2. train cars: 3\n3. train tracks: 1\n4. power lines: 1\n5. mountains: 1\n6. grass: 1\n7. trees: 1\n8. houses: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27560.1, "ram_available_mb": 98212.1, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 27560.1, "ram_available_mb": 98212.1, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.83, "peak": 40.56, "min": 18.91}, "VIN": {"avg": 70.84, "peak": 101.75, "min": 37.27}, "VIN_SYS_5V0": {"avg": 15.07, "peak": 16.15, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 16.24, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 27.83, "energy_joules_est": 72.34, "sample_count": 25, "duration_seconds": 2.599}, "timestamp": "2026-01-19T15:30:16.227293"}
{"image_index": 440, "image_name": "000000050165.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050165.jpg", "image_width": 640, "image_height": 431, "image_resolution": "640x431", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2462.929, "latencies_ms": [2462.929], "images_per_second": 0.406, "prompt_tokens": 1117, "response_tokens_est": 54, "n_tiles": 1, "output_text": " The train is positioned on the left side of the image, moving from left to right. The background features a mountain range, while the foreground shows a grassy field. The train is closer to the viewer than the mountains, and the grassy field is in the middle ground.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 27560.1, "ram_available_mb": 98212.1, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 27560.1, "ram_available_mb": 98212.1, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.77, "peak": 40.57, "min": 17.34}, "VIN": {"avg": 71.74, "peak": 124.1, "min": 28.87}, "VIN_SYS_5V0": {"avg": 15.17, "peak": 16.26, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 16.22, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 27.77, "energy_joules_est": 68.41, "sample_count": 24, "duration_seconds": 2.463}, "timestamp": "2026-01-19T15:30:18.729879"}
{"image_index": 440, "image_name": "000000050165.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050165.jpg", "image_width": 640, "image_height": 431, "image_resolution": "640x431", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1650.78, "latencies_ms": [1650.78], "images_per_second": 0.606, "prompt_tokens": 1111, "response_tokens_est": 22, "n_tiles": 1, "output_text": " A green and white train with red and green cars is traveling through a green field with mountains in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27560.1, "ram_available_mb": 98212.1, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 27560.8, "ram_available_mb": 98211.3, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.85, "peak": 40.57, "min": 16.56}, "VIN": {"avg": 73.96, "peak": 122.53, "min": 29.39}, "VIN_SYS_5V0": {"avg": 15.03, "peak": 16.15, "min": 12.66}, "VDD_CPU_SOC_MSS": {"avg": 16.19, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 30.85, "energy_joules_est": 50.94, "sample_count": 16, "duration_seconds": 1.651}, "timestamp": "2026-01-19T15:30:20.403092"}
{"image_index": 440, "image_name": "000000050165.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050165.jpg", "image_width": 640, "image_height": 431, "image_resolution": "640x431", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1488.795, "latencies_ms": [1488.795], "images_per_second": 0.672, "prompt_tokens": 1109, "response_tokens_est": 16, "n_tiles": 1, "output_text": " The train is green and white, and the sky is blue with white clouds.", "error": null, "sys_before": {"cpu_percent": 50.0, "ram_used_mb": 27560.8, "ram_available_mb": 98211.3, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 27560.8, "ram_available_mb": 98211.3, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.25, "peak": 40.97, "min": 19.7}, "VIN": {"avg": 70.66, "peak": 116.24, "min": 28.32}, "VIN_SYS_5V0": {"avg": 15.11, "peak": 16.15, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 16.22, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 32.25, "energy_joules_est": 48.02, "sample_count": 15, "duration_seconds": 1.489}, "timestamp": "2026-01-19T15:30:21.972249"}
{"image_index": 441, "image_name": "000000050326.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050326.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1854.174, "latencies_ms": [1854.174], "images_per_second": 0.539, "prompt_tokens": 1099, "response_tokens_est": 30, "n_tiles": 1, "output_text": " A man wearing a hat and shorts is standing on the beach with his arms up, and there is a green chair and a kite in the background.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 27560.8, "ram_available_mb": 98211.3, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 27560.8, "ram_available_mb": 98211.3, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.77, "peak": 40.97, "min": 19.71}, "VIN": {"avg": 77.43, "peak": 124.22, "min": 30.03}, "VIN_SYS_5V0": {"avg": 14.89, "peak": 16.05, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 16.01, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 30.77, "energy_joules_est": 57.06, "sample_count": 18, "duration_seconds": 1.854}, "timestamp": "2026-01-19T15:30:23.852459"}
{"image_index": 441, "image_name": "000000050326.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050326.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2501.667, "latencies_ms": [2501.667], "images_per_second": 0.4, "prompt_tokens": 1113, "response_tokens_est": 56, "n_tiles": 1, "output_text": " 1. man: 1\n2. hat: 1\n3. shorts: 1\n4. sand: 1\n5. chair: 1\n6. kite: 1\n7. ocean: 1\n8. sky: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27560.8, "ram_available_mb": 98211.3, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 27560.8, "ram_available_mb": 98211.3, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.7, "peak": 40.57, "min": 17.74}, "VIN": {"avg": 71.35, "peak": 121.79, "min": 29.94}, "VIN_SYS_5V0": {"avg": 15.1, "peak": 16.15, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 16.1, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 27.7, "energy_joules_est": 69.31, "sample_count": 25, "duration_seconds": 2.502}, "timestamp": "2026-01-19T15:30:26.450426"}
{"image_index": 441, "image_name": "000000050326.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050326.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2042.599, "latencies_ms": [2042.599], "images_per_second": 0.49, "prompt_tokens": 1117, "response_tokens_est": 37, "n_tiles": 1, "output_text": " The man is standing on the left side of the image, with the ocean waves on the right side. The green chair is in the foreground, while the kite is in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27560.8, "ram_available_mb": 98211.3, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 27560.8, "ram_available_mb": 98211.3, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.82, "peak": 40.18, "min": 15.38}, "VIN": {"avg": 71.72, "peak": 120.42, "min": 28.24}, "VIN_SYS_5V0": {"avg": 14.8, "peak": 16.05, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.87, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 28.82, "energy_joules_est": 58.88, "sample_count": 20, "duration_seconds": 2.043}, "timestamp": "2026-01-19T15:30:28.536379"}
{"image_index": 441, "image_name": "000000050326.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050326.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1740.255, "latencies_ms": [1740.255], "images_per_second": 0.575, "prompt_tokens": 1111, "response_tokens_est": 26, "n_tiles": 1, "output_text": " A man is standing on a beach with his arms up, holding a cell phone, and a kite is flying in the sky.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27560.8, "ram_available_mb": 98211.3, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 27560.8, "ram_available_mb": 98211.3, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.52, "peak": 40.57, "min": 16.95}, "VIN": {"avg": 72.11, "peak": 121.76, "min": 27.35}, "VIN_SYS_5V0": {"avg": 15.06, "peak": 16.15, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 16.17, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 30.52, "energy_joules_est": 53.13, "sample_count": 17, "duration_seconds": 1.741}, "timestamp": "2026-01-19T15:30:30.313044"}
{"image_index": 441, "image_name": "000000050326.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050326.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1587.619, "latencies_ms": [1587.619], "images_per_second": 0.63, "prompt_tokens": 1109, "response_tokens_est": 20, "n_tiles": 1, "output_text": " The sky is blue and cloudy, and the man is wearing a black shirt and khaki shorts.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27560.8, "ram_available_mb": 98211.3, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 27560.8, "ram_available_mb": 98211.4, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.44, "peak": 40.56, "min": 18.52}, "VIN": {"avg": 80.73, "peak": 127.86, "min": 28.07}, "VIN_SYS_5V0": {"avg": 15.09, "peak": 16.15, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 16.17, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 31.44, "energy_joules_est": 49.93, "sample_count": 16, "duration_seconds": 1.588}, "timestamp": "2026-01-19T15:30:31.983974"}
{"image_index": 442, "image_name": "000000050331.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050331.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1614.313, "latencies_ms": [1614.313], "images_per_second": 0.619, "prompt_tokens": 1099, "response_tokens_est": 21, "n_tiles": 1, "output_text": " The image shows a garden with several potted plants, including broccoli and cabbage, planted in the soil.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 27560.8, "ram_available_mb": 98211.4, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 27560.8, "ram_available_mb": 98211.4, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.42, "peak": 40.95, "min": 18.53}, "VIN": {"avg": 74.76, "peak": 115.81, "min": 30.63}, "VIN_SYS_5V0": {"avg": 14.92, "peak": 16.05, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.99, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 31.42, "energy_joules_est": 50.73, "sample_count": 16, "duration_seconds": 1.615}, "timestamp": "2026-01-19T15:30:33.657857"}
{"image_index": 442, "image_name": "000000050331.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050331.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1566.928, "latencies_ms": [1566.928], "images_per_second": 0.638, "prompt_tokens": 1113, "response_tokens_est": 19, "n_tiles": 1, "output_text": " broccoli: 2, pot: 2, dirt: 1, plant: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27560.8, "ram_available_mb": 98211.4, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 27560.8, "ram_available_mb": 98211.4, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.25, "peak": 40.57, "min": 19.31}, "VIN": {"avg": 78.74, "peak": 125.91, "min": 32.65}, "VIN_SYS_5V0": {"avg": 15.05, "peak": 16.15, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 16.25, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 32.25, "energy_joules_est": 50.54, "sample_count": 15, "duration_seconds": 1.567}, "timestamp": "2026-01-19T15:30:35.230086"}
{"image_index": 442, "image_name": "000000050331.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050331.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2011.657, "latencies_ms": [2011.657], "images_per_second": 0.497, "prompt_tokens": 1117, "response_tokens_est": 37, "n_tiles": 1, "output_text": " The broccoli plants are in the foreground, with the dirt and the pink animal in the background. The broccoli plants are near the orange pots, and the dirt is near the broccoli plants.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27560.8, "ram_available_mb": 98211.4, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 27560.8, "ram_available_mb": 98211.4, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.32, "peak": 40.57, "min": 19.32}, "VIN": {"avg": 71.05, "peak": 103.93, "min": 29.77}, "VIN_SYS_5V0": {"avg": 15.24, "peak": 16.26, "min": 13.49}, "VDD_CPU_SOC_MSS": {"avg": 16.34, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 30.32, "energy_joules_est": 61.0, "sample_count": 20, "duration_seconds": 2.012}, "timestamp": "2026-01-19T15:30:37.316337"}
{"image_index": 442, "image_name": "000000050331.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050331.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2993.283, "latencies_ms": [2993.283], "images_per_second": 0.334, "prompt_tokens": 1111, "response_tokens_est": 73, "n_tiles": 1, "output_text": " The image captures a serene scene of a garden where two large potted plants are thriving. The plants, with their lush green leaves, are the main focus of the image. They are housed in terracotta pots, which are placed on a sandy ground. The background is a simple backdrop of a brown wall, which adds a warm tone to the overall image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27560.8, "ram_available_mb": 98211.4, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 27560.8, "ram_available_mb": 98211.4, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.2, "peak": 40.18, "min": 16.56}, "VIN": {"avg": 72.69, "peak": 127.39, "min": 32.26}, "VIN_SYS_5V0": {"avg": 14.88, "peak": 16.05, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.87, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 26.2, "energy_joules_est": 78.44, "sample_count": 29, "duration_seconds": 2.994}, "timestamp": "2026-01-19T15:30:40.316220"}
{"image_index": 442, "image_name": "000000050331.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050331.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1490.578, "latencies_ms": [1490.578], "images_per_second": 0.671, "prompt_tokens": 1109, "response_tokens_est": 17, "n_tiles": 1, "output_text": " The image shows a garden with green plants in pots, and the lighting is natural.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27560.8, "ram_available_mb": 98211.4, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 27560.8, "ram_available_mb": 98211.4, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.54, "peak": 40.18, "min": 17.74}, "VIN": {"avg": 78.1, "peak": 111.26, "min": 27.79}, "VIN_SYS_5V0": {"avg": 15.26, "peak": 16.26, "min": 13.29}, "VDD_CPU_SOC_MSS": {"avg": 16.38, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 31.54, "energy_joules_est": 47.02, "sample_count": 15, "duration_seconds": 1.491}, "timestamp": "2026-01-19T15:30:41.888932"}
{"image_index": 443, "image_name": "000000050380.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050380.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1825.576, "latencies_ms": [1825.576], "images_per_second": 0.548, "prompt_tokens": 1099, "response_tokens_est": 28, "n_tiles": 1, "output_text": " An elderly man is walking a small brown and white pony with a red bridle, while a young boy sits on the pony's back.", "error": null, "sys_before": {"cpu_percent": 7.1, "ram_used_mb": 27560.8, "ram_available_mb": 98211.4, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 27560.8, "ram_available_mb": 98211.4, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.4, "peak": 40.56, "min": 18.92}, "VIN": {"avg": 71.72, "peak": 111.59, "min": 29.44}, "VIN_SYS_5V0": {"avg": 14.8, "peak": 16.05, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 15.84, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 30.4, "energy_joules_est": 55.51, "sample_count": 18, "duration_seconds": 1.826}, "timestamp": "2026-01-19T15:30:43.772720"}
{"image_index": 443, "image_name": "000000050380.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050380.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2537.687, "latencies_ms": [2537.687], "images_per_second": 0.394, "prompt_tokens": 1113, "response_tokens_est": 56, "n_tiles": 1, "output_text": " 1. person: 2\n2. child: 1\n3. horse: 1\n4. pony: 1\n5. man: 1\n6. woman: 1\n7. chair: 1\n8. building: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27560.8, "ram_available_mb": 98211.4, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 27560.8, "ram_available_mb": 98211.4, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.55, "peak": 40.57, "min": 18.13}, "VIN": {"avg": 67.27, "peak": 130.47, "min": 28.95}, "VIN_SYS_5V0": {"avg": 14.93, "peak": 16.05, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.9, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 27.55, "energy_joules_est": 69.93, "sample_count": 25, "duration_seconds": 2.538}, "timestamp": "2026-01-19T15:30:46.377709"}
{"image_index": 443, "image_name": "000000050380.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050380.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2238.812, "latencies_ms": [2238.812], "images_per_second": 0.447, "prompt_tokens": 1117, "response_tokens_est": 44, "n_tiles": 1, "output_text": " The man is holding the pony's reins, which are in front of him, and the pony is in front of the man. The pony is in the foreground, while the man and the boy are in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27560.8, "ram_available_mb": 98211.4, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 27560.8, "ram_available_mb": 98211.4, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.06, "peak": 40.57, "min": 15.77}, "VIN": {"avg": 70.58, "peak": 128.08, "min": 29.58}, "VIN_SYS_5V0": {"avg": 14.91, "peak": 16.05, "min": 12.25}, "VDD_CPU_SOC_MSS": {"avg": 15.89, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 28.06, "energy_joules_est": 62.84, "sample_count": 22, "duration_seconds": 2.239}, "timestamp": "2026-01-19T15:30:48.675497"}
{"image_index": 443, "image_name": "000000050380.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050380.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1634.761, "latencies_ms": [1634.761], "images_per_second": 0.612, "prompt_tokens": 1111, "response_tokens_est": 21, "n_tiles": 1, "output_text": " An elderly man and a young boy are walking down a cobblestone street with a small brown horse.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 27560.8, "ram_available_mb": 98211.4, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 27560.8, "ram_available_mb": 98211.4, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.7, "peak": 40.18, "min": 16.16}, "VIN": {"avg": 75.64, "peak": 119.77, "min": 29.91}, "VIN_SYS_5V0": {"avg": 14.9, "peak": 16.05, "min": 12.25}, "VDD_CPU_SOC_MSS": {"avg": 16.02, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 30.7, "energy_joules_est": 50.2, "sample_count": 16, "duration_seconds": 1.635}, "timestamp": "2026-01-19T15:30:50.348761"}
{"image_index": 443, "image_name": "000000050380.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050380.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2714.208, "latencies_ms": [2714.208], "images_per_second": 0.368, "prompt_tokens": 1109, "response_tokens_est": 60, "n_tiles": 1, "output_text": " The image features a young boy riding a small brown and white pony, with the pony wearing a red bridle. The setting appears to be a sunny day, with the sun casting shadows on the ground. The pony is standing on a paved area, and the boy is wearing a light blue shirt.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 27560.8, "ram_available_mb": 98211.4, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 27560.8, "ram_available_mb": 98211.4, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.98, "peak": 40.57, "min": 17.35}, "VIN": {"avg": 69.5, "peak": 122.03, "min": 29.47}, "VIN_SYS_5V0": {"avg": 14.88, "peak": 16.05, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 15.93, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 26.98, "energy_joules_est": 73.24, "sample_count": 27, "duration_seconds": 2.714}, "timestamp": "2026-01-19T15:30:53.156167"}
{"image_index": 444, "image_name": "000000050638.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050638.jpg", "image_width": 640, "image_height": 393, "image_resolution": "640x393", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1928.023, "latencies_ms": [1928.023], "images_per_second": 0.519, "prompt_tokens": 1099, "response_tokens_est": 32, "n_tiles": 1, "output_text": " A young boy with blonde hair and blue jeans is walking down a dirt path in a field of blue flowers, carrying a brown teddy bear on his back.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 27560.8, "ram_available_mb": 98211.4, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 27560.8, "ram_available_mb": 98211.4, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.82, "peak": 40.18, "min": 14.59}, "VIN": {"avg": 68.5, "peak": 95.77, "min": 30.0}, "VIN_SYS_5V0": {"avg": 14.7, "peak": 15.95, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 15.71, "peak": 17.32, "min": 12.61}}, "power_watts_avg": 28.82, "energy_joules_est": 55.58, "sample_count": 19, "duration_seconds": 1.928}, "timestamp": "2026-01-19T15:30:55.142903"}
{"image_index": 444, "image_name": "000000050638.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050638.jpg", "image_width": 640, "image_height": 393, "image_resolution": "640x393", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2734.454, "latencies_ms": [2734.454], "images_per_second": 0.366, "prompt_tokens": 1113, "response_tokens_est": 64, "n_tiles": 1, "output_text": " 1. child: 1\n2. blue shirt: 1\n3. blue jeans: 1\n4. brown shoes: 1\n5. brown teddy bear: 1\n6. blue flowers: 1\n7. dirt path: 1\n8. green leaves: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27560.8, "ram_available_mb": 98211.4, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 27560.8, "ram_available_mb": 98211.4, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.84, "peak": 40.57, "min": 17.35}, "VIN": {"avg": 68.88, "peak": 102.59, "min": 29.94}, "VIN_SYS_5V0": {"avg": 14.93, "peak": 16.05, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 16.03, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 26.84, "energy_joules_est": 73.4, "sample_count": 27, "duration_seconds": 2.735}, "timestamp": "2026-01-19T15:30:57.945704"}
{"image_index": 444, "image_name": "000000050638.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050638.jpg", "image_width": 640, "image_height": 393, "image_resolution": "640x393", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2136.245, "latencies_ms": [2136.245], "images_per_second": 0.468, "prompt_tokens": 1117, "response_tokens_est": 40, "n_tiles": 1, "output_text": " The child is positioned on the right side of the image, with the path leading into the distance. The child is standing on the left side of the path, with the flowers on the right side.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27560.8, "ram_available_mb": 98211.4, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 27560.8, "ram_available_mb": 98211.4, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.31, "peak": 40.18, "min": 15.38}, "VIN": {"avg": 67.25, "peak": 123.66, "min": 30.16}, "VIN_SYS_5V0": {"avg": 14.86, "peak": 16.05, "min": 12.15}, "VDD_CPU_SOC_MSS": {"avg": 15.86, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 28.31, "energy_joules_est": 60.49, "sample_count": 21, "duration_seconds": 2.137}, "timestamp": "2026-01-19T15:31:00.136201"}
{"image_index": 444, "image_name": "000000050638.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050638.jpg", "image_width": 640, "image_height": 393, "image_resolution": "640x393", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1734.829, "latencies_ms": [1734.829], "images_per_second": 0.576, "prompt_tokens": 1111, "response_tokens_est": 25, "n_tiles": 1, "output_text": " A young boy with blonde hair is walking down a dirt path in a forest, carrying a teddy bear on his back.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 27560.8, "ram_available_mb": 98211.4, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 27560.8, "ram_available_mb": 98211.4, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.31, "peak": 40.18, "min": 16.56}, "VIN": {"avg": 71.38, "peak": 125.43, "min": 28.9}, "VIN_SYS_5V0": {"avg": 14.9, "peak": 16.05, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 16.0, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 30.31, "energy_joules_est": 52.6, "sample_count": 17, "duration_seconds": 1.735}, "timestamp": "2026-01-19T15:31:01.910767"}
{"image_index": 444, "image_name": "000000050638.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050638.jpg", "image_width": 640, "image_height": 393, "image_resolution": "640x393", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2990.111, "latencies_ms": [2990.111], "images_per_second": 0.334, "prompt_tokens": 1109, "response_tokens_est": 74, "n_tiles": 1, "output_text": " The image features a young boy with blonde hair, wearing a blue and white striped shirt and blue jeans, walking on a dirt path surrounded by a lush field of blue flowers. The lighting is natural and soft, suggesting it is either early morning or late afternoon. The boy is carrying a brown teddy bear on his back, adding a touch of warmth to the scene.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 27560.8, "ram_available_mb": 98211.4, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 27560.8, "ram_available_mb": 98211.4, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.6, "peak": 40.97, "min": 18.91}, "VIN": {"avg": 69.4, "peak": 117.29, "min": 29.97}, "VIN_SYS_5V0": {"avg": 15.05, "peak": 16.15, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 16.02, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 26.6, "energy_joules_est": 79.55, "sample_count": 29, "duration_seconds": 2.991}, "timestamp": "2026-01-19T15:31:04.928276"}
{"image_index": 445, "image_name": "000000050679.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050679.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1613.332, "latencies_ms": [1613.332], "images_per_second": 0.62, "prompt_tokens": 1099, "response_tokens_est": 21, "n_tiles": 1, "output_text": " An orange sits on the asphalt of a parking lot, with a line of cars parked in the background.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 27560.8, "ram_available_mb": 98211.4, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 27560.8, "ram_available_mb": 98211.4, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.8, "peak": 40.57, "min": 16.16}, "VIN": {"avg": 73.31, "peak": 107.54, "min": 30.02}, "VIN_SYS_5V0": {"avg": 15.05, "peak": 16.15, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 16.17, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 30.8, "energy_joules_est": 49.7, "sample_count": 16, "duration_seconds": 1.614}, "timestamp": "2026-01-19T15:31:06.601117"}
{"image_index": 445, "image_name": "000000050679.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050679.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1177.937, "latencies_ms": [1177.937], "images_per_second": 0.849, "prompt_tokens": 1113, "response_tokens_est": 4, "n_tiles": 1, "output_text": " orange: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27560.8, "ram_available_mb": 98211.4, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 2.0, "ram_used_mb": 27560.8, "ram_available_mb": 98211.4, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 33.29, "peak": 40.95, "min": 19.31}, "VIN": {"avg": 77.12, "peak": 105.18, "min": 29.06}, "VIN_SYS_5V0": {"avg": 14.91, "peak": 16.05, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 16.14, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 33.29, "energy_joules_est": 39.23, "sample_count": 12, "duration_seconds": 1.178}, "timestamp": "2026-01-19T15:31:07.858353"}
{"image_index": 445, "image_name": "000000050679.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050679.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2345.324, "latencies_ms": [2345.324], "images_per_second": 0.426, "prompt_tokens": 1117, "response_tokens_est": 46, "n_tiles": 1, "output_text": " The orange is positioned in the foreground, with the parking lot and cars in the background. The orange is located to the left of the white line on the asphalt, and the parking lot extends to the right side of the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27560.8, "ram_available_mb": 98211.4, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 27560.8, "ram_available_mb": 98211.4, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.36, "peak": 40.97, "min": 18.52}, "VIN": {"avg": 67.7, "peak": 122.81, "min": 29.13}, "VIN_SYS_5V0": {"avg": 14.82, "peak": 16.05, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.87, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 29.36, "energy_joules_est": 68.87, "sample_count": 23, "duration_seconds": 2.346}, "timestamp": "2026-01-19T15:31:10.258791"}
{"image_index": 445, "image_name": "000000050679.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050679.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1554.065, "latencies_ms": [1554.065], "images_per_second": 0.643, "prompt_tokens": 1111, "response_tokens_est": 18, "n_tiles": 1, "output_text": " A single orange sits on the ground in a parking lot, surrounded by cars and trees.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27560.8, "ram_available_mb": 98211.4, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 27560.8, "ram_available_mb": 98211.4, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.07, "peak": 40.57, "min": 16.16}, "VIN": {"avg": 78.97, "peak": 119.68, "min": 30.54}, "VIN_SYS_5V0": {"avg": 14.91, "peak": 16.05, "min": 12.25}, "VDD_CPU_SOC_MSS": {"avg": 15.98, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 31.07, "energy_joules_est": 48.3, "sample_count": 15, "duration_seconds": 1.555}, "timestamp": "2026-01-19T15:31:11.837650"}
{"image_index": 445, "image_name": "000000050679.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050679.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1767.983, "latencies_ms": [1767.983], "images_per_second": 0.566, "prompt_tokens": 1109, "response_tokens_est": 27, "n_tiles": 1, "output_text": " The orange is a vibrant orange color, and the asphalt is black. The sky is cloudy, and there are trees in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27560.8, "ram_available_mb": 98211.4, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 27560.8, "ram_available_mb": 98211.4, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.95, "peak": 40.97, "min": 19.7}, "VIN": {"avg": 71.34, "peak": 102.94, "min": 29.02}, "VIN_SYS_5V0": {"avg": 15.08, "peak": 16.15, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 16.16, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 30.95, "energy_joules_est": 54.73, "sample_count": 18, "duration_seconds": 1.768}, "timestamp": "2026-01-19T15:31:13.711794"}
{"image_index": 446, "image_name": "000000050811.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050811.jpg", "image_width": 640, "image_height": 605, "image_resolution": "640x605", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2043.349, "latencies_ms": [2043.349], "images_per_second": 0.489, "prompt_tokens": 1432, "response_tokens_est": 21, "n_tiles": 1, "output_text": " A man in a suit is sitting at a table with a bowl of food and several empty beer bottles.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 27560.8, "ram_available_mb": 98211.4, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 27560.8, "ram_available_mb": 98211.4, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.2, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.38, "peak": 40.18, "min": 16.56}, "VIN": {"avg": 76.1, "peak": 119.42, "min": 29.61}, "VIN_SYS_5V0": {"avg": 15.12, "peak": 16.66, "min": 11.56}, "VDD_CPU_SOC_MSS": {"avg": 16.22, "peak": 17.71, "min": 12.61}}, "power_watts_avg": 31.38, "energy_joules_est": 64.15, "sample_count": 20, "duration_seconds": 2.044}, "timestamp": "2026-01-19T15:31:15.813223"}
{"image_index": 446, "image_name": "000000050811.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050811.jpg", "image_width": 640, "image_height": 605, "image_resolution": "640x605", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2191.464, "latencies_ms": [2191.464], "images_per_second": 0.456, "prompt_tokens": 1446, "response_tokens_est": 27, "n_tiles": 1, "output_text": " man:1, bowl:1, bottle:4, keys:1, remote:1, table:1, wall:1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27560.8, "ram_available_mb": 98211.4, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 27560.8, "ram_available_mb": 98211.4, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.32, "peak": 40.18, "min": 18.92}, "VIN": {"avg": 76.5, "peak": 121.99, "min": 28.88}, "VIN_SYS_5V0": {"avg": 15.19, "peak": 16.66, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 16.28, "peak": 17.71, "min": 13.39}}, "power_watts_avg": 31.32, "energy_joules_est": 68.65, "sample_count": 22, "duration_seconds": 2.192}, "timestamp": "2026-01-19T15:31:18.111533"}
{"image_index": 446, "image_name": "000000050811.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050811.jpg", "image_width": 640, "image_height": 605, "image_resolution": "640x605", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2686.409, "latencies_ms": [2686.409], "images_per_second": 0.372, "prompt_tokens": 1450, "response_tokens_est": 45, "n_tiles": 1, "output_text": " The man is seated at a table with a bowl of food in front of him, which is positioned to his left. The bottles of beer are arranged to his right, with one bottle closer to the camera than the others.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 27560.8, "ram_available_mb": 98211.4, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 27560.8, "ram_available_mb": 98211.4, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.49, "peak": 40.18, "min": 16.95}, "VIN": {"avg": 75.07, "peak": 120.85, "min": 30.79}, "VIN_SYS_5V0": {"avg": 15.04, "peak": 16.46, "min": 11.46}, "VDD_CPU_SOC_MSS": {"avg": 16.13, "peak": 17.71, "min": 12.61}}, "power_watts_avg": 29.49, "energy_joules_est": 79.24, "sample_count": 26, "duration_seconds": 2.687}, "timestamp": "2026-01-19T15:31:20.815055"}
{"image_index": 446, "image_name": "000000050811.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050811.jpg", "image_width": 640, "image_height": 605, "image_resolution": "640x605", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2010.917, "latencies_ms": [2010.917], "images_per_second": 0.497, "prompt_tokens": 1444, "response_tokens_est": 21, "n_tiles": 1, "output_text": " A man in a suit is sitting at a table with a bowl of food and several bottles of beer.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27560.8, "ram_available_mb": 98211.4, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 27560.8, "ram_available_mb": 98211.4, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.95, "peak": 40.57, "min": 17.35}, "VIN": {"avg": 73.79, "peak": 117.6, "min": 29.79}, "VIN_SYS_5V0": {"avg": 15.38, "peak": 16.66, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 16.49, "peak": 17.71, "min": 14.18}}, "power_watts_avg": 31.95, "energy_joules_est": 64.26, "sample_count": 20, "duration_seconds": 2.011}, "timestamp": "2026-01-19T15:31:22.903345"}
{"image_index": 446, "image_name": "000000050811.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050811.jpg", "image_width": 640, "image_height": 605, "image_resolution": "640x605", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1941.293, "latencies_ms": [1941.293], "images_per_second": 0.515, "prompt_tokens": 1442, "response_tokens_est": 18, "n_tiles": 1, "output_text": " The man is wearing a grey suit and white shirt. The table is made of wood.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 27560.8, "ram_available_mb": 98211.4, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 27560.8, "ram_available_mb": 98211.4, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.64, "peak": 40.18, "min": 18.53}, "VIN": {"avg": 75.41, "peak": 123.31, "min": 29.33}, "VIN_SYS_5V0": {"avg": 15.18, "peak": 16.56, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 16.37, "peak": 17.71, "min": 13.39}}, "power_watts_avg": 32.64, "energy_joules_est": 63.37, "sample_count": 19, "duration_seconds": 1.942}, "timestamp": "2026-01-19T15:31:24.879132"}
{"image_index": 447, "image_name": "000000050828.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050828.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1892.57, "latencies_ms": [1892.57], "images_per_second": 0.528, "prompt_tokens": 1099, "response_tokens_est": 32, "n_tiles": 1, "output_text": " The image depicts a hotel room with a large bed, a desk, and a window, all of which are neatly arranged and appear to be in good condition.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27560.8, "ram_available_mb": 98211.4, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 27561.0, "ram_available_mb": 98211.1, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.29, "peak": 40.57, "min": 19.31}, "VIN": {"avg": 70.97, "peak": 125.71, "min": 29.17}, "VIN_SYS_5V0": {"avg": 15.03, "peak": 16.15, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 16.14, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 30.29, "energy_joules_est": 57.34, "sample_count": 19, "duration_seconds": 1.893}, "timestamp": "2026-01-19T15:31:26.869161"}
{"image_index": 447, "image_name": "000000050828.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050828.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2539.721, "latencies_ms": [2539.721], "images_per_second": 0.394, "prompt_tokens": 1113, "response_tokens_est": 56, "n_tiles": 1, "output_text": " 1. bed: 2\n2. pillows: 4\n3. chair: 1\n4. desk: 1\n5. lamp: 1\n6. window: 1\n7. carpet: 1\n8. wall: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27561.0, "ram_available_mb": 98211.1, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 27561.0, "ram_available_mb": 98211.1, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.23, "peak": 40.18, "min": 16.56}, "VIN": {"avg": 70.71, "peak": 119.94, "min": 28.96}, "VIN_SYS_5V0": {"avg": 14.85, "peak": 16.05, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.88, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 27.23, "energy_joules_est": 69.17, "sample_count": 25, "duration_seconds": 2.54}, "timestamp": "2026-01-19T15:31:29.462566"}
{"image_index": 447, "image_name": "000000050828.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050828.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2116.514, "latencies_ms": [2116.514], "images_per_second": 0.472, "prompt_tokens": 1117, "response_tokens_est": 40, "n_tiles": 1, "output_text": " The bed is positioned to the left of the desk, with the desk located in the foreground of the image. The window is situated to the right of the bed, providing natural light to the room.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27561.0, "ram_available_mb": 98211.1, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 27561.0, "ram_available_mb": 98211.1, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.55, "peak": 40.57, "min": 16.16}, "VIN": {"avg": 74.54, "peak": 126.36, "min": 29.33}, "VIN_SYS_5V0": {"avg": 14.92, "peak": 16.05, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.97, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 28.55, "energy_joules_est": 60.45, "sample_count": 21, "duration_seconds": 2.117}, "timestamp": "2026-01-19T15:31:31.655668"}
{"image_index": 447, "image_name": "000000050828.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050828.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1486.157, "latencies_ms": [1486.157], "images_per_second": 0.673, "prompt_tokens": 1111, "response_tokens_est": 15, "n_tiles": 1, "output_text": " A hotel room with a large bed, a desk, and a window.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27561.0, "ram_available_mb": 98211.1, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 27561.0, "ram_available_mb": 98211.1, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.05, "peak": 40.18, "min": 16.16}, "VIN": {"avg": 78.52, "peak": 129.02, "min": 28.69}, "VIN_SYS_5V0": {"avg": 14.74, "peak": 16.05, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 15.8, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 31.05, "energy_joules_est": 46.16, "sample_count": 15, "duration_seconds": 1.487}, "timestamp": "2026-01-19T15:31:33.222090"}
{"image_index": 447, "image_name": "000000050828.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050828.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1710.028, "latencies_ms": [1710.028], "images_per_second": 0.585, "prompt_tokens": 1109, "response_tokens_est": 24, "n_tiles": 1, "output_text": " The room is well-lit with natural light coming through the window, and the walls are painted in a light color.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27561.0, "ram_available_mb": 98211.1, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 27561.0, "ram_available_mb": 98211.1, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.17, "peak": 40.57, "min": 19.71}, "VIN": {"avg": 66.7, "peak": 122.48, "min": 28.96}, "VIN_SYS_5V0": {"avg": 14.83, "peak": 16.05, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.89, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 31.17, "energy_joules_est": 53.32, "sample_count": 17, "duration_seconds": 1.71}, "timestamp": "2026-01-19T15:31:34.994178"}
{"image_index": 448, "image_name": "000000050844.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050844.jpg", "image_width": 334, "image_height": 500, "image_resolution": "334x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1960.279, "latencies_ms": [1960.279], "images_per_second": 0.51, "prompt_tokens": 1100, "response_tokens_est": 34, "n_tiles": 1, "output_text": " Three stuffed animals, including a bear, a snowman, and a bear wearing a hat, are sitting on a blue and white surface with a red and orange background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27561.0, "ram_available_mb": 98211.1, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 27561.0, "ram_available_mb": 98211.2, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.71, "peak": 40.57, "min": 18.53}, "VIN": {"avg": 72.75, "peak": 115.45, "min": 29.81}, "VIN_SYS_5V0": {"avg": 14.87, "peak": 16.05, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 16.06, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 29.71, "energy_joules_est": 58.26, "sample_count": 19, "duration_seconds": 1.961}, "timestamp": "2026-01-19T15:31:36.990244"}
{"image_index": 448, "image_name": "000000050844.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050844.jpg", "image_width": 334, "image_height": 500, "image_resolution": "334x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2974.812, "latencies_ms": [2974.812], "images_per_second": 0.336, "prompt_tokens": 1114, "response_tokens_est": 74, "n_tiles": 1, "output_text": " 1. teddy bear: 3\n2. green hat: 1\n3. red scarf: 1\n4. black and white snowman: 1\n5. red Coca Cola hat: 1\n6. red Coca Cola shirt: 1\n7. green shirt: 1\n8. orange background: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27561.0, "ram_available_mb": 98211.2, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 27561.0, "ram_available_mb": 98211.2, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.3, "peak": 40.56, "min": 17.74}, "VIN": {"avg": 64.78, "peak": 103.07, "min": 30.18}, "VIN_SYS_5V0": {"avg": 14.99, "peak": 16.15, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 16.17, "peak": 17.71, "min": 14.19}}, "power_watts_avg": 26.3, "energy_joules_est": 78.24, "sample_count": 29, "duration_seconds": 2.975}, "timestamp": "2026-01-19T15:31:40.013576"}
{"image_index": 448, "image_name": "000000050844.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050844.jpg", "image_width": 334, "image_height": 500, "image_resolution": "334x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2649.692, "latencies_ms": [2649.692], "images_per_second": 0.377, "prompt_tokens": 1118, "response_tokens_est": 61, "n_tiles": 1, "output_text": " The teddy bear wearing the green hat is positioned to the left of the teddy bear with the red scarf, which is in front of the teddy bear with the black hat. The teddy bear with the red scarf is in the foreground, while the teddy bear with the black hat is in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27561.0, "ram_available_mb": 98211.2, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 27561.0, "ram_available_mb": 98211.2, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.7, "peak": 40.18, "min": 15.77}, "VIN": {"avg": 69.15, "peak": 127.2, "min": 30.64}, "VIN_SYS_5V0": {"avg": 14.97, "peak": 16.15, "min": 12.36}, "VDD_CPU_SOC_MSS": {"avg": 15.98, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 26.7, "energy_joules_est": 70.75, "sample_count": 26, "duration_seconds": 2.65}, "timestamp": "2026-01-19T15:31:42.728439"}
{"image_index": 448, "image_name": "000000050844.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050844.jpg", "image_width": 334, "image_height": 500, "image_resolution": "334x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1423.874, "latencies_ms": [1423.874], "images_per_second": 0.702, "prompt_tokens": 1112, "response_tokens_est": 13, "n_tiles": 1, "output_text": " Three stuffed animals are sitting on a table with a colorful background.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 27561.0, "ram_available_mb": 98211.2, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 27561.0, "ram_available_mb": 98211.2, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.04, "peak": 39.78, "min": 15.77}, "VIN": {"avg": 71.15, "peak": 113.1, "min": 29.33}, "VIN_SYS_5V0": {"avg": 14.83, "peak": 16.15, "min": 12.15}, "VDD_CPU_SOC_MSS": {"avg": 15.97, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 31.04, "energy_joules_est": 44.21, "sample_count": 14, "duration_seconds": 1.424}, "timestamp": "2026-01-19T15:31:44.189589"}
{"image_index": 448, "image_name": "000000050844.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050844.jpg", "image_width": 334, "image_height": 500, "image_resolution": "334x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1878.555, "latencies_ms": [1878.555], "images_per_second": 0.532, "prompt_tokens": 1110, "response_tokens_est": 31, "n_tiles": 1, "output_text": " The teddy bears are in a room with a red and orange background. The teddy bears are made of plush material and are stuffed with a soft filling.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27561.0, "ram_available_mb": 98211.2, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 27561.3, "ram_available_mb": 98210.9, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.54, "peak": 40.97, "min": 19.31}, "VIN": {"avg": 74.27, "peak": 125.24, "min": 27.52}, "VIN_SYS_5V0": {"avg": 14.99, "peak": 16.15, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 16.18, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 30.54, "energy_joules_est": 57.39, "sample_count": 19, "duration_seconds": 1.879}, "timestamp": "2026-01-19T15:31:46.169601"}
{"image_index": 449, "image_name": "000000050896.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050896.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1763.299, "latencies_ms": [1763.299], "images_per_second": 0.567, "prompt_tokens": 1432, "response_tokens_est": 11, "n_tiles": 1, "output_text": " A glass bowl filled with oranges sits on a table.", "error": null, "sys_before": {"cpu_percent": 6.3, "ram_used_mb": 27561.3, "ram_available_mb": 98210.9, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 2.4, "ram_used_mb": 27561.3, "ram_available_mb": 98210.9, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.2, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.56, "peak": 40.18, "min": 16.16}, "VIN": {"avg": 76.42, "peak": 119.33, "min": 30.93}, "VIN_SYS_5V0": {"avg": 15.21, "peak": 16.76, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 16.37, "peak": 17.71, "min": 12.61}}, "power_watts_avg": 32.56, "energy_joules_est": 57.44, "sample_count": 17, "duration_seconds": 1.764}, "timestamp": "2026-01-19T15:31:47.954996"}
{"image_index": 449, "image_name": "000000050896.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050896.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1860.597, "latencies_ms": [1860.597], "images_per_second": 0.537, "prompt_tokens": 1446, "response_tokens_est": 11, "n_tiles": 1, "output_text": " bowl: 1\noranges: 12", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27561.3, "ram_available_mb": 98210.9, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 27561.3, "ram_available_mb": 98210.9, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 34.01, "peak": 41.34, "min": 23.25}, "VIN": {"avg": 72.82, "peak": 129.41, "min": 28.07}, "VIN_SYS_5V0": {"avg": 15.43, "peak": 16.86, "min": 13.49}, "VDD_CPU_SOC_MSS": {"avg": 16.58, "peak": 18.11, "min": 14.57}}, "power_watts_avg": 34.01, "energy_joules_est": 63.3, "sample_count": 18, "duration_seconds": 1.861}, "timestamp": "2026-01-19T15:31:49.835688"}
{"image_index": 449, "image_name": "000000050896.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050896.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2647.243, "latencies_ms": [2647.243], "images_per_second": 0.378, "prompt_tokens": 1450, "response_tokens_est": 43, "n_tiles": 1, "output_text": " The bowl is located in the foreground, and the oranges are placed inside the bowl. The oranges are arranged in a circular pattern, with some oranges placed closer to the center of the bowl and others towards the edges.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27561.3, "ram_available_mb": 98210.9, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 27561.3, "ram_available_mb": 98210.9, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.91, "peak": 40.57, "min": 18.91}, "VIN": {"avg": 74.49, "peak": 127.63, "min": 29.59}, "VIN_SYS_5V0": {"avg": 15.18, "peak": 16.66, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 16.25, "peak": 17.71, "min": 13.79}}, "power_watts_avg": 29.91, "energy_joules_est": 79.19, "sample_count": 26, "duration_seconds": 2.648}, "timestamp": "2026-01-19T15:31:52.551918"}
{"image_index": 449, "image_name": "000000050896.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050896.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1730.877, "latencies_ms": [1730.877], "images_per_second": 0.578, "prompt_tokens": 1444, "response_tokens_est": 10, "n_tiles": 1, "output_text": " A bowl of oranges is sitting on a table.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 27561.3, "ram_available_mb": 98210.9, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 2.3, "ram_used_mb": 27561.3, "ram_available_mb": 98210.9, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.77, "peak": 40.18, "min": 16.16}, "VIN": {"avg": 78.83, "peak": 127.88, "min": 29.35}, "VIN_SYS_5V0": {"avg": 15.19, "peak": 16.56, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 16.3, "peak": 17.71, "min": 13.0}}, "power_watts_avg": 32.77, "energy_joules_est": 56.74, "sample_count": 17, "duration_seconds": 1.731}, "timestamp": "2026-01-19T15:31:54.327244"}
{"image_index": 449, "image_name": "000000050896.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050896.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1780.602, "latencies_ms": [1780.602], "images_per_second": 0.562, "prompt_tokens": 1442, "response_tokens_est": 12, "n_tiles": 1, "output_text": " The oranges are bright orange and the bowl is clear glass.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 27561.3, "ram_available_mb": 98210.9, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 2.4, "ram_used_mb": 27561.3, "ram_available_mb": 98210.9, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 33.95, "peak": 40.97, "min": 22.46}, "VIN": {"avg": 77.25, "peak": 130.75, "min": 28.0}, "VIN_SYS_5V0": {"avg": 15.29, "peak": 16.56, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 16.4, "peak": 17.71, "min": 13.79}}, "power_watts_avg": 33.95, "energy_joules_est": 60.46, "sample_count": 18, "duration_seconds": 1.781}, "timestamp": "2026-01-19T15:31:56.212511"}
{"image_index": 450, "image_name": "000000050943.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050943.jpg", "image_width": 640, "image_height": 319, "image_resolution": "640x319", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1236.131, "latencies_ms": [1236.131], "images_per_second": 0.809, "prompt_tokens": 766, "response_tokens_est": 20, "n_tiles": 1, "output_text": " A surfer is riding a large wave in the ocean, with a cloudy sky in the background.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 27561.3, "ram_available_mb": 98210.9, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 27561.3, "ram_available_mb": 98210.9, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5179.1, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.38, "peak": 37.42, "min": 19.71}, "VIN": {"avg": 74.12, "peak": 121.36, "min": 29.28}, "VIN_SYS_5V0": {"avg": 14.28, "peak": 15.14, "min": 11.87}, "VDD_CPU_SOC_MSS": {"avg": 15.26, "peak": 16.14, "min": 13.0}}, "power_watts_avg": 29.38, "energy_joules_est": 36.33, "sample_count": 12, "duration_seconds": 1.237}, "timestamp": "2026-01-19T15:31:57.474137"}
{"image_index": 450, "image_name": "000000050943.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050943.jpg", "image_width": 640, "image_height": 319, "image_resolution": "640x319", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2233.083, "latencies_ms": [2233.083], "images_per_second": 0.448, "prompt_tokens": 780, "response_tokens_est": 61, "n_tiles": 1, "output_text": " 1. Surfer: 1\n2. Surfboard: 1\n3. Wave: 1\n4. Ocean: 1\n5. Sky: 1\n6. Clouds: 1\n7. Water: 1\n8. Surfboard: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27561.3, "ram_available_mb": 98210.9, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 27561.3, "ram_available_mb": 98210.9, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5180.1, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 25.43, "peak": 37.42, "min": 17.74}, "VIN": {"avg": 71.5, "peak": 116.62, "min": 29.83}, "VIN_SYS_5V0": {"avg": 14.93, "peak": 15.54, "min": 13.39}, "VDD_CPU_SOC_MSS": {"avg": 15.96, "peak": 16.92, "min": 14.57}}, "power_watts_avg": 25.43, "energy_joules_est": 56.8, "sample_count": 22, "duration_seconds": 2.234}, "timestamp": "2026-01-19T15:31:59.774174"}
{"image_index": 450, "image_name": "000000050943.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050943.jpg", "image_width": 640, "image_height": 319, "image_resolution": "640x319", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1700.626, "latencies_ms": [1700.626], "images_per_second": 0.588, "prompt_tokens": 784, "response_tokens_est": 38, "n_tiles": 1, "output_text": " The surfer is positioned in the foreground, riding a wave that dominates the majority of the image. The wave is in the background, with the sky occupying the upper portion of the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27561.3, "ram_available_mb": 98210.9, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 27561.3, "ram_available_mb": 98210.9, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5180.1, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 25.7, "peak": 35.85, "min": 16.16}, "VIN": {"avg": 64.12, "peak": 86.17, "min": 28.21}, "VIN_SYS_5V0": {"avg": 14.55, "peak": 15.24, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.61, "peak": 16.54, "min": 13.79}}, "power_watts_avg": 25.7, "energy_joules_est": 43.71, "sample_count": 17, "duration_seconds": 1.701}, "timestamp": "2026-01-19T15:32:01.560007"}
{"image_index": 450, "image_name": "000000050943.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050943.jpg", "image_width": 640, "image_height": 319, "image_resolution": "640x319", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1484.569, "latencies_ms": [1484.569], "images_per_second": 0.674, "prompt_tokens": 778, "response_tokens_est": 30, "n_tiles": 1, "output_text": " A surfer is riding a large wave in the ocean. The surfer is wearing a black wetsuit and is standing on a surfboard.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27561.3, "ram_available_mb": 98210.9, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 27561.3, "ram_available_mb": 98210.9, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5180.1, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.66, "peak": 36.63, "min": 15.38}, "VIN": {"avg": 68.84, "peak": 124.39, "min": 27.33}, "VIN_SYS_5V0": {"avg": 14.35, "peak": 15.14, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.38, "peak": 16.53, "min": 13.0}}, "power_watts_avg": 26.66, "energy_joules_est": 39.59, "sample_count": 15, "duration_seconds": 1.485}, "timestamp": "2026-01-19T15:32:03.123328"}
{"image_index": 450, "image_name": "000000050943.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050943.jpg", "image_width": 640, "image_height": 319, "image_resolution": "640x319", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1882.699, "latencies_ms": [1882.699], "images_per_second": 0.531, "prompt_tokens": 776, "response_tokens_est": 45, "n_tiles": 1, "output_text": " The image captures a surfer riding a large wave in the ocean, with the surfer wearing a black wetsuit and a white surfboard. The sky is overcast, and the water is a deep blue color.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 27561.3, "ram_available_mb": 98210.9, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 27561.2, "ram_available_mb": 98210.9, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5180.1, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 25.18, "peak": 36.24, "min": 16.16}, "VIN": {"avg": 63.84, "peak": 121.05, "min": 27.46}, "VIN_SYS_5V0": {"avg": 14.47, "peak": 15.24, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 15.54, "peak": 16.54, "min": 13.39}}, "power_watts_avg": 25.18, "energy_joules_est": 47.42, "sample_count": 19, "duration_seconds": 1.883}, "timestamp": "2026-01-19T15:32:05.109100"}
{"image_index": 451, "image_name": "000000051008.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051008.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1447.115, "latencies_ms": [1447.115], "images_per_second": 0.691, "prompt_tokens": 1099, "response_tokens_est": 14, "n_tiles": 1, "output_text": " A cat is sitting on a laptop computer and looking at the screen.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 27561.2, "ram_available_mb": 98210.9, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 27561.2, "ram_available_mb": 98210.9, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.04, "peak": 40.18, "min": 14.59}, "VIN": {"avg": 78.07, "peak": 119.18, "min": 30.06}, "VIN_SYS_5V0": {"avg": 14.68, "peak": 15.95, "min": 11.54}, "VDD_CPU_SOC_MSS": {"avg": 15.81, "peak": 17.32, "min": 12.61}}, "power_watts_avg": 31.04, "energy_joules_est": 44.94, "sample_count": 14, "duration_seconds": 1.448}, "timestamp": "2026-01-19T15:32:06.575755"}
{"image_index": 451, "image_name": "000000051008.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051008.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2525.371, "latencies_ms": [2525.371], "images_per_second": 0.396, "prompt_tokens": 1113, "response_tokens_est": 57, "n_tiles": 1, "output_text": " 1. cat: 1\n2. laptop: 1\n3. keyboard: 1\n4. screen: 1\n5. mouse: 1\n6. mousepad: 1\n7. paper: 1\n8. couch: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27561.2, "ram_available_mb": 98210.9, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 27561.2, "ram_available_mb": 98210.9, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.48, "peak": 40.57, "min": 18.14}, "VIN": {"avg": 68.81, "peak": 101.79, "min": 28.59}, "VIN_SYS_5V0": {"avg": 15.14, "peak": 16.26, "min": 13.29}, "VDD_CPU_SOC_MSS": {"avg": 16.3, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 28.48, "energy_joules_est": 71.93, "sample_count": 25, "duration_seconds": 2.526}, "timestamp": "2026-01-19T15:32:09.177619"}
{"image_index": 451, "image_name": "000000051008.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051008.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2063.507, "latencies_ms": [2063.507], "images_per_second": 0.485, "prompt_tokens": 1117, "response_tokens_est": 38, "n_tiles": 1, "output_text": " The cat is in the foreground, sitting on the left side of the laptop. The laptop is on the right side of the cat. The cat is looking at the screen of the laptop.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27561.2, "ram_available_mb": 98210.9, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 27561.2, "ram_available_mb": 98210.9, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.88, "peak": 40.18, "min": 15.38}, "VIN": {"avg": 65.11, "peak": 125.04, "min": 30.41}, "VIN_SYS_5V0": {"avg": 14.9, "peak": 16.05, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 16.02, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 28.88, "energy_joules_est": 59.6, "sample_count": 20, "duration_seconds": 2.064}, "timestamp": "2026-01-19T15:32:11.253872"}
{"image_index": 451, "image_name": "000000051008.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051008.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1410.764, "latencies_ms": [1410.764], "images_per_second": 0.709, "prompt_tokens": 1111, "response_tokens_est": 14, "n_tiles": 1, "output_text": " A cat is sitting on a laptop computer, looking at the screen.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27561.2, "ram_available_mb": 98210.9, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 2.5, "ram_used_mb": 27561.2, "ram_available_mb": 98210.9, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.47, "peak": 40.18, "min": 18.52}, "VIN": {"avg": 74.03, "peak": 103.62, "min": 29.25}, "VIN_SYS_5V0": {"avg": 15.26, "peak": 16.15, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 16.51, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 32.47, "energy_joules_est": 45.82, "sample_count": 14, "duration_seconds": 1.411}, "timestamp": "2026-01-19T15:32:12.718428"}
{"image_index": 451, "image_name": "000000051008.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051008.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1409.322, "latencies_ms": [1409.322], "images_per_second": 0.71, "prompt_tokens": 1109, "response_tokens_est": 13, "n_tiles": 1, "output_text": " The cat is white and brown, and the laptop is black.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27561.2, "ram_available_mb": 98210.9, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 27561.2, "ram_available_mb": 98210.9, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 33.29, "peak": 40.97, "min": 21.29}, "VIN": {"avg": 73.25, "peak": 105.02, "min": 28.84}, "VIN_SYS_5V0": {"avg": 15.01, "peak": 16.15, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 16.23, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 33.29, "energy_joules_est": 46.94, "sample_count": 14, "duration_seconds": 1.41}, "timestamp": "2026-01-19T15:32:14.177914"}
{"image_index": 452, "image_name": "000000051309.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051309.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1918.056, "latencies_ms": [1918.056], "images_per_second": 0.521, "prompt_tokens": 1099, "response_tokens_est": 33, "n_tiles": 1, "output_text": " In the image, a group of horses, including a brown one, are gathered around a hay feeder in a field, with a house and trees in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27561.2, "ram_available_mb": 98210.9, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 27561.2, "ram_available_mb": 98210.9, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.7, "peak": 40.95, "min": 20.1}, "VIN": {"avg": 70.14, "peak": 106.98, "min": 30.9}, "VIN_SYS_5V0": {"avg": 15.03, "peak": 16.15, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 16.04, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 30.7, "energy_joules_est": 58.91, "sample_count": 19, "duration_seconds": 1.919}, "timestamp": "2026-01-19T15:32:16.165167"}
{"image_index": 452, "image_name": "000000051309.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051309.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1862.201, "latencies_ms": [1862.201], "images_per_second": 0.537, "prompt_tokens": 1113, "response_tokens_est": 30, "n_tiles": 1, "output_text": " horse: 4, hay: 1, fence: 1, trees: 1, house: 1, power lines: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27561.2, "ram_available_mb": 98210.9, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 27561.2, "ram_available_mb": 98210.9, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.14, "peak": 40.56, "min": 17.35}, "VIN": {"avg": 73.55, "peak": 127.66, "min": 29.63}, "VIN_SYS_5V0": {"avg": 14.94, "peak": 16.05, "min": 12.36}, "VDD_CPU_SOC_MSS": {"avg": 16.03, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 30.14, "energy_joules_est": 56.14, "sample_count": 18, "duration_seconds": 1.862}, "timestamp": "2026-01-19T15:32:18.048062"}
{"image_index": 452, "image_name": "000000051309.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051309.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2180.909, "latencies_ms": [2180.909], "images_per_second": 0.459, "prompt_tokens": 1117, "response_tokens_est": 43, "n_tiles": 1, "output_text": " The brown horse is in the foreground, eating hay from a feeder. The brown horse is in the middle ground, eating hay from a feeder. The brown horse is in the background, eating hay from a feeder.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 27561.2, "ram_available_mb": 98210.9, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 27561.2, "ram_available_mb": 98210.9, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.17, "peak": 40.57, "min": 18.52}, "VIN": {"avg": 76.1, "peak": 116.96, "min": 36.55}, "VIN_SYS_5V0": {"avg": 15.17, "peak": 16.15, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 16.27, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 29.17, "energy_joules_est": 63.63, "sample_count": 21, "duration_seconds": 2.181}, "timestamp": "2026-01-19T15:32:20.234231"}
{"image_index": 452, "image_name": "000000051309.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051309.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2320.003, "latencies_ms": [2320.003], "images_per_second": 0.431, "prompt_tokens": 1111, "response_tokens_est": 49, "n_tiles": 1, "output_text": " In a rural setting, a group of horses, including a brown foal, are gathered around a hay feeder, enjoying their meal. The scene is set in a field with a fence in the background, and power lines can be seen above.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27561.2, "ram_available_mb": 98210.9, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 27561.2, "ram_available_mb": 98210.9, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.23, "peak": 40.57, "min": 18.13}, "VIN": {"avg": 71.84, "peak": 126.52, "min": 29.14}, "VIN_SYS_5V0": {"avg": 15.15, "peak": 16.15, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 16.3, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 28.23, "energy_joules_est": 65.51, "sample_count": 23, "duration_seconds": 2.32}, "timestamp": "2026-01-19T15:32:22.625326"}
{"image_index": 452, "image_name": "000000051309.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051309.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2513.142, "latencies_ms": [2513.142], "images_per_second": 0.398, "prompt_tokens": 1109, "response_tokens_est": 55, "n_tiles": 1, "output_text": " The image features a group of horses in a field with a mix of brown and black coats, with the brown horses being the most prominent. The lighting is natural and bright, suggesting it is daytime, and the weather appears to be clear with no visible clouds in the sky.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27561.2, "ram_available_mb": 98210.9, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 27561.2, "ram_available_mb": 98210.9, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.99, "peak": 40.18, "min": 15.77}, "VIN": {"avg": 62.22, "peak": 119.72, "min": 29.04}, "VIN_SYS_5V0": {"avg": 14.9, "peak": 16.05, "min": 12.25}, "VDD_CPU_SOC_MSS": {"avg": 15.83, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 26.99, "energy_joules_est": 67.84, "sample_count": 25, "duration_seconds": 2.514}, "timestamp": "2026-01-19T15:32:25.230829"}
{"image_index": 453, "image_name": "000000051314.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051314.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1625.041, "latencies_ms": [1625.041], "images_per_second": 0.615, "prompt_tokens": 1099, "response_tokens_est": 21, "n_tiles": 1, "output_text": " A man in a black wetsuit is riding a yellow surfboard on a wave in the ocean.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 27561.2, "ram_available_mb": 98210.9, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 27561.2, "ram_available_mb": 98210.9, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.43, "peak": 40.18, "min": 15.38}, "VIN": {"avg": 73.47, "peak": 119.16, "min": 28.66}, "VIN_SYS_5V0": {"avg": 14.77, "peak": 16.05, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.87, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 30.43, "energy_joules_est": 49.47, "sample_count": 16, "duration_seconds": 1.626}, "timestamp": "2026-01-19T15:32:26.912653"}
{"image_index": 453, "image_name": "000000051314.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051314.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2800.72, "latencies_ms": [2800.72], "images_per_second": 0.357, "prompt_tokens": 1113, "response_tokens_est": 67, "n_tiles": 1, "output_text": " 1. Surfer: 1\n2. Surfboard: 1\n3. Ocean: 1\n4. Water: 1\n5. Surfboard: 1\n6. Surfboard: 1\n7. Surfboard: 1\n8. Surfboard: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27561.2, "ram_available_mb": 98210.9, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 27561.2, "ram_available_mb": 98210.9, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.18, "peak": 40.57, "min": 19.31}, "VIN": {"avg": 72.63, "peak": 120.61, "min": 28.95}, "VIN_SYS_5V0": {"avg": 15.06, "peak": 16.05, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 16.07, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 27.18, "energy_joules_est": 76.13, "sample_count": 27, "duration_seconds": 2.801}, "timestamp": "2026-01-19T15:32:29.726250"}
{"image_index": 453, "image_name": "000000051314.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051314.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2145.456, "latencies_ms": [2145.456], "images_per_second": 0.466, "prompt_tokens": 1117, "response_tokens_est": 42, "n_tiles": 1, "output_text": " The surfer is positioned in the foreground, with the ocean and coastline in the background. The surfer is crouched low on the board, with the wave forming a near-perfect arc around him.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27561.2, "ram_available_mb": 98210.9, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 27561.2, "ram_available_mb": 98210.9, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.78, "peak": 39.78, "min": 17.34}, "VIN": {"avg": 69.78, "peak": 98.92, "min": 30.62}, "VIN_SYS_5V0": {"avg": 15.21, "peak": 16.15, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 16.29, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 28.78, "energy_joules_est": 61.76, "sample_count": 21, "duration_seconds": 2.146}, "timestamp": "2026-01-19T15:32:31.914665"}
{"image_index": 453, "image_name": "000000051314.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051314.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1562.607, "latencies_ms": [1562.607], "images_per_second": 0.64, "prompt_tokens": 1111, "response_tokens_est": 19, "n_tiles": 1, "output_text": " A man in a wetsuit is riding a wave on a surfboard in the ocean.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 27561.2, "ram_available_mb": 98210.9, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 27561.2, "ram_available_mb": 98210.9, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.44, "peak": 40.57, "min": 16.95}, "VIN": {"avg": 76.28, "peak": 121.88, "min": 36.52}, "VIN_SYS_5V0": {"avg": 15.18, "peak": 16.15, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 16.25, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 31.44, "energy_joules_est": 49.14, "sample_count": 15, "duration_seconds": 1.563}, "timestamp": "2026-01-19T15:32:33.482437"}
{"image_index": 453, "image_name": "000000051314.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051314.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1929.259, "latencies_ms": [1929.259], "images_per_second": 0.518, "prompt_tokens": 1109, "response_tokens_est": 34, "n_tiles": 1, "output_text": " The surfer is wearing a black wetsuit and is riding a yellow surfboard. The ocean is a greenish-blue color, and the sky is partly cloudy.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27561.2, "ram_available_mb": 98210.9, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 27561.2, "ram_available_mb": 98210.9, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.77, "peak": 40.57, "min": 20.1}, "VIN": {"avg": 71.12, "peak": 107.08, "min": 28.31}, "VIN_SYS_5V0": {"avg": 15.33, "peak": 16.36, "min": 13.69}, "VDD_CPU_SOC_MSS": {"avg": 16.35, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 30.77, "energy_joules_est": 59.38, "sample_count": 19, "duration_seconds": 1.93}, "timestamp": "2026-01-19T15:32:35.468980"}
{"image_index": 454, "image_name": "000000051326.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051326.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2842.354, "latencies_ms": [2842.354], "images_per_second": 0.352, "prompt_tokens": 1099, "response_tokens_est": 68, "n_tiles": 1, "output_text": " The image features a collection of pumpkins, including a large one with a face carved into it, a smaller one with a skull design, and a third pumpkin with a carved face. In addition to the pumpkins, there are flowers in a vase placed among them, and a small figure of a man is positioned on top of the smaller pumpkin.", "error": null, "sys_before": {"cpu_percent": 7.1, "ram_used_mb": 27561.2, "ram_available_mb": 98210.9, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 27561.2, "ram_available_mb": 98210.9, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.3, "peak": 40.57, "min": 16.56}, "VIN": {"avg": 64.66, "peak": 116.85, "min": 29.97}, "VIN_SYS_5V0": {"avg": 14.88, "peak": 16.05, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.87, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 26.3, "energy_joules_est": 74.77, "sample_count": 28, "duration_seconds": 2.843}, "timestamp": "2026-01-19T15:32:38.408678"}
{"image_index": 454, "image_name": "000000051326.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051326.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1979.271, "latencies_ms": [1979.271], "images_per_second": 0.505, "prompt_tokens": 1113, "response_tokens_est": 34, "n_tiles": 1, "output_text": " pumpkin: 3, pumpkin: 2, pumpkin: 1, pumpkin: 1, pumpkin: 1, pumpkin: 1, pumpkin: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27561.2, "ram_available_mb": 98210.9, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 27561.2, "ram_available_mb": 98210.9, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.99, "peak": 39.77, "min": 14.98}, "VIN": {"avg": 67.52, "peak": 112.92, "min": 29.44}, "VIN_SYS_5V0": {"avg": 14.87, "peak": 16.05, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.83, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 28.99, "energy_joules_est": 57.39, "sample_count": 19, "duration_seconds": 1.98}, "timestamp": "2026-01-19T15:32:40.401977"}
{"image_index": 454, "image_name": "000000051326.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051326.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2054.686, "latencies_ms": [2054.686], "images_per_second": 0.487, "prompt_tokens": 1117, "response_tokens_est": 38, "n_tiles": 1, "output_text": " The pumpkin with the face is positioned in the foreground, while the other pumpkins are in the background. The pumpkin with the face is also positioned to the left of the pumpkin with the flowers.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27561.2, "ram_available_mb": 98210.9, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 27561.2, "ram_available_mb": 98211.0, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.41, "peak": 40.56, "min": 18.13}, "VIN": {"avg": 68.88, "peak": 117.72, "min": 29.55}, "VIN_SYS_5V0": {"avg": 15.22, "peak": 16.26, "min": 13.29}, "VDD_CPU_SOC_MSS": {"avg": 16.2, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 29.41, "energy_joules_est": 60.44, "sample_count": 20, "duration_seconds": 2.055}, "timestamp": "2026-01-19T15:32:42.496470"}
{"image_index": 454, "image_name": "000000051326.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051326.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1475.578, "latencies_ms": [1475.578], "images_per_second": 0.678, "prompt_tokens": 1111, "response_tokens_est": 15, "n_tiles": 1, "output_text": " A collection of pumpkins with faces carved into them are displayed on a table.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 27561.2, "ram_available_mb": 98211.0, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 27561.2, "ram_available_mb": 98211.0, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.46, "peak": 40.56, "min": 17.34}, "VIN": {"avg": 76.26, "peak": 129.74, "min": 27.83}, "VIN_SYS_5V0": {"avg": 14.99, "peak": 16.15, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 16.09, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 31.46, "energy_joules_est": 46.43, "sample_count": 15, "duration_seconds": 1.476}, "timestamp": "2026-01-19T15:32:44.064594"}
{"image_index": 454, "image_name": "000000051326.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051326.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1574.361, "latencies_ms": [1574.361], "images_per_second": 0.635, "prompt_tokens": 1109, "response_tokens_est": 18, "n_tiles": 1, "output_text": " The pumpkins are orange, the flowers are pink and white, and the lighting is natural.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27561.2, "ram_available_mb": 98211.0, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 27561.2, "ram_available_mb": 98211.0, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.32, "peak": 40.57, "min": 19.32}, "VIN": {"avg": 72.83, "peak": 113.75, "min": 27.18}, "VIN_SYS_5V0": {"avg": 14.71, "peak": 16.05, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.77, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 31.32, "energy_joules_est": 49.32, "sample_count": 16, "duration_seconds": 1.575}, "timestamp": "2026-01-19T15:32:45.741047"}
{"image_index": 455, "image_name": "000000051598.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051598.jpg", "image_width": 360, "image_height": 640, "image_resolution": "360x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1803.622, "latencies_ms": [1803.622], "images_per_second": 0.554, "prompt_tokens": 1100, "response_tokens_est": 28, "n_tiles": 1, "output_text": " The image depicts a bathroom with a white sink, a mirror, and a door, all of which are in a state of disrepair.", "error": null, "sys_before": {"cpu_percent": 14.3, "ram_used_mb": 27561.2, "ram_available_mb": 98211.0, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 27561.2, "ram_available_mb": 98211.0, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.92, "peak": 40.16, "min": 18.53}, "VIN": {"avg": 74.18, "peak": 120.93, "min": 28.88}, "VIN_SYS_5V0": {"avg": 14.75, "peak": 16.05, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.86, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 29.92, "energy_joules_est": 53.98, "sample_count": 18, "duration_seconds": 1.804}, "timestamp": "2026-01-19T15:32:47.630136"}
{"image_index": 455, "image_name": "000000051598.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051598.jpg", "image_width": 360, "image_height": 640, "image_resolution": "360x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1997.306, "latencies_ms": [1997.306], "images_per_second": 0.501, "prompt_tokens": 1114, "response_tokens_est": 35, "n_tiles": 1, "output_text": " sink: 1, mirror: 1, door: 1, trash bag: 1, toilet: 1, shelf: 1, wall: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27561.2, "ram_available_mb": 98211.0, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 27561.2, "ram_available_mb": 98211.0, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.97, "peak": 40.18, "min": 17.35}, "VIN": {"avg": 67.51, "peak": 95.28, "min": 28.65}, "VIN_SYS_5V0": {"avg": 14.81, "peak": 16.05, "min": 12.15}, "VDD_CPU_SOC_MSS": {"avg": 15.93, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 28.97, "energy_joules_est": 57.87, "sample_count": 20, "duration_seconds": 1.998}, "timestamp": "2026-01-19T15:32:49.709757"}
{"image_index": 455, "image_name": "000000051598.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051598.jpg", "image_width": 360, "image_height": 640, "image_resolution": "360x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2038.22, "latencies_ms": [2038.22], "images_per_second": 0.491, "prompt_tokens": 1118, "response_tokens_est": 37, "n_tiles": 1, "output_text": " The sink is located to the left of the door, and the mirror is above the sink. The bag is on the floor near the door, and the shelf is above the sink.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 27561.2, "ram_available_mb": 98211.0, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 27561.2, "ram_available_mb": 98211.0, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.82, "peak": 40.18, "min": 16.56}, "VIN": {"avg": 74.79, "peak": 122.4, "min": 29.64}, "VIN_SYS_5V0": {"avg": 14.89, "peak": 16.15, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.91, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 28.82, "energy_joules_est": 58.75, "sample_count": 20, "duration_seconds": 2.039}, "timestamp": "2026-01-19T15:32:51.798007"}
{"image_index": 455, "image_name": "000000051598.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051598.jpg", "image_width": 360, "image_height": 640, "image_resolution": "360x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1393.266, "latencies_ms": [1393.266], "images_per_second": 0.718, "prompt_tokens": 1112, "response_tokens_est": 12, "n_tiles": 1, "output_text": " A bathroom with a white sink, mirror, and door.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27561.2, "ram_available_mb": 98211.0, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 27561.2, "ram_available_mb": 98211.0, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.43, "peak": 39.78, "min": 17.35}, "VIN": {"avg": 78.06, "peak": 123.1, "min": 29.25}, "VIN_SYS_5V0": {"avg": 14.95, "peak": 16.26, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 16.0, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 31.43, "energy_joules_est": 43.8, "sample_count": 14, "duration_seconds": 1.394}, "timestamp": "2026-01-19T15:32:53.262311"}
{"image_index": 455, "image_name": "000000051598.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051598.jpg", "image_width": 360, "image_height": 640, "image_resolution": "360x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1470.145, "latencies_ms": [1470.145], "images_per_second": 0.68, "prompt_tokens": 1110, "response_tokens_est": 15, "n_tiles": 1, "output_text": " The room is lit by a yellowish light, and the walls are white.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27561.2, "ram_available_mb": 98211.0, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 27561.2, "ram_available_mb": 98211.0, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.33, "peak": 40.56, "min": 21.29}, "VIN": {"avg": 78.48, "peak": 123.62, "min": 27.35}, "VIN_SYS_5V0": {"avg": 14.85, "peak": 16.15, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 16.01, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 32.33, "energy_joules_est": 47.56, "sample_count": 15, "duration_seconds": 1.471}, "timestamp": "2026-01-19T15:32:54.830739"}
{"image_index": 456, "image_name": "000000051610.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051610.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1512.08, "latencies_ms": [1512.08], "images_per_second": 0.661, "prompt_tokens": 1099, "response_tokens_est": 16, "n_tiles": 1, "output_text": " A little girl is sitting on a bed with a laptop in front of her.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27561.2, "ram_available_mb": 98211.0, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 27561.2, "ram_available_mb": 98211.0, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.91, "peak": 40.57, "min": 19.32}, "VIN": {"avg": 75.62, "peak": 123.71, "min": 29.03}, "VIN_SYS_5V0": {"avg": 14.68, "peak": 15.95, "min": 11.87}, "VDD_CPU_SOC_MSS": {"avg": 15.77, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 31.91, "energy_joules_est": 48.28, "sample_count": 15, "duration_seconds": 1.513}, "timestamp": "2026-01-19T15:32:56.402138"}
{"image_index": 456, "image_name": "000000051610.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051610.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2594.639, "latencies_ms": [2594.639], "images_per_second": 0.385, "prompt_tokens": 1113, "response_tokens_est": 58, "n_tiles": 1, "output_text": " 1. laptop: 1\n2. child: 1\n3. bed: 1\n4. wall: 1\n5. screen: 1\n6. keyboard: 1\n7. laptop screen: 1\n8. laptop body: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27561.2, "ram_available_mb": 98211.0, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 27561.2, "ram_available_mb": 98211.0, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.92, "peak": 40.56, "min": 19.71}, "VIN": {"avg": 64.21, "peak": 114.66, "min": 30.65}, "VIN_SYS_5V0": {"avg": 14.97, "peak": 16.05, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.92, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 27.92, "energy_joules_est": 72.45, "sample_count": 25, "duration_seconds": 2.595}, "timestamp": "2026-01-19T15:32:59.011485"}
{"image_index": 456, "image_name": "000000051610.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051610.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2021.993, "latencies_ms": [2021.993], "images_per_second": 0.495, "prompt_tokens": 1117, "response_tokens_est": 35, "n_tiles": 1, "output_text": " The laptop is on the left side of the bed, and the child is sitting on the right side of the bed. The child is closer to the laptop than the bed.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27561.2, "ram_available_mb": 98211.0, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 27561.2, "ram_available_mb": 98211.0, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.94, "peak": 40.57, "min": 16.56}, "VIN": {"avg": 72.58, "peak": 118.87, "min": 29.78}, "VIN_SYS_5V0": {"avg": 15.09, "peak": 16.15, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 16.08, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 28.94, "energy_joules_est": 58.53, "sample_count": 20, "duration_seconds": 2.022}, "timestamp": "2026-01-19T15:33:01.099252"}
{"image_index": 456, "image_name": "000000051610.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051610.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1520.749, "latencies_ms": [1520.749], "images_per_second": 0.658, "prompt_tokens": 1111, "response_tokens_est": 16, "n_tiles": 1, "output_text": " A little girl is sitting on a bed with a laptop in front of her.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27561.2, "ram_available_mb": 98211.0, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 27561.2, "ram_available_mb": 98211.0, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.04, "peak": 40.18, "min": 16.56}, "VIN": {"avg": 77.52, "peak": 127.06, "min": 28.93}, "VIN_SYS_5V0": {"avg": 14.84, "peak": 16.05, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.83, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 31.04, "energy_joules_est": 47.21, "sample_count": 15, "duration_seconds": 1.521}, "timestamp": "2026-01-19T15:33:02.669738"}
{"image_index": 456, "image_name": "000000051610.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051610.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1623.338, "latencies_ms": [1623.338], "images_per_second": 0.616, "prompt_tokens": 1109, "response_tokens_est": 21, "n_tiles": 1, "output_text": " The image is in black and white, with a white background and a white laptop on a white bed.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27561.2, "ram_available_mb": 98211.0, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 27561.2, "ram_available_mb": 98211.0, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.83, "peak": 40.95, "min": 20.5}, "VIN": {"avg": 70.81, "peak": 93.17, "min": 28.88}, "VIN_SYS_5V0": {"avg": 15.02, "peak": 16.15, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 16.07, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 31.83, "energy_joules_est": 51.68, "sample_count": 16, "duration_seconds": 1.624}, "timestamp": "2026-01-19T15:33:04.349024"}
{"image_index": 457, "image_name": "000000051712.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051712.jpg", "image_width": 640, "image_height": 368, "image_resolution": "640x368", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1587.743, "latencies_ms": [1587.743], "images_per_second": 0.63, "prompt_tokens": 1099, "response_tokens_est": 19, "n_tiles": 1, "output_text": " A skier wearing a blue helmet and goggles is skiing down a snowy slope surrounded by trees.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 27561.2, "ram_available_mb": 98211.0, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 27561.2, "ram_available_mb": 98211.0, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.32, "peak": 40.57, "min": 19.31}, "VIN": {"avg": 71.45, "peak": 120.94, "min": 28.87}, "VIN_SYS_5V0": {"avg": 14.8, "peak": 16.05, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.97, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 31.32, "energy_joules_est": 49.74, "sample_count": 16, "duration_seconds": 1.588}, "timestamp": "2026-01-19T15:33:06.029959"}
{"image_index": 457, "image_name": "000000051712.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051712.jpg", "image_width": 640, "image_height": 368, "image_resolution": "640x368", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2601.423, "latencies_ms": [2601.423], "images_per_second": 0.384, "prompt_tokens": 1113, "response_tokens_est": 59, "n_tiles": 1, "output_text": " 1. skier: 1\n2. ski poles: 2\n3. skis: 2\n4. helmet: 1\n5. goggles: 1\n6. jacket: 1\n7. pants: 1\n8. trees: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 27561.2, "ram_available_mb": 98211.0, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 27561.2, "ram_available_mb": 98211.0, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.77, "peak": 40.57, "min": 18.92}, "VIN": {"avg": 70.96, "peak": 101.18, "min": 33.18}, "VIN_SYS_5V0": {"avg": 14.97, "peak": 16.05, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.89, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 27.77, "energy_joules_est": 72.25, "sample_count": 25, "duration_seconds": 2.602}, "timestamp": "2026-01-19T15:33:08.637905"}
{"image_index": 457, "image_name": "000000051712.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051712.jpg", "image_width": 640, "image_height": 368, "image_resolution": "640x368", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2069.599, "latencies_ms": [2069.599], "images_per_second": 0.483, "prompt_tokens": 1117, "response_tokens_est": 39, "n_tiles": 1, "output_text": " The skier is positioned in the foreground, with the trees in the background. The skier is moving towards the right side of the image, while the trees are located on the left side.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27561.2, "ram_available_mb": 98211.0, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 27565.2, "ram_available_mb": 98207.0, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.41, "peak": 40.18, "min": 17.35}, "VIN": {"avg": 72.7, "peak": 125.94, "min": 29.54}, "VIN_SYS_5V0": {"avg": 15.29, "peak": 16.26, "min": 13.29}, "VDD_CPU_SOC_MSS": {"avg": 16.38, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 29.41, "energy_joules_est": 60.87, "sample_count": 20, "duration_seconds": 2.07}, "timestamp": "2026-01-19T15:33:10.725661"}
{"image_index": 457, "image_name": "000000051712.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051712.jpg", "image_width": 640, "image_height": 368, "image_resolution": "640x368", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1405.836, "latencies_ms": [1405.836], "images_per_second": 0.711, "prompt_tokens": 1111, "response_tokens_est": 13, "n_tiles": 1, "output_text": " A skier is skiing down a snowy slope surrounded by trees.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27565.2, "ram_available_mb": 98207.0, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 2.5, "ram_used_mb": 27565.2, "ram_available_mb": 98207.0, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.14, "peak": 40.57, "min": 17.74}, "VIN": {"avg": 75.9, "peak": 120.74, "min": 29.92}, "VIN_SYS_5V0": {"avg": 15.2, "peak": 16.15, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 16.25, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 32.14, "energy_joules_est": 45.19, "sample_count": 14, "duration_seconds": 1.406}, "timestamp": "2026-01-19T15:33:12.192709"}
{"image_index": 457, "image_name": "000000051712.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051712.jpg", "image_width": 640, "image_height": 368, "image_resolution": "640x368", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1627.425, "latencies_ms": [1627.425], "images_per_second": 0.614, "prompt_tokens": 1109, "response_tokens_est": 20, "n_tiles": 1, "output_text": " The skier is wearing a brown jacket and blue helmet, and the snow is white and fluffy.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27565.2, "ram_available_mb": 98207.0, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 27565.2, "ram_available_mb": 98207.0, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.18, "peak": 40.95, "min": 21.29}, "VIN": {"avg": 74.95, "peak": 122.47, "min": 28.93}, "VIN_SYS_5V0": {"avg": 14.95, "peak": 16.05, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.92, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 32.18, "energy_joules_est": 52.39, "sample_count": 16, "duration_seconds": 1.628}, "timestamp": "2026-01-19T15:33:13.862587"}
{"image_index": 458, "image_name": "000000051738.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051738.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2000.659, "latencies_ms": [2000.659], "images_per_second": 0.5, "prompt_tokens": 1099, "response_tokens_est": 36, "n_tiles": 1, "output_text": " The image depicts a spacious hotel room with a large bed, a blue armchair, and a nightstand with a lamp, all set against a backdrop of a window with curtains.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 27565.2, "ram_available_mb": 98207.0, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 27565.4, "ram_available_mb": 98206.7, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.7, "peak": 40.56, "min": 19.31}, "VIN": {"avg": 70.24, "peak": 107.76, "min": 29.56}, "VIN_SYS_5V0": {"avg": 14.96, "peak": 16.05, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 16.0, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 29.7, "energy_joules_est": 59.44, "sample_count": 20, "duration_seconds": 2.001}, "timestamp": "2026-01-19T15:33:15.947774"}
{"image_index": 458, "image_name": "000000051738.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051738.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2610.07, "latencies_ms": [2610.07], "images_per_second": 0.383, "prompt_tokens": 1113, "response_tokens_est": 59, "n_tiles": 1, "output_text": " 1. bed: 1\n2. lamps: 2\n3. chair: 1\n4. suitcase: 1\n5. ottoman: 1\n6. window: 1\n7. wall: 1\n8. carpet: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27565.4, "ram_available_mb": 98206.7, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 27565.4, "ram_available_mb": 98206.7, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.91, "peak": 40.57, "min": 16.56}, "VIN": {"avg": 68.05, "peak": 121.58, "min": 29.18}, "VIN_SYS_5V0": {"avg": 14.8, "peak": 16.05, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 15.81, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 26.91, "energy_joules_est": 70.25, "sample_count": 26, "duration_seconds": 2.611}, "timestamp": "2026-01-19T15:33:18.654602"}
{"image_index": 458, "image_name": "000000051738.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051738.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2686.118, "latencies_ms": [2686.118], "images_per_second": 0.372, "prompt_tokens": 1117, "response_tokens_est": 62, "n_tiles": 1, "output_text": " The bed is positioned in the center of the room, with the nightstands placed on either side. The armchair is located to the left of the bed, while the window is situated to the right. The lamp on the nightstand is positioned closer to the bed than the one on the other side.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27565.4, "ram_available_mb": 98206.7, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 27570.2, "ram_available_mb": 98202.0, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.76, "peak": 40.18, "min": 14.98}, "VIN": {"avg": 67.62, "peak": 124.78, "min": 28.89}, "VIN_SYS_5V0": {"avg": 14.83, "peak": 16.05, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.88, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 26.76, "energy_joules_est": 71.9, "sample_count": 26, "duration_seconds": 2.687}, "timestamp": "2026-01-19T15:33:21.359513"}
{"image_index": 458, "image_name": "000000051738.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051738.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1551.703, "latencies_ms": [1551.703], "images_per_second": 0.644, "prompt_tokens": 1111, "response_tokens_est": 19, "n_tiles": 1, "output_text": " A hotel room with a large bed, two lamps, a chair, and a suitcase.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27570.2, "ram_available_mb": 98202.0, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 27570.2, "ram_available_mb": 98202.0, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.65, "peak": 40.18, "min": 16.95}, "VIN": {"avg": 77.22, "peak": 124.27, "min": 29.4}, "VIN_SYS_5V0": {"avg": 15.21, "peak": 16.15, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 16.3, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 31.65, "energy_joules_est": 49.13, "sample_count": 15, "duration_seconds": 1.552}, "timestamp": "2026-01-19T15:33:22.927123"}
{"image_index": 458, "image_name": "000000051738.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051738.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1675.15, "latencies_ms": [1675.15], "images_per_second": 0.597, "prompt_tokens": 1109, "response_tokens_est": 24, "n_tiles": 1, "output_text": " The room is well-lit with natural light coming through the window, and the walls are painted in a neutral color.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27570.2, "ram_available_mb": 98202.0, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 27570.2, "ram_available_mb": 98202.0, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.68, "peak": 40.57, "min": 20.89}, "VIN": {"avg": 72.05, "peak": 108.5, "min": 27.3}, "VIN_SYS_5V0": {"avg": 15.18, "peak": 16.15, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 16.28, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 31.68, "energy_joules_est": 53.09, "sample_count": 17, "duration_seconds": 1.676}, "timestamp": "2026-01-19T15:33:24.700888"}
{"image_index": 459, "image_name": "000000051938.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051938.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2134.482, "latencies_ms": [2134.482], "images_per_second": 0.468, "prompt_tokens": 1099, "response_tokens_est": 41, "n_tiles": 1, "output_text": " In the image, a skier is performing a jump on a red rail on a snowy mountain, with other skiers and snowboarders in the background, and a ski lift visible in the distance.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27570.2, "ram_available_mb": 98202.0, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 27570.2, "ram_available_mb": 98202.0, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.8, "peak": 40.57, "min": 17.35}, "VIN": {"avg": 74.79, "peak": 127.83, "min": 28.83}, "VIN_SYS_5V0": {"avg": 14.86, "peak": 16.05, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.78, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 28.8, "energy_joules_est": 61.49, "sample_count": 21, "duration_seconds": 2.135}, "timestamp": "2026-01-19T15:33:26.892747"}
{"image_index": 459, "image_name": "000000051938.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051938.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2887.168, "latencies_ms": [2887.168], "images_per_second": 0.346, "prompt_tokens": 1113, "response_tokens_est": 70, "n_tiles": 1, "output_text": " 1. snowboard: 1\n2. skier: 1\n3. snowboarder: 1\n4. ski lift: 1\n5. ski pole: 1\n6. ski pole holder: 1\n7. ski pole tip: 1\n8. snowboarder's foot: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27570.2, "ram_available_mb": 98202.0, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 27570.2, "ram_available_mb": 98202.0, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.54, "peak": 40.57, "min": 16.56}, "VIN": {"avg": 71.86, "peak": 122.79, "min": 28.72}, "VIN_SYS_5V0": {"avg": 14.98, "peak": 16.15, "min": 12.56}, "VDD_CPU_SOC_MSS": {"avg": 15.99, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 26.54, "energy_joules_est": 76.63, "sample_count": 28, "duration_seconds": 2.887}, "timestamp": "2026-01-19T15:33:29.807272"}
{"image_index": 459, "image_name": "000000051938.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051938.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2741.066, "latencies_ms": [2741.066], "images_per_second": 0.365, "prompt_tokens": 1117, "response_tokens_est": 65, "n_tiles": 1, "output_text": " The main object, a snowboarder, is in the foreground, performing a trick on a red rail. The background features a ski lift and other skiers, indicating a bustling ski resort. The snowboarder is positioned to the left of the red rail, and the ski lift is located further back in the background.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 27570.2, "ram_available_mb": 98202.0, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 27570.2, "ram_available_mb": 98202.0, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.73, "peak": 40.16, "min": 16.56}, "VIN": {"avg": 69.64, "peak": 128.77, "min": 29.0}, "VIN_SYS_5V0": {"avg": 15.06, "peak": 16.15, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 16.03, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 26.73, "energy_joules_est": 73.29, "sample_count": 27, "duration_seconds": 2.742}, "timestamp": "2026-01-19T15:33:32.616861"}
{"image_index": 459, "image_name": "000000051938.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051938.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1576.097, "latencies_ms": [1576.097], "images_per_second": 0.634, "prompt_tokens": 1111, "response_tokens_est": 19, "n_tiles": 1, "output_text": " A group of people are skiing down a snowy hill, with a red structure in the foreground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27570.2, "ram_available_mb": 98202.0, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 27570.2, "ram_available_mb": 98202.0, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.58, "peak": 40.18, "min": 15.77}, "VIN": {"avg": 73.52, "peak": 123.52, "min": 27.47}, "VIN_SYS_5V0": {"avg": 14.82, "peak": 16.05, "min": 12.15}, "VDD_CPU_SOC_MSS": {"avg": 15.9, "peak": 17.32, "min": 13.4}}, "power_watts_avg": 30.58, "energy_joules_est": 48.21, "sample_count": 16, "duration_seconds": 1.577}, "timestamp": "2026-01-19T15:33:34.287697"}
{"image_index": 459, "image_name": "000000051938.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051938.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2121.977, "latencies_ms": [2121.977], "images_per_second": 0.471, "prompt_tokens": 1109, "response_tokens_est": 40, "n_tiles": 1, "output_text": " The image features a vibrant scene of a snow-covered mountain with a clear blue sky, and the snow is a bright white color. The lighting is natural and bright, indicating that it is daytime.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27570.2, "ram_available_mb": 98202.0, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 27570.2, "ram_available_mb": 98202.0, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.0, "peak": 40.57, "min": 18.53}, "VIN": {"avg": 73.04, "peak": 124.5, "min": 30.0}, "VIN_SYS_5V0": {"avg": 14.81, "peak": 16.05, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.88, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 29.0, "energy_joules_est": 61.55, "sample_count": 21, "duration_seconds": 2.122}, "timestamp": "2026-01-19T15:33:36.480972"}
{"image_index": 460, "image_name": "000000051961.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051961.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2485.11, "latencies_ms": [2485.11], "images_per_second": 0.402, "prompt_tokens": 1100, "response_tokens_est": 54, "n_tiles": 1, "output_text": " The image captures a vibrant scene of urban artistry, where a graffiti-covered wall serves as a canvas for a multitude of colors and designs, with a prominent sign reading \"THE ONE N' ONLY\" and a parking meter standing as a silent observer amidst the creative chaos.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27570.2, "ram_available_mb": 98202.0, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 27570.2, "ram_available_mb": 98202.0, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.3, "peak": 40.16, "min": 16.16}, "VIN": {"avg": 60.79, "peak": 124.06, "min": 29.89}, "VIN_SYS_5V0": {"avg": 14.87, "peak": 16.05, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 15.91, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 27.3, "energy_joules_est": 67.87, "sample_count": 24, "duration_seconds": 2.486}, "timestamp": "2026-01-19T15:33:38.985065"}
{"image_index": 460, "image_name": "000000051961.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051961.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2349.274, "latencies_ms": [2349.274], "images_per_second": 0.426, "prompt_tokens": 1114, "response_tokens_est": 50, "n_tiles": 1, "output_text": " 1. Graffiti wall\n2. Graffiti art\n3. Graffiti lettering\n4. Graffiti design\n5. Graffiti paint\n6. Graffiti paintbrush\n7. Graffiti spray\n8. Graffiti can", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27570.2, "ram_available_mb": 98202.0, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 27570.2, "ram_available_mb": 98202.0, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.94, "peak": 40.18, "min": 16.95}, "VIN": {"avg": 65.06, "peak": 101.82, "min": 29.56}, "VIN_SYS_5V0": {"avg": 15.18, "peak": 16.26, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 16.21, "peak": 17.71, "min": 14.18}}, "power_watts_avg": 27.94, "energy_joules_est": 65.65, "sample_count": 23, "duration_seconds": 2.35}, "timestamp": "2026-01-19T15:33:41.375541"}
{"image_index": 460, "image_name": "000000051961.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051961.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2388.506, "latencies_ms": [2388.506], "images_per_second": 0.419, "prompt_tokens": 1118, "response_tokens_est": 51, "n_tiles": 1, "output_text": " The graffiti-covered wall is in the foreground, with the parking meter in the middle ground, and the building with the metal fence in the background. The parking meter is positioned to the right of the graffiti, and the building is behind the parking meter.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27570.2, "ram_available_mb": 98202.0, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 27570.2, "ram_available_mb": 98202.0, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.55, "peak": 40.18, "min": 16.56}, "VIN": {"avg": 71.55, "peak": 123.12, "min": 28.78}, "VIN_SYS_5V0": {"avg": 14.96, "peak": 16.15, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 16.04, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 27.55, "energy_joules_est": 65.81, "sample_count": 24, "duration_seconds": 2.389}, "timestamp": "2026-01-19T15:33:43.867719"}
{"image_index": 460, "image_name": "000000051961.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051961.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2371.376, "latencies_ms": [2371.376], "images_per_second": 0.422, "prompt_tokens": 1112, "response_tokens_est": 50, "n_tiles": 1, "output_text": " The image captures a vibrant scene of urban artistry, where a graffiti-covered wall serves as a canvas for various tags and designs. The wall, adorned with a mix of colors and patterns, stands as a testament to the creative energy of the city.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27570.2, "ram_available_mb": 98202.0, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 27570.2, "ram_available_mb": 98202.0, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.29, "peak": 39.38, "min": 14.98}, "VIN": {"avg": 67.72, "peak": 105.31, "min": 28.99}, "VIN_SYS_5V0": {"avg": 14.82, "peak": 16.15, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 15.94, "peak": 17.32, "min": 12.61}}, "power_watts_avg": 27.29, "energy_joules_est": 64.73, "sample_count": 23, "duration_seconds": 2.372}, "timestamp": "2026-01-19T15:33:46.262561"}
{"image_index": 460, "image_name": "000000051961.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051961.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2166.437, "latencies_ms": [2166.437], "images_per_second": 0.462, "prompt_tokens": 1110, "response_tokens_est": 43, "n_tiles": 1, "output_text": " The image features a vibrant and colorful graffiti-covered wall with a mix of black, white, blue, and red colors. The lighting is natural, likely from the sun, and the weather appears to be clear.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27570.2, "ram_available_mb": 98202.0, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 27570.2, "ram_available_mb": 98202.0, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.71, "peak": 40.57, "min": 16.95}, "VIN": {"avg": 64.31, "peak": 104.0, "min": 32.28}, "VIN_SYS_5V0": {"avg": 15.15, "peak": 16.26, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 16.22, "peak": 17.71, "min": 14.18}}, "power_watts_avg": 28.71, "energy_joules_est": 62.22, "sample_count": 21, "duration_seconds": 2.167}, "timestamp": "2026-01-19T15:33:48.439050"}
{"image_index": 461, "image_name": "000000051976.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051976.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1452.667, "latencies_ms": [1452.667], "images_per_second": 0.688, "prompt_tokens": 1099, "response_tokens_est": 15, "n_tiles": 1, "output_text": " A surfer is riding a wave on a surfboard in the ocean.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27570.2, "ram_available_mb": 98202.0, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 2.5, "ram_used_mb": 27570.2, "ram_available_mb": 98202.0, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.57, "peak": 40.57, "min": 17.34}, "VIN": {"avg": 74.71, "peak": 105.38, "min": 27.34}, "VIN_SYS_5V0": {"avg": 15.17, "peak": 16.15, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 16.22, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 31.57, "energy_joules_est": 45.88, "sample_count": 15, "duration_seconds": 1.453}, "timestamp": "2026-01-19T15:33:49.999157"}
{"image_index": 461, "image_name": "000000051976.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051976.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2638.95, "latencies_ms": [2638.95], "images_per_second": 0.379, "prompt_tokens": 1113, "response_tokens_est": 60, "n_tiles": 1, "output_text": " 1. Surfer: 1\n2. Surfboard: 1\n3. Ocean: 1\n4. Wave: 1\n5. Water: 1\n6. Sky: 1\n7. Clouds: 1\n8. Sunlight: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 27570.2, "ram_available_mb": 98202.0, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 27570.4, "ram_available_mb": 98201.7, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.5, "peak": 40.18, "min": 18.91}, "VIN": {"avg": 67.53, "peak": 84.2, "min": 29.39}, "VIN_SYS_5V0": {"avg": 14.82, "peak": 16.05, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.79, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 27.5, "energy_joules_est": 72.59, "sample_count": 26, "duration_seconds": 2.64}, "timestamp": "2026-01-19T15:33:52.684352"}
{"image_index": 461, "image_name": "000000051976.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051976.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2350.494, "latencies_ms": [2350.494], "images_per_second": 0.425, "prompt_tokens": 1117, "response_tokens_est": 49, "n_tiles": 1, "output_text": " The surfer is positioned in the foreground of the image, riding a wave that is moving towards the right side of the frame. The wave is the main focus of the image, with the surfer positioned on the left side of the wave.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 27570.4, "ram_available_mb": 98201.7, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 27570.4, "ram_available_mb": 98201.7, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.89, "peak": 40.18, "min": 16.16}, "VIN": {"avg": 74.8, "peak": 126.88, "min": 29.39}, "VIN_SYS_5V0": {"avg": 15.02, "peak": 16.15, "min": 12.46}, "VDD_CPU_SOC_MSS": {"avg": 15.97, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 27.89, "energy_joules_est": 65.56, "sample_count": 23, "duration_seconds": 2.351}, "timestamp": "2026-01-19T15:33:55.063383"}
{"image_index": 461, "image_name": "000000051976.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051976.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1370.46, "latencies_ms": [1370.46], "images_per_second": 0.73, "prompt_tokens": 1111, "response_tokens_est": 11, "n_tiles": 1, "output_text": " A surfer is riding a wave in the ocean.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 27570.4, "ram_available_mb": 98201.7, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 2.5, "ram_used_mb": 27570.4, "ram_available_mb": 98201.7, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.77, "peak": 40.57, "min": 16.95}, "VIN": {"avg": 71.54, "peak": 114.43, "min": 29.25}, "VIN_SYS_5V0": {"avg": 15.03, "peak": 16.15, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 16.14, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 31.77, "energy_joules_est": 43.55, "sample_count": 14, "duration_seconds": 1.371}, "timestamp": "2026-01-19T15:33:56.522455"}
{"image_index": 461, "image_name": "000000051976.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051976.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2564.039, "latencies_ms": [2564.039], "images_per_second": 0.39, "prompt_tokens": 1109, "response_tokens_est": 57, "n_tiles": 1, "output_text": " The image captures a surfer riding a wave in the ocean, with the surfer wearing a black wetsuit and the wave displaying a deep blue color. The lighting in the image suggests it is either early morning or late afternoon, and the weather appears to be clear and sunny.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27570.4, "ram_available_mb": 98201.7, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 27570.4, "ram_available_mb": 98201.8, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.14, "peak": 40.97, "min": 19.31}, "VIN": {"avg": 73.71, "peak": 129.26, "min": 29.19}, "VIN_SYS_5V0": {"avg": 14.95, "peak": 16.05, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.92, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 28.14, "energy_joules_est": 72.17, "sample_count": 25, "duration_seconds": 2.565}, "timestamp": "2026-01-19T15:33:59.113122"}
{"image_index": 462, "image_name": "000000052007.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052007.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1619.078, "latencies_ms": [1619.078], "images_per_second": 0.618, "prompt_tokens": 1100, "response_tokens_est": 21, "n_tiles": 1, "output_text": " A double-decker bus is parked at a bus stop, with a man standing next to it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27570.4, "ram_available_mb": 98201.8, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 27570.4, "ram_available_mb": 98201.8, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.78, "peak": 39.78, "min": 16.55}, "VIN": {"avg": 77.43, "peak": 130.78, "min": 28.78}, "VIN_SYS_5V0": {"avg": 15.16, "peak": 16.15, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 16.17, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 30.78, "energy_joules_est": 49.85, "sample_count": 16, "duration_seconds": 1.62}, "timestamp": "2026-01-19T15:34:00.783577"}
{"image_index": 462, "image_name": "000000052007.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052007.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2674.876, "latencies_ms": [2674.876], "images_per_second": 0.374, "prompt_tokens": 1114, "response_tokens_est": 62, "n_tiles": 1, "output_text": " 1. bus: 1\n2. people: 2\n3. flowers: 1\n4. bus stop: 1\n5. bus number: 1\n6. bus route: 1\n7. bus destination: 1\n8. bus license plate: 1", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 27570.4, "ram_available_mb": 98201.8, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 27570.4, "ram_available_mb": 98201.8, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.35, "peak": 40.56, "min": 18.91}, "VIN": {"avg": 68.94, "peak": 112.11, "min": 30.7}, "VIN_SYS_5V0": {"avg": 15.06, "peak": 16.26, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 16.11, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 27.35, "energy_joules_est": 73.17, "sample_count": 26, "duration_seconds": 2.675}, "timestamp": "2026-01-19T15:34:03.471422"}
{"image_index": 462, "image_name": "000000052007.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052007.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2066.31, "latencies_ms": [2066.31], "images_per_second": 0.484, "prompt_tokens": 1118, "response_tokens_est": 39, "n_tiles": 1, "output_text": " The bus is on the left side of the image, with the passengers standing on the right side. The bus is in the foreground, while the bus stop and the building are in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27570.4, "ram_available_mb": 98201.8, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 27570.4, "ram_available_mb": 98201.8, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.13, "peak": 39.77, "min": 16.95}, "VIN": {"avg": 75.48, "peak": 124.65, "min": 30.43}, "VIN_SYS_5V0": {"avg": 15.26, "peak": 16.26, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 16.28, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 29.13, "energy_joules_est": 60.2, "sample_count": 20, "duration_seconds": 2.067}, "timestamp": "2026-01-19T15:34:05.552244"}
{"image_index": 462, "image_name": "000000052007.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052007.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1487.116, "latencies_ms": [1487.116], "images_per_second": 0.672, "prompt_tokens": 1112, "response_tokens_est": 17, "n_tiles": 1, "output_text": " A double-decker bus is parked at a bus stop on a city street.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 27570.4, "ram_available_mb": 98201.8, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 27570.4, "ram_available_mb": 98201.8, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.54, "peak": 39.39, "min": 18.14}, "VIN": {"avg": 80.05, "peak": 130.99, "min": 29.19}, "VIN_SYS_5V0": {"avg": 15.26, "peak": 16.26, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 16.35, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 31.54, "energy_joules_est": 46.91, "sample_count": 15, "duration_seconds": 1.487}, "timestamp": "2026-01-19T15:34:07.115870"}
{"image_index": 462, "image_name": "000000052007.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052007.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2175.729, "latencies_ms": [2175.729], "images_per_second": 0.46, "prompt_tokens": 1110, "response_tokens_est": 42, "n_tiles": 1, "output_text": " The image features a yellow double-decker bus with a black front, parked on a street with a red brick sidewalk. The sky is overcast, and the lighting is natural, suggesting it is daytime.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 27570.4, "ram_available_mb": 98201.8, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 27570.4, "ram_available_mb": 98201.8, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.06, "peak": 40.18, "min": 19.32}, "VIN": {"avg": 66.49, "peak": 122.72, "min": 29.46}, "VIN_SYS_5V0": {"avg": 14.92, "peak": 16.15, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 16.01, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 29.06, "energy_joules_est": 63.24, "sample_count": 21, "duration_seconds": 2.176}, "timestamp": "2026-01-19T15:34:09.304800"}
{"image_index": 463, "image_name": "000000052017.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052017.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1612.923, "latencies_ms": [1612.923], "images_per_second": 0.62, "prompt_tokens": 1099, "response_tokens_est": 21, "n_tiles": 1, "output_text": " A yellow and red biplane with the registration number SP-AWE is flying in the cloudy sky.", "error": null, "sys_before": {"cpu_percent": 14.3, "ram_used_mb": 27570.4, "ram_available_mb": 98201.8, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 27570.4, "ram_available_mb": 98201.8, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.05, "peak": 40.57, "min": 17.35}, "VIN": {"avg": 75.32, "peak": 115.78, "min": 29.92}, "VIN_SYS_5V0": {"avg": 15.17, "peak": 16.15, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 16.26, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 31.05, "energy_joules_est": 50.1, "sample_count": 16, "duration_seconds": 1.614}, "timestamp": "2026-01-19T15:34:10.974282"}
{"image_index": 463, "image_name": "000000052017.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052017.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2620.065, "latencies_ms": [2620.065], "images_per_second": 0.382, "prompt_tokens": 1113, "response_tokens_est": 59, "n_tiles": 1, "output_text": " 1. airplane: 1\n2. tail: 1\n3. wing: 2\n4. propeller: 1\n5. propeller blade: 1\n6. engine: 1\n7. landing gear: 1\n8. propeller shaft: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27570.4, "ram_available_mb": 98201.8, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 27572.1, "ram_available_mb": 98200.0, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.4, "peak": 40.97, "min": 17.73}, "VIN": {"avg": 64.56, "peak": 127.01, "min": 27.52}, "VIN_SYS_5V0": {"avg": 14.93, "peak": 16.15, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.93, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 27.4, "energy_joules_est": 71.8, "sample_count": 26, "duration_seconds": 2.62}, "timestamp": "2026-01-19T15:34:13.678375"}
{"image_index": 463, "image_name": "000000052017.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052017.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1807.117, "latencies_ms": [1807.117], "images_per_second": 0.553, "prompt_tokens": 1117, "response_tokens_est": 27, "n_tiles": 1, "output_text": " The yellow and red biplane is positioned in the foreground, flying from left to right, while the cloudy sky is in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27572.1, "ram_available_mb": 98200.0, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 27572.1, "ram_available_mb": 98200.0, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.39, "peak": 40.16, "min": 15.38}, "VIN": {"avg": 74.65, "peak": 118.83, "min": 28.03}, "VIN_SYS_5V0": {"avg": 14.76, "peak": 16.05, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.77, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 29.39, "energy_joules_est": 53.12, "sample_count": 18, "duration_seconds": 1.808}, "timestamp": "2026-01-19T15:34:15.558319"}
{"image_index": 463, "image_name": "000000052017.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052017.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1543.951, "latencies_ms": [1543.951], "images_per_second": 0.648, "prompt_tokens": 1111, "response_tokens_est": 17, "n_tiles": 1, "output_text": " A yellow and red biplane is flying in the sky, with a cloudy background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27572.1, "ram_available_mb": 98200.0, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 27572.1, "ram_available_mb": 98200.0, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.2, "peak": 40.18, "min": 17.35}, "VIN": {"avg": 70.76, "peak": 119.88, "min": 27.54}, "VIN_SYS_5V0": {"avg": 14.81, "peak": 16.05, "min": 12.15}, "VDD_CPU_SOC_MSS": {"avg": 15.85, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 31.2, "energy_joules_est": 48.2, "sample_count": 15, "duration_seconds": 1.545}, "timestamp": "2026-01-19T15:34:17.131888"}
{"image_index": 463, "image_name": "000000052017.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052017.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1876.884, "latencies_ms": [1876.884], "images_per_second": 0.533, "prompt_tokens": 1109, "response_tokens_est": 31, "n_tiles": 1, "output_text": " The airplane is painted in vibrant yellow and red, with a blue propeller and a red tail. The sky is overcast, with a grayish hue.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27572.1, "ram_available_mb": 98200.0, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 27572.1, "ram_available_mb": 98200.0, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.97, "peak": 40.97, "min": 20.49}, "VIN": {"avg": 72.03, "peak": 115.94, "min": 36.85}, "VIN_SYS_5V0": {"avg": 15.16, "peak": 16.15, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 16.25, "peak": 17.32, "min": 13.78}}, "power_watts_avg": 30.97, "energy_joules_est": 58.14, "sample_count": 18, "duration_seconds": 1.877}, "timestamp": "2026-01-19T15:34:19.015639"}
{"image_index": 464, "image_name": "000000052412.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052412.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2057.439, "latencies_ms": [2057.439], "images_per_second": 0.486, "prompt_tokens": 1099, "response_tokens_est": 39, "n_tiles": 1, "output_text": " The image captures an aerial view of a bustling airport, with a large parking lot brimming with cars, a building with a distinctive red roof, and a prominent airplane wing visible in the foreground.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 27572.1, "ram_available_mb": 98200.0, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 27572.1, "ram_available_mb": 98200.0, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.68, "peak": 40.18, "min": 18.91}, "VIN": {"avg": 75.65, "peak": 125.87, "min": 29.13}, "VIN_SYS_5V0": {"avg": 15.27, "peak": 16.15, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 16.4, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 29.68, "energy_joules_est": 61.08, "sample_count": 20, "duration_seconds": 2.058}, "timestamp": "2026-01-19T15:34:21.110982"}
{"image_index": 464, "image_name": "000000052412.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052412.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2004.894, "latencies_ms": [2004.894], "images_per_second": 0.499, "prompt_tokens": 1113, "response_tokens_est": 36, "n_tiles": 1, "output_text": " airplane wing: 1, car: 100, building: 1, parking lot: 100, trees: 10, sky: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27572.1, "ram_available_mb": 98200.0, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 27572.1, "ram_available_mb": 98200.0, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.27, "peak": 40.57, "min": 17.35}, "VIN": {"avg": 70.75, "peak": 103.79, "min": 31.59}, "VIN_SYS_5V0": {"avg": 15.04, "peak": 16.15, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 16.14, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 29.27, "energy_joules_est": 58.7, "sample_count": 20, "duration_seconds": 2.005}, "timestamp": "2026-01-19T15:34:23.196071"}
{"image_index": 464, "image_name": "000000052412.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052412.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2080.51, "latencies_ms": [2080.51], "images_per_second": 0.481, "prompt_tokens": 1117, "response_tokens_est": 38, "n_tiles": 1, "output_text": " The airplane wing is on the left side of the image, while the parking lot is on the right side. The parking lot is in the foreground, with the cityscape in the background.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 27572.1, "ram_available_mb": 98200.0, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 27572.1, "ram_available_mb": 98200.1, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.04, "peak": 40.18, "min": 16.56}, "VIN": {"avg": 73.04, "peak": 119.64, "min": 32.88}, "VIN_SYS_5V0": {"avg": 14.88, "peak": 16.05, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 16.0, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 29.04, "energy_joules_est": 60.43, "sample_count": 20, "duration_seconds": 2.081}, "timestamp": "2026-01-19T15:34:25.280499"}
{"image_index": 464, "image_name": "000000052412.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052412.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3045.635, "latencies_ms": [3045.635], "images_per_second": 0.328, "prompt_tokens": 1111, "response_tokens_est": 77, "n_tiles": 1, "output_text": " The image captures a bustling airport scene from an aerial perspective. The foreground is dominated by a large parking lot filled with numerous cars, indicating a busy day at the airport. In the background, a large building with a distinctive red roof stands out, likely serving as a terminal or a hangar. The clear blue sky above and the distant cityscape further emphasize the location as an airport.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 27572.1, "ram_available_mb": 98200.1, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 27572.1, "ram_available_mb": 98200.1, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.27, "peak": 40.18, "min": 17.73}, "VIN": {"avg": 70.04, "peak": 123.71, "min": 28.11}, "VIN_SYS_5V0": {"avg": 15.13, "peak": 16.26, "min": 13.29}, "VDD_CPU_SOC_MSS": {"avg": 16.21, "peak": 17.71, "min": 14.57}}, "power_watts_avg": 26.27, "energy_joules_est": 80.02, "sample_count": 30, "duration_seconds": 3.046}, "timestamp": "2026-01-19T15:34:28.409767"}
{"image_index": 464, "image_name": "000000052412.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052412.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2569.24, "latencies_ms": [2569.24], "images_per_second": 0.389, "prompt_tokens": 1109, "response_tokens_est": 57, "n_tiles": 1, "output_text": " The image is a high-angle aerial view of a large parking lot, with a clear blue sky and white clouds in the background. The parking lot is filled with numerous cars of various colors, and there are several buildings in the vicinity, including a large one with a red roof.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 27572.1, "ram_available_mb": 98200.1, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 27572.1, "ram_available_mb": 98200.1, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.91, "peak": 40.16, "min": 15.38}, "VIN": {"avg": 69.53, "peak": 122.88, "min": 28.96}, "VIN_SYS_5V0": {"avg": 14.86, "peak": 16.05, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.94, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 26.91, "energy_joules_est": 69.15, "sample_count": 25, "duration_seconds": 2.57}, "timestamp": "2026-01-19T15:34:31.016093"}
{"image_index": 465, "image_name": "000000052413.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052413.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1504.209, "latencies_ms": [1504.209], "images_per_second": 0.665, "prompt_tokens": 1099, "response_tokens_est": 17, "n_tiles": 1, "output_text": " A person is holding a pink flip phone with a picture of a girl on it.", "error": null, "sys_before": {"cpu_percent": 14.3, "ram_used_mb": 27572.1, "ram_available_mb": 98200.1, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 27572.1, "ram_available_mb": 98200.1, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.33, "peak": 40.57, "min": 16.55}, "VIN": {"avg": 72.38, "peak": 113.02, "min": 28.81}, "VIN_SYS_5V0": {"avg": 15.08, "peak": 16.15, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 16.17, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 31.33, "energy_joules_est": 47.15, "sample_count": 15, "duration_seconds": 1.505}, "timestamp": "2026-01-19T15:34:32.585001"}
{"image_index": 465, "image_name": "000000052413.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052413.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2692.067, "latencies_ms": [2692.067], "images_per_second": 0.371, "prompt_tokens": 1113, "response_tokens_est": 57, "n_tiles": 1, "output_text": " 1. person: 2\n2. cell phone: 1\n3. cup: 1\n4. book: 1\n5. bag: 1\n6. person: 1\n7. person: 1\n8. person: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27572.1, "ram_available_mb": 98200.1, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 27572.1, "ram_available_mb": 98200.1, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.23, "peak": 40.97, "min": 18.91}, "VIN": {"avg": 65.73, "peak": 103.55, "min": 29.47}, "VIN_SYS_5V0": {"avg": 14.76, "peak": 16.05, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.79, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 27.23, "energy_joules_est": 73.32, "sample_count": 26, "duration_seconds": 2.693}, "timestamp": "2026-01-19T15:34:35.291647"}
{"image_index": 465, "image_name": "000000052413.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052413.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2675.023, "latencies_ms": [2675.023], "images_per_second": 0.374, "prompt_tokens": 1117, "response_tokens_est": 60, "n_tiles": 1, "output_text": " The pink flip phone is held in the foreground by a person's hand, with the person's legs and another person's legs in the background. The pink flip phone is positioned to the left of the person's legs, and the person's legs are positioned to the right of the pink flip phone.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 27572.1, "ram_available_mb": 98200.1, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 27572.1, "ram_available_mb": 98200.1, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.84, "peak": 40.18, "min": 16.56}, "VIN": {"avg": 72.11, "peak": 124.83, "min": 28.71}, "VIN_SYS_5V0": {"avg": 15.02, "peak": 16.15, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 15.99, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 26.84, "energy_joules_est": 71.81, "sample_count": 26, "duration_seconds": 2.675}, "timestamp": "2026-01-19T15:34:37.999507"}
{"image_index": 465, "image_name": "000000052413.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052413.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1895.499, "latencies_ms": [1895.499], "images_per_second": 0.528, "prompt_tokens": 1111, "response_tokens_est": 32, "n_tiles": 1, "output_text": " A person is holding a pink flip phone with a picture of a girl on it. The person is sitting on a couch with a book in front of them.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27572.1, "ram_available_mb": 98200.1, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 27572.1, "ram_available_mb": 98200.1, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.32, "peak": 40.57, "min": 16.16}, "VIN": {"avg": 70.03, "peak": 111.06, "min": 28.91}, "VIN_SYS_5V0": {"avg": 15.03, "peak": 16.15, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 16.14, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 29.32, "energy_joules_est": 55.59, "sample_count": 19, "duration_seconds": 1.896}, "timestamp": "2026-01-19T15:34:39.979923"}
{"image_index": 465, "image_name": "000000052413.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052413.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1965.705, "latencies_ms": [1965.705], "images_per_second": 0.509, "prompt_tokens": 1109, "response_tokens_est": 34, "n_tiles": 1, "output_text": " The image is taken in a dimly lit room with a pink flip phone in the foreground. The phone is held by a person wearing blue jeans and white sneakers.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27572.1, "ram_available_mb": 98200.1, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 27572.4, "ram_available_mb": 98199.8, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.51, "peak": 40.57, "min": 16.95}, "VIN": {"avg": 70.08, "peak": 127.1, "min": 30.1}, "VIN_SYS_5V0": {"avg": 14.87, "peak": 16.05, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 15.87, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 29.51, "energy_joules_est": 58.02, "sample_count": 19, "duration_seconds": 1.966}, "timestamp": "2026-01-19T15:34:41.959741"}
{"image_index": 466, "image_name": "000000052462.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052462.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1700.189, "latencies_ms": [1700.189], "images_per_second": 0.588, "prompt_tokens": 1099, "response_tokens_est": 25, "n_tiles": 1, "output_text": " Two zebras are standing in a dry grass field, with one zebra looking directly at the camera and the other looking away.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 27572.4, "ram_available_mb": 98199.8, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 27572.4, "ram_available_mb": 98199.8, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.89, "peak": 40.57, "min": 18.14}, "VIN": {"avg": 78.44, "peak": 120.74, "min": 29.36}, "VIN_SYS_5V0": {"avg": 15.19, "peak": 16.15, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 16.28, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 30.89, "energy_joules_est": 52.54, "sample_count": 17, "duration_seconds": 1.701}, "timestamp": "2026-01-19T15:34:43.734657"}
{"image_index": 466, "image_name": "000000052462.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052462.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1194.223, "latencies_ms": [1194.223], "images_per_second": 0.837, "prompt_tokens": 1113, "response_tokens_est": 4, "n_tiles": 1, "output_text": " zebra: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27572.4, "ram_available_mb": 98199.8, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 2.1, "ram_used_mb": 27572.4, "ram_available_mb": 98199.8, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.6, "peak": 40.57, "min": 18.13}, "VIN": {"avg": 73.47, "peak": 123.68, "min": 29.08}, "VIN_SYS_5V0": {"avg": 14.78, "peak": 16.05, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.95, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 32.6, "energy_joules_est": 38.96, "sample_count": 12, "duration_seconds": 1.195}, "timestamp": "2026-01-19T15:34:44.992101"}
{"image_index": 466, "image_name": "000000052462.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052462.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2474.062, "latencies_ms": [2474.062], "images_per_second": 0.404, "prompt_tokens": 1117, "response_tokens_est": 53, "n_tiles": 1, "output_text": " The zebras are positioned in the foreground of the image, with the one on the left slightly closer to the camera than the one on the right. The background of the image features a grassy field with trees and hills, providing a natural habitat for the zebras.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 27572.4, "ram_available_mb": 98199.8, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 27572.4, "ram_available_mb": 98199.8, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.27, "peak": 40.95, "min": 19.7}, "VIN": {"avg": 68.01, "peak": 102.21, "min": 29.94}, "VIN_SYS_5V0": {"avg": 14.94, "peak": 16.05, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.93, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 29.27, "energy_joules_est": 72.43, "sample_count": 24, "duration_seconds": 2.474}, "timestamp": "2026-01-19T15:34:47.491019"}
{"image_index": 466, "image_name": "000000052462.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052462.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1537.296, "latencies_ms": [1537.296], "images_per_second": 0.65, "prompt_tokens": 1111, "response_tokens_est": 18, "n_tiles": 1, "output_text": " Two zebras are standing in a tall, dry grass field, looking at the camera.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 27572.4, "ram_available_mb": 98199.8, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 27572.4, "ram_available_mb": 98199.8, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.41, "peak": 40.57, "min": 16.56}, "VIN": {"avg": 69.71, "peak": 85.23, "min": 27.97}, "VIN_SYS_5V0": {"avg": 15.18, "peak": 16.15, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 16.27, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 31.41, "energy_joules_est": 48.31, "sample_count": 15, "duration_seconds": 1.538}, "timestamp": "2026-01-19T15:34:49.057970"}
{"image_index": 466, "image_name": "000000052462.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052462.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1645.495, "latencies_ms": [1645.495], "images_per_second": 0.608, "prompt_tokens": 1109, "response_tokens_est": 22, "n_tiles": 1, "output_text": " The zebras are black and white with a pattern of stripes, and the grass is a golden brown color.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27572.4, "ram_available_mb": 98199.8, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 27572.4, "ram_available_mb": 98199.8, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.03, "peak": 40.95, "min": 20.49}, "VIN": {"avg": 75.5, "peak": 106.76, "min": 29.66}, "VIN_SYS_5V0": {"avg": 15.16, "peak": 16.15, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 16.24, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 32.03, "energy_joules_est": 52.71, "sample_count": 16, "duration_seconds": 1.646}, "timestamp": "2026-01-19T15:34:50.729752"}
{"image_index": 467, "image_name": "000000052507.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052507.jpg", "image_width": 640, "image_height": 438, "image_resolution": "640x438", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1493.331, "latencies_ms": [1493.331], "images_per_second": 0.67, "prompt_tokens": 1099, "response_tokens_est": 16, "n_tiles": 1, "output_text": " A man is walking on the beach with a yellow surfboard in his hand.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 27572.4, "ram_available_mb": 98199.8, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 27572.3, "ram_available_mb": 98199.8, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.14, "peak": 40.95, "min": 19.7}, "VIN": {"avg": 74.83, "peak": 107.02, "min": 29.32}, "VIN_SYS_5V0": {"avg": 15.02, "peak": 16.15, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 16.19, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 32.14, "energy_joules_est": 48.01, "sample_count": 15, "duration_seconds": 1.494}, "timestamp": "2026-01-19T15:34:52.307869"}
{"image_index": 467, "image_name": "000000052507.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052507.jpg", "image_width": 640, "image_height": 438, "image_resolution": "640x438", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2624.78, "latencies_ms": [2624.78], "images_per_second": 0.381, "prompt_tokens": 1113, "response_tokens_est": 59, "n_tiles": 1, "output_text": " 1. Surfboard: 1\n2. Man: 1\n3. Ocean: 2\n4. Waves: 2\n5. Sunlight: 1\n6. Sky: 1\n7. Water: 1\n8. Sand: 1", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 27572.3, "ram_available_mb": 98199.8, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 27572.3, "ram_available_mb": 98199.8, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.47, "peak": 40.57, "min": 17.73}, "VIN": {"avg": 65.68, "peak": 85.76, "min": 27.84}, "VIN_SYS_5V0": {"avg": 14.85, "peak": 16.05, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.83, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 27.47, "energy_joules_est": 72.11, "sample_count": 26, "duration_seconds": 2.625}, "timestamp": "2026-01-19T15:34:55.024413"}
{"image_index": 467, "image_name": "000000052507.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052507.jpg", "image_width": 640, "image_height": 438, "image_resolution": "640x438", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2142.899, "latencies_ms": [2142.899], "images_per_second": 0.467, "prompt_tokens": 1117, "response_tokens_est": 40, "n_tiles": 1, "output_text": " The man is standing in the foreground of the image, with the ocean waves in the background. The yellow surfboard is held in front of the man, and the water is splashing around him.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27572.3, "ram_available_mb": 98199.8, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 27572.3, "ram_available_mb": 98199.8, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.24, "peak": 39.78, "min": 15.38}, "VIN": {"avg": 72.41, "peak": 114.9, "min": 30.02}, "VIN_SYS_5V0": {"avg": 14.76, "peak": 15.95, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.75, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 28.24, "energy_joules_est": 60.53, "sample_count": 21, "duration_seconds": 2.143}, "timestamp": "2026-01-19T15:34:57.216744"}
{"image_index": 467, "image_name": "000000052507.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052507.jpg", "image_width": 640, "image_height": 438, "image_resolution": "640x438", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1616.504, "latencies_ms": [1616.504], "images_per_second": 0.619, "prompt_tokens": 1111, "response_tokens_est": 20, "n_tiles": 1, "output_text": " A man is walking on the beach with a yellow surfboard. The ocean is in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27572.3, "ram_available_mb": 98199.8, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 27572.3, "ram_available_mb": 98199.8, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.78, "peak": 40.16, "min": 16.95}, "VIN": {"avg": 72.38, "peak": 121.6, "min": 28.92}, "VIN_SYS_5V0": {"avg": 14.9, "peak": 16.05, "min": 12.46}, "VDD_CPU_SOC_MSS": {"avg": 15.99, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 30.78, "energy_joules_est": 49.77, "sample_count": 16, "duration_seconds": 1.617}, "timestamp": "2026-01-19T15:34:58.893018"}
{"image_index": 467, "image_name": "000000052507.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052507.jpg", "image_width": 640, "image_height": 438, "image_resolution": "640x438", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2082.537, "latencies_ms": [2082.537], "images_per_second": 0.48, "prompt_tokens": 1109, "response_tokens_est": 37, "n_tiles": 1, "output_text": " The image features a man in the ocean, holding a yellow surfboard. The sky is clear and blue, and the water is a deep blue with white waves crashing onto the shore.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27572.3, "ram_available_mb": 98199.8, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 27572.3, "ram_available_mb": 98199.8, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.61, "peak": 40.57, "min": 19.32}, "VIN": {"avg": 73.07, "peak": 119.46, "min": 30.42}, "VIN_SYS_5V0": {"avg": 14.95, "peak": 16.05, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.92, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 29.61, "energy_joules_est": 61.67, "sample_count": 20, "duration_seconds": 2.083}, "timestamp": "2026-01-19T15:35:00.985038"}
{"image_index": 468, "image_name": "000000052565.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052565.jpg", "image_width": 640, "image_height": 458, "image_resolution": "640x458", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1510.925, "latencies_ms": [1510.925], "images_per_second": 0.662, "prompt_tokens": 1099, "response_tokens_est": 17, "n_tiles": 1, "output_text": " A black and white cow is standing on a beach with the ocean in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27572.3, "ram_available_mb": 98199.8, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 27572.3, "ram_available_mb": 98199.8, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.52, "peak": 40.56, "min": 17.34}, "VIN": {"avg": 68.4, "peak": 96.94, "min": 27.34}, "VIN_SYS_5V0": {"avg": 15.11, "peak": 16.15, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 16.27, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 31.52, "energy_joules_est": 47.65, "sample_count": 15, "duration_seconds": 1.512}, "timestamp": "2026-01-19T15:35:02.559010"}
{"image_index": 468, "image_name": "000000052565.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052565.jpg", "image_width": 640, "image_height": 458, "image_resolution": "640x458", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1202.877, "latencies_ms": [1202.877], "images_per_second": 0.831, "prompt_tokens": 1113, "response_tokens_est": 4, "n_tiles": 1, "output_text": " cow: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27572.3, "ram_available_mb": 98199.8, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 2.2, "ram_used_mb": 27572.3, "ram_available_mb": 98199.8, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 33.12, "peak": 40.57, "min": 19.7}, "VIN": {"avg": 74.53, "peak": 116.7, "min": 29.25}, "VIN_SYS_5V0": {"avg": 14.78, "peak": 16.05, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 16.11, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 33.12, "energy_joules_est": 39.85, "sample_count": 12, "duration_seconds": 1.203}, "timestamp": "2026-01-19T15:35:03.817484"}
{"image_index": 468, "image_name": "000000052565.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052565.jpg", "image_width": 640, "image_height": 458, "image_resolution": "640x458", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1907.062, "latencies_ms": [1907.062], "images_per_second": 0.524, "prompt_tokens": 1117, "response_tokens_est": 30, "n_tiles": 1, "output_text": " The cow is standing in the foreground, close to the camera, on the beach. The water is in the background, far away from the camera.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27572.3, "ram_available_mb": 98199.8, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 27572.3, "ram_available_mb": 98199.8, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.33, "peak": 40.95, "min": 19.7}, "VIN": {"avg": 68.2, "peak": 110.16, "min": 30.06}, "VIN_SYS_5V0": {"avg": 14.82, "peak": 15.95, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.85, "peak": 16.92, "min": 13.79}}, "power_watts_avg": 31.33, "energy_joules_est": 59.76, "sample_count": 19, "duration_seconds": 1.908}, "timestamp": "2026-01-19T15:35:05.799853"}
{"image_index": 468, "image_name": "000000052565.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052565.jpg", "image_width": 640, "image_height": 458, "image_resolution": "640x458", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1487.386, "latencies_ms": [1487.386], "images_per_second": 0.672, "prompt_tokens": 1111, "response_tokens_est": 15, "n_tiles": 1, "output_text": " A black and white cow stands on a beach, looking at the camera.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27572.3, "ram_available_mb": 98199.8, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 27572.3, "ram_available_mb": 98199.8, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.15, "peak": 40.18, "min": 16.95}, "VIN": {"avg": 76.55, "peak": 123.65, "min": 27.51}, "VIN_SYS_5V0": {"avg": 14.72, "peak": 16.05, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 15.85, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 31.15, "energy_joules_est": 46.35, "sample_count": 15, "duration_seconds": 1.488}, "timestamp": "2026-01-19T15:35:07.369328"}
{"image_index": 468, "image_name": "000000052565.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052565.jpg", "image_width": 640, "image_height": 458, "image_resolution": "640x458", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1592.894, "latencies_ms": [1592.894], "images_per_second": 0.628, "prompt_tokens": 1109, "response_tokens_est": 19, "n_tiles": 1, "output_text": " The cow is black and white, standing on a sandy beach with the ocean in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27572.3, "ram_available_mb": 98199.8, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 27572.3, "ram_available_mb": 98199.8, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.54, "peak": 40.57, "min": 20.11}, "VIN": {"avg": 70.92, "peak": 119.2, "min": 29.47}, "VIN_SYS_5V0": {"avg": 14.77, "peak": 16.05, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.9, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 31.54, "energy_joules_est": 50.25, "sample_count": 16, "duration_seconds": 1.593}, "timestamp": "2026-01-19T15:35:09.042950"}
{"image_index": 469, "image_name": "000000052591.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052591.jpg", "image_width": 359, "image_height": 640, "image_resolution": "359x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1796.705, "latencies_ms": [1796.705], "images_per_second": 0.557, "prompt_tokens": 1100, "response_tokens_est": 28, "n_tiles": 1, "output_text": " A woman in a white blouse and black pants is standing on skis in the snow, holding ski poles and wearing a scarf.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27572.3, "ram_available_mb": 98199.8, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 27572.6, "ram_available_mb": 98199.6, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.42, "peak": 40.57, "min": 18.91}, "VIN": {"avg": 73.59, "peak": 118.21, "min": 29.38}, "VIN_SYS_5V0": {"avg": 14.81, "peak": 15.95, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 15.97, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 30.42, "energy_joules_est": 54.67, "sample_count": 18, "duration_seconds": 1.797}, "timestamp": "2026-01-19T15:35:10.921172"}
{"image_index": 469, "image_name": "000000052591.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052591.jpg", "image_width": 359, "image_height": 640, "image_resolution": "359x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2635.92, "latencies_ms": [2635.92], "images_per_second": 0.379, "prompt_tokens": 1114, "response_tokens_est": 60, "n_tiles": 1, "output_text": " 1. woman: 1\n2. ski poles: 2\n3. skis: 2\n4. backpack: 1\n5. trees: 2\n6. clouds: 1\n7. snow: 1\n8. woman's hair: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27572.6, "ram_available_mb": 98199.6, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 27572.6, "ram_available_mb": 98199.6, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.96, "peak": 40.18, "min": 17.34}, "VIN": {"avg": 66.15, "peak": 122.8, "min": 29.22}, "VIN_SYS_5V0": {"avg": 14.82, "peak": 16.05, "min": 12.15}, "VDD_CPU_SOC_MSS": {"avg": 16.02, "peak": 17.32, "min": 13.4}}, "power_watts_avg": 26.96, "energy_joules_est": 71.08, "sample_count": 26, "duration_seconds": 2.636}, "timestamp": "2026-01-19T15:35:13.625300"}
{"image_index": 469, "image_name": "000000052591.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052591.jpg", "image_width": 359, "image_height": 640, "image_resolution": "359x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2042.947, "latencies_ms": [2042.947], "images_per_second": 0.489, "prompt_tokens": 1118, "response_tokens_est": 37, "n_tiles": 1, "output_text": " The woman is standing in the foreground, with the ski poles and trees in the background. The woman is holding the ski poles in her left hand, and the trees are behind her.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27572.6, "ram_available_mb": 98199.6, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 27572.6, "ram_available_mb": 98199.6, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.68, "peak": 40.18, "min": 15.77}, "VIN": {"avg": 65.92, "peak": 123.15, "min": 28.94}, "VIN_SYS_5V0": {"avg": 14.83, "peak": 16.05, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 16.04, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 28.68, "energy_joules_est": 58.61, "sample_count": 20, "duration_seconds": 2.043}, "timestamp": "2026-01-19T15:35:15.707317"}
{"image_index": 469, "image_name": "000000052591.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052591.jpg", "image_width": 359, "image_height": 640, "image_resolution": "359x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1710.905, "latencies_ms": [1710.905], "images_per_second": 0.584, "prompt_tokens": 1112, "response_tokens_est": 25, "n_tiles": 1, "output_text": " A woman in a white blouse and black pants is standing on a snowy hill, holding ski poles and wearing skis.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27572.6, "ram_available_mb": 98199.6, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 27572.6, "ram_available_mb": 98199.6, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.31, "peak": 39.78, "min": 17.35}, "VIN": {"avg": 69.12, "peak": 103.08, "min": 28.82}, "VIN_SYS_5V0": {"avg": 15.05, "peak": 16.26, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 16.28, "peak": 17.71, "min": 13.79}}, "power_watts_avg": 30.31, "energy_joules_est": 51.88, "sample_count": 17, "duration_seconds": 1.712}, "timestamp": "2026-01-19T15:35:17.473004"}
{"image_index": 469, "image_name": "000000052591.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052591.jpg", "image_width": 359, "image_height": 640, "image_resolution": "359x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2132.068, "latencies_ms": [2132.068], "images_per_second": 0.469, "prompt_tokens": 1110, "response_tokens_est": 41, "n_tiles": 1, "output_text": " The image is in black and white, with a clear contrast between the subject and the background. The subject is wearing a white blouse and black pants, and the background features a snowy landscape with trees.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27572.6, "ram_available_mb": 98199.6, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 27572.6, "ram_available_mb": 98199.6, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.99, "peak": 40.57, "min": 18.14}, "VIN": {"avg": 73.69, "peak": 128.39, "min": 30.57}, "VIN_SYS_5V0": {"avg": 14.97, "peak": 16.15, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 16.2, "peak": 17.71, "min": 13.79}}, "power_watts_avg": 28.99, "energy_joules_est": 61.82, "sample_count": 21, "duration_seconds": 2.133}, "timestamp": "2026-01-19T15:35:19.661073"}
{"image_index": 470, "image_name": "000000052891.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052891.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2017.941, "latencies_ms": [2017.941], "images_per_second": 0.496, "prompt_tokens": 1099, "response_tokens_est": 36, "n_tiles": 1, "output_text": " A dog with a black, white, and gray coat is holding a yellow frisbee in its mouth on a sandy beach with the ocean and a small island in the background.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 27572.6, "ram_available_mb": 98199.6, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 27572.6, "ram_available_mb": 98199.6, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.94, "peak": 40.16, "min": 16.56}, "VIN": {"avg": 72.41, "peak": 121.57, "min": 29.72}, "VIN_SYS_5V0": {"avg": 14.86, "peak": 16.05, "min": 12.25}, "VDD_CPU_SOC_MSS": {"avg": 16.0, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 28.94, "energy_joules_est": 58.41, "sample_count": 20, "duration_seconds": 2.018}, "timestamp": "2026-01-19T15:35:21.749782"}
{"image_index": 470, "image_name": "000000052891.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052891.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2023.993, "latencies_ms": [2023.993], "images_per_second": 0.494, "prompt_tokens": 1113, "response_tokens_est": 36, "n_tiles": 1, "output_text": " dog: 1, frisbee: 1, beach: 1, ocean: 1, island: 1, sand: 1, sky: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27572.6, "ram_available_mb": 98199.6, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 27572.6, "ram_available_mb": 98199.6, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.0, "peak": 40.18, "min": 16.95}, "VIN": {"avg": 71.37, "peak": 125.16, "min": 29.35}, "VIN_SYS_5V0": {"avg": 14.86, "peak": 16.05, "min": 12.15}, "VDD_CPU_SOC_MSS": {"avg": 16.04, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 29.0, "energy_joules_est": 58.7, "sample_count": 20, "duration_seconds": 2.024}, "timestamp": "2026-01-19T15:35:23.831407"}
{"image_index": 470, "image_name": "000000052891.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052891.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2407.209, "latencies_ms": [2407.209], "images_per_second": 0.415, "prompt_tokens": 1117, "response_tokens_est": 51, "n_tiles": 1, "output_text": " The dog is in the foreground, holding a yellow frisbee in its mouth. The beach is in the background, with the ocean and a small island visible. The dog is looking towards the camera, which is positioned to the left of the dog.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 27572.6, "ram_available_mb": 98199.6, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 27572.6, "ram_available_mb": 98199.6, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.59, "peak": 40.18, "min": 16.95}, "VIN": {"avg": 69.02, "peak": 120.91, "min": 28.91}, "VIN_SYS_5V0": {"avg": 14.92, "peak": 16.05, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.85, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 27.59, "energy_joules_est": 66.43, "sample_count": 24, "duration_seconds": 2.408}, "timestamp": "2026-01-19T15:35:26.327361"}
{"image_index": 470, "image_name": "000000052891.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052891.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1744.271, "latencies_ms": [1744.271], "images_per_second": 0.573, "prompt_tokens": 1111, "response_tokens_est": 25, "n_tiles": 1, "output_text": " A dog is playing on the beach with a frisbee. The beach is sandy and there are waves in the ocean.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 27572.6, "ram_available_mb": 98199.6, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 27572.6, "ram_available_mb": 98199.6, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.87, "peak": 40.18, "min": 15.38}, "VIN": {"avg": 70.58, "peak": 121.68, "min": 29.3}, "VIN_SYS_5V0": {"avg": 14.76, "peak": 16.05, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.86, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 29.87, "energy_joules_est": 52.12, "sample_count": 17, "duration_seconds": 1.745}, "timestamp": "2026-01-19T15:35:28.107355"}
{"image_index": 470, "image_name": "000000052891.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052891.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1753.029, "latencies_ms": [1753.029], "images_per_second": 0.57, "prompt_tokens": 1109, "response_tokens_est": 26, "n_tiles": 1, "output_text": " The dog is black, white, and gray, and the beach is sandy. The sky is cloudy and the ocean is blue.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 27572.6, "ram_available_mb": 98199.6, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 27572.6, "ram_available_mb": 98199.6, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.03, "peak": 40.57, "min": 18.92}, "VIN": {"avg": 73.17, "peak": 104.07, "min": 29.42}, "VIN_SYS_5V0": {"avg": 15.04, "peak": 16.15, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 16.21, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 31.03, "energy_joules_est": 54.41, "sample_count": 17, "duration_seconds": 1.754}, "timestamp": "2026-01-19T15:35:29.884292"}
{"image_index": 471, "image_name": "000000052996.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052996.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2080.439, "latencies_ms": [2080.439], "images_per_second": 0.481, "prompt_tokens": 1099, "response_tokens_est": 39, "n_tiles": 1, "output_text": " In a kitchen, a group of women, including a woman in a camouflage uniform, are gathered around a table with a large pot of food, while a woman in a white shirt stands nearby.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27572.6, "ram_available_mb": 98199.6, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 27570.7, "ram_available_mb": 98201.5, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.17, "peak": 40.95, "min": 18.53}, "VIN": {"avg": 69.3, "peak": 100.18, "min": 29.35}, "VIN_SYS_5V0": {"avg": 15.03, "peak": 16.15, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 16.16, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 29.17, "energy_joules_est": 60.7, "sample_count": 21, "duration_seconds": 2.081}, "timestamp": "2026-01-19T15:35:32.073824"}
{"image_index": 471, "image_name": "000000052996.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052996.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2532.497, "latencies_ms": [2532.497], "images_per_second": 0.395, "prompt_tokens": 1113, "response_tokens_est": 56, "n_tiles": 1, "output_text": " 1. woman: 2\n2. woman: 2\n3. woman: 2\n4. woman: 2\n5. woman: 2\n6. woman: 2\n7. woman: 2\n8. woman: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27570.7, "ram_available_mb": 98201.5, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 27570.7, "ram_available_mb": 98201.5, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.12, "peak": 40.18, "min": 15.77}, "VIN": {"avg": 67.71, "peak": 114.96, "min": 28.93}, "VIN_SYS_5V0": {"avg": 14.82, "peak": 15.95, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.83, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 27.12, "energy_joules_est": 68.69, "sample_count": 25, "duration_seconds": 2.533}, "timestamp": "2026-01-19T15:35:34.665516"}
{"image_index": 471, "image_name": "000000052996.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052996.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2770.696, "latencies_ms": [2770.696], "images_per_second": 0.361, "prompt_tokens": 1117, "response_tokens_est": 65, "n_tiles": 1, "output_text": " The main objects are positioned in the foreground of the image, with the kitchen appliances and countertop in the background. The woman in the camouflage uniform is standing near the countertop, while the woman in the striped shirt is standing near the refrigerator. The woman in the floral shirt is standing near the woman in the camouflage uniform.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27570.7, "ram_available_mb": 98201.5, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 27570.7, "ram_available_mb": 98201.5, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.63, "peak": 40.18, "min": 16.16}, "VIN": {"avg": 65.69, "peak": 125.41, "min": 28.68}, "VIN_SYS_5V0": {"avg": 14.91, "peak": 16.05, "min": 12.25}, "VDD_CPU_SOC_MSS": {"avg": 15.95, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 26.63, "energy_joules_est": 73.8, "sample_count": 27, "duration_seconds": 2.771}, "timestamp": "2026-01-19T15:35:37.476342"}
{"image_index": 471, "image_name": "000000052996.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052996.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1674.43, "latencies_ms": [1674.43], "images_per_second": 0.597, "prompt_tokens": 1111, "response_tokens_est": 23, "n_tiles": 1, "output_text": " A group of women in military uniforms are standing in a kitchen, with a large pot of food on the counter.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 27570.7, "ram_available_mb": 98201.5, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 27571.4, "ram_available_mb": 98200.7, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.22, "peak": 40.18, "min": 16.16}, "VIN": {"avg": 74.29, "peak": 122.37, "min": 28.59}, "VIN_SYS_5V0": {"avg": 14.96, "peak": 16.15, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 16.12, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 30.22, "energy_joules_est": 50.61, "sample_count": 17, "duration_seconds": 1.675}, "timestamp": "2026-01-19T15:35:39.256004"}
{"image_index": 471, "image_name": "000000052996.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052996.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1566.273, "latencies_ms": [1566.273], "images_per_second": 0.638, "prompt_tokens": 1109, "response_tokens_est": 18, "n_tiles": 1, "output_text": " The kitchen is well-lit with natural light, and the walls are made of concrete.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 27571.4, "ram_available_mb": 98200.7, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 27571.4, "ram_available_mb": 98200.7, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.76, "peak": 40.57, "min": 17.35}, "VIN": {"avg": 73.26, "peak": 105.86, "min": 27.74}, "VIN_SYS_5V0": {"avg": 14.6, "peak": 15.95, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.75, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 30.76, "energy_joules_est": 48.19, "sample_count": 16, "duration_seconds": 1.567}, "timestamp": "2026-01-19T15:35:40.924629"}
{"image_index": 472, "image_name": "000000053505.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053505.jpg", "image_width": 428, "image_height": 640, "image_resolution": "428x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1756.775, "latencies_ms": [1756.775], "images_per_second": 0.569, "prompt_tokens": 1100, "response_tokens_est": 26, "n_tiles": 1, "output_text": " The image depicts a bathroom with a toilet, a shelf above it holding various toiletries, and a shower area with a curtain.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27571.4, "ram_available_mb": 98200.7, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 27571.7, "ram_available_mb": 98200.5, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.64, "peak": 40.18, "min": 18.53}, "VIN": {"avg": 69.96, "peak": 123.71, "min": 29.54}, "VIN_SYS_5V0": {"avg": 14.77, "peak": 16.15, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 15.93, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 30.64, "energy_joules_est": 53.85, "sample_count": 17, "duration_seconds": 1.758}, "timestamp": "2026-01-19T15:35:42.701137"}
{"image_index": 472, "image_name": "000000053505.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053505.jpg", "image_width": 428, "image_height": 640, "image_resolution": "428x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2523.63, "latencies_ms": [2523.63], "images_per_second": 0.396, "prompt_tokens": 1114, "response_tokens_est": 57, "n_tiles": 1, "output_text": " 1. toilet: 1\n2. shelf: 3\n3. bottles: 4\n4. towel: 2\n5. toilet paper: 1\n6. phone: 1\n7. mirror: 1\n8. wall: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27571.7, "ram_available_mb": 98200.5, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 27571.7, "ram_available_mb": 98200.5, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.67, "peak": 40.16, "min": 18.13}, "VIN": {"avg": 72.38, "peak": 119.68, "min": 28.43}, "VIN_SYS_5V0": {"avg": 15.13, "peak": 16.26, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 16.27, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 27.67, "energy_joules_est": 69.84, "sample_count": 25, "duration_seconds": 2.524}, "timestamp": "2026-01-19T15:35:45.304434"}
{"image_index": 472, "image_name": "000000053505.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053505.jpg", "image_width": 428, "image_height": 640, "image_resolution": "428x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2149.849, "latencies_ms": [2149.849], "images_per_second": 0.465, "prompt_tokens": 1118, "response_tokens_est": 41, "n_tiles": 1, "output_text": " The toilet is located in the foreground of the image, with the shelf above it in the middle ground. The sink is situated to the left of the toilet, while the towel rack is to the right.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27571.7, "ram_available_mb": 98200.5, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 27582.4, "ram_available_mb": 98189.8, "ram_percent": 21.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.16, "peak": 40.16, "min": 15.38}, "VIN": {"avg": 74.34, "peak": 126.35, "min": 29.08}, "VIN_SYS_5V0": {"avg": 14.88, "peak": 16.05, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 15.99, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 28.16, "energy_joules_est": 60.55, "sample_count": 21, "duration_seconds": 2.15}, "timestamp": "2026-01-19T15:35:47.497227"}
{"image_index": 472, "image_name": "000000053505.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053505.jpg", "image_width": 428, "image_height": 640, "image_resolution": "428x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1477.697, "latencies_ms": [1477.697], "images_per_second": 0.677, "prompt_tokens": 1112, "response_tokens_est": 16, "n_tiles": 1, "output_text": " A bathroom with a toilet, sink, and shower is shown in the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27582.4, "ram_available_mb": 98189.8, "ram_percent": 21.9}, "sys_after": {"cpu_percent": 9.6, "ram_used_mb": 27667.3, "ram_available_mb": 98104.9, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.02, "peak": 39.78, "min": 16.95}, "VIN": {"avg": 75.63, "peak": 124.26, "min": 31.02}, "VIN_SYS_5V0": {"avg": 15.1, "peak": 16.26, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 17.11, "peak": 18.89, "min": 14.18}}, "power_watts_avg": 31.02, "energy_joules_est": 45.85, "sample_count": 15, "duration_seconds": 1.478}, "timestamp": "2026-01-19T15:35:49.065410"}
{"image_index": 472, "image_name": "000000053505.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053505.jpg", "image_width": 428, "image_height": 640, "image_resolution": "428x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1710.406, "latencies_ms": [1710.406], "images_per_second": 0.585, "prompt_tokens": 1110, "response_tokens_est": 24, "n_tiles": 1, "output_text": " The bathroom is well-lit with a warm yellow light, and the walls are painted in a light beige color.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27667.3, "ram_available_mb": 98104.9, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 11.0, "ram_used_mb": 27685.9, "ram_available_mb": 98086.3, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.8, "peak": 40.57, "min": 18.91}, "VIN": {"avg": 75.0, "peak": 130.73, "min": 32.08}, "VIN_SYS_5V0": {"avg": 14.93, "peak": 16.15, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 18.69, "peak": 20.47, "min": 15.76}}, "power_watts_avg": 30.8, "energy_joules_est": 52.69, "sample_count": 17, "duration_seconds": 1.711}, "timestamp": "2026-01-19T15:35:50.833186"}
{"image_index": 473, "image_name": "000000053529.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053529.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1620.891, "latencies_ms": [1620.891], "images_per_second": 0.617, "prompt_tokens": 1099, "response_tokens_est": 21, "n_tiles": 1, "output_text": " A dog wearing a green hat is sitting in the back seat of a car, looking out the window.", "error": null, "sys_before": {"cpu_percent": 16.7, "ram_used_mb": 27685.9, "ram_available_mb": 98086.3, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 10.5, "ram_used_mb": 27696.5, "ram_available_mb": 98075.7, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.44, "peak": 40.56, "min": 18.52}, "VIN": {"avg": 76.78, "peak": 111.32, "min": 32.14}, "VIN_SYS_5V0": {"avg": 14.97, "peak": 16.05, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 18.75, "peak": 20.07, "min": 16.15}}, "power_watts_avg": 31.44, "energy_joules_est": 50.98, "sample_count": 16, "duration_seconds": 1.622}, "timestamp": "2026-01-19T15:35:52.502326"}
{"image_index": 473, "image_name": "000000053529.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053529.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1987.939, "latencies_ms": [1987.939], "images_per_second": 0.503, "prompt_tokens": 1113, "response_tokens_est": 35, "n_tiles": 1, "output_text": " dog: 1, hat: 1, car: 1, window: 1, light: 1, shamrock: 1, person: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27696.5, "ram_available_mb": 98075.7, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 11.1, "ram_used_mb": 27712.2, "ram_available_mb": 98060.0, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.7, "peak": 40.56, "min": 18.91}, "VIN": {"avg": 72.86, "peak": 129.79, "min": 31.75}, "VIN_SYS_5V0": {"avg": 14.99, "peak": 16.15, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 18.74, "peak": 20.07, "min": 16.54}}, "power_watts_avg": 29.7, "energy_joules_est": 59.06, "sample_count": 20, "duration_seconds": 1.989}, "timestamp": "2026-01-19T15:35:54.587962"}
{"image_index": 473, "image_name": "000000053529.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053529.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2337.92, "latencies_ms": [2337.92], "images_per_second": 0.428, "prompt_tokens": 1117, "response_tokens_est": 48, "n_tiles": 1, "output_text": " The dog is in the driver's seat, which is on the left side of the vehicle. The person is in the passenger seat, which is on the right side of the vehicle. The dog is closer to the camera than the person.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 27712.2, "ram_available_mb": 98060.0, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 11.0, "ram_used_mb": 27727.7, "ram_available_mb": 98044.5, "ram_percent": 22.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.78, "peak": 40.16, "min": 16.16}, "VIN": {"avg": 75.59, "peak": 119.59, "min": 32.26}, "VIN_SYS_5V0": {"avg": 14.81, "peak": 16.05, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 18.59, "peak": 19.68, "min": 15.76}}, "power_watts_avg": 27.78, "energy_joules_est": 64.96, "sample_count": 23, "duration_seconds": 2.338}, "timestamp": "2026-01-19T15:35:56.981252"}
{"image_index": 473, "image_name": "000000053529.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053529.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1631.721, "latencies_ms": [1631.721], "images_per_second": 0.613, "prompt_tokens": 1111, "response_tokens_est": 21, "n_tiles": 1, "output_text": " A dog wearing a green hat is sitting in the back seat of a car, looking out the window.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27727.7, "ram_available_mb": 98044.5, "ram_percent": 22.0}, "sys_after": {"cpu_percent": 10.5, "ram_used_mb": 27743.4, "ram_available_mb": 98028.7, "ram_percent": 22.1}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.73, "peak": 40.16, "min": 16.16}, "VIN": {"avg": 75.53, "peak": 128.72, "min": 34.15}, "VIN_SYS_5V0": {"avg": 14.96, "peak": 16.15, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 18.7, "peak": 19.68, "min": 16.15}}, "power_watts_avg": 30.73, "energy_joules_est": 50.15, "sample_count": 16, "duration_seconds": 1.632}, "timestamp": "2026-01-19T15:35:58.650887"}
{"image_index": 473, "image_name": "000000053529.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053529.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2365.514, "latencies_ms": [2365.514], "images_per_second": 0.423, "prompt_tokens": 1109, "response_tokens_est": 48, "n_tiles": 1, "output_text": " The image features a dog wearing a green hat and a green shamrock on the side of a vehicle, with a person in the driver's seat. The vehicle is decorated with green and orange balloons, and the lighting suggests it is daytime.", "error": null, "sys_before": {"cpu_percent": 40.0, "ram_used_mb": 27743.4, "ram_available_mb": 98028.7, "ram_percent": 22.1}, "sys_after": {"cpu_percent": 13.4, "ram_used_mb": 27770.7, "ram_available_mb": 98001.5, "ram_percent": 22.1}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.45, "peak": 40.56, "min": 19.31}, "VIN": {"avg": 70.7, "peak": 108.43, "min": 33.52}, "VIN_SYS_5V0": {"avg": 15.01, "peak": 16.15, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 19.02, "peak": 21.25, "min": 16.54}}, "power_watts_avg": 28.45, "energy_joules_est": 67.32, "sample_count": 23, "duration_seconds": 2.366}, "timestamp": "2026-01-19T15:36:01.047615"}
{"image_index": 474, "image_name": "000000053624.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053624.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1742.557, "latencies_ms": [1742.557], "images_per_second": 0.574, "prompt_tokens": 1099, "response_tokens_est": 26, "n_tiles": 1, "output_text": " In the image, a large elephant is standing in a shallow pool of water, with a log and a fence in the background.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 27770.7, "ram_available_mb": 98001.5, "ram_percent": 22.1}, "sys_after": {"cpu_percent": 7.3, "ram_used_mb": 27786.2, "ram_available_mb": 97986.0, "ram_percent": 22.1}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.45, "peak": 40.56, "min": 16.55}, "VIN": {"avg": 74.23, "peak": 107.27, "min": 29.72}, "VIN_SYS_5V0": {"avg": 15.11, "peak": 16.15, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 17.9, "peak": 19.68, "min": 15.75}}, "power_watts_avg": 30.45, "energy_joules_est": 53.09, "sample_count": 17, "duration_seconds": 1.743}, "timestamp": "2026-01-19T15:36:02.826056"}
{"image_index": 474, "image_name": "000000053624.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053624.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1885.158, "latencies_ms": [1885.158], "images_per_second": 0.53, "prompt_tokens": 1113, "response_tokens_est": 31, "n_tiles": 1, "output_text": " elephant: 1\nwater: 1\nrocks: 3\ntree: 1\nfence: 1\npeople: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27786.2, "ram_available_mb": 97986.0, "ram_percent": 22.1}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 27784.2, "ram_available_mb": 97988.0, "ram_percent": 22.1}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.94, "peak": 40.57, "min": 18.91}, "VIN": {"avg": 72.2, "peak": 118.36, "min": 29.37}, "VIN_SYS_5V0": {"avg": 15.0, "peak": 16.15, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 16.16, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 29.94, "energy_joules_est": 56.45, "sample_count": 19, "duration_seconds": 1.885}, "timestamp": "2026-01-19T15:36:04.808832"}
{"image_index": 474, "image_name": "000000053624.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053624.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2682.408, "latencies_ms": [2682.408], "images_per_second": 0.373, "prompt_tokens": 1117, "response_tokens_est": 61, "n_tiles": 1, "output_text": " The elephant is positioned in the foreground, with the water body and rocks in the middle ground, and the people and fence in the background. The elephant is facing the water body, with its trunk extended towards it, and the rocks are positioned in the foreground, with the water body in the middle ground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27784.2, "ram_available_mb": 97988.0, "ram_percent": 22.1}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 27780.3, "ram_available_mb": 97991.9, "ram_percent": 22.1}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.93, "peak": 40.18, "min": 16.56}, "VIN": {"avg": 68.53, "peak": 104.8, "min": 31.35}, "VIN_SYS_5V0": {"avg": 14.8, "peak": 15.95, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.96, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 26.93, "energy_joules_est": 72.25, "sample_count": 26, "duration_seconds": 2.683}, "timestamp": "2026-01-19T15:36:07.525835"}
{"image_index": 474, "image_name": "000000053624.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053624.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1653.034, "latencies_ms": [1653.034], "images_per_second": 0.605, "prompt_tokens": 1111, "response_tokens_est": 22, "n_tiles": 1, "output_text": " In a zoo enclosure, a large elephant is drinking water from a pool, with a log and rocks nearby.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27776.4, "ram_available_mb": 97995.7, "ram_percent": 22.1}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 27773.7, "ram_available_mb": 97998.4, "ram_percent": 22.1}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.9, "peak": 40.57, "min": 16.56}, "VIN": {"avg": 72.77, "peak": 117.28, "min": 28.45}, "VIN_SYS_5V0": {"avg": 15.06, "peak": 16.15, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 16.24, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 30.9, "energy_joules_est": 51.09, "sample_count": 16, "duration_seconds": 1.653}, "timestamp": "2026-01-19T15:36:09.196287"}
{"image_index": 474, "image_name": "000000053624.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053624.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1493.73, "latencies_ms": [1493.73], "images_per_second": 0.669, "prompt_tokens": 1109, "response_tokens_est": 16, "n_tiles": 1, "output_text": " The elephant is gray, the water is blue, and the ground is brown.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27773.7, "ram_available_mb": 97998.4, "ram_percent": 22.1}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 27773.5, "ram_available_mb": 97998.7, "ram_percent": 22.1}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.25, "peak": 40.97, "min": 19.7}, "VIN": {"avg": 73.74, "peak": 99.76, "min": 28.84}, "VIN_SYS_5V0": {"avg": 15.18, "peak": 16.15, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 16.27, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 32.25, "energy_joules_est": 48.19, "sample_count": 15, "duration_seconds": 1.494}, "timestamp": "2026-01-19T15:36:10.763437"}
{"image_index": 475, "image_name": "000000053626.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053626.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1874.893, "latencies_ms": [1874.893], "images_per_second": 0.533, "prompt_tokens": 1099, "response_tokens_est": 31, "n_tiles": 1, "output_text": " Four people are standing on a snowy mountain, wearing ski gear and holding ski poles, with a clear blue sky and snow-covered mountains in the background.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 27773.5, "ram_available_mb": 97998.7, "ram_percent": 22.1}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 27773.7, "ram_available_mb": 97998.5, "ram_percent": 22.1}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.19, "peak": 40.95, "min": 19.31}, "VIN": {"avg": 71.83, "peak": 107.22, "min": 27.13}, "VIN_SYS_5V0": {"avg": 14.86, "peak": 16.05, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.98, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 30.19, "energy_joules_est": 56.62, "sample_count": 19, "duration_seconds": 1.875}, "timestamp": "2026-01-19T15:36:12.748645"}
{"image_index": 475, "image_name": "000000053626.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053626.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2625.832, "latencies_ms": [2625.832], "images_per_second": 0.381, "prompt_tokens": 1113, "response_tokens_est": 59, "n_tiles": 1, "output_text": " 1. skis: 4\n2. ski poles: 4\n3. skiers: 4\n4. jackets: 4\n5. hats: 4\n6. goggles: 4\n7. gloves: 4\n8. snow: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27773.7, "ram_available_mb": 97998.5, "ram_percent": 22.1}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 27774.6, "ram_available_mb": 97997.5, "ram_percent": 22.1}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.94, "peak": 40.18, "min": 16.16}, "VIN": {"avg": 72.24, "peak": 116.56, "min": 29.8}, "VIN_SYS_5V0": {"avg": 14.79, "peak": 16.05, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 15.78, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 26.94, "energy_joules_est": 70.75, "sample_count": 26, "duration_seconds": 2.626}, "timestamp": "2026-01-19T15:36:15.447247"}
{"image_index": 475, "image_name": "000000053626.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053626.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2469.042, "latencies_ms": [2469.042], "images_per_second": 0.405, "prompt_tokens": 1117, "response_tokens_est": 53, "n_tiles": 1, "output_text": " The skiers are positioned in the foreground of the image, with the mountains in the background. The skier on the left is slightly closer to the camera than the skier in the middle, while the skier on the right is positioned farthest from the camera.", "error": null, "sys_before": {"cpu_percent": 50.0, "ram_used_mb": 27774.6, "ram_available_mb": 97997.5, "ram_percent": 22.1}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 27774.9, "ram_available_mb": 97997.3, "ram_percent": 22.1}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.32, "peak": 40.18, "min": 15.38}, "VIN": {"avg": 69.52, "peak": 118.69, "min": 30.02}, "VIN_SYS_5V0": {"avg": 14.89, "peak": 16.05, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 15.95, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 27.32, "energy_joules_est": 67.47, "sample_count": 24, "duration_seconds": 2.469}, "timestamp": "2026-01-19T15:36:17.955840"}
{"image_index": 475, "image_name": "000000053626.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053626.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1521.978, "latencies_ms": [1521.978], "images_per_second": 0.657, "prompt_tokens": 1111, "response_tokens_est": 17, "n_tiles": 1, "output_text": " Four people are standing on a snowy mountain, wearing ski gear and holding ski poles.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27774.9, "ram_available_mb": 97997.3, "ram_percent": 22.1}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 27775.3, "ram_available_mb": 97996.9, "ram_percent": 22.1}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.28, "peak": 40.56, "min": 16.16}, "VIN": {"avg": 74.81, "peak": 116.66, "min": 28.82}, "VIN_SYS_5V0": {"avg": 15.09, "peak": 16.15, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 16.17, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 31.28, "energy_joules_est": 47.62, "sample_count": 15, "duration_seconds": 1.522}, "timestamp": "2026-01-19T15:36:19.523664"}
{"image_index": 475, "image_name": "000000053626.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053626.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2502.696, "latencies_ms": [2502.696], "images_per_second": 0.4, "prompt_tokens": 1109, "response_tokens_est": 55, "n_tiles": 1, "output_text": " The image features a group of four skiers standing on a snowy mountain, with the clear blue sky above them and the snow-covered mountains in the background. The skiers are wearing colorful ski gear, including jackets, pants, and helmets, and are holding ski poles.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27775.3, "ram_available_mb": 97996.9, "ram_percent": 22.1}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 27776.1, "ram_available_mb": 97996.0, "ram_percent": 22.1}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.83, "peak": 40.97, "min": 17.34}, "VIN": {"avg": 69.8, "peak": 115.16, "min": 28.88}, "VIN_SYS_5V0": {"avg": 15.03, "peak": 16.15, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 16.19, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 27.83, "energy_joules_est": 69.66, "sample_count": 25, "duration_seconds": 2.503}, "timestamp": "2026-01-19T15:36:22.128704"}
{"image_index": 476, "image_name": "000000053909.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053909.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2097.599, "latencies_ms": [2097.599], "images_per_second": 0.477, "prompt_tokens": 1100, "response_tokens_est": 39, "n_tiles": 1, "output_text": " A person is holding a black smartphone in their hand, and the screen displays a photo of a tree with the time \"9:45\" and the date \"Mon, 11\".", "error": null, "sys_before": {"cpu_percent": 18.2, "ram_used_mb": 27776.1, "ram_available_mb": 97996.0, "ram_percent": 22.1}, "sys_after": {"cpu_percent": 10.0, "ram_used_mb": 27968.5, "ram_available_mb": 97803.7, "ram_percent": 22.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.04, "peak": 40.16, "min": 14.98}, "VIN": {"avg": 69.85, "peak": 106.04, "min": 32.8}, "VIN_SYS_5V0": {"avg": 14.95, "peak": 16.05, "min": 11.87}, "VDD_CPU_SOC_MSS": {"avg": 16.82, "peak": 18.51, "min": 13.0}}, "power_watts_avg": 28.04, "energy_joules_est": 58.83, "sample_count": 21, "duration_seconds": 2.098}, "timestamp": "2026-01-19T15:36:24.315950"}
{"image_index": 476, "image_name": "000000053909.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053909.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2527.037, "latencies_ms": [2527.037], "images_per_second": 0.396, "prompt_tokens": 1114, "response_tokens_est": 56, "n_tiles": 1, "output_text": " 1. hand: 1\n2. smartphone: 1\n3. keyboard: 1\n4. screen: 1\n5. finger: 1\n6. wrist: 1\n7. table: 1\n8. light: 1", "error": null, "sys_before": {"cpu_percent": 75.0, "ram_used_mb": 27992.1, "ram_available_mb": 97780.1, "ram_percent": 22.3}, "sys_after": {"cpu_percent": 7.7, "ram_used_mb": 28062.3, "ram_available_mb": 97709.9, "ram_percent": 22.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.06, "peak": 40.16, "min": 15.76}, "VIN": {"avg": 75.09, "peak": 122.65, "min": 28.59}, "VIN_SYS_5V0": {"avg": 15.1, "peak": 16.15, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 16.98, "peak": 18.11, "min": 15.76}}, "power_watts_avg": 27.06, "energy_joules_est": 68.4, "sample_count": 25, "duration_seconds": 2.528}, "timestamp": "2026-01-19T15:36:26.915653"}
{"image_index": 476, "image_name": "000000053909.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053909.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2110.223, "latencies_ms": [2110.223], "images_per_second": 0.474, "prompt_tokens": 1118, "response_tokens_est": 39, "n_tiles": 1, "output_text": " The smartphone is held in a person's hand, which is positioned in the foreground of the image. The smartphone is in front of a keyboard, which is located in the background of the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 28062.3, "ram_available_mb": 97709.9, "ram_percent": 22.3}, "sys_after": {"cpu_percent": 16.5, "ram_used_mb": 28272.8, "ram_available_mb": 97499.3, "ram_percent": 22.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.33, "peak": 40.16, "min": 15.38}, "VIN": {"avg": 76.39, "peak": 125.69, "min": 32.45}, "VIN_SYS_5V0": {"avg": 14.98, "peak": 16.15, "min": 12.25}, "VDD_CPU_SOC_MSS": {"avg": 16.84, "peak": 20.08, "min": 14.18}}, "power_watts_avg": 28.33, "energy_joules_est": 59.8, "sample_count": 21, "duration_seconds": 2.111}, "timestamp": "2026-01-19T15:36:29.100031"}
{"image_index": 476, "image_name": "000000053909.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053909.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1607.477, "latencies_ms": [1607.477], "images_per_second": 0.622, "prompt_tokens": 1112, "response_tokens_est": 22, "n_tiles": 1, "output_text": " A person is holding a black smartphone in their hand, and the screen is displaying a picture of a tree.", "error": null, "sys_before": {"cpu_percent": 100.0, "ram_used_mb": 28272.8, "ram_available_mb": 97499.3, "ram_percent": 22.5}, "sys_after": {"cpu_percent": 19.2, "ram_used_mb": 28143.3, "ram_available_mb": 97628.8, "ram_percent": 22.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.56, "peak": 39.39, "min": 17.73}, "VIN": {"avg": 75.31, "peak": 105.71, "min": 29.8}, "VIN_SYS_5V0": {"avg": 15.26, "peak": 16.26, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 19.58, "peak": 23.22, "min": 15.36}}, "power_watts_avg": 30.56, "energy_joules_est": 49.14, "sample_count": 16, "duration_seconds": 1.608}, "timestamp": "2026-01-19T15:36:30.776579"}
{"image_index": 476, "image_name": "000000053909.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053909.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1387.88, "latencies_ms": [1387.88], "images_per_second": 0.721, "prompt_tokens": 1110, "response_tokens_est": 12, "n_tiles": 1, "output_text": " The phone is black and the screen is reflecting a tree.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 28143.3, "ram_available_mb": 97628.8, "ram_percent": 22.4}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 28167.8, "ram_available_mb": 97604.3, "ram_percent": 22.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.08, "peak": 40.16, "min": 18.53}, "VIN": {"avg": 73.94, "peak": 119.72, "min": 29.4}, "VIN_SYS_5V0": {"avg": 14.9, "peak": 16.15, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 16.56, "peak": 18.5, "min": 13.79}}, "power_watts_avg": 32.08, "energy_joules_est": 44.54, "sample_count": 14, "duration_seconds": 1.388}, "timestamp": "2026-01-19T15:36:32.239987"}
{"image_index": 477, "image_name": "000000053994.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053994.jpg", "image_width": 481, "image_height": 640, "image_resolution": "481x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2655.935, "latencies_ms": [2655.935], "images_per_second": 0.377, "prompt_tokens": 1432, "response_tokens_est": 44, "n_tiles": 1, "output_text": " A red and blue parking meter is on the sidewalk, with a sign that says \"DENVER'S HAD HOME\" and another sign that says \"CAMPAIGN TO END HOMELESSNESS\".", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 28167.8, "ram_available_mb": 97604.3, "ram_percent": 22.4}, "sys_after": {"cpu_percent": 7.7, "ram_used_mb": 28213.5, "ram_available_mb": 97558.7, "ram_percent": 22.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.2, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.94, "peak": 40.56, "min": 19.7}, "VIN": {"avg": 71.99, "peak": 114.3, "min": 29.52}, "VIN_SYS_5V0": {"avg": 15.28, "peak": 16.76, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 16.76, "peak": 18.5, "min": 13.79}}, "power_watts_avg": 29.94, "energy_joules_est": 79.54, "sample_count": 26, "duration_seconds": 2.657}, "timestamp": "2026-01-19T15:36:34.940193"}
{"image_index": 477, "image_name": "000000053994.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053994.jpg", "image_width": 481, "image_height": 640, "image_resolution": "481x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2343.825, "latencies_ms": [2343.825], "images_per_second": 0.427, "prompt_tokens": 1446, "response_tokens_est": 33, "n_tiles": 1, "output_text": " 1. parking meter\n2. sign\n3. fence\n4. bushes\n5. flowers\n6. trees\n7. sidewalk\n8. grass", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 28213.5, "ram_available_mb": 97558.7, "ram_percent": 22.4}, "sys_after": {"cpu_percent": 23.8, "ram_used_mb": 28273.5, "ram_available_mb": 97498.7, "ram_percent": 22.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.66, "peak": 40.18, "min": 16.95}, "VIN": {"avg": 75.29, "peak": 119.14, "min": 29.56}, "VIN_SYS_5V0": {"avg": 15.4, "peak": 16.76, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 19.14, "peak": 24.8, "min": 14.57}}, "power_watts_avg": 30.66, "energy_joules_est": 71.87, "sample_count": 23, "duration_seconds": 2.344}, "timestamp": "2026-01-19T15:36:37.331256"}
{"image_index": 477, "image_name": "000000053994.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053994.jpg", "image_width": 481, "image_height": 640, "image_resolution": "481x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2461.214, "latencies_ms": [2461.214], "images_per_second": 0.406, "prompt_tokens": 1450, "response_tokens_est": 38, "n_tiles": 1, "output_text": " The parking meter is located on the right side of the image, with a sign on the left side. The sign is positioned in the foreground, while the parking meter is in the background.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 28273.5, "ram_available_mb": 97498.7, "ram_percent": 22.5}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 28255.3, "ram_available_mb": 97516.9, "ram_percent": 22.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.53, "peak": 40.57, "min": 17.74}, "VIN": {"avg": 73.98, "peak": 126.64, "min": 29.88}, "VIN_SYS_5V0": {"avg": 15.31, "peak": 16.66, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 16.49, "peak": 17.71, "min": 14.18}}, "power_watts_avg": 30.53, "energy_joules_est": 75.16, "sample_count": 24, "duration_seconds": 2.462}, "timestamp": "2026-01-19T15:36:39.822309"}
{"image_index": 477, "image_name": "000000053994.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053994.jpg", "image_width": 481, "image_height": 640, "image_resolution": "481x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2694.595, "latencies_ms": [2694.595], "images_per_second": 0.371, "prompt_tokens": 1444, "response_tokens_est": 47, "n_tiles": 1, "output_text": " A red and blue parking meter is on the side of the road, with a sign that says \"DENVER'S HAD HOME\" and a sign that says \"CAMPAIGN TO END HOMELESSNESS\".", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 28255.3, "ram_available_mb": 97516.9, "ram_percent": 22.5}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 28254.8, "ram_available_mb": 97517.4, "ram_percent": 22.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.78, "peak": 40.18, "min": 17.74}, "VIN": {"avg": 73.19, "peak": 120.49, "min": 30.01}, "VIN_SYS_5V0": {"avg": 15.35, "peak": 16.66, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 16.37, "peak": 17.71, "min": 13.79}}, "power_watts_avg": 29.78, "energy_joules_est": 80.26, "sample_count": 26, "duration_seconds": 2.695}, "timestamp": "2026-01-19T15:36:42.529654"}
{"image_index": 477, "image_name": "000000053994.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053994.jpg", "image_width": 481, "image_height": 640, "image_resolution": "481x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2023.71, "latencies_ms": [2023.71], "images_per_second": 0.494, "prompt_tokens": 1442, "response_tokens_est": 21, "n_tiles": 1, "output_text": " The parking meter is red and white, and it is located on a sidewalk next to a green wall.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 28254.8, "ram_available_mb": 97517.4, "ram_percent": 22.5}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 28252.3, "ram_available_mb": 97519.9, "ram_percent": 22.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.14, "peak": 40.18, "min": 17.74}, "VIN": {"avg": 72.44, "peak": 127.78, "min": 28.94}, "VIN_SYS_5V0": {"avg": 15.43, "peak": 16.66, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 16.52, "peak": 17.71, "min": 14.18}}, "power_watts_avg": 32.14, "energy_joules_est": 65.05, "sample_count": 20, "duration_seconds": 2.024}, "timestamp": "2026-01-19T15:36:44.607860"}
{"image_index": 478, "image_name": "000000054123.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054123.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1837.68, "latencies_ms": [1837.68], "images_per_second": 0.544, "prompt_tokens": 1099, "response_tokens_est": 29, "n_tiles": 1, "output_text": " In the image, a group of zebras are grazing in a grassy field, their black and white stripes contrasting with the green and brown vegetation.", "error": null, "sys_before": {"cpu_percent": 15.4, "ram_used_mb": 28252.3, "ram_available_mb": 97519.9, "ram_percent": 22.5}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 28249.4, "ram_available_mb": 97522.7, "ram_percent": 22.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.45, "peak": 40.97, "min": 18.92}, "VIN": {"avg": 68.81, "peak": 128.45, "min": 29.42}, "VIN_SYS_5V0": {"avg": 14.96, "peak": 16.05, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.97, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 30.45, "energy_joules_est": 55.98, "sample_count": 18, "duration_seconds": 1.838}, "timestamp": "2026-01-19T15:36:46.489536"}
{"image_index": 478, "image_name": "000000054123.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054123.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1433.071, "latencies_ms": [1433.071], "images_per_second": 0.698, "prompt_tokens": 1113, "response_tokens_est": 14, "n_tiles": 1, "output_text": " zebra: 3\ngrass: 1\ntrees: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 28249.4, "ram_available_mb": 97522.7, "ram_percent": 22.5}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 28245.2, "ram_available_mb": 97527.0, "ram_percent": 22.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.39, "peak": 40.57, "min": 18.14}, "VIN": {"avg": 74.25, "peak": 107.82, "min": 27.91}, "VIN_SYS_5V0": {"avg": 15.17, "peak": 16.15, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 16.28, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 32.39, "energy_joules_est": 46.43, "sample_count": 14, "duration_seconds": 1.433}, "timestamp": "2026-01-19T15:36:47.951117"}
{"image_index": 478, "image_name": "000000054123.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054123.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2348.847, "latencies_ms": [2348.847], "images_per_second": 0.426, "prompt_tokens": 1117, "response_tokens_est": 49, "n_tiles": 1, "output_text": " The zebras are positioned in the foreground of the image, with the background featuring a mix of green and brown vegetation. The zebras are standing close to each other, with some of them grazing on the grass while others are facing different directions.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 28245.2, "ram_available_mb": 97527.0, "ram_percent": 22.5}, "sys_after": {"cpu_percent": 7.6, "ram_used_mb": 28167.4, "ram_available_mb": 97604.8, "ram_percent": 22.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.96, "peak": 40.95, "min": 19.31}, "VIN": {"avg": 69.13, "peak": 98.09, "min": 30.38}, "VIN_SYS_5V0": {"avg": 15.18, "peak": 16.15, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 16.38, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 28.96, "energy_joules_est": 68.03, "sample_count": 23, "duration_seconds": 2.349}, "timestamp": "2026-01-19T15:36:50.341315"}
{"image_index": 478, "image_name": "000000054123.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054123.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1752.274, "latencies_ms": [1752.274], "images_per_second": 0.571, "prompt_tokens": 1111, "response_tokens_est": 26, "n_tiles": 1, "output_text": " In a grassy field, a group of zebras graze peacefully, their black and white stripes contrasting with the green and brown vegetation.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 28167.4, "ram_available_mb": 97604.8, "ram_percent": 22.4}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 28154.6, "ram_available_mb": 97617.5, "ram_percent": 22.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.47, "peak": 40.57, "min": 16.56}, "VIN": {"avg": 72.8, "peak": 117.98, "min": 28.91}, "VIN_SYS_5V0": {"avg": 15.06, "peak": 16.15, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 16.23, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 30.47, "energy_joules_est": 53.4, "sample_count": 17, "duration_seconds": 1.753}, "timestamp": "2026-01-19T15:36:52.112076"}
{"image_index": 478, "image_name": "000000054123.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054123.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2144.823, "latencies_ms": [2144.823], "images_per_second": 0.466, "prompt_tokens": 1109, "response_tokens_est": 42, "n_tiles": 1, "output_text": " The image features a group of zebras grazing in a grassy field with a clear blue sky in the background. The zebras have black and white stripes, and the grass is a mix of green and brown.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 28154.6, "ram_available_mb": 97617.5, "ram_percent": 22.4}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 28150.8, "ram_available_mb": 97621.3, "ram_percent": 22.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.37, "peak": 40.95, "min": 19.31}, "VIN": {"avg": 67.99, "peak": 116.62, "min": 29.18}, "VIN_SYS_5V0": {"avg": 15.22, "peak": 16.15, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 16.23, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 29.37, "energy_joules_est": 63.0, "sample_count": 21, "duration_seconds": 2.145}, "timestamp": "2026-01-19T15:36:54.299692"}
{"image_index": 479, "image_name": "000000054164.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054164.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1567.202, "latencies_ms": [1567.202], "images_per_second": 0.638, "prompt_tokens": 1099, "response_tokens_est": 19, "n_tiles": 1, "output_text": " A young man in a black wetsuit is skillfully riding a wave on a surfboard.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 28150.8, "ram_available_mb": 97621.3, "ram_percent": 22.4}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 28151.1, "ram_available_mb": 97621.1, "ram_percent": 22.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.92, "peak": 40.56, "min": 16.95}, "VIN": {"avg": 71.88, "peak": 115.69, "min": 28.75}, "VIN_SYS_5V0": {"avg": 14.93, "peak": 16.15, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 16.07, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 30.92, "energy_joules_est": 48.47, "sample_count": 16, "duration_seconds": 1.568}, "timestamp": "2026-01-19T15:36:55.970472"}
{"image_index": 479, "image_name": "000000054164.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054164.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2582.585, "latencies_ms": [2582.585], "images_per_second": 0.387, "prompt_tokens": 1113, "response_tokens_est": 58, "n_tiles": 1, "output_text": " 1. person: 1\n2. surfboard: 1\n3. wave: 1\n4. water: 1\n5. sky: 0\n6. surfboard: 1\n7. person: 1\n8. water: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 28151.1, "ram_available_mb": 97621.1, "ram_percent": 22.4}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 28151.4, "ram_available_mb": 97620.7, "ram_percent": 22.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.62, "peak": 40.56, "min": 18.14}, "VIN": {"avg": 72.2, "peak": 123.09, "min": 28.8}, "VIN_SYS_5V0": {"avg": 14.87, "peak": 16.05, "min": 11.87}, "VDD_CPU_SOC_MSS": {"avg": 15.94, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 27.62, "energy_joules_est": 71.35, "sample_count": 25, "duration_seconds": 2.583}, "timestamp": "2026-01-19T15:36:58.577571"}
{"image_index": 479, "image_name": "000000054164.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054164.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2255.221, "latencies_ms": [2255.221], "images_per_second": 0.443, "prompt_tokens": 1117, "response_tokens_est": 44, "n_tiles": 1, "output_text": " The surfer is positioned in the foreground of the image, with the wave in the background. The surfer is riding the wave from left to right, with the wave's crest to the right of the surfer.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 28151.4, "ram_available_mb": 97620.7, "ram_percent": 22.4}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 28143.7, "ram_available_mb": 97628.5, "ram_percent": 22.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.24, "peak": 40.18, "min": 16.55}, "VIN": {"avg": 68.52, "peak": 103.1, "min": 29.37}, "VIN_SYS_5V0": {"avg": 15.05, "peak": 16.15, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 16.11, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 28.24, "energy_joules_est": 63.69, "sample_count": 22, "duration_seconds": 2.255}, "timestamp": "2026-01-19T15:37:00.861989"}
{"image_index": 479, "image_name": "000000054164.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054164.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1618.822, "latencies_ms": [1618.822], "images_per_second": 0.618, "prompt_tokens": 1111, "response_tokens_est": 21, "n_tiles": 1, "output_text": " A surfer in a black wetsuit is riding a wave on a surfboard in the ocean.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 28143.7, "ram_available_mb": 97628.5, "ram_percent": 22.4}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 28144.4, "ram_available_mb": 97627.8, "ram_percent": 22.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.98, "peak": 40.57, "min": 16.55}, "VIN": {"avg": 66.95, "peak": 89.12, "min": 30.0}, "VIN_SYS_5V0": {"avg": 15.11, "peak": 16.15, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 16.14, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 30.98, "energy_joules_est": 50.17, "sample_count": 16, "duration_seconds": 1.619}, "timestamp": "2026-01-19T15:37:02.519501"}
{"image_index": 479, "image_name": "000000054164.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054164.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1960.306, "latencies_ms": [1960.306], "images_per_second": 0.51, "prompt_tokens": 1109, "response_tokens_est": 34, "n_tiles": 1, "output_text": " The surfer is wearing a black wetsuit and is riding a wave on a white surfboard. The ocean is a deep blue color and the sky is clear.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 28144.4, "ram_available_mb": 97627.8, "ram_percent": 22.4}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 28141.2, "ram_available_mb": 97631.0, "ram_percent": 22.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.27, "peak": 40.56, "min": 19.71}, "VIN": {"avg": 72.06, "peak": 124.96, "min": 28.6}, "VIN_SYS_5V0": {"avg": 15.05, "peak": 16.15, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 16.22, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 30.27, "energy_joules_est": 59.35, "sample_count": 19, "duration_seconds": 1.961}, "timestamp": "2026-01-19T15:37:04.496624"}
{"image_index": 480, "image_name": "000000054592.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054592.jpg", "image_width": 640, "image_height": 418, "image_resolution": "640x418", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1605.11, "latencies_ms": [1605.11], "images_per_second": 0.623, "prompt_tokens": 1099, "response_tokens_est": 20, "n_tiles": 1, "output_text": " Two people are standing on a snowy mountain, one of them is holding a pair of skis.", "error": null, "sys_before": {"cpu_percent": 14.3, "ram_used_mb": 28141.2, "ram_available_mb": 97631.0, "ram_percent": 22.4}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 28141.2, "ram_available_mb": 97630.9, "ram_percent": 22.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.15, "peak": 40.57, "min": 17.74}, "VIN": {"avg": 74.14, "peak": 123.3, "min": 27.59}, "VIN_SYS_5V0": {"avg": 15.16, "peak": 16.15, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 16.27, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 31.15, "energy_joules_est": 50.02, "sample_count": 16, "duration_seconds": 1.606}, "timestamp": "2026-01-19T15:37:06.167933"}
{"image_index": 480, "image_name": "000000054592.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054592.jpg", "image_width": 640, "image_height": 418, "image_resolution": "640x418", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2607.062, "latencies_ms": [2607.062], "images_per_second": 0.384, "prompt_tokens": 1113, "response_tokens_est": 56, "n_tiles": 1, "output_text": " 1. person: 2\n2. backpack: 1\n3. ski: 1\n4. snow: 1\n5. pole: 1\n6. sun: 1\n7. mountain: 1\n8. sky: 1", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 28141.2, "ram_available_mb": 97630.9, "ram_percent": 22.4}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 28141.4, "ram_available_mb": 97630.8, "ram_percent": 22.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.08, "peak": 40.18, "min": 17.34}, "VIN": {"avg": 68.36, "peak": 117.23, "min": 30.06}, "VIN_SYS_5V0": {"avg": 14.75, "peak": 15.95, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.76, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 27.08, "energy_joules_est": 70.61, "sample_count": 26, "duration_seconds": 2.608}, "timestamp": "2026-01-19T15:37:08.871237"}
{"image_index": 480, "image_name": "000000054592.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054592.jpg", "image_width": 640, "image_height": 418, "image_resolution": "640x418", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2860.314, "latencies_ms": [2860.314], "images_per_second": 0.35, "prompt_tokens": 1117, "response_tokens_est": 68, "n_tiles": 1, "output_text": " The skier on the left is positioned closer to the camera, while the skier on the right is farther away. The skier on the left is also closer to the foreground, while the skier on the right is in the background. The skier on the left is also positioned in front of the skier on the right.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 28141.4, "ram_available_mb": 97630.8, "ram_percent": 22.4}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 28141.0, "ram_available_mb": 97631.2, "ram_percent": 22.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.17, "peak": 40.16, "min": 14.98}, "VIN": {"avg": 68.13, "peak": 99.76, "min": 29.26}, "VIN_SYS_5V0": {"avg": 14.84, "peak": 15.95, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.79, "peak": 17.32, "min": 12.61}}, "power_watts_avg": 26.17, "energy_joules_est": 74.86, "sample_count": 28, "duration_seconds": 2.861}, "timestamp": "2026-01-19T15:37:11.785441"}
{"image_index": 480, "image_name": "000000054592.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054592.jpg", "image_width": 640, "image_height": 418, "image_resolution": "640x418", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1658.167, "latencies_ms": [1658.167], "images_per_second": 0.603, "prompt_tokens": 1111, "response_tokens_est": 22, "n_tiles": 1, "output_text": " Two people are standing on a snowy mountain at sunset, one of them is holding a pair of skis.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 28141.0, "ram_available_mb": 97631.2, "ram_percent": 22.4}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 28141.2, "ram_available_mb": 97630.9, "ram_percent": 22.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.75, "peak": 40.16, "min": 15.76}, "VIN": {"avg": 78.19, "peak": 130.31, "min": 32.26}, "VIN_SYS_5V0": {"avg": 14.98, "peak": 16.05, "min": 12.36}, "VDD_CPU_SOC_MSS": {"avg": 16.04, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 30.75, "energy_joules_est": 51.0, "sample_count": 16, "duration_seconds": 1.658}, "timestamp": "2026-01-19T15:37:13.449930"}
{"image_index": 480, "image_name": "000000054592.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054592.jpg", "image_width": 640, "image_height": 418, "image_resolution": "640x418", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2437.966, "latencies_ms": [2437.966], "images_per_second": 0.41, "prompt_tokens": 1109, "response_tokens_est": 53, "n_tiles": 1, "output_text": " The image features two individuals dressed in white snow gear, with one of them holding a pair of skis. The sun is setting in the background, casting a warm glow on the scene. The snow is pristine and untouched, and the sky is a clear blue.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 28141.2, "ram_available_mb": 97630.9, "ram_percent": 22.4}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 28141.3, "ram_available_mb": 97630.8, "ram_percent": 22.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.29, "peak": 40.97, "min": 18.52}, "VIN": {"avg": 68.54, "peak": 115.67, "min": 29.9}, "VIN_SYS_5V0": {"avg": 15.12, "peak": 16.15, "min": 13.29}, "VDD_CPU_SOC_MSS": {"avg": 16.16, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 28.29, "energy_joules_est": 68.98, "sample_count": 24, "duration_seconds": 2.438}, "timestamp": "2026-01-19T15:37:15.954856"}
{"image_index": 481, "image_name": "000000054593.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054593.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1631.692, "latencies_ms": [1631.692], "images_per_second": 0.613, "prompt_tokens": 1099, "response_tokens_est": 21, "n_tiles": 1, "output_text": " A baseball player in a red shirt and white pants is swinging a bat at a ball in a game.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 28141.3, "ram_available_mb": 97630.8, "ram_percent": 22.4}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 28131.1, "ram_available_mb": 97641.1, "ram_percent": 22.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.5, "peak": 40.16, "min": 15.77}, "VIN": {"avg": 71.64, "peak": 124.06, "min": 30.13}, "VIN_SYS_5V0": {"avg": 14.86, "peak": 16.05, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 15.97, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 30.5, "energy_joules_est": 49.78, "sample_count": 16, "duration_seconds": 1.632}, "timestamp": "2026-01-19T15:37:17.631852"}
{"image_index": 481, "image_name": "000000054593.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054593.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2723.583, "latencies_ms": [2723.583], "images_per_second": 0.367, "prompt_tokens": 1113, "response_tokens_est": 63, "n_tiles": 1, "output_text": " 1. baseball bat: 1\n2. baseball glove: 1\n3. baseball: 1\n4. baseball player: 1\n5. catcher: 1\n6. umpire: 1\n7. fence: 1\n8. spectator: 1", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 28131.1, "ram_available_mb": 97641.1, "ram_percent": 22.4}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 28131.7, "ram_available_mb": 97640.5, "ram_percent": 22.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.21, "peak": 40.56, "min": 17.73}, "VIN": {"avg": 70.44, "peak": 126.32, "min": 27.43}, "VIN_SYS_5V0": {"avg": 15.06, "peak": 16.15, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 16.2, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 27.21, "energy_joules_est": 74.12, "sample_count": 27, "duration_seconds": 2.724}, "timestamp": "2026-01-19T15:37:20.444984"}
{"image_index": 481, "image_name": "000000054593.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054593.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2395.881, "latencies_ms": [2395.881], "images_per_second": 0.417, "prompt_tokens": 1117, "response_tokens_est": 48, "n_tiles": 1, "output_text": " The batter is positioned in the foreground, with the catcher and umpire behind him. The ball is in the air, and the batter is ready to swing. The fence is in the background, separating the field from the spectators.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 28131.7, "ram_available_mb": 97640.5, "ram_percent": 22.4}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 28133.0, "ram_available_mb": 97639.1, "ram_percent": 22.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.99, "peak": 39.77, "min": 14.98}, "VIN": {"avg": 65.08, "peak": 114.15, "min": 28.01}, "VIN_SYS_5V0": {"avg": 14.74, "peak": 16.05, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.78, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 26.99, "energy_joules_est": 64.67, "sample_count": 24, "duration_seconds": 2.396}, "timestamp": "2026-01-19T15:37:22.946302"}
{"image_index": 481, "image_name": "000000054593.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054593.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1660.01, "latencies_ms": [1660.01], "images_per_second": 0.602, "prompt_tokens": 1111, "response_tokens_est": 21, "n_tiles": 1, "output_text": " A baseball game is taking place on a sunny day, with a fence in the background and spectators watching.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 28133.0, "ram_available_mb": 97639.1, "ram_percent": 22.4}, "sys_after": {"cpu_percent": 10.4, "ram_used_mb": 28321.2, "ram_available_mb": 97451.0, "ram_percent": 22.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.16, "peak": 40.18, "min": 14.98}, "VIN": {"avg": 70.79, "peak": 94.4, "min": 31.06}, "VIN_SYS_5V0": {"avg": 14.82, "peak": 15.95, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 16.27, "peak": 18.11, "min": 12.61}}, "power_watts_avg": 30.16, "energy_joules_est": 50.09, "sample_count": 16, "duration_seconds": 1.661}, "timestamp": "2026-01-19T15:37:24.618823"}
{"image_index": 481, "image_name": "000000054593.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054593.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2283.747, "latencies_ms": [2283.747], "images_per_second": 0.438, "prompt_tokens": 1109, "response_tokens_est": 41, "n_tiles": 1, "output_text": " The image captures a vibrant scene of a baseball game, with the players dressed in crisp white and red uniforms, the sun casting a warm glow on the field, and the lush green trees in the background.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 28321.2, "ram_available_mb": 97451.0, "ram_percent": 22.5}, "sys_after": {"cpu_percent": 14.5, "ram_used_mb": 28392.5, "ram_available_mb": 97379.6, "ram_percent": 22.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.45, "peak": 40.56, "min": 19.7}, "VIN": {"avg": 71.61, "peak": 108.24, "min": 37.18}, "VIN_SYS_5V0": {"avg": 15.1, "peak": 16.36, "min": 13.69}, "VDD_CPU_SOC_MSS": {"avg": 18.79, "peak": 21.25, "min": 16.93}}, "power_watts_avg": 28.45, "energy_joules_est": 64.99, "sample_count": 22, "duration_seconds": 2.284}, "timestamp": "2026-01-19T15:37:26.908361"}
{"image_index": 482, "image_name": "000000054605.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054605.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2078.74, "latencies_ms": [2078.74], "images_per_second": 0.481, "prompt_tokens": 1432, "response_tokens_est": 23, "n_tiles": 1, "output_text": " A large glass milkshake with whipped cream and a straw sits on a table next to a slice of cake.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 28392.5, "ram_available_mb": 97379.6, "ram_percent": 22.6}, "sys_after": {"cpu_percent": 6.6, "ram_used_mb": 28403.9, "ram_available_mb": 97368.2, "ram_percent": 22.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.2, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.17, "peak": 40.56, "min": 16.55}, "VIN": {"avg": 71.28, "peak": 101.6, "min": 29.39}, "VIN_SYS_5V0": {"avg": 15.51, "peak": 16.76, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 17.96, "peak": 20.07, "min": 14.18}}, "power_watts_avg": 31.17, "energy_joules_est": 64.82, "sample_count": 21, "duration_seconds": 2.079}, "timestamp": "2026-01-19T15:37:29.095292"}
{"image_index": 482, "image_name": "000000054605.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054605.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2991.866, "latencies_ms": [2991.866], "images_per_second": 0.334, "prompt_tokens": 1446, "response_tokens_est": 57, "n_tiles": 1, "output_text": " 1. glass: 1\n2. cake: 1\n3. fork: 2\n4. knife: 1\n5. napkin: 1\n6. table: 1\n7. chair: 1\n8. person: 1", "error": null, "sys_before": {"cpu_percent": 40.0, "ram_used_mb": 28403.9, "ram_available_mb": 97368.2, "ram_percent": 22.6}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 28396.6, "ram_available_mb": 97375.6, "ram_percent": 22.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.6, "peak": 40.18, "min": 16.95}, "VIN": {"avg": 70.42, "peak": 119.35, "min": 28.88}, "VIN_SYS_5V0": {"avg": 15.06, "peak": 16.56, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 16.21, "peak": 17.71, "min": 13.0}}, "power_watts_avg": 28.6, "energy_joules_est": 85.58, "sample_count": 29, "duration_seconds": 2.992}, "timestamp": "2026-01-19T15:37:32.114356"}
{"image_index": 482, "image_name": "000000054605.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054605.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2157.718, "latencies_ms": [2157.718], "images_per_second": 0.463, "prompt_tokens": 1450, "response_tokens_est": 27, "n_tiles": 1, "output_text": " The milkshake is to the left of the cake, the cake is in the foreground, and the table is in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 28396.6, "ram_available_mb": 97375.6, "ram_percent": 22.6}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 28412.4, "ram_available_mb": 97359.8, "ram_percent": 22.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.57, "peak": 40.56, "min": 16.95}, "VIN": {"avg": 78.13, "peak": 133.41, "min": 30.54}, "VIN_SYS_5V0": {"avg": 15.42, "peak": 16.76, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 16.59, "peak": 18.11, "min": 14.18}}, "power_watts_avg": 31.57, "energy_joules_est": 68.13, "sample_count": 21, "duration_seconds": 2.158}, "timestamp": "2026-01-19T15:37:34.310739"}
{"image_index": 482, "image_name": "000000054605.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054605.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1998.537, "latencies_ms": [1998.537], "images_per_second": 0.5, "prompt_tokens": 1444, "response_tokens_est": 21, "n_tiles": 1, "output_text": " A woman is sitting at a table in a restaurant with a milkshake and a slice of cake.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 28412.4, "ram_available_mb": 97359.8, "ram_percent": 22.6}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 28422.7, "ram_available_mb": 97349.5, "ram_percent": 22.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.4, "peak": 40.57, "min": 18.53}, "VIN": {"avg": 73.92, "peak": 116.66, "min": 28.01}, "VIN_SYS_5V0": {"avg": 15.34, "peak": 16.66, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 16.55, "peak": 18.11, "min": 13.79}}, "power_watts_avg": 32.4, "energy_joules_est": 64.77, "sample_count": 20, "duration_seconds": 1.999}, "timestamp": "2026-01-19T15:37:36.396348"}
{"image_index": 482, "image_name": "000000054605.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054605.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2843.642, "latencies_ms": [2843.642], "images_per_second": 0.352, "prompt_tokens": 1442, "response_tokens_est": 52, "n_tiles": 1, "output_text": " The image is taken in a restaurant with a warm and inviting atmosphere. The lighting is soft and natural, coming from the windows in the background. The table is made of dark wood, and the chairs are upholstered in a light-colored fabric.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 28422.7, "ram_available_mb": 97349.5, "ram_percent": 22.6}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 28431.5, "ram_available_mb": 97340.7, "ram_percent": 22.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.18, "peak": 40.57, "min": 18.53}, "VIN": {"avg": 70.29, "peak": 120.69, "min": 28.71}, "VIN_SYS_5V0": {"avg": 15.1, "peak": 16.56, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 16.27, "peak": 17.71, "min": 13.0}}, "power_watts_avg": 29.18, "energy_joules_est": 82.99, "sample_count": 28, "duration_seconds": 2.844}, "timestamp": "2026-01-19T15:37:39.296694"}
{"image_index": 483, "image_name": "000000054628.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054628.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1918.999, "latencies_ms": [1918.999], "images_per_second": 0.521, "prompt_tokens": 1100, "response_tokens_est": 33, "n_tiles": 1, "output_text": " A large, multi-tiered wedding cake with blue and white decorations sits on a table in a room with a chandelier and other tables in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 28431.5, "ram_available_mb": 97340.7, "ram_percent": 22.6}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 28442.8, "ram_available_mb": 97329.4, "ram_percent": 22.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.38, "peak": 40.56, "min": 16.56}, "VIN": {"avg": 62.89, "peak": 98.73, "min": 29.12}, "VIN_SYS_5V0": {"avg": 14.92, "peak": 16.05, "min": 12.25}, "VDD_CPU_SOC_MSS": {"avg": 16.12, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 29.38, "energy_joules_est": 56.39, "sample_count": 19, "duration_seconds": 1.919}, "timestamp": "2026-01-19T15:37:41.284421"}
{"image_index": 483, "image_name": "000000054628.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054628.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2601.262, "latencies_ms": [2601.262], "images_per_second": 0.384, "prompt_tokens": 1114, "response_tokens_est": 59, "n_tiles": 1, "output_text": " 1. cake: 1\n2. tablecloth: 1\n3. flowers: 1\n4. table: 1\n5. chairs: 1\n6. chandelier: 1\n7. windows: 1\n8. people: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 28442.8, "ram_available_mb": 97329.4, "ram_percent": 22.6}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 28436.5, "ram_available_mb": 97335.6, "ram_percent": 22.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.99, "peak": 40.57, "min": 17.34}, "VIN": {"avg": 71.53, "peak": 126.01, "min": 28.6}, "VIN_SYS_5V0": {"avg": 14.94, "peak": 16.15, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 16.08, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 26.99, "energy_joules_est": 70.22, "sample_count": 26, "duration_seconds": 2.602}, "timestamp": "2026-01-19T15:37:43.992045"}
{"image_index": 483, "image_name": "000000054628.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054628.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2140.61, "latencies_ms": [2140.61], "images_per_second": 0.467, "prompt_tokens": 1118, "response_tokens_est": 41, "n_tiles": 1, "output_text": " The cake is positioned in the foreground, with the tablecloth and chairs in the background. The chandelier is located above the table, while the guests are seated at the tables in the far background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 28436.5, "ram_available_mb": 97335.6, "ram_percent": 22.6}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 28454.1, "ram_available_mb": 97318.0, "ram_percent": 22.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.2, "peak": 40.16, "min": 14.98}, "VIN": {"avg": 76.6, "peak": 128.7, "min": 28.75}, "VIN_SYS_5V0": {"avg": 14.82, "peak": 16.05, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.92, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 28.2, "energy_joules_est": 60.38, "sample_count": 21, "duration_seconds": 2.141}, "timestamp": "2026-01-19T15:37:46.180965"}
{"image_index": 483, "image_name": "000000054628.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054628.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1606.045, "latencies_ms": [1606.045], "images_per_second": 0.623, "prompt_tokens": 1112, "response_tokens_est": 21, "n_tiles": 1, "output_text": " A large wedding cake with blue and white decorations sits on a table in a room with tables and chairs.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 28454.1, "ram_available_mb": 97318.0, "ram_percent": 22.6}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 28458.7, "ram_available_mb": 97313.5, "ram_percent": 22.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.58, "peak": 39.39, "min": 16.95}, "VIN": {"avg": 62.25, "peak": 108.97, "min": 29.09}, "VIN_SYS_5V0": {"avg": 15.04, "peak": 16.15, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 16.21, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 30.58, "energy_joules_est": 49.13, "sample_count": 16, "duration_seconds": 1.607}, "timestamp": "2026-01-19T15:37:47.850163"}
{"image_index": 483, "image_name": "000000054628.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054628.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1587.089, "latencies_ms": [1587.089], "images_per_second": 0.63, "prompt_tokens": 1110, "response_tokens_est": 20, "n_tiles": 1, "output_text": " The cake is white and blue, with gold accents, and is lit by a chandelier.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 28458.7, "ram_available_mb": 97313.5, "ram_percent": 22.6}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 28449.3, "ram_available_mb": 97322.9, "ram_percent": 22.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.19, "peak": 40.16, "min": 18.92}, "VIN": {"avg": 70.42, "peak": 121.43, "min": 27.19}, "VIN_SYS_5V0": {"avg": 14.99, "peak": 16.15, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 16.14, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 31.19, "energy_joules_est": 49.52, "sample_count": 16, "duration_seconds": 1.588}, "timestamp": "2026-01-19T15:37:49.525244"}
{"image_index": 484, "image_name": "000000054654.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054654.jpg", "image_width": 407, "image_height": 640, "image_resolution": "407x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1824.456, "latencies_ms": [1824.456], "images_per_second": 0.548, "prompt_tokens": 1100, "response_tokens_est": 29, "n_tiles": 1, "output_text": " A woman wearing a blue sweater with red and white patterns is standing in a kitchen and holding a plate with a piece of food on it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 28449.3, "ram_available_mb": 97322.9, "ram_percent": 22.6}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 28474.9, "ram_available_mb": 97297.2, "ram_percent": 22.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.16, "peak": 40.57, "min": 18.14}, "VIN": {"avg": 72.96, "peak": 122.28, "min": 27.53}, "VIN_SYS_5V0": {"avg": 14.85, "peak": 15.95, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 16.1, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 30.16, "energy_joules_est": 55.03, "sample_count": 18, "duration_seconds": 1.825}, "timestamp": "2026-01-19T15:37:51.404480"}
{"image_index": 484, "image_name": "000000054654.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054654.jpg", "image_width": 407, "image_height": 640, "image_resolution": "407x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2682.051, "latencies_ms": [2682.051], "images_per_second": 0.373, "prompt_tokens": 1114, "response_tokens_est": 59, "n_tiles": 1, "output_text": " 1. person: 1\n2. plate: 1\n3. pot: 1\n4. spoon: 1\n5. food: 1\n6. wall: 1\n7. poster: 1\n8. wall-mounted items: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 28474.9, "ram_available_mb": 97297.2, "ram_percent": 22.6}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 28479.8, "ram_available_mb": 97292.4, "ram_percent": 22.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.93, "peak": 40.18, "min": 17.74}, "VIN": {"avg": 67.1, "peak": 124.54, "min": 29.05}, "VIN_SYS_5V0": {"avg": 14.94, "peak": 16.15, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.99, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 26.93, "energy_joules_est": 72.24, "sample_count": 26, "duration_seconds": 2.682}, "timestamp": "2026-01-19T15:37:54.109883"}
{"image_index": 484, "image_name": "000000054654.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054654.jpg", "image_width": 407, "image_height": 640, "image_resolution": "407x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2106.383, "latencies_ms": [2106.383], "images_per_second": 0.475, "prompt_tokens": 1118, "response_tokens_est": 40, "n_tiles": 1, "output_text": " The person is standing in the foreground of the image, with the stove and pot in the middle ground. The food is placed on the plate in the foreground, while the poster is in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 28479.8, "ram_available_mb": 97292.4, "ram_percent": 22.6}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 28493.6, "ram_available_mb": 97278.6, "ram_percent": 22.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.48, "peak": 40.18, "min": 16.16}, "VIN": {"avg": 69.86, "peak": 101.19, "min": 28.65}, "VIN_SYS_5V0": {"avg": 15.11, "peak": 16.26, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 16.18, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 28.48, "energy_joules_est": 60.0, "sample_count": 21, "duration_seconds": 2.107}, "timestamp": "2026-01-19T15:37:56.292031"}
{"image_index": 484, "image_name": "000000054654.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054654.jpg", "image_width": 407, "image_height": 640, "image_resolution": "407x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1541.342, "latencies_ms": [1541.342], "images_per_second": 0.649, "prompt_tokens": 1112, "response_tokens_est": 18, "n_tiles": 1, "output_text": " A woman wearing a sweater is standing in a kitchen and holding a plate of food.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 28493.6, "ram_available_mb": 97278.6, "ram_percent": 22.7}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 28501.5, "ram_available_mb": 97270.6, "ram_percent": 22.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.89, "peak": 39.78, "min": 16.16}, "VIN": {"avg": 76.89, "peak": 123.68, "min": 29.16}, "VIN_SYS_5V0": {"avg": 14.9, "peak": 16.26, "min": 12.15}, "VDD_CPU_SOC_MSS": {"avg": 16.06, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 30.89, "energy_joules_est": 47.63, "sample_count": 15, "duration_seconds": 1.542}, "timestamp": "2026-01-19T15:37:57.866137"}
{"image_index": 484, "image_name": "000000054654.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054654.jpg", "image_width": 407, "image_height": 640, "image_resolution": "407x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1956.73, "latencies_ms": [1956.73], "images_per_second": 0.511, "prompt_tokens": 1110, "response_tokens_est": 35, "n_tiles": 1, "output_text": " The image is taken in a kitchen with a warm and cozy atmosphere. The lighting is natural, coming from the window in the background, and the colors are vibrant and inviting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 28501.5, "ram_available_mb": 97270.6, "ram_percent": 22.7}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 28514.4, "ram_available_mb": 97257.7, "ram_percent": 22.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.44, "peak": 40.57, "min": 20.49}, "VIN": {"avg": 72.51, "peak": 112.96, "min": 29.81}, "VIN_SYS_5V0": {"avg": 15.17, "peak": 16.15, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 16.33, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 30.44, "energy_joules_est": 59.59, "sample_count": 19, "duration_seconds": 1.957}, "timestamp": "2026-01-19T15:37:59.846886"}
{"image_index": 485, "image_name": "000000054931.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054931.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1728.967, "latencies_ms": [1728.967], "images_per_second": 0.578, "prompt_tokens": 1100, "response_tokens_est": 25, "n_tiles": 1, "output_text": " A woman in a pink shirt and black pants is holding a rope and walking next to a white horse in a dirt area.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 28514.4, "ram_available_mb": 97257.7, "ram_percent": 22.7}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 28529.6, "ram_available_mb": 97242.6, "ram_percent": 22.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.52, "peak": 40.18, "min": 17.74}, "VIN": {"avg": 70.54, "peak": 111.43, "min": 30.35}, "VIN_SYS_5V0": {"avg": 15.05, "peak": 16.05, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 16.17, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 30.52, "energy_joules_est": 52.79, "sample_count": 17, "duration_seconds": 1.73}, "timestamp": "2026-01-19T15:38:01.623536"}
{"image_index": 485, "image_name": "000000054931.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054931.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2539.364, "latencies_ms": [2539.364], "images_per_second": 0.394, "prompt_tokens": 1114, "response_tokens_est": 57, "n_tiles": 1, "output_text": " 1. woman: 1\n2. horse: 1\n3. rope: 1\n4. fence: 1\n5. boots: 1\n6. shirt: 1\n7. pants: 1\n8. leash: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 28529.6, "ram_available_mb": 97242.6, "ram_percent": 22.7}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 28549.8, "ram_available_mb": 97222.4, "ram_percent": 22.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.47, "peak": 40.56, "min": 18.52}, "VIN": {"avg": 68.47, "peak": 124.78, "min": 30.11}, "VIN_SYS_5V0": {"avg": 15.04, "peak": 16.15, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 16.08, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 27.47, "energy_joules_est": 69.77, "sample_count": 25, "duration_seconds": 2.54}, "timestamp": "2026-01-19T15:38:04.229660"}
{"image_index": 485, "image_name": "000000054931.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054931.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2059.474, "latencies_ms": [2059.474], "images_per_second": 0.486, "prompt_tokens": 1118, "response_tokens_est": 38, "n_tiles": 1, "output_text": " The woman is standing in the foreground of the image, with the horse in the background. The horse is positioned to the left of the woman, and the fence is visible in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 28549.8, "ram_available_mb": 97222.4, "ram_percent": 22.7}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 28565.4, "ram_available_mb": 97206.7, "ram_percent": 22.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.84, "peak": 40.16, "min": 15.77}, "VIN": {"avg": 75.82, "peak": 128.88, "min": 30.7}, "VIN_SYS_5V0": {"avg": 14.99, "peak": 16.15, "min": 12.25}, "VDD_CPU_SOC_MSS": {"avg": 16.02, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 28.84, "energy_joules_est": 59.41, "sample_count": 20, "duration_seconds": 2.06}, "timestamp": "2026-01-19T15:38:06.308561"}
{"image_index": 485, "image_name": "000000054931.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054931.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1646.61, "latencies_ms": [1646.61], "images_per_second": 0.607, "prompt_tokens": 1112, "response_tokens_est": 23, "n_tiles": 1, "output_text": " A woman in a pink shirt and black pants is walking a horse in a dirt area with trees in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 28565.4, "ram_available_mb": 97206.7, "ram_percent": 22.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 28573.4, "ram_available_mb": 97198.8, "ram_percent": 22.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.87, "peak": 39.38, "min": 17.73}, "VIN": {"avg": 78.61, "peak": 129.86, "min": 29.01}, "VIN_SYS_5V0": {"avg": 15.27, "peak": 16.26, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 16.44, "peak": 17.71, "min": 14.18}}, "power_watts_avg": 30.87, "energy_joules_est": 50.84, "sample_count": 16, "duration_seconds": 1.647}, "timestamp": "2026-01-19T15:38:07.983904"}
{"image_index": 485, "image_name": "000000054931.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054931.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1715.093, "latencies_ms": [1715.093], "images_per_second": 0.583, "prompt_tokens": 1110, "response_tokens_est": 25, "n_tiles": 1, "output_text": " The image is taken in a sunny day with a clear sky, and the woman is wearing a pink shirt and black pants.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 28573.4, "ram_available_mb": 97198.8, "ram_percent": 22.7}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 28589.9, "ram_available_mb": 97182.3, "ram_percent": 22.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.1, "peak": 40.57, "min": 19.31}, "VIN": {"avg": 73.42, "peak": 107.41, "min": 30.24}, "VIN_SYS_5V0": {"avg": 15.17, "peak": 16.26, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 16.3, "peak": 17.71, "min": 14.18}}, "power_watts_avg": 31.1, "energy_joules_est": 53.35, "sample_count": 17, "duration_seconds": 1.716}, "timestamp": "2026-01-19T15:38:09.751132"}
{"image_index": 486, "image_name": "000000054967.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054967.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2364.124, "latencies_ms": [2364.124], "images_per_second": 0.423, "prompt_tokens": 1100, "response_tokens_est": 50, "n_tiles": 1, "output_text": " The image captures a bustling city street with a mix of vehicles and pedestrians, including a prominent yellow traffic sign with a camera icon, a sign indicating a 7AM-7PM parking restriction, and a street sign with a blue and white design.", "error": null, "sys_before": {"cpu_percent": 22.2, "ram_used_mb": 28589.9, "ram_available_mb": 97182.3, "ram_percent": 22.7}, "sys_after": {"cpu_percent": 6.6, "ram_used_mb": 28440.3, "ram_available_mb": 97331.9, "ram_percent": 22.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.21, "peak": 40.56, "min": 18.53}, "VIN": {"avg": 70.96, "peak": 104.92, "min": 29.9}, "VIN_SYS_5V0": {"avg": 15.06, "peak": 16.15, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 16.33, "peak": 17.71, "min": 13.79}}, "power_watts_avg": 28.21, "energy_joules_est": 66.71, "sample_count": 23, "duration_seconds": 2.365}, "timestamp": "2026-01-19T15:38:12.138541"}
{"image_index": 486, "image_name": "000000054967.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054967.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2554.814, "latencies_ms": [2554.814], "images_per_second": 0.391, "prompt_tokens": 1114, "response_tokens_est": 58, "n_tiles": 1, "output_text": " 1. sign: 2\n2. traffic light: 1\n3. street sign: 1\n4. car: 2\n5. building: 2\n6. tree: 1\n7. person: 1\n8. road: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 28440.3, "ram_available_mb": 97331.9, "ram_percent": 22.6}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 28435.7, "ram_available_mb": 97336.5, "ram_percent": 22.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.33, "peak": 40.16, "min": 16.95}, "VIN": {"avg": 69.77, "peak": 122.2, "min": 28.84}, "VIN_SYS_5V0": {"avg": 15.13, "peak": 16.26, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 16.27, "peak": 17.71, "min": 14.18}}, "power_watts_avg": 27.33, "energy_joules_est": 69.85, "sample_count": 25, "duration_seconds": 2.556}, "timestamp": "2026-01-19T15:38:14.742737"}
{"image_index": 486, "image_name": "000000054967.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054967.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2449.932, "latencies_ms": [2449.932], "images_per_second": 0.408, "prompt_tokens": 1118, "response_tokens_est": 53, "n_tiles": 1, "output_text": " The traffic sign is located on the left side of the image, while the yellow sign is on the right side. The traffic sign is closer to the camera than the yellow sign. The traffic sign is in the foreground, while the yellow sign is in the background.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 28435.7, "ram_available_mb": 97336.5, "ram_percent": 22.6}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 28443.8, "ram_available_mb": 97328.3, "ram_percent": 22.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.43, "peak": 40.18, "min": 16.16}, "VIN": {"avg": 71.23, "peak": 111.18, "min": 30.02}, "VIN_SYS_5V0": {"avg": 14.96, "peak": 16.15, "min": 12.46}, "VDD_CPU_SOC_MSS": {"avg": 16.14, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 27.43, "energy_joules_est": 67.21, "sample_count": 24, "duration_seconds": 2.45}, "timestamp": "2026-01-19T15:38:17.238889"}
{"image_index": 486, "image_name": "000000054967.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054967.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1996.059, "latencies_ms": [1996.059], "images_per_second": 0.501, "prompt_tokens": 1112, "response_tokens_est": 36, "n_tiles": 1, "output_text": " The image captures a bustling city street during the day, with cars and buses navigating through the traffic. The skyline of tall buildings looms in the background, suggesting a metropolitan area.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 28443.8, "ram_available_mb": 97328.3, "ram_percent": 22.6}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 28455.6, "ram_available_mb": 97316.6, "ram_percent": 22.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.76, "peak": 39.78, "min": 16.55}, "VIN": {"avg": 71.84, "peak": 126.38, "min": 27.44}, "VIN_SYS_5V0": {"avg": 15.0, "peak": 16.26, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 16.18, "peak": 17.71, "min": 13.79}}, "power_watts_avg": 28.76, "energy_joules_est": 57.42, "sample_count": 20, "duration_seconds": 1.997}, "timestamp": "2026-01-19T15:38:19.320674"}
{"image_index": 486, "image_name": "000000054967.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054967.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2427.558, "latencies_ms": [2427.558], "images_per_second": 0.412, "prompt_tokens": 1110, "response_tokens_est": 52, "n_tiles": 1, "output_text": " The image features a city street with a mix of modern and older buildings, and the sky is overcast with a grayish hue. The street is lined with trees and there are several traffic signs visible, including a bike lane sign and a pedestrian crossing sign.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 28455.6, "ram_available_mb": 97316.6, "ram_percent": 22.6}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 28458.7, "ram_available_mb": 97313.5, "ram_percent": 22.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.33, "peak": 40.18, "min": 16.16}, "VIN": {"avg": 63.98, "peak": 121.3, "min": 28.57}, "VIN_SYS_5V0": {"avg": 14.85, "peak": 16.05, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.91, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 27.33, "energy_joules_est": 66.35, "sample_count": 24, "duration_seconds": 2.428}, "timestamp": "2026-01-19T15:38:21.811831"}
{"image_index": 487, "image_name": "000000055002.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055002.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1536.28, "latencies_ms": [1536.28], "images_per_second": 0.651, "prompt_tokens": 1099, "response_tokens_est": 18, "n_tiles": 1, "output_text": " A person is standing in a bathroom with a stainless steel toilet and a blue toilet brush.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 28458.7, "ram_available_mb": 97313.5, "ram_percent": 22.6}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 28461.9, "ram_available_mb": 97310.3, "ram_percent": 22.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.07, "peak": 40.57, "min": 15.77}, "VIN": {"avg": 63.72, "peak": 87.84, "min": 29.18}, "VIN_SYS_5V0": {"avg": 14.91, "peak": 16.05, "min": 12.25}, "VDD_CPU_SOC_MSS": {"avg": 16.09, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 31.07, "energy_joules_est": 47.75, "sample_count": 15, "duration_seconds": 1.537}, "timestamp": "2026-01-19T15:38:23.390249"}
{"image_index": 487, "image_name": "000000055002.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055002.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1482.078, "latencies_ms": [1482.078], "images_per_second": 0.675, "prompt_tokens": 1113, "response_tokens_est": 16, "n_tiles": 1, "output_text": " toilet: 1\ntoilet brush: 1\nperson: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 28461.9, "ram_available_mb": 97310.3, "ram_percent": 22.6}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 28481.0, "ram_available_mb": 97291.1, "ram_percent": 22.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.56, "peak": 40.95, "min": 20.49}, "VIN": {"avg": 80.58, "peak": 125.63, "min": 28.48}, "VIN_SYS_5V0": {"avg": 15.09, "peak": 16.15, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 16.27, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 32.56, "energy_joules_est": 48.27, "sample_count": 15, "duration_seconds": 1.482}, "timestamp": "2026-01-19T15:38:24.955134"}
{"image_index": 487, "image_name": "000000055002.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055002.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2195.645, "latencies_ms": [2195.645], "images_per_second": 0.455, "prompt_tokens": 1117, "response_tokens_est": 43, "n_tiles": 1, "output_text": " The toilet is located in the center of the image, with the person's feet visible in the foreground. The blue brush is placed to the left of the toilet, while the metal bars are located to the right.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 28481.0, "ram_available_mb": 97291.1, "ram_percent": 22.6}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 28493.8, "ram_available_mb": 97278.4, "ram_percent": 22.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.99, "peak": 40.56, "min": 18.13}, "VIN": {"avg": 69.46, "peak": 123.1, "min": 31.15}, "VIN_SYS_5V0": {"avg": 14.89, "peak": 16.05, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.96, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 28.99, "energy_joules_est": 63.67, "sample_count": 22, "duration_seconds": 2.196}, "timestamp": "2026-01-19T15:38:27.246811"}
{"image_index": 487, "image_name": "000000055002.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055002.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1480.955, "latencies_ms": [1480.955], "images_per_second": 0.675, "prompt_tokens": 1111, "response_tokens_est": 15, "n_tiles": 1, "output_text": " A person is standing in a bathroom with a toilet and a blue brush.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 28493.8, "ram_available_mb": 97278.4, "ram_percent": 22.7}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 28484.0, "ram_available_mb": 97288.2, "ram_percent": 22.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.91, "peak": 40.18, "min": 15.77}, "VIN": {"avg": 73.68, "peak": 124.53, "min": 27.36}, "VIN_SYS_5V0": {"avg": 14.7, "peak": 16.05, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.88, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 30.91, "energy_joules_est": 45.79, "sample_count": 15, "duration_seconds": 1.481}, "timestamp": "2026-01-19T15:38:28.814312"}
{"image_index": 487, "image_name": "000000055002.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055002.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1548.985, "latencies_ms": [1548.985], "images_per_second": 0.646, "prompt_tokens": 1109, "response_tokens_est": 18, "n_tiles": 1, "output_text": " The bathroom is well-lit with natural light, and the floor is made of tiles.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 28484.0, "ram_available_mb": 97288.2, "ram_percent": 22.6}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 28511.6, "ram_available_mb": 97260.5, "ram_percent": 22.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.35, "peak": 40.57, "min": 19.7}, "VIN": {"avg": 77.4, "peak": 128.16, "min": 31.47}, "VIN_SYS_5V0": {"avg": 14.92, "peak": 16.05, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 16.11, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 32.35, "energy_joules_est": 50.12, "sample_count": 15, "duration_seconds": 1.549}, "timestamp": "2026-01-19T15:38:30.379494"}
{"image_index": 488, "image_name": "000000055022.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055022.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1586.787, "latencies_ms": [1586.787], "images_per_second": 0.63, "prompt_tokens": 1100, "response_tokens_est": 21, "n_tiles": 1, "output_text": " A pink bicycle is parked in a store with other bicycles, and a woman is walking in the background.", "error": null, "sys_before": {"cpu_percent": 18.2, "ram_used_mb": 28511.6, "ram_available_mb": 97260.5, "ram_percent": 22.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 28526.8, "ram_available_mb": 97245.4, "ram_percent": 22.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.17, "peak": 40.16, "min": 21.67}, "VIN": {"avg": 77.36, "peak": 124.73, "min": 28.76}, "VIN_SYS_5V0": {"avg": 15.29, "peak": 16.26, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 16.46, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 32.17, "energy_joules_est": 51.07, "sample_count": 16, "duration_seconds": 1.587}, "timestamp": "2026-01-19T15:38:32.044026"}
{"image_index": 488, "image_name": "000000055022.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055022.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2672.736, "latencies_ms": [2672.736], "images_per_second": 0.374, "prompt_tokens": 1114, "response_tokens_est": 62, "n_tiles": 1, "output_text": " 1. Bicycle: 1\n2. Wall: 1\n3. Floor: 1\n4. Bike: 1\n5. Bike: 1\n6. Bike: 1\n7. Bike: 1\n8. Bike: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 28526.8, "ram_available_mb": 97245.4, "ram_percent": 22.7}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 28539.2, "ram_available_mb": 97233.0, "ram_percent": 22.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.43, "peak": 40.56, "min": 18.91}, "VIN": {"avg": 68.88, "peak": 102.88, "min": 29.36}, "VIN_SYS_5V0": {"avg": 15.02, "peak": 16.15, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.97, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 27.43, "energy_joules_est": 73.32, "sample_count": 26, "duration_seconds": 2.673}, "timestamp": "2026-01-19T15:38:34.733577"}
{"image_index": 488, "image_name": "000000055022.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055022.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2130.697, "latencies_ms": [2130.697], "images_per_second": 0.469, "prompt_tokens": 1118, "response_tokens_est": 42, "n_tiles": 1, "output_text": " The pink bicycle is positioned in the foreground, with the other bicycles in the background. The pink bicycle is to the left of the other bicycles, and the other bicycles are to the right of the pink bicycle.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 28539.2, "ram_available_mb": 97233.0, "ram_percent": 22.7}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 28554.4, "ram_available_mb": 97217.8, "ram_percent": 22.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.78, "peak": 39.78, "min": 17.35}, "VIN": {"avg": 75.18, "peak": 116.2, "min": 29.97}, "VIN_SYS_5V0": {"avg": 15.23, "peak": 16.26, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 16.25, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 28.78, "energy_joules_est": 61.33, "sample_count": 21, "duration_seconds": 2.131}, "timestamp": "2026-01-19T15:38:36.918182"}
{"image_index": 488, "image_name": "000000055022.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055022.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2467.518, "latencies_ms": [2467.518], "images_per_second": 0.405, "prompt_tokens": 1112, "response_tokens_est": 54, "n_tiles": 1, "output_text": " The image captures a quaint bicycle shop with a variety of bicycles on display. The shop is adorned with a pink bicycle, which stands out against the backdrop of other bicycles. The shop's interior is visible through the open door, revealing a glimpse of the bustling street outside.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 28554.4, "ram_available_mb": 97217.8, "ram_percent": 22.7}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 28562.4, "ram_available_mb": 97209.8, "ram_percent": 22.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.35, "peak": 39.78, "min": 16.56}, "VIN": {"avg": 66.12, "peak": 101.21, "min": 30.03}, "VIN_SYS_5V0": {"avg": 15.06, "peak": 16.26, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 16.14, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 27.35, "energy_joules_est": 67.5, "sample_count": 24, "duration_seconds": 2.468}, "timestamp": "2026-01-19T15:38:39.414524"}
{"image_index": 488, "image_name": "000000055022.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055022.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2217.039, "latencies_ms": [2217.039], "images_per_second": 0.451, "prompt_tokens": 1110, "response_tokens_est": 45, "n_tiles": 1, "output_text": " The image features a collection of bicycles, with a prominent pink bicycle in the foreground, and a variety of colors and materials, including wood and metal. The lighting is bright and natural, coming from the window in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 28562.4, "ram_available_mb": 97209.8, "ram_percent": 22.7}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 28576.3, "ram_available_mb": 97195.9, "ram_percent": 22.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.24, "peak": 39.77, "min": 16.95}, "VIN": {"avg": 74.59, "peak": 127.8, "min": 29.49}, "VIN_SYS_5V0": {"avg": 15.11, "peak": 16.15, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 16.25, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 28.24, "energy_joules_est": 62.62, "sample_count": 22, "duration_seconds": 2.217}, "timestamp": "2026-01-19T15:38:41.696006"}
{"image_index": 489, "image_name": "000000055072.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055072.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1598.215, "latencies_ms": [1598.215], "images_per_second": 0.626, "prompt_tokens": 1099, "response_tokens_est": 20, "n_tiles": 1, "output_text": " A giraffe stands in a dry grassy field, its long neck and legs contrasting with the sparse vegetation.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 28576.3, "ram_available_mb": 97195.9, "ram_percent": 22.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 28587.6, "ram_available_mb": 97184.5, "ram_percent": 22.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.68, "peak": 40.56, "min": 16.16}, "VIN": {"avg": 73.02, "peak": 106.68, "min": 29.49}, "VIN_SYS_5V0": {"avg": 14.88, "peak": 16.05, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.99, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 30.68, "energy_joules_est": 49.04, "sample_count": 16, "duration_seconds": 1.598}, "timestamp": "2026-01-19T15:38:43.371833"}
{"image_index": 489, "image_name": "000000055072.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055072.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1726.436, "latencies_ms": [1726.436], "images_per_second": 0.579, "prompt_tokens": 1113, "response_tokens_est": 24, "n_tiles": 1, "output_text": " giraffe: 1, tree: 2, bush: 1, sky: 1, grass: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 28587.6, "ram_available_mb": 97184.5, "ram_percent": 22.7}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 28598.8, "ram_available_mb": 97173.4, "ram_percent": 22.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.77, "peak": 40.18, "min": 18.91}, "VIN": {"avg": 71.58, "peak": 116.52, "min": 28.7}, "VIN_SYS_5V0": {"avg": 14.86, "peak": 16.05, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 16.05, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 30.77, "energy_joules_est": 53.13, "sample_count": 17, "duration_seconds": 1.727}, "timestamp": "2026-01-19T15:38:45.150235"}
{"image_index": 489, "image_name": "000000055072.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055072.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2280.985, "latencies_ms": [2280.985], "images_per_second": 0.438, "prompt_tokens": 1117, "response_tokens_est": 46, "n_tiles": 1, "output_text": " The giraffe is positioned in the foreground of the image, with the background consisting of a grassy field and trees. The giraffe is facing towards the right side of the image, with its body oriented towards the left side of the frame.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 28598.8, "ram_available_mb": 97173.4, "ram_percent": 22.7}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 28606.7, "ram_available_mb": 97165.5, "ram_percent": 22.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.71, "peak": 40.56, "min": 18.53}, "VIN": {"avg": 73.18, "peak": 121.74, "min": 28.9}, "VIN_SYS_5V0": {"avg": 15.01, "peak": 16.15, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 16.07, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 28.71, "energy_joules_est": 65.5, "sample_count": 22, "duration_seconds": 2.281}, "timestamp": "2026-01-19T15:38:47.445627"}
{"image_index": 489, "image_name": "000000055072.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055072.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1887.045, "latencies_ms": [1887.045], "images_per_second": 0.53, "prompt_tokens": 1111, "response_tokens_est": 31, "n_tiles": 1, "output_text": " A giraffe stands in a dry savanna, surrounded by trees and bushes. The giraffe is tall and has a long neck, with a brown and white coat.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 28606.7, "ram_available_mb": 97165.5, "ram_percent": 22.7}, "sys_after": {"cpu_percent": 6.6, "ram_used_mb": 28409.8, "ram_available_mb": 97362.4, "ram_percent": 22.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.55, "peak": 40.57, "min": 16.95}, "VIN": {"avg": 72.99, "peak": 106.97, "min": 30.05}, "VIN_SYS_5V0": {"avg": 15.15, "peak": 16.15, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 16.45, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 29.55, "energy_joules_est": 55.77, "sample_count": 19, "duration_seconds": 1.887}, "timestamp": "2026-01-19T15:38:49.427174"}
{"image_index": 489, "image_name": "000000055072.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055072.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1692.63, "latencies_ms": [1692.63], "images_per_second": 0.591, "prompt_tokens": 1109, "response_tokens_est": 23, "n_tiles": 1, "output_text": " The giraffe is brown and white with a long neck and legs. The sky is cloudy and the grass is dry.", "error": null, "sys_before": {"cpu_percent": 40.0, "ram_used_mb": 28409.8, "ram_available_mb": 97362.4, "ram_percent": 22.6}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 28411.0, "ram_available_mb": 97361.1, "ram_percent": 22.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.03, "peak": 40.16, "min": 16.16}, "VIN": {"avg": 73.66, "peak": 124.78, "min": 27.53}, "VIN_SYS_5V0": {"avg": 14.75, "peak": 16.05, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.93, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 30.03, "energy_joules_est": 50.85, "sample_count": 17, "duration_seconds": 1.693}, "timestamp": "2026-01-19T15:38:51.202794"}
{"image_index": 490, "image_name": "000000055150.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055150.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1842.144, "latencies_ms": [1842.144], "images_per_second": 0.543, "prompt_tokens": 1099, "response_tokens_est": 29, "n_tiles": 1, "output_text": " A young girl with red hair and a little girl with blonde hair are sitting on a luggage cart filled with suitcases in a parking lot.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 28411.0, "ram_available_mb": 97361.1, "ram_percent": 22.6}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 28426.3, "ram_available_mb": 97345.9, "ram_percent": 22.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.98, "peak": 40.56, "min": 17.74}, "VIN": {"avg": 68.79, "peak": 115.25, "min": 31.41}, "VIN_SYS_5V0": {"avg": 14.84, "peak": 16.05, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.97, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 29.98, "energy_joules_est": 55.24, "sample_count": 18, "duration_seconds": 1.843}, "timestamp": "2026-01-19T15:38:53.089117"}
{"image_index": 490, "image_name": "000000055150.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055150.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2669.903, "latencies_ms": [2669.903], "images_per_second": 0.375, "prompt_tokens": 1113, "response_tokens_est": 62, "n_tiles": 1, "output_text": " 1. car: 2\n2. luggage: 3\n3. suitcase: 1\n4. child: 2\n5. bag: 1\n6. suitcase handle: 1\n7. car mirror: 1\n8. car side mirror: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 28426.3, "ram_available_mb": 97345.9, "ram_percent": 22.6}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 28438.5, "ram_available_mb": 97333.7, "ram_percent": 22.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.37, "peak": 40.56, "min": 18.13}, "VIN": {"avg": 68.94, "peak": 108.39, "min": 29.19}, "VIN_SYS_5V0": {"avg": 15.07, "peak": 16.15, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 16.14, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 27.37, "energy_joules_est": 73.08, "sample_count": 26, "duration_seconds": 2.67}, "timestamp": "2026-01-19T15:38:55.779594"}
{"image_index": 490, "image_name": "000000055150.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055150.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2147.618, "latencies_ms": [2147.618], "images_per_second": 0.466, "prompt_tokens": 1117, "response_tokens_est": 42, "n_tiles": 1, "output_text": " The luggage is positioned in the foreground, with the children sitting on it. The luggage is located to the left of the children, and the children are sitting on the luggage, which is in front of them.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 28438.5, "ram_available_mb": 97333.7, "ram_percent": 22.6}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 28447.0, "ram_available_mb": 97325.2, "ram_percent": 22.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.74, "peak": 40.57, "min": 16.56}, "VIN": {"avg": 68.69, "peak": 121.73, "min": 30.31}, "VIN_SYS_5V0": {"avg": 15.18, "peak": 16.15, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 16.25, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 28.74, "energy_joules_est": 61.73, "sample_count": 21, "duration_seconds": 2.148}, "timestamp": "2026-01-19T15:38:57.968457"}
{"image_index": 490, "image_name": "000000055150.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055150.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1524.702, "latencies_ms": [1524.702], "images_per_second": 0.656, "prompt_tokens": 1111, "response_tokens_est": 17, "n_tiles": 1, "output_text": " A young girl with red hair is sitting on a luggage cart in a parking lot.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 28447.0, "ram_available_mb": 97325.2, "ram_percent": 22.6}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 28462.9, "ram_available_mb": 97309.3, "ram_percent": 22.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.49, "peak": 40.56, "min": 16.95}, "VIN": {"avg": 72.09, "peak": 108.36, "min": 28.9}, "VIN_SYS_5V0": {"avg": 15.02, "peak": 16.15, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 16.25, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 31.49, "energy_joules_est": 48.02, "sample_count": 15, "duration_seconds": 1.525}, "timestamp": "2026-01-19T15:38:59.532077"}
{"image_index": 490, "image_name": "000000055150.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055150.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2267.286, "latencies_ms": [2267.286], "images_per_second": 0.441, "prompt_tokens": 1109, "response_tokens_est": 44, "n_tiles": 1, "output_text": " The image is taken on a rainy day, with the wet pavement reflecting the overcast sky. The colors in the image are muted, with the gray of the pavement and the blue of the sky dominating the scene.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 28462.9, "ram_available_mb": 97309.3, "ram_percent": 22.6}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 28475.0, "ram_available_mb": 97297.2, "ram_percent": 22.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.06, "peak": 40.95, "min": 19.7}, "VIN": {"avg": 68.8, "peak": 104.94, "min": 30.09}, "VIN_SYS_5V0": {"avg": 14.96, "peak": 15.95, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 16.05, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 29.06, "energy_joules_est": 65.9, "sample_count": 22, "duration_seconds": 2.268}, "timestamp": "2026-01-19T15:39:01.820269"}
{"image_index": 491, "image_name": "000000055167.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055167.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1579.467, "latencies_ms": [1579.467], "images_per_second": 0.633, "prompt_tokens": 1099, "response_tokens_est": 20, "n_tiles": 1, "output_text": " A yellow and blue bus is driving down a street in a town with a mountain in the background.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 28475.0, "ram_available_mb": 97297.2, "ram_percent": 22.6}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 28484.4, "ram_available_mb": 97287.7, "ram_percent": 22.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.88, "peak": 40.57, "min": 16.55}, "VIN": {"avg": 77.81, "peak": 128.19, "min": 30.19}, "VIN_SYS_5V0": {"avg": 15.08, "peak": 16.15, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 16.14, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 30.88, "energy_joules_est": 48.79, "sample_count": 16, "duration_seconds": 1.58}, "timestamp": "2026-01-19T15:39:03.499838"}
{"image_index": 491, "image_name": "000000055167.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055167.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2655.085, "latencies_ms": [2655.085], "images_per_second": 0.377, "prompt_tokens": 1113, "response_tokens_est": 58, "n_tiles": 1, "output_text": " 1. bus: 1\n2. street lamp: 1\n3. building: 1\n4. tree: 1\n5. flower bed: 1\n6. bench: 1\n7. car: 1\n8. mountain: 1", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 28484.4, "ram_available_mb": 97287.7, "ram_percent": 22.6}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 28499.3, "ram_available_mb": 97272.9, "ram_percent": 22.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.14, "peak": 40.57, "min": 18.14}, "VIN": {"avg": 67.86, "peak": 129.19, "min": 29.75}, "VIN_SYS_5V0": {"avg": 14.8, "peak": 16.05, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.82, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 27.14, "energy_joules_est": 72.07, "sample_count": 26, "duration_seconds": 2.655}, "timestamp": "2026-01-19T15:39:06.206258"}
{"image_index": 491, "image_name": "000000055167.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055167.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2680.484, "latencies_ms": [2680.484], "images_per_second": 0.373, "prompt_tokens": 1117, "response_tokens_est": 62, "n_tiles": 1, "output_text": " The yellow and blue bus is parked on the left side of the street, while the white van is driving on the right side. The bus is closer to the camera than the van, and the van is further away from the camera. The bus is in the foreground, while the van is in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 28499.3, "ram_available_mb": 97272.9, "ram_percent": 22.7}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 28511.6, "ram_available_mb": 97260.6, "ram_percent": 22.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.91, "peak": 40.18, "min": 15.77}, "VIN": {"avg": 69.28, "peak": 124.35, "min": 28.83}, "VIN_SYS_5V0": {"avg": 14.99, "peak": 16.15, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.99, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 26.91, "energy_joules_est": 72.15, "sample_count": 26, "duration_seconds": 2.681}, "timestamp": "2026-01-19T15:39:08.908264"}
{"image_index": 491, "image_name": "000000055167.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055167.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1660.892, "latencies_ms": [1660.892], "images_per_second": 0.602, "prompt_tokens": 1111, "response_tokens_est": 23, "n_tiles": 1, "output_text": " A yellow and blue bus drives down a street in a small town, passing by a row of shops and houses.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 28511.6, "ram_available_mb": 97260.6, "ram_percent": 22.7}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 28529.6, "ram_available_mb": 97242.6, "ram_percent": 22.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.0, "peak": 40.56, "min": 16.56}, "VIN": {"avg": 79.28, "peak": 119.99, "min": 35.98}, "VIN_SYS_5V0": {"avg": 15.26, "peak": 16.15, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 16.44, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 31.0, "energy_joules_est": 51.5, "sample_count": 16, "duration_seconds": 1.661}, "timestamp": "2026-01-19T15:39:10.575856"}
{"image_index": 491, "image_name": "000000055167.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055167.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2037.388, "latencies_ms": [2037.388], "images_per_second": 0.491, "prompt_tokens": 1109, "response_tokens_est": 38, "n_tiles": 1, "output_text": " The image features a vibrant yellow and blue bus driving down a street, with a clear blue sky overhead. The bus is surrounded by lush green grass and colorful flowers, creating a picturesque scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 28529.6, "ram_available_mb": 97242.6, "ram_percent": 22.7}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 28549.7, "ram_available_mb": 97222.5, "ram_percent": 22.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.08, "peak": 40.95, "min": 20.1}, "VIN": {"avg": 76.9, "peak": 131.37, "min": 29.57}, "VIN_SYS_5V0": {"avg": 15.25, "peak": 16.26, "min": 13.39}, "VDD_CPU_SOC_MSS": {"avg": 16.4, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 30.08, "energy_joules_est": 61.3, "sample_count": 20, "duration_seconds": 2.038}, "timestamp": "2026-01-19T15:39:12.654440"}
{"image_index": 492, "image_name": "000000055299.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055299.jpg", "image_width": 640, "image_height": 429, "image_resolution": "640x429", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1741.861, "latencies_ms": [1741.861], "images_per_second": 0.574, "prompt_tokens": 1099, "response_tokens_est": 26, "n_tiles": 1, "output_text": " A bird stands on a rock in the foreground of a beach, with the ocean and mountains in the background under a cloudy sky.", "error": null, "sys_before": {"cpu_percent": 14.3, "ram_used_mb": 28549.7, "ram_available_mb": 97222.5, "ram_percent": 22.7}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 28562.1, "ram_available_mb": 97210.1, "ram_percent": 22.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.43, "peak": 40.56, "min": 16.95}, "VIN": {"avg": 69.25, "peak": 122.74, "min": 30.25}, "VIN_SYS_5V0": {"avg": 15.07, "peak": 16.15, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 16.19, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 30.43, "energy_joules_est": 53.03, "sample_count": 17, "duration_seconds": 1.743}, "timestamp": "2026-01-19T15:39:14.438041"}
{"image_index": 492, "image_name": "000000055299.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055299.jpg", "image_width": 640, "image_height": 429, "image_resolution": "640x429", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2532.397, "latencies_ms": [2532.397], "images_per_second": 0.395, "prompt_tokens": 1113, "response_tokens_est": 56, "n_tiles": 1, "output_text": " 1. bird: 1\n2. rocks: 1\n3. beach: 1\n4. water: 1\n5. mountains: 1\n6. trees: 1\n7. pier: 1\n8. clouds: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 28562.1, "ram_available_mb": 97210.1, "ram_percent": 22.7}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 28576.6, "ram_available_mb": 97195.6, "ram_percent": 22.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.66, "peak": 40.57, "min": 18.13}, "VIN": {"avg": 69.48, "peak": 106.27, "min": 28.99}, "VIN_SYS_5V0": {"avg": 15.02, "peak": 16.15, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 16.13, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 27.66, "energy_joules_est": 70.06, "sample_count": 25, "duration_seconds": 2.533}, "timestamp": "2026-01-19T15:39:17.048200"}
{"image_index": 492, "image_name": "000000055299.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055299.jpg", "image_width": 640, "image_height": 429, "image_resolution": "640x429", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2183.114, "latencies_ms": [2183.114], "images_per_second": 0.458, "prompt_tokens": 1117, "response_tokens_est": 42, "n_tiles": 1, "output_text": " The bird is in the foreground, standing on a rock, while the beach, ocean, and mountains are in the background. The bird is near the water, and the mountains are far away from the camera.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 28576.6, "ram_available_mb": 97195.6, "ram_percent": 22.7}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 28580.7, "ram_available_mb": 97191.5, "ram_percent": 22.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.46, "peak": 40.18, "min": 15.77}, "VIN": {"avg": 75.35, "peak": 127.63, "min": 32.95}, "VIN_SYS_5V0": {"avg": 14.91, "peak": 16.05, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 15.98, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 28.46, "energy_joules_est": 62.14, "sample_count": 21, "duration_seconds": 2.184}, "timestamp": "2026-01-19T15:39:19.238394"}
{"image_index": 492, "image_name": "000000055299.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055299.jpg", "image_width": 640, "image_height": 429, "image_resolution": "640x429", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1481.425, "latencies_ms": [1481.425], "images_per_second": 0.675, "prompt_tokens": 1111, "response_tokens_est": 16, "n_tiles": 1, "output_text": " A bird stands on a rock by the sea, with mountains in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 28580.7, "ram_available_mb": 97191.5, "ram_percent": 22.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 28597.0, "ram_available_mb": 97175.1, "ram_percent": 22.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.7, "peak": 40.57, "min": 17.73}, "VIN": {"avg": 74.52, "peak": 121.64, "min": 29.37}, "VIN_SYS_5V0": {"avg": 15.2, "peak": 16.26, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 16.43, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 31.7, "energy_joules_est": 46.98, "sample_count": 15, "duration_seconds": 1.482}, "timestamp": "2026-01-19T15:39:20.806738"}
{"image_index": 492, "image_name": "000000055299.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055299.jpg", "image_width": 640, "image_height": 429, "image_resolution": "640x429", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1697.977, "latencies_ms": [1697.977], "images_per_second": 0.589, "prompt_tokens": 1109, "response_tokens_est": 23, "n_tiles": 1, "output_text": " The image features a beach with a bird perched on a rock, the sky is cloudy and the water is blue.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 28597.0, "ram_available_mb": 97175.1, "ram_percent": 22.7}, "sys_after": {"cpu_percent": 6.9, "ram_used_mb": 28417.4, "ram_available_mb": 97354.8, "ram_percent": 22.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.87, "peak": 40.57, "min": 19.32}, "VIN": {"avg": 74.21, "peak": 123.41, "min": 28.18}, "VIN_SYS_5V0": {"avg": 14.83, "peak": 16.05, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 16.17, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 30.87, "energy_joules_est": 52.43, "sample_count": 17, "duration_seconds": 1.698}, "timestamp": "2026-01-19T15:39:22.584690"}
{"image_index": 493, "image_name": "000000055528.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055528.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1694.594, "latencies_ms": [1694.594], "images_per_second": 0.59, "prompt_tokens": 1099, "response_tokens_est": 24, "n_tiles": 1, "output_text": " A man wearing glasses and a black shirt is sitting in a chair and holding a brown paper bag and a blue pen.", "error": null, "sys_before": {"cpu_percent": 14.3, "ram_used_mb": 28417.4, "ram_available_mb": 97354.8, "ram_percent": 22.6}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 28411.8, "ram_available_mb": 97360.4, "ram_percent": 22.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.68, "peak": 40.56, "min": 17.74}, "VIN": {"avg": 71.59, "peak": 101.89, "min": 27.97}, "VIN_SYS_5V0": {"avg": 14.87, "peak": 16.05, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 16.1, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 30.68, "energy_joules_est": 52.01, "sample_count": 17, "duration_seconds": 1.695}, "timestamp": "2026-01-19T15:39:24.358353"}
{"image_index": 493, "image_name": "000000055528.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055528.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2553.252, "latencies_ms": [2553.252], "images_per_second": 0.392, "prompt_tokens": 1113, "response_tokens_est": 57, "n_tiles": 1, "output_text": " 1. person: 1\n2. chair: 1\n3. remote control: 1\n4. bag: 1\n5. glasses: 1\n6. tv: 1\n7. wall: 1\n8. cord: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 28411.8, "ram_available_mb": 97360.4, "ram_percent": 22.6}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 28422.4, "ram_available_mb": 97349.8, "ram_percent": 22.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.53, "peak": 40.57, "min": 17.74}, "VIN": {"avg": 71.35, "peak": 114.54, "min": 29.25}, "VIN_SYS_5V0": {"avg": 14.98, "peak": 16.05, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 16.1, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 27.53, "energy_joules_est": 70.3, "sample_count": 25, "duration_seconds": 2.554}, "timestamp": "2026-01-19T15:39:26.957734"}
{"image_index": 493, "image_name": "000000055528.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055528.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2049.092, "latencies_ms": [2049.092], "images_per_second": 0.488, "prompt_tokens": 1117, "response_tokens_est": 38, "n_tiles": 1, "output_text": " The man is sitting on the left side of the couch, with the remote control on the right side. The bag is in front of the man, and the remote control is behind him.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 28422.4, "ram_available_mb": 97349.8, "ram_percent": 22.6}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 28433.2, "ram_available_mb": 97339.0, "ram_percent": 22.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.0, "peak": 40.18, "min": 16.16}, "VIN": {"avg": 72.27, "peak": 117.82, "min": 28.54}, "VIN_SYS_5V0": {"avg": 15.1, "peak": 16.15, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 16.18, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 29.0, "energy_joules_est": 59.44, "sample_count": 20, "duration_seconds": 2.05}, "timestamp": "2026-01-19T15:39:29.052526"}
{"image_index": 493, "image_name": "000000055528.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055528.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1594.208, "latencies_ms": [1594.208], "images_per_second": 0.627, "prompt_tokens": 1111, "response_tokens_est": 20, "n_tiles": 1, "output_text": " A man wearing glasses and a black shirt is sitting in a chair and holding a bag of chips.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 28433.2, "ram_available_mb": 97339.0, "ram_percent": 22.6}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 28449.4, "ram_available_mb": 97322.8, "ram_percent": 22.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.98, "peak": 40.57, "min": 16.95}, "VIN": {"avg": 71.58, "peak": 102.16, "min": 28.99}, "VIN_SYS_5V0": {"avg": 15.03, "peak": 16.15, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 16.17, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 30.98, "energy_joules_est": 49.4, "sample_count": 16, "duration_seconds": 1.595}, "timestamp": "2026-01-19T15:39:30.718268"}
{"image_index": 493, "image_name": "000000055528.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055528.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1616.001, "latencies_ms": [1616.001], "images_per_second": 0.619, "prompt_tokens": 1109, "response_tokens_est": 20, "n_tiles": 1, "output_text": " The man is wearing glasses and a black shirt, and the room is lit by a light source.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 28449.4, "ram_available_mb": 97322.8, "ram_percent": 22.6}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 28457.8, "ram_available_mb": 97314.4, "ram_percent": 22.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.37, "peak": 40.57, "min": 18.92}, "VIN": {"avg": 76.59, "peak": 120.75, "min": 28.93}, "VIN_SYS_5V0": {"avg": 14.92, "peak": 16.15, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.99, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 31.37, "energy_joules_est": 50.71, "sample_count": 16, "duration_seconds": 1.616}, "timestamp": "2026-01-19T15:39:32.383550"}
{"image_index": 494, "image_name": "000000055950.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055950.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1714.675, "latencies_ms": [1714.675], "images_per_second": 0.583, "prompt_tokens": 1100, "response_tokens_est": 25, "n_tiles": 1, "output_text": " A man wearing an orange shirt and black shorts is playing tennis on a green court with a blue and white tennis racket.", "error": null, "sys_before": {"cpu_percent": 22.2, "ram_used_mb": 28457.8, "ram_available_mb": 97314.4, "ram_percent": 22.6}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 28477.5, "ram_available_mb": 97294.7, "ram_percent": 22.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.03, "peak": 40.97, "min": 19.71}, "VIN": {"avg": 74.37, "peak": 122.53, "min": 29.09}, "VIN_SYS_5V0": {"avg": 15.01, "peak": 16.15, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 16.19, "peak": 17.71, "min": 13.79}}, "power_watts_avg": 31.03, "energy_joules_est": 53.22, "sample_count": 17, "duration_seconds": 1.715}, "timestamp": "2026-01-19T15:39:34.161598"}
{"image_index": 494, "image_name": "000000055950.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055950.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2893.112, "latencies_ms": [2893.112], "images_per_second": 0.346, "prompt_tokens": 1114, "response_tokens_est": 71, "n_tiles": 1, "output_text": " 1. Tennis racket: 1\n2. Tennis ball: 1\n3. Player: 1\n4. Tennis shoes: 1\n5. Tennis shorts: 1\n6. Tennis shirt: 1\n7. Tennis cap: 1\n8. Tennis net: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 28477.5, "ram_available_mb": 97294.7, "ram_percent": 22.6}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 28489.5, "ram_available_mb": 97282.6, "ram_percent": 22.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 26.84, "peak": 40.57, "min": 18.52}, "VIN": {"avg": 65.48, "peak": 124.14, "min": 29.07}, "VIN_SYS_5V0": {"avg": 15.07, "peak": 16.15, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 16.18, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 26.84, "energy_joules_est": 77.66, "sample_count": 28, "duration_seconds": 2.893}, "timestamp": "2026-01-19T15:39:37.068381"}
{"image_index": 494, "image_name": "000000055950.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055950.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2274.723, "latencies_ms": [2274.723], "images_per_second": 0.44, "prompt_tokens": 1118, "response_tokens_est": 47, "n_tiles": 1, "output_text": " The tennis player is positioned on the left side of the image, with the tennis ball in the center and the net in the foreground. The player is closer to the camera than the ball, which is in the middle of the image.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 28489.5, "ram_available_mb": 97282.6, "ram_percent": 22.7}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 28498.6, "ram_available_mb": 97273.6, "ram_percent": 22.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.28, "peak": 40.18, "min": 16.56}, "VIN": {"avg": 61.97, "peak": 107.41, "min": 29.15}, "VIN_SYS_5V0": {"avg": 15.18, "peak": 16.26, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 16.36, "peak": 17.71, "min": 14.18}}, "power_watts_avg": 28.28, "energy_joules_est": 64.34, "sample_count": 22, "duration_seconds": 2.275}, "timestamp": "2026-01-19T15:39:39.354968"}
{"image_index": 494, "image_name": "000000055950.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055950.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1495.271, "latencies_ms": [1495.271], "images_per_second": 0.669, "prompt_tokens": 1112, "response_tokens_est": 17, "n_tiles": 1, "output_text": " A man in an orange shirt and black shorts is playing tennis on a green court.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 28498.6, "ram_available_mb": 97273.6, "ram_percent": 22.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 28510.8, "ram_available_mb": 97261.4, "ram_percent": 22.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.25, "peak": 39.39, "min": 17.35}, "VIN": {"avg": 74.12, "peak": 123.84, "min": 30.07}, "VIN_SYS_5V0": {"avg": 15.25, "peak": 16.26, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 16.4, "peak": 17.71, "min": 14.57}}, "power_watts_avg": 31.25, "energy_joules_est": 46.74, "sample_count": 15, "duration_seconds": 1.496}, "timestamp": "2026-01-19T15:39:40.916295"}
{"image_index": 494, "image_name": "000000055950.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055950.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1552.586, "latencies_ms": [1552.586], "images_per_second": 0.644, "prompt_tokens": 1110, "response_tokens_est": 18, "n_tiles": 1, "output_text": " The tennis player is wearing an orange shirt and black shorts, and the court is green.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 28510.8, "ram_available_mb": 97261.4, "ram_percent": 22.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 28527.0, "ram_available_mb": 97245.2, "ram_percent": 22.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.04, "peak": 40.56, "min": 19.7}, "VIN": {"avg": 77.66, "peak": 124.83, "min": 29.83}, "VIN_SYS_5V0": {"avg": 14.99, "peak": 16.15, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 16.17, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 32.04, "energy_joules_est": 49.76, "sample_count": 15, "duration_seconds": 1.553}, "timestamp": "2026-01-19T15:39:42.487768"}
{"image_index": 495, "image_name": "000000056127.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000056127.jpg", "image_width": 580, "image_height": 640, "image_resolution": "580x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2371.32, "latencies_ms": [2371.32], "images_per_second": 0.422, "prompt_tokens": 1432, "response_tokens_est": 33, "n_tiles": 1, "output_text": " The image depicts a compact kitchen area with a variety of appliances and utensils, including a sink, stove, and refrigerator, all arranged in a functional and efficient manner.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 28527.0, "ram_available_mb": 97245.2, "ram_percent": 22.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 28546.8, "ram_available_mb": 97225.4, "ram_percent": 22.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.2, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.12, "peak": 40.56, "min": 20.89}, "VIN": {"avg": 71.23, "peak": 121.75, "min": 30.89}, "VIN_SYS_5V0": {"avg": 15.46, "peak": 16.76, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 16.57, "peak": 18.11, "min": 14.18}}, "power_watts_avg": 31.12, "energy_joules_est": 73.81, "sample_count": 23, "duration_seconds": 2.372}, "timestamp": "2026-01-19T15:39:44.894913"}
{"image_index": 495, "image_name": "000000056127.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000056127.jpg", "image_width": 580, "image_height": 640, "image_resolution": "580x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3128.271, "latencies_ms": [3128.271], "images_per_second": 0.32, "prompt_tokens": 1446, "response_tokens_est": 63, "n_tiles": 1, "output_text": " 1. Kitchen counter: 1\n2. Shelf: 1\n3. Cabinet: 2\n4. Countertop: 1\n5. Sink: 1\n6. Faucet: 1\n7. Cabinet door: 1\n8. Chair: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 28546.8, "ram_available_mb": 97225.4, "ram_percent": 22.7}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 28554.9, "ram_available_mb": 97217.2, "ram_percent": 22.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.51, "peak": 40.18, "min": 17.74}, "VIN": {"avg": 71.13, "peak": 132.24, "min": 35.53}, "VIN_SYS_5V0": {"avg": 15.25, "peak": 16.66, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 16.31, "peak": 17.71, "min": 13.79}}, "power_watts_avg": 28.51, "energy_joules_est": 89.2, "sample_count": 30, "duration_seconds": 3.129}, "timestamp": "2026-01-19T15:39:48.027541"}
{"image_index": 495, "image_name": "000000056127.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000056127.jpg", "image_width": 580, "image_height": 640, "image_resolution": "580x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2717.432, "latencies_ms": [2717.432], "images_per_second": 0.368, "prompt_tokens": 1450, "response_tokens_est": 48, "n_tiles": 1, "output_text": " The kitchen is located in the center of the image, with the sink and stove on the right side and the counter on the left. The refrigerator is positioned in the background, while the door is located on the left side of the image.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 28554.9, "ram_available_mb": 97217.2, "ram_percent": 22.7}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 28553.4, "ram_available_mb": 97218.8, "ram_percent": 22.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.37, "peak": 40.18, "min": 17.35}, "VIN": {"avg": 70.86, "peak": 127.56, "min": 31.23}, "VIN_SYS_5V0": {"avg": 15.35, "peak": 16.76, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 16.49, "peak": 18.11, "min": 14.18}}, "power_watts_avg": 29.37, "energy_joules_est": 79.82, "sample_count": 27, "duration_seconds": 2.718}, "timestamp": "2026-01-19T15:39:50.839956"}
{"image_index": 495, "image_name": "000000056127.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000056127.jpg", "image_width": 580, "image_height": 640, "image_resolution": "580x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2701.677, "latencies_ms": [2701.677], "images_per_second": 0.37, "prompt_tokens": 1444, "response_tokens_est": 47, "n_tiles": 1, "output_text": " The image captures a cozy kitchen area within a camper van, featuring a variety of appliances and storage options. The kitchen is well-equipped with a sink, stove, and refrigerator, all neatly arranged within the confines of the van.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 28553.4, "ram_available_mb": 97218.8, "ram_percent": 22.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 28573.0, "ram_available_mb": 97199.2, "ram_percent": 22.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.4, "peak": 40.57, "min": 15.77}, "VIN": {"avg": 72.79, "peak": 111.86, "min": 31.94}, "VIN_SYS_5V0": {"avg": 15.13, "peak": 16.56, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 16.28, "peak": 17.71, "min": 13.0}}, "power_watts_avg": 29.4, "energy_joules_est": 79.44, "sample_count": 26, "duration_seconds": 2.702}, "timestamp": "2026-01-19T15:39:53.547420"}
{"image_index": 495, "image_name": "000000056127.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000056127.jpg", "image_width": 580, "image_height": 640, "image_resolution": "580x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2443.006, "latencies_ms": [2443.006], "images_per_second": 0.409, "prompt_tokens": 1442, "response_tokens_est": 38, "n_tiles": 1, "output_text": " The interior of a camper van is painted in a warm yellow color, with a black and white checkered floor. The lighting is bright and natural, coming from the ceiling lights.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 28573.0, "ram_available_mb": 97199.2, "ram_percent": 22.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 28577.0, "ram_available_mb": 97195.2, "ram_percent": 22.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.55, "peak": 40.18, "min": 17.74}, "VIN": {"avg": 72.65, "peak": 122.39, "min": 29.18}, "VIN_SYS_5V0": {"avg": 15.43, "peak": 16.76, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 16.58, "peak": 18.11, "min": 14.57}}, "power_watts_avg": 30.55, "energy_joules_est": 74.64, "sample_count": 24, "duration_seconds": 2.443}, "timestamp": "2026-01-19T15:39:56.046300"}
{"image_index": 496, "image_name": "000000056288.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000056288.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1630.197, "latencies_ms": [1630.197], "images_per_second": 0.613, "prompt_tokens": 1099, "response_tokens_est": 22, "n_tiles": 1, "output_text": " A sandwich with lettuce, tomato, and ham is on a paper plate with pickles and mustard on a desk.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 28577.0, "ram_available_mb": 97195.2, "ram_percent": 22.7}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 28597.1, "ram_available_mb": 97175.1, "ram_percent": 22.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.1, "peak": 40.57, "min": 17.35}, "VIN": {"avg": 80.03, "peak": 125.78, "min": 29.49}, "VIN_SYS_5V0": {"avg": 15.0, "peak": 16.05, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 16.21, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 31.1, "energy_joules_est": 50.72, "sample_count": 16, "duration_seconds": 1.631}, "timestamp": "2026-01-19T15:39:57.725033"}
{"image_index": 496, "image_name": "000000056288.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000056288.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2595.022, "latencies_ms": [2595.022], "images_per_second": 0.385, "prompt_tokens": 1113, "response_tokens_est": 58, "n_tiles": 1, "output_text": " 1. sandwich: 1\n2. lettuce: 1\n3. tomato: 1\n4. onion: 1\n5. pickle: 2\n6. mustard: 1\n7. paper plate: 1\n8. computer monitor: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 28597.1, "ram_available_mb": 97175.1, "ram_percent": 22.7}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 28412.0, "ram_available_mb": 97360.2, "ram_percent": 22.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.78, "peak": 40.97, "min": 19.31}, "VIN": {"avg": 71.46, "peak": 117.25, "min": 41.21}, "VIN_SYS_5V0": {"avg": 15.12, "peak": 16.15, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 16.36, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 27.78, "energy_joules_est": 72.1, "sample_count": 25, "duration_seconds": 2.595}, "timestamp": "2026-01-19T15:40:00.326224"}
{"image_index": 496, "image_name": "000000056288.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000056288.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1986.572, "latencies_ms": [1986.572], "images_per_second": 0.503, "prompt_tokens": 1117, "response_tokens_est": 36, "n_tiles": 1, "output_text": " The sandwich is located in the foreground of the image, with the computer monitor in the background. The plate is placed on a desk, and the sandwich is positioned on the plate.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 28412.0, "ram_available_mb": 97360.2, "ram_percent": 22.6}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 28411.3, "ram_available_mb": 97360.9, "ram_percent": 22.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.29, "peak": 40.16, "min": 17.74}, "VIN": {"avg": 72.94, "peak": 118.76, "min": 28.72}, "VIN_SYS_5V0": {"avg": 15.17, "peak": 16.26, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 16.3, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 29.29, "energy_joules_est": 58.2, "sample_count": 20, "duration_seconds": 1.987}, "timestamp": "2026-01-19T15:40:02.407605"}
{"image_index": 496, "image_name": "000000056288.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000056288.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1533.184, "latencies_ms": [1533.184], "images_per_second": 0.652, "prompt_tokens": 1111, "response_tokens_est": 17, "n_tiles": 1, "output_text": " A sandwich is on a paper plate on a desk with a computer in the background.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 28411.3, "ram_available_mb": 97360.9, "ram_percent": 22.6}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 28424.1, "ram_available_mb": 97348.0, "ram_percent": 22.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.15, "peak": 40.16, "min": 16.16}, "VIN": {"avg": 78.42, "peak": 122.29, "min": 31.08}, "VIN_SYS_5V0": {"avg": 14.79, "peak": 16.05, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.93, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 31.15, "energy_joules_est": 47.77, "sample_count": 15, "duration_seconds": 1.534}, "timestamp": "2026-01-19T15:40:03.966266"}
{"image_index": 496, "image_name": "000000056288.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000056288.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2042.841, "latencies_ms": [2042.841], "images_per_second": 0.49, "prompt_tokens": 1109, "response_tokens_est": 38, "n_tiles": 1, "output_text": " The sandwich is on a white paper plate, and the plate is on a wooden table. The sandwich is on a white napkin, and the napkin is on a white paper plate.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 28424.1, "ram_available_mb": 97348.0, "ram_percent": 22.6}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 28429.8, "ram_available_mb": 97342.3, "ram_percent": 22.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.2, "peak": 40.97, "min": 20.1}, "VIN": {"avg": 68.75, "peak": 127.64, "min": 29.73}, "VIN_SYS_5V0": {"avg": 15.15, "peak": 16.15, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 16.28, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 30.2, "energy_joules_est": 61.71, "sample_count": 20, "duration_seconds": 2.043}, "timestamp": "2026-01-19T15:40:06.050130"}
{"image_index": 497, "image_name": "000000056344.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000056344.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2100.813, "latencies_ms": [2100.813], "images_per_second": 0.476, "prompt_tokens": 1099, "response_tokens_est": 40, "n_tiles": 1, "output_text": " The image depicts a desk with two computer monitors, a keyboard, a mouse, and a smartphone, all arranged in a way that suggests a workspace or a tech-savvy individual's desk setup.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 28429.8, "ram_available_mb": 97342.3, "ram_percent": 22.6}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 28441.9, "ram_available_mb": 97330.3, "ram_percent": 22.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.74, "peak": 40.57, "min": 16.95}, "VIN": {"avg": 70.32, "peak": 107.83, "min": 28.79}, "VIN_SYS_5V0": {"avg": 14.98, "peak": 16.05, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 16.16, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 28.74, "energy_joules_est": 60.4, "sample_count": 21, "duration_seconds": 2.102}, "timestamp": "2026-01-19T15:40:08.243152"}
{"image_index": 497, "image_name": "000000056344.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000056344.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1974.572, "latencies_ms": [1974.572], "images_per_second": 0.506, "prompt_tokens": 1113, "response_tokens_est": 34, "n_tiles": 1, "output_text": " monitor: 2, keyboard: 1, mouse: 1, phone: 1, tablet: 1, camera: 1, laptop: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 28441.9, "ram_available_mb": 97330.3, "ram_percent": 22.6}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 28453.5, "ram_available_mb": 97318.6, "ram_percent": 22.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.38, "peak": 40.18, "min": 16.16}, "VIN": {"avg": 74.71, "peak": 120.5, "min": 32.54}, "VIN_SYS_5V0": {"avg": 14.87, "peak": 16.05, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 16.04, "peak": 17.32, "min": 13.0}}, "power_watts_avg": 29.38, "energy_joules_est": 58.03, "sample_count": 19, "duration_seconds": 1.975}, "timestamp": "2026-01-19T15:40:10.223422"}
{"image_index": 497, "image_name": "000000056344.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000056344.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2451.857, "latencies_ms": [2451.857], "images_per_second": 0.408, "prompt_tokens": 1117, "response_tokens_est": 54, "n_tiles": 1, "output_text": " The keyboard is positioned in the foreground, close to the camera, while the two monitors are placed in the background, farther away from the camera. The mouse is situated near the keyboard, and the tablet is positioned to the right of the keyboard, both in the foreground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 28453.5, "ram_available_mb": 97318.6, "ram_percent": 22.6}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 28477.1, "ram_available_mb": 97295.1, "ram_percent": 22.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.14, "peak": 40.18, "min": 18.91}, "VIN": {"avg": 69.16, "peak": 90.46, "min": 28.58}, "VIN_SYS_5V0": {"avg": 15.24, "peak": 16.26, "min": 13.49}, "VDD_CPU_SOC_MSS": {"avg": 16.37, "peak": 17.32, "min": 14.96}}, "power_watts_avg": 28.14, "energy_joules_est": 69.01, "sample_count": 24, "duration_seconds": 2.452}, "timestamp": "2026-01-19T15:40:12.719122"}
{"image_index": 497, "image_name": "000000056344.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000056344.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3264.112, "latencies_ms": [3264.112], "images_per_second": 0.306, "prompt_tokens": 1111, "response_tokens_est": 77, "n_tiles": 1, "output_text": " The image captures a scene of a well-equipped workspace, with a desk that houses two computer monitors, a keyboard, a mouse, and a tablet. The monitors are displaying various screens, suggesting that the user is engaged in multiple tasks simultaneously. The desk is situated in a room with a white wall, and the lighting in the room is dim, creating a focused and productive atmosphere.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 28477.1, "ram_available_mb": 97295.1, "ram_percent": 22.6}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 28487.5, "ram_available_mb": 97284.6, "ram_percent": 22.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 24.97, "peak": 40.56, "min": 16.16}, "VIN": {"avg": 63.84, "peak": 116.34, "min": 28.76}, "VIN_SYS_5V0": {"avg": 14.78, "peak": 16.15, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 15.77, "peak": 17.32, "min": 13.79}}, "power_watts_avg": 24.97, "energy_joules_est": 81.52, "sample_count": 32, "duration_seconds": 3.265}, "timestamp": "2026-01-19T15:40:16.039312"}
{"image_index": 497, "image_name": "000000056344.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000056344.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2487.364, "latencies_ms": [2487.364], "images_per_second": 0.402, "prompt_tokens": 1109, "response_tokens_est": 54, "n_tiles": 1, "output_text": " The image depicts a desk with two computer monitors, a keyboard, a mouse, and a tablet. The monitors are displaying various web pages, and the desk is illuminated by a blue light. The overall color scheme of the image is dominated by shades of blue and white.", "error": null, "sys_before": {"cpu_percent": 40.0, "ram_used_mb": 28487.5, "ram_available_mb": 97284.6, "ram_percent": 22.7}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 28504.0, "ram_available_mb": 97268.2, "ram_percent": 22.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5565.5, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 27.5, "peak": 40.57, "min": 15.38}, "VIN": {"avg": 72.29, "peak": 119.15, "min": 34.32}, "VIN_SYS_5V0": {"avg": 15.01, "peak": 16.05, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.98, "peak": 17.32, "min": 13.39}}, "power_watts_avg": 27.5, "energy_joules_est": 68.42, "sample_count": 24, "duration_seconds": 2.488}, "timestamp": "2026-01-19T15:40:18.531970"}
{"image_index": 498, "image_name": "000000056350.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000056350.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2268.959, "latencies_ms": [2268.959], "images_per_second": 0.441, "prompt_tokens": 1432, "response_tokens_est": 31, "n_tiles": 1, "output_text": " The image captures a unique perspective of a bathroom, where the floor is adorned with a large mural of a group of people, creating an intriguing visual effect.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 28504.0, "ram_available_mb": 97268.2, "ram_percent": 22.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 28512.8, "ram_available_mb": 97259.4, "ram_percent": 22.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.2, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 30.99, "peak": 40.56, "min": 16.95}, "VIN": {"avg": 74.54, "peak": 113.16, "min": 29.47}, "VIN_SYS_5V0": {"avg": 15.62, "peak": 16.86, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 16.77, "peak": 18.11, "min": 14.57}}, "power_watts_avg": 30.99, "energy_joules_est": 70.33, "sample_count": 22, "duration_seconds": 2.269}, "timestamp": "2026-01-19T15:40:20.824161"}
{"image_index": 498, "image_name": "000000056350.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000056350.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3056.147, "latencies_ms": [3056.147], "images_per_second": 0.327, "prompt_tokens": 1446, "response_tokens_est": 61, "n_tiles": 1, "output_text": " 1. Toilet: 1\n2. Floor: 1\n3. Wall: 1\n4. Tile: 1\n5. People: 1\n6. Shoes: 1\n7. Bathroom: 1\n8. Urinal: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 28512.8, "ram_available_mb": 97259.4, "ram_percent": 22.7}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 28524.6, "ram_available_mb": 97247.5, "ram_percent": 22.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 28.64, "peak": 40.56, "min": 18.52}, "VIN": {"avg": 70.91, "peak": 118.36, "min": 28.89}, "VIN_SYS_5V0": {"avg": 15.32, "peak": 16.76, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 16.34, "peak": 18.11, "min": 14.18}}, "power_watts_avg": 28.64, "energy_joules_est": 87.54, "sample_count": 30, "duration_seconds": 3.057}, "timestamp": "2026-01-19T15:40:23.952104"}
{"image_index": 498, "image_name": "000000056350.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000056350.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2712.325, "latencies_ms": [2712.325], "images_per_second": 0.369, "prompt_tokens": 1450, "response_tokens_est": 47, "n_tiles": 1, "output_text": " The toilet is located in the upper right corner of the image, while the group of people is positioned in the lower left corner. The floor is the closest object to the viewer, while the toilet is the farthest object in the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 28524.6, "ram_available_mb": 97247.5, "ram_percent": 22.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 28536.4, "ram_available_mb": 97235.8, "ram_percent": 22.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 29.12, "peak": 40.16, "min": 16.16}, "VIN": {"avg": 71.81, "peak": 121.28, "min": 28.24}, "VIN_SYS_5V0": {"avg": 15.11, "peak": 16.56, "min": 12.15}, "VDD_CPU_SOC_MSS": {"avg": 16.17, "peak": 17.71, "min": 13.0}}, "power_watts_avg": 29.12, "energy_joules_est": 79.0, "sample_count": 27, "duration_seconds": 2.713}, "timestamp": "2026-01-19T15:40:26.756261"}
{"image_index": 498, "image_name": "000000056350.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000056350.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1850.055, "latencies_ms": [1850.055], "images_per_second": 0.541, "prompt_tokens": 1444, "response_tokens_est": 15, "n_tiles": 1, "output_text": " A group of people are standing on a tiled floor in a bathroom.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 28536.4, "ram_available_mb": 97235.8, "ram_percent": 22.7}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 28552.7, "ram_available_mb": 97219.5, "ram_percent": 22.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.34, "peak": 40.18, "min": 15.77}, "VIN": {"avg": 79.06, "peak": 120.9, "min": 29.31}, "VIN_SYS_5V0": {"avg": 15.17, "peak": 16.56, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 16.4, "peak": 17.71, "min": 13.0}}, "power_watts_avg": 32.34, "energy_joules_est": 59.85, "sample_count": 18, "duration_seconds": 1.851}, "timestamp": "2026-01-19T15:40:28.637769"}
{"image_index": 498, "image_name": "000000056350.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000056350.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2340.135, "latencies_ms": [2340.135], "images_per_second": 0.427, "prompt_tokens": 1442, "response_tokens_est": 34, "n_tiles": 1, "output_text": " The image is taken in a well-lit bathroom with white tiles on the floor and walls. The lighting is bright and even, and the tiles are clean and shiny.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 28552.7, "ram_available_mb": 97219.5, "ram_percent": 22.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 28564.5, "ram_available_mb": 97207.7, "ram_percent": 22.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.68, "peak": 40.57, "min": 20.1}, "VIN": {"avg": 75.0, "peak": 117.52, "min": 28.69}, "VIN_SYS_5V0": {"avg": 15.4, "peak": 16.76, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 16.55, "peak": 18.11, "min": 14.18}}, "power_watts_avg": 31.68, "energy_joules_est": 74.15, "sample_count": 23, "duration_seconds": 2.341}, "timestamp": "2026-01-19T15:40:31.044866"}
{"image_index": 499, "image_name": "000000056545.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000056545.jpg", "image_width": 505, "image_height": 640, "image_resolution": "505x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1954.229, "latencies_ms": [1954.229], "images_per_second": 0.512, "prompt_tokens": 1432, "response_tokens_est": 18, "n_tiles": 1, "output_text": " A bird with a gray body and black beak is perched on a branch in a forest.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 28564.5, "ram_available_mb": 97207.7, "ram_percent": 22.7}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 28579.9, "ram_available_mb": 97192.2, "ram_percent": 22.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.2, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 31.95, "peak": 40.16, "min": 16.95}, "VIN": {"avg": 75.67, "peak": 124.48, "min": 28.7}, "VIN_SYS_5V0": {"avg": 15.34, "peak": 16.76, "min": 12.15}, "VDD_CPU_SOC_MSS": {"avg": 16.47, "peak": 18.11, "min": 13.39}}, "power_watts_avg": 31.95, "energy_joules_est": 62.47, "sample_count": 19, "duration_seconds": 1.955}, "timestamp": "2026-01-19T15:40:33.033635"}
{"image_index": 499, "image_name": "000000056545.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000056545.jpg", "image_width": 505, "image_height": 640, "image_resolution": "505x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1543.882, "latencies_ms": [1543.882], "images_per_second": 0.648, "prompt_tokens": 1446, "response_tokens_est": 4, "n_tiles": 1, "output_text": " bird: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 28579.9, "ram_available_mb": 97192.2, "ram_percent": 22.7}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 28599.0, "ram_available_mb": 97173.2, "ram_percent": 22.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 34.84, "peak": 40.56, "min": 20.49}, "VIN": {"avg": 83.14, "peak": 124.6, "min": 29.06}, "VIN_SYS_5V0": {"avg": 15.56, "peak": 16.66, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 16.82, "peak": 18.11, "min": 14.18}}, "power_watts_avg": 34.84, "energy_joules_est": 53.8, "sample_count": 15, "duration_seconds": 1.544}, "timestamp": "2026-01-19T15:40:34.594867"}
{"image_index": 499, "image_name": "000000056545.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000056545.jpg", "image_width": 505, "image_height": 640, "image_resolution": "505x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2177.99, "latencies_ms": [2177.99], "images_per_second": 0.459, "prompt_tokens": 1450, "response_tokens_est": 27, "n_tiles": 1, "output_text": " The bird is positioned in the foreground, perched on a branch, while the background is filled with green foliage, creating a natural setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 28599.0, "ram_available_mb": 97173.2, "ram_percent": 22.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 28611.2, "ram_available_mb": 97160.9, "ram_percent": 22.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 33.89, "peak": 41.34, "min": 23.25}, "VIN": {"avg": 77.46, "peak": 125.64, "min": 29.95}, "VIN_SYS_5V0": {"avg": 15.56, "peak": 16.76, "min": 13.49}, "VDD_CPU_SOC_MSS": {"avg": 16.83, "peak": 18.11, "min": 14.96}}, "power_watts_avg": 33.89, "energy_joules_est": 73.82, "sample_count": 21, "duration_seconds": 2.178}, "timestamp": "2026-01-19T15:40:36.782287"}
{"image_index": 499, "image_name": "000000056545.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000056545.jpg", "image_width": 505, "image_height": 640, "image_resolution": "505x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1912.214, "latencies_ms": [1912.214], "images_per_second": 0.523, "prompt_tokens": 1444, "response_tokens_est": 18, "n_tiles": 1, "output_text": " A bird with a gray body and black beak is perched on a branch in a forest.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 28611.2, "ram_available_mb": 97160.9, "ram_percent": 22.7}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 28630.2, "ram_available_mb": 97142.0, "ram_percent": 22.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 33.09, "peak": 40.56, "min": 19.7}, "VIN": {"avg": 77.04, "peak": 134.98, "min": 28.85}, "VIN_SYS_5V0": {"avg": 15.52, "peak": 16.76, "min": 13.29}, "VDD_CPU_SOC_MSS": {"avg": 16.8, "peak": 18.11, "min": 14.57}}, "power_watts_avg": 33.09, "energy_joules_est": 63.29, "sample_count": 19, "duration_seconds": 1.913}, "timestamp": "2026-01-19T15:40:38.763312"}
{"image_index": 499, "image_name": "000000056545.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000056545.jpg", "image_width": 505, "image_height": 640, "image_resolution": "505x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2050.157, "latencies_ms": [2050.157], "images_per_second": 0.488, "prompt_tokens": 1442, "response_tokens_est": 22, "n_tiles": 1, "output_text": " The bird is gray and black, and the photo was taken in a forest with sunlight shining through the trees.", "error": null, "sys_before": {"cpu_percent": 50.0, "ram_used_mb": 28630.2, "ram_available_mb": 97142.0, "ram_percent": 22.8}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 28646.1, "ram_available_mb": 97126.1, "ram_percent": 22.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 7158.0, "gpu_max_mem_alloc_mb": 5953.3, "gpu_max_mem_reserved_mb": 7158.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VDD_GPU": {"avg": 32.56, "peak": 40.57, "min": 19.71}, "VIN": {"avg": 75.26, "peak": 118.63, "min": 30.58}, "VIN_SYS_5V0": {"avg": 15.21, "peak": 16.56, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 16.4, "peak": 17.71, "min": 13.39}}, "power_watts_avg": 32.56, "energy_joules_est": 66.78, "sample_count": 20, "duration_seconds": 2.051}, "timestamp": "2026-01-19T15:40:40.839154"}
